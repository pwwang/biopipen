{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A set of processes/pipelines for bioinformatics based on pipen Installation pip install -U biopipen Usage Use as APIs from pipen import Proc , Pipen from biopipen.ns.bed import BedLiftOver MyBedLiftOver = Proc . from_proc ( BedLiftOver ) if __name__ == \"__main__\" : Pipen () . set_start ( MyBedLiftOver ) . run () Use as pipen-cli-run plugin \u276f pipen run bed BedLiftOver --help Usage: pipen [ -h | -h+ ] [ options ] Liftover a BED file using liftOver Use ` @configfile ` to load default values for the options. Pipeline Options: --name NAME The name for the pipeline, will affect the default workdir and outdir. [ default: BedLiftOver_pipeline ] --profile PROFILE The default profile from the configuration to run the pipeline. This profile will be used unless a profile is specified in the process or in the .run method of pipen. You can check the available profiles by running ` pipen profile ` --outdir OUTDIR The output directory of the pipeline [ default: ./<name>_results ] --forks FORKS How many jobs to run simultaneously by the scheduler --scheduler SCHEDULER The scheduler to run the jobs Namespace <envs>: --envs ENVS Environment variables for the process [ default: { 'liftover' : 'liftOver' , 'chain' : '' }] --envs.liftover LIFTOVER The path to liftOver [ default: liftOver ] --envs.chain CHAIN The map chain file for liftover [ default: ] Namespace < in >: --in.inbed INBED [ INBED ... ] The input BED file Namespace <out>: --out.outbed OUTBED The output BED file [ default: {{ in .inbed | basename }}] Options: -h, --help, -h+, --help+ show help message ( with + to show more options ) and exit","title":"Home"},{"location":"#installation","text":"pip install -U biopipen","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#use-as-apis","text":"from pipen import Proc , Pipen from biopipen.ns.bed import BedLiftOver MyBedLiftOver = Proc . from_proc ( BedLiftOver ) if __name__ == \"__main__\" : Pipen () . set_start ( MyBedLiftOver ) . run ()","title":"Use as APIs"},{"location":"#use-as-pipen-cli-run-plugin","text":"\u276f pipen run bed BedLiftOver --help Usage: pipen [ -h | -h+ ] [ options ] Liftover a BED file using liftOver Use ` @configfile ` to load default values for the options. Pipeline Options: --name NAME The name for the pipeline, will affect the default workdir and outdir. [ default: BedLiftOver_pipeline ] --profile PROFILE The default profile from the configuration to run the pipeline. This profile will be used unless a profile is specified in the process or in the .run method of pipen. You can check the available profiles by running ` pipen profile ` --outdir OUTDIR The output directory of the pipeline [ default: ./<name>_results ] --forks FORKS How many jobs to run simultaneously by the scheduler --scheduler SCHEDULER The scheduler to run the jobs Namespace <envs>: --envs ENVS Environment variables for the process [ default: { 'liftover' : 'liftOver' , 'chain' : '' }] --envs.liftover LIFTOVER The path to liftOver [ default: liftOver ] --envs.chain CHAIN The map chain file for liftover [ default: ] Namespace < in >: --in.inbed INBED [ INBED ... ] The input BED file Namespace <out>: --out.outbed OUTBED The output BED file [ default: {{ in .inbed | basename }}] Options: -h, --help, -h+, --help+ show help message ( with + to show more options ) and exit","title":"Use as pipen-cli-run plugin"},{"location":"CHANGELOG/","text":"Change Log 0.34.16 fix(scrna.SeuratClusterStats): update documentation without mentioning table feat(scrna.SeuratPreparing): allow seurat object as input chore: bump pipen to 0.17.21 0.34.15 feat(scrna.MarkersFinder): allow using other metadata columns from object for enrichment plot of all subcases chore: update dependencies 0.34.14 fix(scrna.CellTypeAnnotation): update logging for celltypist command execution fix(scrna.MarkersFinder): fix FindMarkers calling with subsetting when sctransform was used 0.34.13 fix(scrna.SeuratClusterStats): improve error handling in feature plotting when save_code (due to upgrade to ggplot2 v4) feat(MarkersFinder): use scplotter::MarkersPlot (wrapped by biopipen.utils::VizDEGs to visualize markers ci: update CACHE_NUMBER for conda environment to force install latest dependencies 0.34.12 chore: update Dockerfiles to use multi-stage building 0.34.11 fix(scrna_metabolic_landscape): fix report paging issue docs(scrna.MarkersFinder): fix links in docs 0.34.10 ci: correct condition for deleting old test intermediate cache docs(scrna.SeuratPreparing): enhance cell_qc parameter description in SeuratPreparing docs(scrna.ModuleScoreCalculator): update link format in ModuleScoreCalculator docstring 0.34.9 chore(deps): update dependencies fix(scrna.CellCellCommunication): handle numpy product attribute error feat(scrna.ModuleScoreCalculator): add post mutaters functionality to allow compound modules based on added modules docs(scrna.MarkersFinder): correct URL in documentation feat(scrna.CellTypeAnnotation): add support for additional direct cell type annotations feat(scrna.MarkersFinder): enhance enrichment plot descriptions chore(scrna.CellCellCommunicationPlots): set default case to \"Cell-Cell Communication\" feat(scrna.CellCellCommunicationPlots): add table output option for ccc data feat(regulatory.MotifAffinityTest): add variant column support in MotifAffinityTest so only paired variant-motif can be output fix(regulatory.VariantMotifPlot): correct argument in ensure_regulator_motifs function 0.34.8 fix(scrna.MarkersFinder): improve marker processing and enrichment checks when result has no markers/enrichments feat(scrna.PseudoBulkDEG): add ncores and cache parameters feat(scrna.MarkersFinder): enhance heatmap plotting options fix(scrna.ScFGSEA): improve handling of cases with NA gene ranks chore: update package versions 0.34.7 chore(deps): bump pipen-report to 0.23.8 fix(scrna): update naming convention in expand_each function fix(scrna.PseudoBulkDEG): improve error handling chore(deps): bump version to 0.17.14 0.34.6 chore(scrna.CellTypeAnnotation): print the command for celltypist feat(tcr.ScRepLoading): enhance to auto-detect data type chore(tcr.ScRepCombiningExpression): rename TCR_presence to VDJ_presence in metadata of output Seurat object chore(deps): update xqute to version 0.10.4 chore(docker): add procps-ng for free command for pipen-runinfo chore(deps): update pipen version to 0.17.12 chore(deps): bump pipen-poplog to 0.3.4 chore(deps): update simplug to version 0.5.1 chore(deps): bump pipen-args to 0.17.4 to void overriding the default plugin options 0.34.5 fix(scrna.SeuratClusterstats): update string formatting for statistical comparison descriptions ci: revert CACHE_NUMBER to 1 for conda to force install biopipen.utils.R v0.2.9 0.34.4 chore(scrna): update error flag in MarkersFinder and PseudoBulkDEG classes chore(deps): bump pipen to 0.17.11 0.34.3 BREAKING(scrna.MetabolicFeatures): use : instead of , to separate groups for comparisons feat(tcr.ScRepCombiningExpression): add TCR presence indicator to combined expression object fix(scrna.CellTypeAnnotation): fix identification of default ident if rds or qs is given as input for celltypist fix(scrna.CellTypeAnnotation): ensure n_neighbors is set in neighbors params fix(scrna.SeuratClusterStats): correct typo in group_by variable name fix(tcr.ScRepLoading): fix an issue where barcode is not the first column when loaded for 10X data fix(tcr.TCRClustering): fix input collection for clustering fix(tcr.TESSA): refine TCR input preparation fix(scrna.SeuratClusterStats): exclude NA entities in groupings chore(scrna.CellTypeAnnotation): fix dead links for model files for celltypist chore: remove SCP-plot.R chore(ci): comment out cache deletion condition chore(scrna_metabolic_landscape): enlarge font size for list items in introduction of report templates chore(scrna.SeuratClusterStats): adjust plot settings for better visualization chore(tcr.TCRClustering): swap importing BLOSUM62 matrix from newer and older version of biopython for GIANA chore(scrna_metabolic_landscape): adjust plot dimensions chore(scrna.SeuratClusterStats): enhance descriptions of plots chore(deps): update versions for copier, pipen-report, and pipen-verbose docs(scrna): update mutater documentation to include clone selectors ci: update cache number for conda environments to force updates 0.34.2 feat(scrna): add PseudoBulkDEG process for differential gene expression analysis fix(test.Seurat): update pipeline function to set starts for PrepareSeurat chore(scrna.ScFGSEA): rename allpathway_plots to alleach_plots chore(scrna/tcr): update parameter naming for consistency chore(scrna.TopExpressingGenes): use common report template chore(test): update pipeline function to remove report enabling test(scrna): merge tests for Seurat processes to avoid repeatedly load pbmc3k dataset test(scrna): move map2ref tests out of tests/scrna/Seurat for being tested locally only test(scrna): add set.seed to PrepareSeurat script ci(docker): update Dockerfiles to use dynamic REF_NAME argument ci: update build condition and add caching for test intermediates 0.34.1 fix(tcr.ClonalStats): update envs assignment to handle todot parameter chore(deps): bump up pipen to 0.17.8 fix(SeuratClusterStats): fix when features are given as a dict (used in heatmap) feat(MarkersFinder): add enrichment plot across all subsets by each or all ident.1 in group.by feat(ScFGSEA): add support for all pathway plots for all subsets by each chore(TopExpressingGenes): adjust plot height for bar plots docs(SeuratPreparing): update docs for support loading loom files 0.34.0 New Features feat(scrna): add ScVelo analysis for RNA velocity and Slingshot for trajectory inference feat(tcr): add ScRepCombiningExpression for combining TCR/BCR and expression data, and ScRepLoading for multiple TCR/BCR data formats feat(scrna): add ScRepLoading with support for multiple formats and improved logging feat(plot): add Plot class and associated R script for data visualization feat(bam): add SamtoolsView for BAM file processing feat(utils): implement Reporter class for generating JSON reports for processes feat(scrna): support qs2 format for input and output in various processes feat(scrna.SeuratPreparing): add mutaters parameter for metadata mutation feat(scrna.MarkersFinder): enhance parameter handling and marker processing feat: add common Svelte report template for job reporting using JSON reports Enhancements enh(bam.CNAClinic): change envs.binsize to bp instead of kbp enh(cnv): replace ggplot2 with plotthis for improved plotting in AneuploidyScoreSummary and TMADScoreSummary enh(scrna): adopt biopipen.utils.R v0.1.0 across multiple processes enh(scrna_metabolic_landscape): improve flexibility and stability of metabolic landscape analysis enh(scrna.ExprImputation): improve error handling and threshold handling for cell imputation enh(tcr.ClonalStats): support qs2 format for output Bug Fixes fix(cnv.TMADScore): correct output filename fix(bam): fix report template and argument handling in CNVpytor, CNAClinic, and ControlFREEC fix(cnv.AneuploidyScore): replace ggplot with plotthis and add error handling for chromosome detection fix(cnv.AneuploidyScoreSummary): rename 'rows' to 'rows_by' for clarity in heatmap function fix(delim.SampleInfo): fix plot functions being registered twice to gglogger and ensure reporter saves to correct directory fix(scrna): replace readRDS/saveRDS with biopipen.utils functions for consistency across processes fix(scrna.SeuratClusterStats): improve data handling and plot saving functionality Refactoring refactor(scrna.CellCellCommunicationPlots): use scplotter::CCCPlot refactor(tcr.CDR3AAPhyschem): adopt input from ScRepCombiningExpression refactor: remove utility R scripts and use biopipen.utils.R package Development & Infrastructure chore(deps): update dependencies and specify versions for bioconductor packages chore(docker): refactor Dockerfiles to streamline base image usage and dependency installation chore: use filter 'r' for R input/output paths instead of 'quote' in scripts test: add comprehensive tests for bam, cnv, and scrna processes (local tests only for some) test: add docker-test job and Dockerfile for test image building ci: update environment cache and reference data cache 0.33.1 fix(delim.SampleInfo): fix when plot_type ending with \"plot\" feat(scrna.LoomTo10X): add LoomTo10X to convert loom format of scRNA-seq data to 10X format ci: update conditions for build and deploy jobs based on event type 0.33.0 chore(dependencies): update pipen (v0.17) and related package versions in pyproject.toml fix: update all template filters in script to adopt pipen 0.17, which passes in.file etc as a MountedPath ci: add caching for conda environments to improve workflow efficiency test: improve test output grouping for better readability chore: add descriptive summaries for fgsea and enrichr results (#158) chore(snp.PlinkFromVcf): enhance type annotations and set default for keep_allele_order feat(snp.Plink2GTMat): enhance genotype coding options and improve documentation feat(stats.ChowTest): separate groups in output and add pvalues for the coefficient for the subregressions fix(utils/misc.py): enhance error messages in command execution for better debugging fix(web.Download): enhance output filename generation by adding URL decoding and improved slugification fix(cellranger.CellRangerCount): fix inconsistency between in.id and in.fastqs chore(snp.PlinkFilter): remove unnecessary docstring from PlinkFilter script fix(cellranger.CellRangerSummary): use plotthis and biopipen.utils.R for plotting, logging and report content generation test(cellranger): add tests for CellRangerCount and CellRangerSummary with data download setup chore: comment out dev-dependencies section in pyproject.toml feat(scrna.CellCellCommunication): add subset and split_by options for CellCellCommunication and update conversion logic 0.32.3 chore: add descriptive summaries for fgsea and enrichr results 0.32.2 chore: update dependencies to latest versions feat: add PDF output option for SampleInfo plots feat: add PDF output options for violin and scatter plots in Seurat preparation scripts feat: add PDF output options for volcano, dotplot, venn, and upset plots; update filters for type hints feat: add PDF output option for Enrichr plots in TopExpressingGenes script feat: add PDF output options for UMAP plots in SeuratMap2Ref script; update image handling in misc.liq feat: add PDF output options for cluster size distribution, shared clusters, and sample diversity plots; update plotting functions to handle multiple output formats feat: add PDF output options for various Immunarch scripts; enhance reporting with downloadable PDF files feat: add PDF output options for cluster size distribution, dimension plots, and feature plots; enhance reporting with downloadable PDF files feat: add PDF output options for radar and bar plots; enhance reporting with downloadable PDF files feat: add PDF output options for CloneResidency script; enhance reporting with downloadable PDF files feat: add PDF output options for GSEA table and enrichment plots; enhance reporting with downloadable PDF files feat: add PDF output options for pie charts, heatmaps, Venn plots, and UpSet plots; enhance reporting with downloadable PDF files feat: add PDF output options for Enrichr plots; enhance reporting with downloadable PDF files feat: add PDF output options for estimated coefficients and distribution plots; enhance reporting with downloadable PDF files chore: add gcc to cnvkit pipeline docker deps 0.32.1 fix(scrna.ScFGSEA): fix case gmtfile not working fix(TopExpressingGenes): add InlineNotification component to TopExpressingGenes.svelte feat(scrna.SeuratPreparing): add envs.species so that percent.mt , percent.ribo , percent.hb and percent.plat can be correctly calculated for mouse fix(scrna.SeuratClusterStats): fix kind not being added to the figure file name for plots of features 0.32.0 deps: update pipen-runinfo dependency to version 0.8.1 feat(scrna): add CellCellCommunication and CellCellCommunicationPlots fix(scrna.SeuratMap2Ref): fix report and add stats to report fix(utils.single_cell.R): fix categorical data when converting seurat to anndata format refactor(scrna.Seurat2AnnData): abstract seurat_to_anndata() for reuse enh(tcr.TCRClustering): make GIANA compatible with latest BioPython (v1.84) fix(tcr.TCRClstering): fix clusTCR error due to scipy update (v1.14) 0.31.7 deps: bump pipen-args to 0.16 chore: update pyright configuration to include biopipen/**/*.py feat(bam): add BamSubsetByBed process for subsetting bam file by regions in a bed file feat(bed): add BedtoolsMakeWindows process for generating windows from a BED file or genome size file 0.31.6 deps: pin the channels of conda dependencies for tests feat(vcf): adopt truvari v4+ for related processes feat(regulatory): add VariantMotifPlot to plot motif and surrounding sequences with mutations refactor(regulatory.MotifAffinityTest): optimize code base ci: add verbosal output for tests 0.31.5 deps: update pipen to version 0.15.3 and xqute to version 0.5.2 feat(bam): add BamSampling process for sampling a fraction of reads from a bam file feat(protein): add the protein module and Prodigy and ProdigySummary to calculate the binding affinity of a complex structure ci: do not print verbose logs for tests chore(bam.BamMerge): use logger instead of print for logging 0.31.4 deps: bump pipen-report to 0.20.1 (pipen to 0.15.2) fix(plot.VennDiagram): update default devpars and fix issues with computed data fix(scrna.SeuratMap2Ref): fix identifying the normalization method of reference 0.31.3 test: fix test not failing when tests failed test: fix gene name conversion tests due to external API change fix(tcr.CDR3AAPhyschem): fix when chain is not available fix(tcr.TCRClustering): fix when chain is not available 0.31.2 fix(tcr.CDR3AAPhyschem): use sequence from TRB chain only fix(tcr.TCRClustering): fix for multi-chain TCRs, use TRB only if on_multi is false 0.31.1 enh(scrna.SeuratMap2Ref): check if reference has SCTModel if SCTransform'ed (likely prepared by old Seurat) 0.31.0 deps: bump pipen to 0.15.0 0.30.0 scrna/tcr BREAKING(scrna): move clustree plots from SeuratClustering/SeuratSubClustering to SeuratClusterStats feat(scrna.CellTypeAnnotation): allow to merge/not to merge (envs.merge) the clusters with the same labels predicted feat(scrna.SeuratPreparing): add scDblFinder to detect doublets feat(scrna.SeuratMap2Ref): add envs.skip_if_normalized option to skip normalization if query is already normalized using the same method as the reference refactor(tcr.Immunarch): source the files for Immunarch scripts for better debugging refactor(scnra.SeuratClustering): refactor the script for better debugging refactor(scnra.SeuratPreparing): refactor the script for better debugging fix(scrna): fix resolution expansion for SeuratClustering and SeuratSubClustering fix(cellranger.CellRangerCount): fix falsy envs.create_bam not working for cellranger v7 fix(scrna): Fix generating PrepSCTFindMarkers command when no previous commands present tests(scrna.ScFGSEA): fix unavailable urls to GMT files chore(scrna.SeuratMap2Ref): optimize memory usage chore(scrna.MetaMarkers): remove plugin_opts.poplog_max chore(tcr.CloneResidency): improve logging when handling subjects other fix(stats.Mediation): fix when NAs in the data feat(plot): add Scatter for scatter plots tests: use single conda env for tests ci: fix CI due to conda env changes docs(web): update docs of envs.tool for Download/DownloadList feat(web): add GCloudStorageDownloadFile and GCloudStorageDownloadBucket to download files from GCP chore(regulatory.MotifAffinityTest): use template filter source_r to source R files tests(regulatory.MotifAffinityTest): rename regulation to regulatory chore: use template filter source_r to source R files fix(stats): handle case when p-value is 0 for MetaPvalue and MetaPvalue1 0.29.2 chore(stats.Mediation): make better logging strategy for various number of cases chore(scrna.SeuratClusterStats): use ident label length to adjust default height for feature plots fix(scrna.MetaMarkers): fix seurat object not updated when expanding cases and run PrepSCTFindMarkers when necessary before calling meta-markers fix(scrna.MarkersFinder): fix fetching command when composing the PrepSCTFindMarkers command fix(scrna_metabolic_landscape): handle null values in for loop in MetabolicFeatures and MetabolicFeaturesIntraSubset for report generation 0.29.1 BREAKING: rename namespace regulation to regulatory choir(plot.Manhattan): default envs.title to None (don't add title to the plot by default) enh(plot.Manhattan): give warnings instead of errors about zooming chromosomes not existing fix(plot.Manhattan): fix envs.ylabel not working feat(stats): add Mediation for mediation analysis feat(plot.QQPlot): add support for custom theoratical values tests(plot.QQPlot): add tests chore(snp.MatrixEQTL): allow pvalue cutoffs to be greater than 1 (but 1 will be used anyway) fix(snp.PlinkIBD): add --keep-allele-order to keep the allele order fix(delim.SampleInfo): fix numbers not split up when each is specified. enh(delim.SampleInfo): make sizes of pie charts proportional to number of samples when each is specified enh(scrna.MarkersFinder): run PrepSCTFindMarkers when necessary before calling FindMarkers feat(scrna.SeuratPreparing): add option to cache Seurat object at different steps feat(scrna.SeuratPreparing): allow doubletfinder to run with a different number of cores chore(scrna.SeuratClustering): record PrepSCTFindMarkers command in sobj@commands tests(scrna.SeuratClusterStats): use less stringent p-value cutoff for DEG/MarkersFinder tests(scrna.SeuratPreparing): add doubletfinder in tests 0.29.0 Depedencies deps: update pipen-filters to version 0.13 deps: add meme to env_bio.yml for tests deps: bump pipen-board to 0.15.2 ci: update GitHub Actions versions and dependencies BREAKING BREAKING: merge namespace bcftools to vcf New Features/Processes feat(snp): add PlinkFromVcf to convert VCF files to PLINK format feat(snp): add Plink2GTMat for converting PLINK files to genotype matrix feat(snp): add PlinkIBD analysis for identity by descent feat(snp): add PlinkHWE for Hardy-Weinberg Equilibrium test feat(snp): add PlinkHet for calculating sample heterozygosity feat(snp): add PlinkCallRate for calculating call rate of samples and variants feat(snp): add PlinkFilter process for filtering PLINK files feat(snp): add PlinkFreq process for calculating and filtering by allele frequencies feat(snp): add PlinkUpdateName proc to update variant names in PLINK files feat(gene): add GenePromoters for retrieving gene promoter regions and tests for it feat(bed): add BedtoolsIntersect process for finding the intersection of two BED files feat(regulation): add the namespace and MotifScan to use fimo for motif scanning feat(regulation): add MotifAffinityTest to test the affinity of motifs to the sequences and the affinity change due the mutations. feat(cnv.AneuploidyScore): allow BED and VCF files as in.segfile feat(cnv.TMADScore): allow BED and VCF files as in.segfile feat(cnvkit): allow user home directory (~) to be used in envs.ref in mulitple processes feat(plot): add ManhattanPlot to for support for plotting Manhattan plots feat(plot): add QQPlot proc for generating QQ-plot or PP-plot feat(vcf): add BcftoolsView process for viewing, subsetting, and filtering VCF files feat(vcf): add run_bcftools function for running bcftools with given arguments feat(vcf.BcftoolsSort): allow sorting contigs based on a chrom size file feat(vcf.BcftoolsFilter): allow indexing output file feat(vcf.BcftoolsAnnotate): allow providing annotation file as input file and allow indexing output file feat(stats): add MetaPvalue1 to combine pvalues from the same file feat(stats.MetaPvalue): add envs.keep_single flag to keep the single p-values feat(utils.misc.R): add run_command function for R feat(utils.reference): allow tabix_index to index infile directly feat(snp.MatrixEQTL): add envs.match_samples flag to subset snp, expr and cov data with common samples feat(snp.MatrixEQTL): fix cov data being wrongly transposed feat(snp.MatrixEQTL): allow extra columns when snp and gene position file is BED feat(tests): add lazy loading for reference data download and --local flag for downloading more references locally Refactors/Improvements refactor(utils.gene): redesign gene_name_conversion functions for both python and R refactor(gene.GeneNameConversion): use R for implementation refactor(misc.Shell): save envs.cmd to a file and run it to fix the escaping issues of the command enh(snp.MatrixEQTL): use rtracklayer to read the position files Minor choir(misc.Str2File): add default for in.name choir(gene.GeneNameConversion): allow envs.notfound to be ignore or skip when envs.output is append or replace choir(utils.misc.py): flush output for command printing in run_command function choir(utils.misc.py): print command with a new line for run_command function choir(snp.PlinkFromVcf): indicate sex is to be handled choir(cellranger_pipeline): remove unused import choir(bam.CNVpytor): implement cnvnator2vcf directly instead of using cnvnator2vcf.pl Tests tests(utils.gene): update tests for gene_name_conversion tests(gene.GeneNameConversion): use right environment for tests tests(snp): add tests for plink related processes tests(snp): disable report generation for plink related tests tests(regulation): specify envs.genome for MotifAffinityTest tests: add python package mygene to conda environment biopipen-r tests: add tests for bcftools processes tests: do not download reference data for hg38 at CI tests: update bioconductor-ggmanh dependency to version 1.9.6 tests: add bcftools to conda environment dependencies for tests Docs docs(MatrixEQTL): fix choice items of envs.model docs(cellranger_pipeline): fix types of some items in docs, which should be 'list', instead of 'type=list' Fixes fix(utils.reference): avoid index file to be created again for the same infile for tabix_index function fix(utils.reference): pass -f to bgzip or gunzip to overwrite the output if exists fix(vcf): fix passing vcffile as a string in fix_vcffile in VcfFix_utils.py fix(cnvkit_pipeline): fix sex in process channels fix(cnv.AneuploidyScoreSummary): fix when Sample column is already in metafile fix(cnv.TMADScoreSummary): fix when Sample column is already in metafile Immunopipe-related fix(tcr.TCRClusterStats): fix envs.shared_clusters.heatmap_meta being broken by envs.shared_clusters.sample_order choir(scrna.SeuratMap2Ref): present better error message when envs.use or values of envs.MapQuery.refdata not in reference fix(scrna.MarkersFinder): run PrepSCTFindMarkers when needed choir(scrna.SeuratClustering): use FindClusters to run for multiple resolutions choir(scrna.SeuratSubClustering): use FindClusters to run for multiple resolutions feat(scrna.SeuratClustering): add clustree plot feat(scrna.SeuratSubClustering): add clustree plot tests(scrna.SeuratClusterStats): add assertion for clustree plot generation 0.28.1 fix(scrna.CellsDistribution): fix devpars and hm_devpars not working 0.28.0 tests(scrna.CellTypeAnnotation): add tests for CellTypeAnnotation using scCATCH feat(cellranger_pipeline): add docker image building for cellranger pipeline chore(cellranger.CellRangerCount): add envs.create_bam to control whether create bams (supporting cellranger v8) chore(cellranger): add in.id to CellRangerCount and CellRangerVdj to specify sample ids chore(cellranger.CellRangerSummary): set the default value of report_paging to 8 0.27.9 feat(tcr.TCRClusterStats): add sample_order to set sample order on heatmap and cluster_rows to switch row clustering on/off 0.27.8 fix(scrna.SeuratClusterStats): fix selected columns not unique for stats feat(scrna.SeuratMap2Ref): allow non-SCTransform'ed reference feat(scrna.SeuratMap2Ref): allow splitting query object for mapping (pwwang/immunopipe#61) deps: update pipen-board to version 0.15.1 0.27.7 fix(utils.gsea): fix gsea table not being printed for runFGSEA fix(core.filters): fix slugified pathway plot file name in report fix(scrna_metabolic_landscape): fix mutaters not working fix(scrna_metabolic_landscape.MetabolicFeatures/MetabolicFeaturesIntraSubset): skip groups with less than 5 cells in do_one_group and save a warning file under the case fix(utils.gsea): do not switch 1st and 2nd columns when 2nd column is numeric for localizeGmtfile chore: fix typo in class name ExprImpution to ExprImputation choir(tests): remove KEGG_metabolism.gmt for prep_reference.py tests(scrna_metabolic_landscape): fix tests 0.27.6 fix(scrna_metabolic_landscape.MetabolicFeatures): fix return value of groups with less than 5 cells in do_one_group choir(utils.gsea): avoid printing NULL for runFGSEA tests: use the return value of pipen.run() to test the success 0.27.5 fix(scrna.Subset10X/SeuratTo10X): correct the paths to the scripts feat(testing): allow to enable report for testing pipelines feat(scrna.SeuratPreparing): add envs.cell_qc_per_sample to filter cells before merging instead after test: add tests to scrna.SeuratTo10X and scrna.SeuratPreparing fix(scrna.SeuratClusterStats): fix color palette for ridge plots 0.27.4 feat: add plot.ROC choir(delim.SampleInfo): add alpha to the colors of the plots using biopipen color pallete feat: add snp.MatrixEQTL docs(tcr/scrna/scrna_metabolic_landscape): update links of images in docs 0.27.3 deps: temporary fix copier breaks with pyyaml-include v2 (copier-org/copier#1568) deps: bump pipen-poplog to 0.1.2 (quick fix for populating logs when job fails) choir(scrna.ScFGSEA): Skip cases when no cells found (pwwang/immunopipe#50) choir(scrna.MarkersFinder): Skip cases when no cells found (pwwang/immunopipe#50) choir(scrna.MetaMarkers): Skip cases when no cells found (pwwang/immunopipe#50) feat(scrna.SeuratPreparing): support DoubletFinder 0.27.2 fix(utils.misc.py): inherit envs when env passed for run_command() fix(scrna.RadarPlots): fix mutaters not working feat(tcr.CloneResidency): support envs.upset_ymax to set the max value of y axis in upset bar plot. feat(tcr.TCRDock): add process choir(utils.misc.py): update level to DEBUG for python logger (leaving the filtering to pipen-poplog) choir(stats.DiffCoexpr): change log_warn to debug for some verbosal logging messages refactor(snp.PlinkSimulation): make the configuration files as input so multiple simulations could run in parallel easily. 0.27.1 BREAKING(scrna.SeuratMap2Ref): rename envs.name to envs.ident so envs.MapQuery.refdata is not required anymore. It will be inferred from envs.ident and envs.use . 0.27.0 deps: bump pipen to 0.14.5 deps: bump datar to 0.15.6 depr(scrna.MarkersFinder): remove use_presto as it's used by Seurat v5 by default enh(tcr.CloneResidency): support log scale for y axis of upset bar plots enh(scrna.SeuratClusterStats): allow to rotate labels in circos plot (pwwang/immunopipe#48) enh(scrna.SeuratClusterStats): use pal_biopipen for ident colors in circos plot fix(scrna.CellsDistribution): fix the row order of the heatmaps fix(scrna.SeuratClusterStats): fix when split-by is specified feat(scrna.CellsDistribution): support prefix_each feat(scrna.MarkersFinder): allow set max number of genes to plot in dotplots feat(scrna.MarkersFinder): support setting detailed arguments for overlapping plots feat(scrna.MarkersFinder): support prefix_group feat(scrna.ScFGSEA): support prefix_each feat(scrna.RadarPlots): support prefix_each and subset choir(scrna.SeuratClusterStats): use logger instead of print for log messages choir(tcr.TCRClustering): print session info for clustcr script choir(scrna.MarkersFinder): flatten toc when no section and no ident-1 specified choir: extract case expansion pattern (scrna.CellsDistribution, scrna.MarkersFinder, scrna.MetaMarkers, scrna.RadarPlots, scrna.ScFGSEA, scrna.TopExpressingGenes) docs: add more detailed docs for envs.section tests: add assertion for success of the pipelines tests: add tests for utils.misc.R tests: add r-presto to env_r.yml tests: simplify tests for r functions tests: pin scipy to 1.8.0 for clustcr in env_r.yml tests: refactor tests for core.filters using unittest tests: refactor tests for utils.common_docstrs using unittest tests: refactor tests for utils.gene using unittest tests: refactor tests for utils.mutate_helpers using unittest tests: refactor tests for utils.single_cell using unittest 0.26.2 deps: bump datar-pandas to 0.5.5 to dismiss deprecated warnings 0.26.1 deps: bump pipen to 0.14.3 deps: pin ggplot2 to 3.4 due to breaking changes of 3.5 for test fix(utils.misc.R): replace latin and greek characters with closest ascii chars for slugify() feat(scrna.TopExpressingGenes): support subset fix(scrna.CellsDistribution): fix the row order of the heatmaps. enh(tcr.CloneResidency): add legend for multiplets in upset plots. feat(scrna.SeuratClusterStats): add circos plot for cell composition stats (pwwang/immunopipe#46). 0.26.0 deps: bump pipen to 0.14.1 deps: bump pipen-report to 0.18.1 fix(scrna.CellsDistribution): fix multiple cells_by columns and speed up plotting choir(tcr.CloneResidency): mark singletons in Venn diagrams more clear fix(scrna.RadarPlots): fix the order of groups on radar plots choir(scrna.RadarPlots): transpose the count/percentage table to save to files fix(scrna.MarkersFinder): fix generating report json file when no significant genes found choir(scrna.MarkersFinder): Plot maximum 20 genes in dotplots choir(scrna.MarkersFinder): Do not convert dashs in case names to dots feat(utils.misc): add logger for python (which allows pipen-poplog to populate logs to running log) feat: add rnaseq.UnitConversion and tests feat: add rnaseq.Simulation to simulate RNAseq data feat: add snp module and snp.PlinkSimulation process feat: add stats module feat: add stats.ChowTest feat: add stats.LiquidAssoc feat: add stats.DiffCoexpr feat: add stats.MetaPvalue 0.25.4 deps: bump datar to 0.15.4 (support pandas 2.2) fix(utils.single_cell.R): fix immdata_from_expanded missing other data columns fix(tcr.Immunarch): fix mutaters not working when no subset is set fix(scrna.CellsDistribution): fix hm_devpars not working 0.25.3 fix(scrna.CellTypeAnnotation): keep factor meta data when input and output are RDS for celltypist 0.25.2 fix(scrna_metabolic_landscape.MetabolicPathwayHeterogeneity): fix output directory path is not slugified chore(tcr.Immunarch): change case filling log to debug level 0.25.1 scrna.CellTypeAnnotation: leave the meta data as is in celltypist wrapper 0.25.0 deps: bump pipen to 0.13.2 feat: add scrna.AnnData2Seurat and scrna.Seurat2AnnData scrna.MarkersFinder: allow to cache FindAllMarkers results scrna.CellTypeAnnotation: support celltypist (#111) scrna.SeuratSubClustering: add envs_depth = 1 to replace whole envs.cases when new case assigned test: add tests for celltypist of CellTypeAnnotation , AnnData2Seurat and Seurat2AnnData 0.24.2 deps: bump pipen-report to 0.17.3 chore: use internal slugify instead of slugify library scrna.SeuratPreparing: fix displaying filters in report scrna.SeuratPreparing: fix logging Seurat procedure arguments cellranger: add CellRangerSummary cell_ranger: use Iframe in report to have loading indicators cellranger_pipeline: add CellRangerCountPipeline and CellRangerVdjPipeline 0.24.1 tcr.Immunarch: update spectratyping output file extension to png 0.24.0 deps: bump up deps by pipen 0.13 deps: add pipen-poplog to populate job logs to running log deps: bump pipen-poplog to 0.0.2 feat: add utils.caching.R cellranger: fix inferring sample name when fastqs from mulitple lanes scrna.SeuratClustering/SeuratSubClustering: cache Seurat procedures step by step scrna.MetaMarkers: limit log messages to be populated to 15 scrna.SeuratPreparing: log procedure arguments at debug level scrna_metabolic_landscape.MetabolicFeaturesIntraSubset: use logger to log so can be poplutated to running log scrna_metabolic_landscape.MetabolicPathwayActivity: use logger to log so can be poplutated to running log tcr.ImmunarchLoading: add logs for steps tcr.TCRClustering: use logger to log so can be poplutated to running log tcr.TESSA: log command at debug level tcr.Immunarch: add plot_type for divs to support boxplots tcr.TCRClustering: fix log_info not found tcr.Immunarch: make poplog_max 999 to pop all job logs to running log scrna_metabolic_landscape.MetabolicFeaturesIntraSubset: change log level for groups from warning to info 0.23.8 scrna.SeuratPreparing: log Seurat procedure arguments scrna.ScFGSEA: add subset to filter cells 0.23.7 scrna.SeuratPreparing: update log message for transformation/scaling step scrna_metabolic_landscape.MetabolicPathwayHeterogeneity: add utils.gsea script source to support localizeGmtfile 0.23.6 feat: support url for gmtfile wherever GSEA is performed (#113) utils.gsea.R: fix file path in gsea.R tcr.Immunarch: add error message for empty filtered/subset data in diversity scrna.SeuratPreparing: correct description of default assay in docstr scrna.SeuratPreparing: run also the normal normalization procedures when SCTransform is used (useful for visualization purposes on RNA assay) scrna.SeuratClustering: add related issue link to PrepSCTFindMarkers scrna.ModuleScoreCalculator: document the names added by cell cycle score (pwwang/immunopipe#34) scrna.SeuratPreparing: support sample names as reference for IntegrateLayers 0.23.5 scrna.SeuratClusterStats: fix when frac or frac_ofall is true and no group-by nor split-by is specified for stats core.filters: fix when no enriched items found for report component enrichr scrna.MarkersFinder: fix when no enriched items found scrna.MetaMarkers: fix when no enriched items found scrna.TopExpressingGenes: fix when no enriched items found utils.gsea.R: fix when no enriched items found for runEnrichr scrna_metabolic_landscript: fix adding report when ncores > 1 0.23.4 scrna.TopExpressingGenes: fix colnames while pulling average expression scrna.CellsDistribution: fix when cells_by has multiple column names scrna.CellTypeAnnotation: fix the order of the clusters for direct method scrna.SeuratClusterStats: add position options for bar plots for stats scrna.RadarPlots: add colors to set the colors of the loops in radar and bar plots tcr.Immunarch: add split_by and split_order to put subplots together in one single plots 0.23.3 tcr.ImmunarchLoading: change mode from single to paired by default 0.23.2 scrna.RadarPlots: fix test error when not enough observations scrna.RadarPlots: add n and mean to test table 0.23.1 scrna.RadarPlots: fix error when generating report for tests when breakdown is not provided 0.23.0 deps: bump pipen to 0.12.5 deps: bump pipen-report to 0.16.3 deps: Update seurat to 5.0.1 in test env file chore: Add /tmp to .gitignore scrna.MarkersFinder: Add envs.use_presto to use presto to speed up finding markers scrna.MarkersFinder: Fix a bug when subsetting cells scrna.MarkersFinder: Set envs.dbs to KEGG_2021_Human and MSigDB_Hallmark_2020 by default scrna.MarkersFinder: Fix FindAllMarkers/FindMarkers for SCTransform'ed data scrna.SeuratPreparing: Fix handling of empty path in RNAData scrna.SeuratPreparing: Set envs.gene_qc.min_cells to 0 by default (instead of 3) scrna.SeuratPreparing: Add sample integration procedures scrna.SeuratPreparing: Allow to filter genes directly scrna.SeuratClustering: Add options to limit string and numeric output length to have more exact caching signature scrna.SeuratClustering: Set default random.seed to 8525 for FindClusters scrna.SeuratClustering: Allow multiple resolutions for FindClusters scrna.SeuratClustering: Print table of idents in log for found clusters scrna.SeuratClustering: Move integration procedues to SeuratPreparing and do only clustering scrna.SeuratClustering: Update tests scrna.SeuratClustering: Make the cluster labels start with \"c1\" instead of \"0\" scrna.SeuratClustering: Default reduction of RunUMAP and FindNeighbors to pca scrna.SeuratClustering: Fix test scrna.SeuratClustering: Print less verbosal log scrna.SeuratClusterStats: Add ngenes to plot the number of genes expressed scrna.SeuratClusterStats: Add barplot for features and allow aggregation of features scrna.SeuratClusterStats: Fix matching kind for plots of features scrna.SeuratClusterStats: Use new umap for plotting feature and dimplots for sub-clustering scrna.SeuratClusterStats: Use default assay for plotting of number of genes expressed scrna.SeuratClusterStats: Add envs.mutaters to mutate meta data scrna.SeuratClusterStats: Add histograms to plot number of cells against another variable scrna.SeuratClusterStats: Fix reduction for subclustering for dimplots scrna.SeuratClusterStats: Subset seurat object for featureplots when ident is subclusters scrna.SeuratClusterStats: Fix argument layer not excluded for heatmaps in features scrna.SeuratClusterStats: Add frac_ofall and transpose for stats to calculate fraction within group or against all cells, and transpose ident and group, respectively scrna.ModuleScoreCalculator: Fix features not being passed to AddModuleScore as a list scrna.ModuleScoreCalculator: Support calculating diffusion map components scrna.SeuratMap2Ref: Rename envs.alias to `envs.name scrna.SeuratMap2Ref: Set default value of envs.MappingScore.ndim to 30 scrna.SeuratMap2Ref: Add envs.ncores for parallelization scrna.SeuratMap2Ref: Remove preset MapQuery arguments scrna.SeuratMap2Ref: Raise an error when envs.MapQuery.refdata is not provided scrna.SeuratMap2Ref: Default envs.use to the key of envs.MapQuery.refdata with single key scrna.SeuratMap2Ref: Use layer instead of slot in docstring (Seurat v5) scrna.SeuratMap2Ref: Make sure the column of cluster labels is a factor scrna.ScFGSEA: Allow to ignore small group when fgsea fails due to all NAs for pre-ranks scrna.ScFGSEA: Use default assay and use layer instead of slot (Seurat v5) scrna.TopExpressingGenes: Use default assay of Seurat object and fix column names of average expression (Seurat v5) scrna.TopExpressingGenes: Change default enrichment gene sets to KEGG_2021_Human and MSigDB_Hallmark_2020 scrna.MetaMarkers: Change default enrichment gene sets to KEGG_2021_Human and MSigDB_Hallmark_2020 scrna.MetaMarkers: Give better message when tests for genes fail scrna.MetaMarkers: Give error message when not enough cells in case scrna.CellsDistribution: Allow to order clusters by envs.cluster_orderby scrna.CellsDistribution: Add heatmaps scrna.SeuratSubClustering: Add process scrna_metabolic_landscape: Add InlineNotification component to imports for report scrna_metabolic_landscape.MetabolicFeatures: Fix when default assay is SCT scrna_metabolic_landscape.MetabolicFeaturesIntraSubset: Fix when default assay is SCT scrna_metabolic_landscape.MetabolicPathwayActivity: Fix when default assay is SCT scrna_metabolic_landscape.MetabolicPathwayActivity: Use default assay of Seurat object scrna_metabolic_landscape.MetabolicPathwayHeterogenetiy: Fix when default assay is SCT scrna.CellTypeAnnotation: Use layer instead of slot of Seurat object (Seurat v5) for sctype tcr.ImmunarchLoading: Allow empty path in TCRData column in input file tcr.ImmunarchLoading: Do not hide envs.mode anymore in docs tcr.CloneResidency: Fix stringifying the subject in case it is a factor tcr.CloneResidency: Make section works in report tcr.Immunarch: Support paired chain data for VJ conjuction plots tcr.TESSA: Change envs.assay to None to use default assay of Seurat object scrna_basic: remove scrna_basic pipeline, use immunopipe instead scrna.GeneExpressionInvestigation: Remove deprecated code scrna.Write10X: Use layer instead of slot (Seurat v5) scrna.ExprImputation: Use default assay of seurat object scrna.SeuratTo10X: Rename Write10X to SeuratTo10X scrna.SeuratSubClustering: Fix original reduction being poluted by subclustering scrna.SeuratClusterStats: Add avgheatmap to plot more elegant heatmap for average gene expressions scrna.SeuratClusterStats: Fix ident not working for dimplots scrna.SeuratClusterStats: Fix for hists when x is a factor/character vector scrna.SeuratClusterStats: Add cluster_orderby to order clusters for features scrna.SeuratClusterStats: Add na_group to keep NA values in group-by scrna.SeuratClusterStats: Allow avgheatmap to plot features other than gene expressions scrna.SeuratClusterStats: Add mutate_helpers.R source file scrna.SeuratClusterStats: Fix data binding for avgheatmap in features utils.mutate_helpers: Change arguments id_col and compare_col to id and compare, respectively utils.mutate_helpers: Fix that subset can't be an expression for expanded family utils.mutate_helpers: Add top to select top entities (e.g clones) scrna.RadarPlots: Add breakdown and test to break down the cell distribution and run statistic test on the fractions 0.22.8 scrna_metabolic_landscape.MetabolicPathwayActivity: Fix useNames = NA being deprecated in matrixStats v1.2 (more locations) scrna_metabolic_landscape.MetabolicPathwayActivity: Fix heatmap column_split scrna_metabolic_landscape.MetabolicFeaturesIntraSubset: Sort groups when being processed utils.gsea: Fix useNames = NA in rowSds for matrixStats v1.2 utils.mutate_helpers: Fix tests 0.22.7 scrna_metabolic_landscape.MetabolicPathwayActivity: Fix useNames = NA being deprecated in matrixStats v1.2 0.22.6 deps: Bump pipen-board to 0.13.10 (pipen-report to 0.16.2) 0.22.5 docs: Bump pipen-board to 0.13.9 (pipen-report to 0.16.1) cellranger.CellRangerCount: Update iframe height in report cellranger.CellRangerVdj: Update iframe height in report 0.22.4 utils.mutate_helpers: Update docs 0.22.3 utils.mutate_helpers: Return ids only when subset is true and group is not NA for uniq = TRUE in expanded, collapsed, emerged and vanished 0.22.2 docs: Update logo and favicon docs: Update logo height in README.md core.filters: Add exclude argument to dict_to_cli_args filter cellranger: Add CellRangerCount and CellRangerVdj scrna.CellTypeAnnotation: Allow using NA to exclude clusters from output Seurat object scrna.SeuratClusterStats: Fix path of expression table file scrna.MarkersFinder: Use FindAllMarkers if ident.1 is not specified scrna.CellsDistribution: Don't add rownames to the output table file utils.mutate_helpers: Add debug and each to expanded, collapsed, vanished and emerged 0.22.1 scrna.CellsDistribution: Export table with distinct columns scrna.SeuratMetadataMutater: Warn about existing columns in seurat object tcr.ImmunarchLoading: Change metacols to extracols so essential columns get exported tcr.Attach2Seurat: Detach prefix from template in code tcr.CDR3AAPhyschem: Detach prefix from template in code tcr.Immunarch: Use immdata$prefix as prefix by default tcr.TCRClustering: Use immdata$prefix as prefix by default tcr.TESSA: Allow in.immdata to be either an RDS file of immunarch object or a text file of cell-level expanded data 0.22.0 Bump pipen-board to 0.13.8 (pipen-report to 0.16) Use render_job filter to generate report utils: Add biopipen palette scrna.SeuratClusterStats: Add subset for dimplots to scrna.CellsDistribution: Add descr for cases in report scrna.CellsDistribution: Save the table only with necessary columns scrna.MarkersFinder: Add dot plot scrna.MetaMarkers: Use logger to log messages scrna.SeuratClustering: Use logger to log messages scrna.SeuratClustering: Add cache option to cache the clustering results if nothing changed except ncores delim.SampleInfo: Fix handling of null exclude_cols 0.21.2 tcr.Immunarch: Add V-J junction circos plots tcr.Immunarch: Refactor logging statements using r-logger 0.21.1 deps: Update pipen-board and pygments versions docs: Adopt mkdocs-rtd 0.0.10 docs: Fix internal reference in API docs delim.SampleInfo: Refactor data subset logic in SampleInfo class 0.21.0 tcr.Immunarch: Fix empty groups in diversity plot after filtering tcr.Immunarch: Add in.metafile to allow other meta info (i.e. seurat clusters) for future subsetting tcr.Immunarch: Change envs.mutaters now on expanded (cell-level) data tcr.Immunarch: Add subset for cases to do analysis on a subset of data tcr.Immunarch: Add separate_by also works on other diversity plots tcr.Immunarch: Add ymin and ymax to align diversity plots by separate_by tcr.Immunarch: Add ncol to specify # columns in the combined plots scrna.RadarPlots: Fix envs.order not working scrna.MarkersFinder: Add overlap to find overlapping markers between cases (pwwang/immunopipe#24) scrna.MarkersFinder: Add subset for each case to subset cells scrna.MarkersFinder: Add dot plots for cases scrna.CellsDistribution: Allow multiple columns for cells_by scrna.CellsDistribution: Add subset for cases to subset cells cnv.AneuploidyScoreSummary: Ignore .call suffix to get sample name by default cnv.AneuploidyScoreSummary: Fix image path in report while envs.group_cols is a string (not an array) utils.single_cell.R: Add functions to expand, filter and restore immunarch objects utils.common_docstrs: Extract some common docstrings for procs utils.misc.R: Use r-logger for logging for R scripts utils.mutate_helpers.R: Add include_emerged for expanded() and include_vanished for collapsed() utils.mutate_helpers.R: Fix tests tests: Add r-logger to test dependencies 0.20.7 (delim.SampleInfo) Add distinct to case to perform stats on distinct records (scrna_basic) Fix docker image building 0.20.6 \u2b06\ufe0f Bump pipen-board to 0.13.4 \u2728 [scrna.MarkersFinder] Allow to set assay for Seurat::FindMarkers() \u2728 [scrna.CellsDistribution] Add venn/upset plot for overlapping cell groups in different cases 0.20.5 \u2b06\ufe0f Bump pipen-board to 0.13.3 \ud83c\udfd7\ufe0f [tcr.CloneResidency] Rename envs.sample_groups to envs.section to be consistent with other processes \ud83c\udfd7\ufe0f [tcr.CloneResidency] Allow additional metadata from input for more flexible case definition (i.e. analysis for specific seurat clusters) \ud83d\udcdd [scrna.ScFGSEA] Remove the link in the summary of the docstring (since they are not transformed in the report) \ud83c\udfa8 [tcr.CDR3AAPhyschem] Give better error message when wrong group items are given 0.20.4 \ud83d\udc1b [scrna.SeuratClusterStats] Fix toc not saved correct (causing report not including the right sections) 0.20.3 \ud83d\udc1b [scrna.SeuratPreparing] Fix when cell_qc is None \ud83c\udfa8 [scrna.MarkersFinder] Add margins to volcano plot \ud83d\udc1b [scrna.SeuratClusterStats] Fix ident in cases of envs.dimplots not working 0.20.2 \ud83d\ude91 [scrna.SeuratPreparing] Fix % in docstring to crash the pipeline 0.20.1 \ud83d\udcdd [scrna_basic/scrna_metabolic_landscape/scrna/tcr] Update docstring \ud83c\udfa8 [scrna.MarkersFinder] Try include more genes in volcano plot (pwwang/immunopipe#17) \ud83c\udfa8 [scrna.CellsDistribution] Give better error message in CellsDistribution if group value not found (pwwang/immunopipe#16) \ud83d\ude9a [tcr.TCRClusterStats] Rename TCRClusteringStats to TCRClusterStats (pwwang/immunopipe#15) 0.20.0 \u2b06\ufe0f Bump pipen to 0.12 0.19.0 \u2b06\ufe0f Bump pipen-report 0.13.1 (pwwang/immunopipe#9, 2) \u2b06\ufe0f Bump pipen-board to 0.12.5 \ud83d\udc84 [docs] Hide unnecessary items in nav bar \ud83d\udc84 [docs] Get docs, especially API docs, formatted better \ud83d\udc1b [delim.SampleInfo] Fix order in pie charts \ud83c\udfa8 [delim.SampleInfo] Add stricter checker for input file (pwwang/immunopipe#13) \ud83c\udfa8 [scrna.SeuratPreparing] Improve QC plots \ud83d\udcdd [scrna.SeuratPreparing] Fix type annotation for envs.features_defaults.ncol in docstring \ud83d\udc1b [scrna.CellsDistribution] Fix the cluster order in pie charts \ud83d\udc1b [scrna.SeuratClusterStats] Fix the cluster order in pie charts \ud83c\udfa8 [scrna.SeuratClusterStats] Indicate the case name in logs when pie is enable for group-by \u2728 [scrna.SeuratClusterStats] Allow mutiple columns in the file for envs.features_defaults.features \u2728 [scrna.SeuratClustering] Add number of clusters at the end of log \ud83e\ude79 [scrna.ModuleScoreCalculator] Set default assay to RNA in case module scores only caculated using integrated features \ud83d\udcdd [tcr.Immunarch] Fix docstring for envs.div.args \ud83c\udfa8 [tcr.CloneResidency] Allow order to be optional \ud83c\udfa8 [tcr.Immunarch] Allow to skip overlap and gene usage analyses by setting method to none (pwwang/immunopipe#11, pwwang/immunopipe#12) \ud83d\udc1b [tcr.TCRClusteringStats] Don't cluster on heatmap when there are only 2 samples \ud83d\udc1b [scrna_metabolic_landscape.MetabolicFeatures] Import Seurat explictly to avoid satijalab/seurat#2853 \ud83d\udc1b [scrna_metabolic_landscape.MetabolicPathwayActivity] Fix when NA values in data for heatmap \ud83d\udc1b [scrna_metabolic_landscape.MetabolicPathwayHeterogeneity] Fix error when no significant pathways selected 0.18.3 \ud83d\udc1b [scrna.MarkersFinder] Fix when either ident is empty 0.18.2 \ud83d\udc1b [tcr.CDR3AAphyschem.R] Fix a bug when the min length of CDR3 seqs > 12 0.18.1 \u2b06\ufe0f Bump datar to 0.15.3 \ud83c\udfa8 [scrna.MetaMarkers/ScFGSEA/SeuratClusterStats] Remove tidyseurat:: prefix for filter \u2728 [tcr.TESSA] Allow the results to be saved to seurat object \ud83d\udcdd [tcr.TESSA] Fix docs about envs.assay 0.18.0 \ud83d\udd27 Update .gitignore \u2b06\ufe0f Bump pipen to 0.11 \u2b06\ufe0f Bump datar to 0.15.2 \ud83d\udea8 Make line length to 88 for linting \u2728 [core.filters] Add skip argument to r() \ud83d\ude91 [tcr.TESSA] Fix type annotation for envs.max_iter \ud83d\udc1b [delim.SampleInfo] Allow unique: prefix for on in stats cases; fix sample order in plots \u267b\ufe0f [scrna.SeuratClusterStats] Redesign envs \u2728 [scrna.MarkersFinder] Add volcano plot \u2728 [tcr.TESSA] Add envs.assay for seurat object input \ud83d\udc1b [tcr.TESSA] Fix when a V-gene/J-gene is missing \u2705 [gsea.FGSEA] Fix tests \ud83d\udeb8 [scrna.SeuratClustering] Add clear message when k.weight is too large for IntegrateData \u23ce 0.17.7 \u2705 [tests] Allow pass FORCE=true to run local-only tests \u2705 [tests] Fix receiving VERBOSE and FORCE in test script \ud83d\ude91 [tcr.ImmunarchLoading] Fix when Sample is the only column in meta \u2728 [tcr.TESSA] Add process and test 0.17.6 \ud83d\udc77 Fix CI for publishing the package \u2b06\ufe0f Bump pipen-board to 0.11.5 \ud83d\ude91 [scrna.SeuratClusterStats] Adjust default width and height for plots \ud83d\ude91 [scrna.CellTypeAnnotation] Keep order of clusters after hitype annotation 0.17.5 \ud83d\udc77 Do not run CI build for publish job \ud83c\udfa8 [tcr.TCRClustering] Add TCR_Cluster_Size1 in addition to TCR_Cluster_Size in out.clusterfile to represent #cells and #CDR3 seqs \u2b06\ufe0f Bump up dependencies 0.17.4 \u2728 [tcr.TCRClustering] Add TCR_Cluster_Size in out.clusterfile \ud83d\udca5 [scrna.SeuratClusterStats] Rename envs.exprs to envs.features 0.17.3 \u2b06\ufe0f Bump pipen-report to 0.12.8 \ud83d\udcdd [delim.SampleInfo] Show h1 in report only when stats specified \ud83d\udcdd [delim.SampleInfo] Fix parsing excluded_cols in report 0.17.2 \ud83d\udcdd [delim.SampleInfo] Add report template 0.17.1 \ud83c\udfa8 [scrna.CellTypeAnnotation] Change seurat_clusters_old to seurat_clusters_id to save old seurat_clusters \ud83d\udca5 [csv] Rename to delim \ud83d\ude9a [csv.BindRows] Rename to delim.RowsBinder \u2728 [utils.mutate_helpers.R] Add paired() to identify paired records \u2728 [delim.SampleInfo] Add process 0.17.0 \u2b06\ufe0f Bump pipen-board to 0.11.4 \ud83d\udcdd [docs] Update logo \ud83d\udcdd [docs] Add css due to mkdocs-rtd change \ud83d\udca5 [core.filters] Default sortkeys to False for filter r \ud83d\udc1b [scrna.ModuleScoreCalculator] Fix aggregation values of programs \ud83d\udc1b [scrna.SeuratClusterStats] Fix typo for default stats \ud83d\udc1b [scrna.ModuleScoreCalculator] Fix name for cell cycle scores \ud83d\udc1b [scrna.CellsDistribution] Fix when cells_by or group_by is not an identifier \ud83d\ude91 [utils.mutate_helpers.R] Allow accessing metadata using . \u2728 [scrna.ModuleScoreCalculator] Add proc 0.16.7 \ud83d\udd25 [scrna.SeuratMetadataMutater] Remove unnecessary in.mutaters \ud83d\udcdd [docs] Use kmdocs-rtd for documentation \ud83d\udcdd [scrna_basic] Fix docs \ud83d\udcdd [docs] Fix CI when files in docs/ changes \ud83d\udcdd [docs] Fix CI when CI config file changes \ud83d\udcdd [scrna_basic] Update docs for processes \ud83d\udd27 [scrna_basic] Update example config file \ud83d\udcdd [docs] Add logo and favicon \ud83d\udcdd [docs] Fix font-sizes in APIs \ud83d\udcdd [docs] Fix logo size in README 0.16.6 \ud83d\ude91 [scrna] Hotfix for docstring when parsed by argparse help 0.16.5 \ud83d\udca5 [scrna.SeuratMetadataMutater] Move mutaters from in to envs \ud83d\udd25 [scrna.CellsDistribution] Remove unnecessary in.casefile \ud83d\ude91 [scrna.CellTypeAnnotation] Hotfix when envs.hitype_db as a file starts with \"hitypedb_\" 0.16.4 \ud83d\ude91 [scrna.CellTypeAnnotation] Hotfix passing envs.newcol \u2b06\ufe0f Bump pipen-report to 0.12.7 0.16.3 \ud83d\udcdd [scrna_metabolic_landscape] Update docstring \u2728 [tcr.CDR3AAPhyschem] Allow envs.subset_cols to be separated by comma \u2728 [scrna.CellTypeAnnotation] Add envs.newcol to keep original idents 0.16.2 \ud83d\udea8 Add .lintr for R lintr \u2b06\ufe0f Bump pipen-board to 0.11.1 \ud83d\udc84 [report] Separate enrichr_report \ud83d\udc84 [scrna.CellsDistribution] Fix reports \ud83d\udc84 [scrna.CellsDistribution] Reorganize report \ud83d\udc84 [scrna.MarkersFinder] Reorganize report \ud83d\udc84 [scrna.ScFGSEA] Reorganize report \ud83d\udc84 [scrna.TopExpressingGenes] Reorganize report \ud83d\udea8 [scrna.TopExpressingGenes] Fix linting issues in script \ud83d\udd27 [scrna.MarkersFinder] Set envs.prefix_each to True by default \ud83d\udd27 [scrna.TopExpressingGenes] Set envs.prefix_each to True by default \u2728 [scrna.MetaMarkers] Add proc\u23ce 0.16.1 \ud83d\udea8 Fix some linting issues \u2b06\ufe0f Bump pipen-board to 0.11 \ud83c\udfa8 [scrna.CellTypeAnnotation] Rename seurat_clusters.old to seurat_clusters_old to save the old clusters for sctype \ud83d\udc1b [scrna.CellTypeAnnotation] Fix saving annotated cell type to text file for sccatch \ud83c\udfa8 [scrna.CellTypeAnnotation] Save old clustering to seurat_clusters_old for sccatch \ud83c\udfa8 [scrna.CellTypeAnnotation] Save old clustering to seurat_clusters_old for direct method \ud83d\udcdd [scrna.CellTypeAnnotation] Fix links in docs for sccatch \u2728 [scrna.SeuratClusterStats] Allow envs.exprs.genes to be genes directly (separated by \",\") \ud83d\udc84 [docs] Update API doc styles for dark mode \u2728 [tcr.TCRClustering] Save the souce code of GIANA with this package \u2728 [tcr.TCRClusteringStats] Allow multiple cases \ud83d\udcdd [tcr.ImmunarchLoading] Update docstring \u2728 [utils] Add mutate_helpers to identify expanded, collapsed, emerged and vanished clones \ud83d\udc1b [utils/misc.R] Fix list_setdefault and list_update when value is NULL \ud83d\udc1b [scrna.TopExpressionGenes] Fix expanding cases \u2728 [scrna.SeuratClustering] Allow envs.FindIntegrationAnchors.reference to be a string separated by comma \u2728 [scrna.ScFGSEA] Allow multiple cases \u2728 [scrna.MarkersFinder] Allow to use mutate_helpers in envs.mutaters \ud83c\udfa8 [scrna.CellsDistribution] Redesign envs to support multiple cases \ud83d\udc84 [tcr.Immunarch] Fix report generation for rarefraction analysis \ud83d\udd27 [tcr.Immunarch] Change envs to be less error prone \ud83d\udc84 [scrna.CellsDistribution] Fix reports \ud83d\udc84 [scrna.ScFGSEA] Fix reports \u2705 [tests] Fix tests 0.16.0 \u2b06\ufe0f Bump pipen-board to 0.10 \ud83d\udc84 [docs] Update docs styles \ud83d\udea8 [core/testing] Remove unused importings \ud83c\udfa8 [scrna] Rename RNADir to RNAData for input data \ud83d\udc1b [gsea.GSEA] Replace doc.string with doc_string to avoid over parsing by pipen-args \ud83c\udfa8 [tcr.Immunarch] Refactor and split into modules \ud83c\udfa8 [scrna.CellTypeAnnotation] Rename CellTypeAnnotate to CellTypeAnnotation and add hitype \ud83c\udfa8 [tcr.ImmunarchLoading] Make it compatible with immunarch 0.9 \ud83c\udfa8 [scrna.MakersFinder] Support multiple cases \ud83c\udfa8 [scrna.TopExpressionGenes] Support multiple cases \ud83d\udc1b [scrna.RadarPlots] Fix section and devpars not passed to script \ud83d\udc1b [scrna.SeuratClustering] Fix PCA on each sample \ud83c\udfa8 [scrna.ExprImpution] Rename from ExprImpute to ExprImputation \ud83d\udc77 [scrna.CellTypeAnnotation] Add r-hitype to env_r.yml for testing \ud83d\udc1b [scrna.CellTypeAnnotation] Fix typos for hitype script \ud83d\udc1b [scrna.CellTypeAnnotation] Fix startsWith in hitype script \ud83c\udfa8 [scrna_basic] Rename ScrnaBasicAnnotate to ScrnaBasicAnnotation \ud83d\udcdd [scrna_basic] Update docs \ud83d\udc1b [cnvkit_pipeline] Fix docker image building \ud83d\udcdd [cnvkit_pipeline] Fix docs 0.15.2 \u2b06\ufe0f Bump pipen-board to 0.9.1 \u2728 [scrna.RadarPlots] Add process \ud83c\udfa8 [tcr.Immunarch] Separate diversity in script into a different file \u2728 [scrna.TopExpressingGenes] Add process \ud83c\udfa8 [scrna.CellsDistribution] Use a different color palette \ud83c\udfa8 [scrna.SeuratClusterStats] Warn about heatmap without downsampling 0.15.1 \u2b06\ufe0f Bump pipen-board to 0.8.0 \u2b06\ufe0f Bump pipen-report to 0.12.5 (to fix the pydantic error) \ud83c\udfa8 [tcr.CloneResidency] Add indicators during running \ud83c\udfa8 [tcr.CloneResidency] Allow multiple cases add mutaters for metadata \ud83d\udc1b [misc.File2Proc] Check if input file exists \ud83c\udfa8 [tcr.Immunarch] Allow cases for trackings and add mutaters for metadata 0.15.0 \u2b06\ufe0f Bump pipen to 0.10.6 \u2b06\ufe0f Bump pipen-board to 0.7.8 \u2796 Retire cmdy at all places (#54) \u2705 [core.filters] Add run.env to test \u2705 [core.filters] Add test for dashify=True \ud83c\udfa8 [scrna.MarkersFinder] Make envs.sigmarkers case wise for scrna.MarkersFinder (#53) 0.14.3 \u2b06\ufe0f Bump pipen to 0.10.5 \ud83d\udd27 [scrna_metabolic_landscape] Make proc group options for process readonly \ud83c\udfa8 [scrna_metabolic_landscape.MetabolicFeatures] Add indicators during computation 0.14.2 \u2b06\ufe0f Bump pipen-board to 0.7.4 \u2b06\ufe0f Bump pipen-report to 0.12.3 \u26a1\ufe0f Replace do.call with do_call in R scripts to improve performance \ud83d\udc1b [scrna.CellTypeAnnotate] Fix when no cell types is given for direct annotation \ud83d\udc1b [cnv.AneuploidyScore] Fix when envs.cn_tranform is a list of thresholds 0.14.1 \u2b06\ufe0f Bump pipen-board to 0.7.3 \u2b06\ufe0f Bump other dependencies \ud83c\udfa8 [scrna] Add type=int for envs.ncores in docstrings \ud83d\ude91 [tcr.CloneResidency] Dismiss warnings from pivot_wider 0.14.0 \u2b06\ufe0f Bump pipen-board to 0.6.3 \ud83d\udd27 Fix make-examples.sh for docker images for pipelines \ud83d\ude91 [scrna_basic] Fix \"Issued certificate has expired\" in making examples for docker \u2728 [tcr.CDR3AAphyschem] Add process \u2728 [cnv.TMADScore] Add TMADScore and TMADScoreSummary \ud83d\ude91 [cnv.TMADScore] Fix wrong envs.seg_transform received in script \ud83d\udcdd [cnv.TMADScoreSummary] Add report template \u2728 [cnv.TMADScoreSummary] Support grouping by 2 groups hierarchically \ud83d\udca5 [cnv.AneuploidyScore] Change envs.include_sex to envs.excl_chroms so exclusion of chroms is more flexible \ud83d\ude91 [cnv.AneuploidyScoreSummary] Adjust with of CAA plot based on number of samples \u2728 [cnv.AneuploidyScoreSummary] Support grouping by 2 groups hierarchically \u2b06\ufe0f Bump pipen-board to 0.6.3 0.13.0 \u2b06\ufe0f Bump pipen-board to 0.5.8 \u267b\ufe0f [scrna_basic] Change detault tag from dev to master for docker image \ud83d\udcdd [scrna_basic] Change detault tag from dev to master in docs \ud83d\udd27 [scrna_basic] Change detault tag from dev to master in entry.sh \ud83d\udd27 [scrna_basic] Fix make-examples.sh when running indenpendently \ud83d\udd27 [scrna_basic] Add plugin_opts.report_no_collapse in board.html \ud83d\udea7 [cnvkit_pipeline] Init docker building \u2699\ufe0f [cnvkit_pipeline] Make examples \u2699\ufe0f [cnvkit_pipeline] Update example.json for pipen-board \ud83d\udd27 [cnvkit_pipeline] Fix example in docker image \ud83d\udcdd [scrna_metabolic_landscape] Update docstrings to adopt pipen-board \ud83d\udcdd [utils.misc] Add docstring for run_command \ud83d\udc1b [cnvkit.CNVkitGuessBaits] Use a better way to determine python of cnvkit.py 0.12.0 \u2b06\ufe0f Bump pipen to 0.10 \u2b06\ufe0f Bump pipen-runinfo to 0.1.1 \u2b06\ufe0f Bump pipen-report to 0.12 and pipen-runinfo to 0.2 \u2b06\ufe0f Bump pipen-args to 0.10.2 \u2b06\ufe0f Bump pipen-board to 0.5.6 \ud83d\udcdd Use flag instead action=store_true in docstring \u2705 [utils.gene] Fix tests \ud83c\udfa8 [scrna.SeuratMap2Ref] Add envs.MappingScore \u2728 [scrna.SeuratMap2Ref] Add report template \ud83d\udc84 [scrna.SeuratMap2Ref] Make figures in 2 columns in report \u2728 [scrna.CellTypeAnnotate] Add ScCATCH for cell type annotation \ud83c\udfa8 [scrna.CellTypeAnnotate] Warn when no cell types are given \ud83d\udc1b [cnvkit] Fix when some arguments are None \ud83d\udcdd [cnvkit_pipeline] Update docstrings to adopt latest pipen-annotate and pipen-board \ud83d\udcdd [cnv] Update docstring \ud83d\ude91 [cnv.AneuploidyScoreSummary] Fix when envs.group_col is None but in.metafile is given \ud83d\udc77 [scrna_basic] Init docker image building action \ud83d\udc77 [scrna_basic] Fix dockhub credentials \ud83d\udcdd [scrna_basic] Update docstrings to adopt latest pipen-annotate and pipen-board \ud83d\udcdd [scrna_basic] Add documentation \ud83d\udd27 [scrna_basic] Update configuration for docker image building 0.11.0 \u2b06\ufe0f Bump pipen to 0.9 \u2b06\ufe0f Drop support for python3.7 \u2795 Add pipen-board as dependency \u2728 Add board.toml for pipen-board to run \ud83d\udc1b [cnvkit.CNVkitCoverage] Fix error when generating flat reference \ud83c\udfa8 [bed.BedConsensus] Use bedtools genomecov to calculate the consensus regions \ud83d\udc1b [core.filters] Keep list of dict in python as list of list in R \u2728 [scrna_metabolic_landscape] Allow multiple subsettings for the data \u2728 [scrna_basic] Initialize the pipeline \ud83d\udc1b [bed.Bed2Vcf] Fix OrderedDiot not found \ud83c\udfa8 [cnvkit_pipeline] Import cached_property directly \ud83d\udc1b [scrna.SeuratPerparing] Fix when input contains a single sample \ud83c\udfa8 [tests] Use --reuse instead of --former \ud83d\udc1b [vcf.VcfSplitSamples] Fix missing mutations for extract samples \ud83c\udfa8 [scrna_metabolic_landscape.MetabolicPathwayHeterogeneity] Add progress indicator \ud83c\udfa8 [scrna.SeuratClustering] Allow sample names to be assigned for reference for FindIntegrationAnchors \ud83c\udfa8 [scrna_metabolic_landscape.MetabolicPathwayActivity] Add merged heatmaps for subsets \ud83d\udc1b [scrna_metabolic_landscape.MetabolicPathwayIntraSubsets] Fix fetching subsetting_comparison and limit nproc for FGSEA to 1 \ud83c\udfa8 [scrna_metabolic_landscape.MetabolicPathwayFeatures] Ignore NAs in subsets \ud83c\udfa8 [scrna_metabolic_landscape] Adopt pipen-args 0.9.7 \u2728 [scrna.SeuratMap2Ref] Add process \u2796 [utils] Retire cmdy \u2728 [bed.BedtoolsMerge] Add process \ud83c\udfa8 [core.testing] Use --cache to control of reusing previous run \ud83c\udfa8 [csv.BindRows] Allow to add filename \ud83d\udccc [scrna_basic] Adopt pipen-board 0.1.2 \ud83d\udc1b [web.Download] Fix when args is Diot \ud83c\udfa8 [cnvkit.CNVkitCall] Detach cmdy \u2728 [bam.BamSplitChroms] Add process \u2728 [bam.Merge] Add process and test \ud83d\udc1b [core] Fix repr filter in templates for Diot objects \ud83d\udc1b [docs] Add mygene dep for building utils.gene \u2705 [vcf.TruvariBench] Pin truvari to v3.4.0 for tests 0.10.0 \u2b06\ufe0f Adopt pipen-report 0.7 for report templates \u26a1\ufe0f Add todot and sortkeys arguments for filter r \ud83d\udc1b Set default lang for processes using bash \u26a1\ufe0f Update docstrings for processes for pipen-cli-config \u26a1\ufe0f [scrna.ExprImpute] Add progress indicators for alra \ud83d\udc1b [scrna.ExprImpute] Set default assay to RNA for rmagic 0.9.0 \u2b06\ufe0f Bump up pipen to 0.6 0.8.0 \ud83d\ude80 [vcf.VcfAnno] Add VcfAnno to use vcfanno to annotate VCF files \u2728 [tcgamaf.Maf2Vcf] Add Variant_Classification and Variant_Type to output vcf \u2728 [vcf.VcfFix] Allow gziped vcf as input \ud83e\uddf9 Remove tests for core pipeline (not needed any more) 0.7.1 \u2b06\ufe0f Upgrade pipen-filters to 0.2 \ud83d\udc7d\ufe0f Adopt pipen-filters 0.2 in reports \ud83d\udd27 Rename scrna_metabolic namespace to scrna_metabolic_landscape in entry points \u2728 [scrna.MarkersFinder] Add each for cases to run on each value of metadata variable each \u2728 [tcgamaf.Maf2Vcf] Add proc \u2728 [bcftools.BcftoolsSort] Add proc 0.7.0 \ud83e\uddd1\u200d\ud83d\udcbb [tcr.Immunarch] Allow separating samples for rarefraction analysis \u2728 [scrna.SeuratClusterStats] Add expression matrix to output \ud83e\uddd1\u200d\ud83d\udcbb [tcr.Immunarch] Allow align_x and log scale for rarefraction analysis \u2728 [cnv.AneuploidyScgitoreSummary] Add heatmaps \ud83e\uddd1\u200d\ud83d\udcbb [tcr.Immunarch] Allow separating samples for rarefraction analysis \u2728 [scrna.SeuratClusterStats] Add expression matrix to output \ud83e\uddd1\u200d\ud83d\udcbb [tcr.Immunarch] Allow align_x and log scale for rarefraction analysis \u2728 [cnv.AneuploidyScoreSummary] Add heatmaps \ud83d\udc1b [cnv.Aneuploidy] Fix when only one arm has signals for a chromosome \u2728 [cnvkit.CNVkitGuessBaits] Add proc \u267b\ufe0f [cnvkit_pipeline] Refactor and add docs \ud83c\udfa8 [cnvkit_pipeline] Use process decorator to define processes \u2728 [scrna.SeuratClusterStats] Allow groupby other metadata column than Sample in cell stats \u2728 [scrna.ExprImput] Add ALRA and set as default \ud83c\udfa8 [scrna.scrna_metabolic_landscape] Move from scrna_metabolic and use Seurat object directly instead of sce \ud83d\udc1b [scrna.SeuratClustering] Fix when there are fewer cells \u2728 [scrna.CellTypeAnnotate] Add proc and tests \u2728 [scrna.SeuratClusterStats] Allow subsetting for cell stats \u2705 [vcf.Vcf2Bed] Fix test \u2705 [tests] Add refgenes for testing \ud83d\udc1b [tests] Fix reference preparing \u2705 [tests] Add sctype db for tests \u2705 [tests] Try not patch using lastest poetry \u2705 [tests] Build test deps and fix tests \ud83d\udc77 [tests] Exclude test_scrna_metabolic_landscape from CI \u2b06\ufe0f Upgrade pipen-cli-run to 0.4.1 \u2b06\ufe0f Upgrade pipen to 0.3.11 0.6.2 \ud83c\udfa8 [scripts.utils.vcf] Use format keys for samples \u2728 [vcf.VcfFix] Dedent envs.helpers automatically and allow it to be list of strings \ud83e\uddd1\u200d\ud83d\udcbb [tcr.CloneResidency] Add count table and allow grouping samples in the report \ud83e\uddd1\u200d\ud83d\udcbb [cnvkit.CNVkitCall] Allow not passing threshold \ud83e\uddd1\u200d\ud83d\udcbb [cnvkit.CNVkitCall] Allow setting cutoff to fetch significant genes for enrichment analysis \ud83e\uddd1\u200d\ud83d\udcbb [scrna.SeuratPreparing/SeuratClustering] Do QC in SeuratPreparing only and prepare clustering in SeuratClustering \u2728 [cnvkit_pipeline] Allow customization of colnames in metafile \ud83d\udc9a Fix CI (conda-incubator/setup-miniconda#274) 0.6.1 \u2728 [cnvkit_pipeline] Allow purity for each sample \u2728 [tcr.ImmunarchSplitIdents] Add proc \u2728 [vcf.VcfSplitSamples] Add proc \ud83c\udfd7\ufe0f [cnvkit.CNVkitCall] Pass purity as input instead of envs \u2728 [vcf.VcfIntersect] Add proc \u2728 [vcf.VcfSampleSplits] Add envs.private to keep only private sites for each sample \ud83d\udd27 Fix setup.py file type \u2705 Fix tests for utils.gene \ud83d\udea8 Ignore template strings in python scripts for pyright 0.6.0 \u2728 [cnv] Add AneuploidyScore and AneuploidyScoreSummary \u2728 [scrna.Write10X] Add Write10X \u2728 [cnv.AneuploidyScore] Add envs.include_sex \ud83d\udc1b [scrna.SeuratSubset] Fix when envs.groupby is not given \u2728 [cnvkit.CNVkitHeatmap] Add envs.order for sample order in the heatmap \u2728 [bam.CNAClinic] Add bam.CNAClinic \u2728 [bam.CNAClinic] Add report \u2728 [cnv.AneuploidyScore] Allow a list of thresholds for envs.cn_transform \u2728 [scrna.SeuratSplit] Add scrna.SeuratSplit \u270f\ufe0f [core] Fix typo in core.proc.Pipeline \ud83d\udc7d\ufe0f Refactor pipeline modules with pipen-cli-run 0.3 \ud83d\udc9a Use mamba in CI 0.5.3 \u2728 [scrna.SeuratClusterStats] Allow features to be a file for expression plots \u2728 [tcr.CloneSizeQQPlot] Add process \ud83e\ude79 [tcr.Immunarch] Fix bad characters in the \u201cMotif Analysis\u201d section in report (#43) 0.5.2 \u2b06\ufe0f Pump pipen-args to 0.3 \ud83e\ude79 [scrna.CellsDistribution] Filter NA cells.by 0.5.1 \ud83d\udc9a Fix CI \ud83d\udea8 Add and fix linting \u2b06\ufe0f Pump pipen-report to 0.4.5 0.5.0 \u2705 [vcf.VcfFix] Add chrom size fixes \u2728 [utils.reference] Add bam_index \ud83d\udc1b [bam.CNVpytor] Fix vcf-fix only adds last contig and fix header with snp data \u2728 [vcf.Vcf2Bed] Add process and test \ud83d\udc1b [bed.BedConsensus] Fix final weighting issue \ud83e\ude79 [All] Use %>% instead of |> in all R scripts for backward compatibility \ud83d\udc1b [scrna_metabolic] Don't turn \"Ident\" to \"seurat_clusters\" for grouping.groupby in config \ud83c\udfd7\ufe0f [tests] Add prefix \"biopipen-\" to conda environment names \u2705 [tests] Enable pipen-report only when necessary 0.4.9 \ud83d\udc77 [test] Reverse immunarch in env_r \u2728 [bam.CNVpytor] Add filters \u2728 [cnvkit/cnvkit_pipeline] Add processes and pipeline \ud83d\udc1b [bam.cnvkit] Fix filter direction \ud83d\ude91 [scrna_metabolic] Fix nproc for runFGSEA for MetabolicPathwayHeterogeneity 0.4.8 \ud83e\ude79 [core] Add default for config.exe.bedtools \ud83e\ude79 [scrna.ScFGSEA] Don't convert sparse matrix to avoid \"problem too large\" error 0.4.7 \ud83d\udc1b [scrna.SeuratPreparing] Fix new data preparing when errored 0.4.6 \u2728 [vcf.TruvariBench] Allow multimatch to be passed \u2728 [vcf.TruvariConsistency] Add report 0.4.5 \u2728 [bam.CNVpytor] Generate and fix VCF file as result \ud83d\udcdd [vcf.TruvariBench] Update docs to show other arguments for truvari bench \u2728 [vcf.TruvariBench] Allow sizemax to be passed \u2728 [bed.BedConsensus] Add process and tests \u2728 [core] Add ref.genome to configurations \u26a1\ufe0f [bed.BedConsensus] Parallelize and speed up \ud83d\udc9a [test] Add bedtools to env bio \ud83d\udc9a [test] Add chromsome sizes to reference \ud83d\udc9a [test] Add r-gsea_r to env r \ud83d\udc9a [scrna.ScFGSEA] Fix tests\u23ce 0.4.4 \ud83d\udc1b [scrna.SeuratPreparing] Fix after tidyseurat being used \ud83d\udc1b [scrna.SeuratPreparing] Fix object Sample not found \ud83d\udcdd [Housekeeping] Fix API docs \ud83d\udcdd [Housekeeping] Make apis show neater docs 0.4.3 \u2728 [scrna] Add filter for cases in CellsDistribution, MarkersFinder and ScFGSEA \u2728 [utils] Allow gg object for ggs in plot.R \ud83d\udc1b [scrna_metabolic] Fix reports \ud83d\udc1b [scrna_metabolic] Fix multiple cases \ud83d\udc1b [scrna_metabolic] Fix rmagic for normalization \u26a1\ufe0f [scrna.SeuratClusterStats] Add common gene list \u26a1\ufe0f [scrna.MarkersFinder] Add filter2 to filter after mutaters \ud83d\udc1b [tcr.Immunarch] Fix missing library tibble in script \u26a1\ufe0f [scrna.ScFGSEA] Make ident hierarchical 0.4.2 \ud83d\udc9a [Housekeeping] Fix CI deploy \u26a1\ufe0f [processes] Use faster do_call() instead of do.call() \ud83d\udcdd [tcr] Fix some docstrings with {{ and }} \u2705 [vcf.TruvariBench] Add ref for test \ud83e\ude79 [tcr.TCRClustering] FIx VGeneScores.txt being generated in current dir \ud83d\udcdd [scrna.SeuratPreparing] Update docstring and refactor script \u2728 [scrna.SeuratClustering] Allow dims to be expanded in arguments \ud83d\udcdd [scrna.MarkersFinder] Adopt reduced case configuration level 0.4.1 General \ud83d\udc77 [Housekeeping] Add deploy in CI \ud83d\ude9a [Housekeeping] Move tests/test_tcr/TCRClustering to tests/test_tcr/TCRClusteringStats \ud83d\udd27 [Tests] Add r-tidyseurat to env_r.toml Processes \ud83e\ude79 [scrna.CellsDistribution] Reduce envs.cases levels \ud83e\ude79 [scrna.CellsDistribution] Allow acurate sizes to be used in orderby \ud83e\ude79 [scrna.ScFGSEA] Reduce envs.cases levels \u2728 [scrna.ScFGSEA] Allow {ident} or {cluster} as placeholder in cases \u2728 [scrna.SeuratClusterStats] Add dimplots \ud83d\ude91 [scrna.SeuratClusterStats] Limit 20 genes by default \ud83d\udc1b [tcr.ImmunarchLoading] Fix multiple \"Source\" columns in data \ud83e\ude79 [tcr.TCRClustering] Make clusterfile as a meta file that can be used by SeuratMetadataMutater \u2728 [tcr.TCRClusteringStats] Add shared clusters by grouping \ud83d\udcdd [tcr.TCRClusteringStats] Don't show shared TCR clusters between groups if not configured \ud83d\udcdd [gsea.FGSEA] Limit pagesize to 10 in report \u2728 [vcf.TruvariBenchSummary] Add process and test \u2728 [vcf.TruvariBenchSummary] Add default input_data \u270f\ufe0f [bed.Bed2Vcf] Fix typos in doc \u2728 [bed.Bed2Vcf] Allow records to be skipped \u2705 [vcf.TruvariBench] Add ref for test 0.4.0 \u2728 [scrna.CellsDistribution] Add process and test \ud83d\uddd1\ufe0f Remove namespaces (use ns instead) 0.3.2 \u2705 Allow tests to run locally only \ud83d\udc9a Add pipen-args for tests \u2705 [plot.Heatmap] Fix test \u2705 [pipeline.scrna_metabolic] Add ARGS in run.env \u2705 [scrna.ScFGSEA] Add test \u2728 [tcr.TCRClusteringStats] Add process \u2705 [tcr.TCRClustering] Use env r for testing \u2705 [tcr.TCRClustering] Add test \u2705 [pipeline.scrna_metabolic] Add test \u2705 [gsea.GSEA] Add tests \u2705 [gsea.FGSEA] Add tests \u2705 [plot.Heatmap] Add tests \u2705 [gene.GeneNameConversion] Add tests \u2705 [utils.gene] Add tests \ud83d\udc9a [bed.Bed2Vcf] Fix test \u2705 [vcf.VcfFix] Add test \u2705 [misc.File2Proc] Use base container for test \u2705 [misc.File2Proc] Fix test \ud83e\ude79 [scrna.ExprImpute] Use if-statement for requirements \u2728 [scrna.SeuratClusterStats] Add process and test 0.3.1 \ud83d\uddd1\ufe0f Deprecate biopipen.namespaces , use biopipen.ns instead \u2728 [bed.Bed2Vcf] Add bed.Bed2Vcf \u2728 [vcf.VcfFix] Add vcf.VcfFix \ud83d\udc1b [vcf.vcfFix] Fix when a flag in INFO \u2728 [vcf.TruvariBench] Add vcf.TruvariBench \u2728 [vcf.TruvariConsistency] Add vcf.TruvariConsistency \ud83d\udc1b [utils.reference] Fix typo in tabix_index \ud83d\udc1b [vcf.VcfIndex] Fix vcf.VcfIndex \u2728 [bed.Bed2Vcf] Allow to ignore non-existing contigs and index the output file \u2728 [misc.Shell] Add misc.Shell to run a shell command 0.3.0 \u267b\ufe0f Refactor some processes for immunopipe \ud83e\ude79 [scrna.SeuratPreparing] Remove tmp datadir for scrna.SeuratPreparing if exsits \ud83e\ude79 [scrna.SeuratPreparing] Add a TODO comment in scrna.SeuratPreparing (#26) \u2728 [scrna.Subset10X] Add scrna.Subset10X \ud83d\udca5 [tcr.Immunarch] Merge tcr.ImmunarchBasic and tcr.ImmunarchAdvanced into tcr.Immunarch \ud83e\ude79 [tcr.VJUsage] Fix R script being generated at current direct for tcr.VJUsage \u2728 [scrna.SeuratMetadataMutater] Add scrna.SeuratMetadataMutater \ud83d\udc1b [tcr.Immunarch] Fix clonotype tracking not selecting top clones by given top N \u267b\ufe0f [pipeline.scrna_metabolic] Refactor scrna_metabolic \ud83d\udcdd [pipeline.scrna_metabolic] Update docs for scrna_metabolic pipeline \u2728 [pipeline.scrna_metabolic] Allow scrna_metabolic pipeline to handle multiple cases \ud83d\ude91 [scrna.ExprImpute] Fix reticulate not using right python \ud83d\ude91 [scrna.SeuratMetadataMutater] Fix error when input mutaters in None \ud83d\ude91 [scrna_metabolic.MetabolicInputs] Fix diot not imported in script 0.2.1 User rtoml over toml 0.2.0 \ud83d\udccc Pin deps for docs Don't link non-existing files for misc.Glob2Dir Upgrade datar to 0.8 \u2b06\ufe0f Upgrade pipen to v0.3 \u26a1\ufe0f Load 10X TCR and RNA-seq data files more robustly for scrna.SeuratPreparing and tcr.ImmunarchLoading 0.1.9 \ud83d\udc1b Load all_config_annotations.csv if filtered_contig_annotations.csv doesn't exist for tcr.ImmunarchLoad \ud83d\udc1b Calculate diversity for all clones only if filtering by clone sizes failed for tcr.ImmunarchAdvanced \ud83d\ude91 Fix seurat object creating when expressions are named \"Gene Expression\" for scrna.SeuratPreparing \u2728 Add tcr.TCRClustering \u2728 Add raw to immdata for tcr.immunarchLoading \u2728 Add on_raw env to tcr.TCRClustering \u2728 Add bam.ControlFREEC 0.1.8 \u2728 Add tcr.Attach2Seurat 0.1.7 \u2795 Add datar dep for scrna_metabolic pipeline \ud83d\ude91 Fix scrna_metabolic.MetabolicPathwayActivity \u2728 Add bcftools.BcftoolsFilter \ud83d\udc7d\ufe0f Don't wrap job report in report_jobs report macro (to adopt pipen-report 0.2) \u2728 Add more options for scrna.DimPlots 0.1.6 \u2728 Convert CNVpytor results to gff and bed \ud83d\ude91 Make scrna_metabolic pipeline work standalone \u2795 Add datar dep for scrna_metabolic pipeline \ud83d\ude91 Fix scrna_metabolic.MetabolicPathwayActivity \u2728 Add bcftools.BcftoolsFilter 0.1.5 \u2728 Add features and fix issues for immunopipe 0.0.4 \u2728 Add some vcf processes 0.1.4 \ud83d\udc1b Fix bam.CNVpytor when snpfile is not provided \u2728 Add metabolic pathway analysis for single-cell RNA-seq data 0.1.3 Add gsea.GSEA and scrna.SCImpute Add gene name conversions Add gsea.FGSEA Add venn plots and refactor ImmunarchFilter Add plot.Heatmap Reuse plot.Heatmap for scrna.GeneExpressionInvestigation Attach metadata to seurat object in scrna.SeuratPreparing Add envs.group_subset for scrna.GeneExpressionInvestigation Fix typo for scrna.GeneExpressionInvestigation Add docs 0.1.2 \u2728 Add envs.qc for scrna.SeuratPreparing 0.1.1 Finish processes for immunopipe 0.1.0 Adopt pipen 0.2+","title":"Change log"},{"location":"CHANGELOG/#change-log","text":"","title":"Change Log"},{"location":"CHANGELOG/#03416","text":"fix(scrna.SeuratClusterStats): update documentation without mentioning table feat(scrna.SeuratPreparing): allow seurat object as input chore: bump pipen to 0.17.21","title":"0.34.16"},{"location":"CHANGELOG/#03415","text":"feat(scrna.MarkersFinder): allow using other metadata columns from object for enrichment plot of all subcases chore: update dependencies","title":"0.34.15"},{"location":"CHANGELOG/#03414","text":"fix(scrna.CellTypeAnnotation): update logging for celltypist command execution fix(scrna.MarkersFinder): fix FindMarkers calling with subsetting when sctransform was used","title":"0.34.14"},{"location":"CHANGELOG/#03413","text":"fix(scrna.SeuratClusterStats): improve error handling in feature plotting when save_code (due to upgrade to ggplot2 v4) feat(MarkersFinder): use scplotter::MarkersPlot (wrapped by biopipen.utils::VizDEGs to visualize markers ci: update CACHE_NUMBER for conda environment to force install latest dependencies","title":"0.34.13"},{"location":"CHANGELOG/#03412","text":"chore: update Dockerfiles to use multi-stage building","title":"0.34.12"},{"location":"CHANGELOG/#03411","text":"fix(scrna_metabolic_landscape): fix report paging issue docs(scrna.MarkersFinder): fix links in docs","title":"0.34.11"},{"location":"CHANGELOG/#03410","text":"ci: correct condition for deleting old test intermediate cache docs(scrna.SeuratPreparing): enhance cell_qc parameter description in SeuratPreparing docs(scrna.ModuleScoreCalculator): update link format in ModuleScoreCalculator docstring","title":"0.34.10"},{"location":"CHANGELOG/#0349","text":"chore(deps): update dependencies fix(scrna.CellCellCommunication): handle numpy product attribute error feat(scrna.ModuleScoreCalculator): add post mutaters functionality to allow compound modules based on added modules docs(scrna.MarkersFinder): correct URL in documentation feat(scrna.CellTypeAnnotation): add support for additional direct cell type annotations feat(scrna.MarkersFinder): enhance enrichment plot descriptions chore(scrna.CellCellCommunicationPlots): set default case to \"Cell-Cell Communication\" feat(scrna.CellCellCommunicationPlots): add table output option for ccc data feat(regulatory.MotifAffinityTest): add variant column support in MotifAffinityTest so only paired variant-motif can be output fix(regulatory.VariantMotifPlot): correct argument in ensure_regulator_motifs function","title":"0.34.9"},{"location":"CHANGELOG/#0348","text":"fix(scrna.MarkersFinder): improve marker processing and enrichment checks when result has no markers/enrichments feat(scrna.PseudoBulkDEG): add ncores and cache parameters feat(scrna.MarkersFinder): enhance heatmap plotting options fix(scrna.ScFGSEA): improve handling of cases with NA gene ranks chore: update package versions","title":"0.34.8"},{"location":"CHANGELOG/#0347","text":"chore(deps): bump pipen-report to 0.23.8 fix(scrna): update naming convention in expand_each function fix(scrna.PseudoBulkDEG): improve error handling chore(deps): bump version to 0.17.14","title":"0.34.7"},{"location":"CHANGELOG/#0346","text":"chore(scrna.CellTypeAnnotation): print the command for celltypist feat(tcr.ScRepLoading): enhance to auto-detect data type chore(tcr.ScRepCombiningExpression): rename TCR_presence to VDJ_presence in metadata of output Seurat object chore(deps): update xqute to version 0.10.4 chore(docker): add procps-ng for free command for pipen-runinfo chore(deps): update pipen version to 0.17.12 chore(deps): bump pipen-poplog to 0.3.4 chore(deps): update simplug to version 0.5.1 chore(deps): bump pipen-args to 0.17.4 to void overriding the default plugin options","title":"0.34.6"},{"location":"CHANGELOG/#0345","text":"fix(scrna.SeuratClusterstats): update string formatting for statistical comparison descriptions ci: revert CACHE_NUMBER to 1 for conda to force install biopipen.utils.R v0.2.9","title":"0.34.5"},{"location":"CHANGELOG/#0344","text":"chore(scrna): update error flag in MarkersFinder and PseudoBulkDEG classes chore(deps): bump pipen to 0.17.11","title":"0.34.4"},{"location":"CHANGELOG/#0343","text":"BREAKING(scrna.MetabolicFeatures): use : instead of , to separate groups for comparisons feat(tcr.ScRepCombiningExpression): add TCR presence indicator to combined expression object fix(scrna.CellTypeAnnotation): fix identification of default ident if rds or qs is given as input for celltypist fix(scrna.CellTypeAnnotation): ensure n_neighbors is set in neighbors params fix(scrna.SeuratClusterStats): correct typo in group_by variable name fix(tcr.ScRepLoading): fix an issue where barcode is not the first column when loaded for 10X data fix(tcr.TCRClustering): fix input collection for clustering fix(tcr.TESSA): refine TCR input preparation fix(scrna.SeuratClusterStats): exclude NA entities in groupings chore(scrna.CellTypeAnnotation): fix dead links for model files for celltypist chore: remove SCP-plot.R chore(ci): comment out cache deletion condition chore(scrna_metabolic_landscape): enlarge font size for list items in introduction of report templates chore(scrna.SeuratClusterStats): adjust plot settings for better visualization chore(tcr.TCRClustering): swap importing BLOSUM62 matrix from newer and older version of biopython for GIANA chore(scrna_metabolic_landscape): adjust plot dimensions chore(scrna.SeuratClusterStats): enhance descriptions of plots chore(deps): update versions for copier, pipen-report, and pipen-verbose docs(scrna): update mutater documentation to include clone selectors ci: update cache number for conda environments to force updates","title":"0.34.3"},{"location":"CHANGELOG/#0342","text":"feat(scrna): add PseudoBulkDEG process for differential gene expression analysis fix(test.Seurat): update pipeline function to set starts for PrepareSeurat chore(scrna.ScFGSEA): rename allpathway_plots to alleach_plots chore(scrna/tcr): update parameter naming for consistency chore(scrna.TopExpressingGenes): use common report template chore(test): update pipeline function to remove report enabling test(scrna): merge tests for Seurat processes to avoid repeatedly load pbmc3k dataset test(scrna): move map2ref tests out of tests/scrna/Seurat for being tested locally only test(scrna): add set.seed to PrepareSeurat script ci(docker): update Dockerfiles to use dynamic REF_NAME argument ci: update build condition and add caching for test intermediates","title":"0.34.2"},{"location":"CHANGELOG/#0341","text":"fix(tcr.ClonalStats): update envs assignment to handle todot parameter chore(deps): bump up pipen to 0.17.8 fix(SeuratClusterStats): fix when features are given as a dict (used in heatmap) feat(MarkersFinder): add enrichment plot across all subsets by each or all ident.1 in group.by feat(ScFGSEA): add support for all pathway plots for all subsets by each chore(TopExpressingGenes): adjust plot height for bar plots docs(SeuratPreparing): update docs for support loading loom files","title":"0.34.1"},{"location":"CHANGELOG/#0340","text":"","title":"0.34.0"},{"location":"CHANGELOG/#new-features","text":"feat(scrna): add ScVelo analysis for RNA velocity and Slingshot for trajectory inference feat(tcr): add ScRepCombiningExpression for combining TCR/BCR and expression data, and ScRepLoading for multiple TCR/BCR data formats feat(scrna): add ScRepLoading with support for multiple formats and improved logging feat(plot): add Plot class and associated R script for data visualization feat(bam): add SamtoolsView for BAM file processing feat(utils): implement Reporter class for generating JSON reports for processes feat(scrna): support qs2 format for input and output in various processes feat(scrna.SeuratPreparing): add mutaters parameter for metadata mutation feat(scrna.MarkersFinder): enhance parameter handling and marker processing feat: add common Svelte report template for job reporting using JSON reports","title":"New Features"},{"location":"CHANGELOG/#enhancements","text":"enh(bam.CNAClinic): change envs.binsize to bp instead of kbp enh(cnv): replace ggplot2 with plotthis for improved plotting in AneuploidyScoreSummary and TMADScoreSummary enh(scrna): adopt biopipen.utils.R v0.1.0 across multiple processes enh(scrna_metabolic_landscape): improve flexibility and stability of metabolic landscape analysis enh(scrna.ExprImputation): improve error handling and threshold handling for cell imputation enh(tcr.ClonalStats): support qs2 format for output","title":"Enhancements"},{"location":"CHANGELOG/#bug-fixes","text":"fix(cnv.TMADScore): correct output filename fix(bam): fix report template and argument handling in CNVpytor, CNAClinic, and ControlFREEC fix(cnv.AneuploidyScore): replace ggplot with plotthis and add error handling for chromosome detection fix(cnv.AneuploidyScoreSummary): rename 'rows' to 'rows_by' for clarity in heatmap function fix(delim.SampleInfo): fix plot functions being registered twice to gglogger and ensure reporter saves to correct directory fix(scrna): replace readRDS/saveRDS with biopipen.utils functions for consistency across processes fix(scrna.SeuratClusterStats): improve data handling and plot saving functionality","title":"Bug Fixes"},{"location":"CHANGELOG/#refactoring","text":"refactor(scrna.CellCellCommunicationPlots): use scplotter::CCCPlot refactor(tcr.CDR3AAPhyschem): adopt input from ScRepCombiningExpression refactor: remove utility R scripts and use biopipen.utils.R package","title":"Refactoring"},{"location":"CHANGELOG/#development-infrastructure","text":"chore(deps): update dependencies and specify versions for bioconductor packages chore(docker): refactor Dockerfiles to streamline base image usage and dependency installation chore: use filter 'r' for R input/output paths instead of 'quote' in scripts test: add comprehensive tests for bam, cnv, and scrna processes (local tests only for some) test: add docker-test job and Dockerfile for test image building ci: update environment cache and reference data cache","title":"Development &amp; Infrastructure"},{"location":"CHANGELOG/#0331","text":"fix(delim.SampleInfo): fix when plot_type ending with \"plot\" feat(scrna.LoomTo10X): add LoomTo10X to convert loom format of scRNA-seq data to 10X format ci: update conditions for build and deploy jobs based on event type","title":"0.33.1"},{"location":"CHANGELOG/#0330","text":"chore(dependencies): update pipen (v0.17) and related package versions in pyproject.toml fix: update all template filters in script to adopt pipen 0.17, which passes in.file etc as a MountedPath ci: add caching for conda environments to improve workflow efficiency test: improve test output grouping for better readability chore: add descriptive summaries for fgsea and enrichr results (#158) chore(snp.PlinkFromVcf): enhance type annotations and set default for keep_allele_order feat(snp.Plink2GTMat): enhance genotype coding options and improve documentation feat(stats.ChowTest): separate groups in output and add pvalues for the coefficient for the subregressions fix(utils/misc.py): enhance error messages in command execution for better debugging fix(web.Download): enhance output filename generation by adding URL decoding and improved slugification fix(cellranger.CellRangerCount): fix inconsistency between in.id and in.fastqs chore(snp.PlinkFilter): remove unnecessary docstring from PlinkFilter script fix(cellranger.CellRangerSummary): use plotthis and biopipen.utils.R for plotting, logging and report content generation test(cellranger): add tests for CellRangerCount and CellRangerSummary with data download setup chore: comment out dev-dependencies section in pyproject.toml feat(scrna.CellCellCommunication): add subset and split_by options for CellCellCommunication and update conversion logic","title":"0.33.0"},{"location":"CHANGELOG/#0323","text":"chore: add descriptive summaries for fgsea and enrichr results","title":"0.32.3"},{"location":"CHANGELOG/#0322","text":"chore: update dependencies to latest versions feat: add PDF output option for SampleInfo plots feat: add PDF output options for violin and scatter plots in Seurat preparation scripts feat: add PDF output options for volcano, dotplot, venn, and upset plots; update filters for type hints feat: add PDF output option for Enrichr plots in TopExpressingGenes script feat: add PDF output options for UMAP plots in SeuratMap2Ref script; update image handling in misc.liq feat: add PDF output options for cluster size distribution, shared clusters, and sample diversity plots; update plotting functions to handle multiple output formats feat: add PDF output options for various Immunarch scripts; enhance reporting with downloadable PDF files feat: add PDF output options for cluster size distribution, dimension plots, and feature plots; enhance reporting with downloadable PDF files feat: add PDF output options for radar and bar plots; enhance reporting with downloadable PDF files feat: add PDF output options for CloneResidency script; enhance reporting with downloadable PDF files feat: add PDF output options for GSEA table and enrichment plots; enhance reporting with downloadable PDF files feat: add PDF output options for pie charts, heatmaps, Venn plots, and UpSet plots; enhance reporting with downloadable PDF files feat: add PDF output options for Enrichr plots; enhance reporting with downloadable PDF files feat: add PDF output options for estimated coefficients and distribution plots; enhance reporting with downloadable PDF files chore: add gcc to cnvkit pipeline docker deps","title":"0.32.2"},{"location":"CHANGELOG/#0321","text":"fix(scrna.ScFGSEA): fix case gmtfile not working fix(TopExpressingGenes): add InlineNotification component to TopExpressingGenes.svelte feat(scrna.SeuratPreparing): add envs.species so that percent.mt , percent.ribo , percent.hb and percent.plat can be correctly calculated for mouse fix(scrna.SeuratClusterStats): fix kind not being added to the figure file name for plots of features","title":"0.32.1"},{"location":"CHANGELOG/#0320","text":"deps: update pipen-runinfo dependency to version 0.8.1 feat(scrna): add CellCellCommunication and CellCellCommunicationPlots fix(scrna.SeuratMap2Ref): fix report and add stats to report fix(utils.single_cell.R): fix categorical data when converting seurat to anndata format refactor(scrna.Seurat2AnnData): abstract seurat_to_anndata() for reuse enh(tcr.TCRClustering): make GIANA compatible with latest BioPython (v1.84) fix(tcr.TCRClstering): fix clusTCR error due to scipy update (v1.14)","title":"0.32.0"},{"location":"CHANGELOG/#0317","text":"deps: bump pipen-args to 0.16 chore: update pyright configuration to include biopipen/**/*.py feat(bam): add BamSubsetByBed process for subsetting bam file by regions in a bed file feat(bed): add BedtoolsMakeWindows process for generating windows from a BED file or genome size file","title":"0.31.7"},{"location":"CHANGELOG/#0316","text":"deps: pin the channels of conda dependencies for tests feat(vcf): adopt truvari v4+ for related processes feat(regulatory): add VariantMotifPlot to plot motif and surrounding sequences with mutations refactor(regulatory.MotifAffinityTest): optimize code base ci: add verbosal output for tests","title":"0.31.6"},{"location":"CHANGELOG/#0315","text":"deps: update pipen to version 0.15.3 and xqute to version 0.5.2 feat(bam): add BamSampling process for sampling a fraction of reads from a bam file feat(protein): add the protein module and Prodigy and ProdigySummary to calculate the binding affinity of a complex structure ci: do not print verbose logs for tests chore(bam.BamMerge): use logger instead of print for logging","title":"0.31.5"},{"location":"CHANGELOG/#0314","text":"deps: bump pipen-report to 0.20.1 (pipen to 0.15.2) fix(plot.VennDiagram): update default devpars and fix issues with computed data fix(scrna.SeuratMap2Ref): fix identifying the normalization method of reference","title":"0.31.4"},{"location":"CHANGELOG/#0313","text":"test: fix test not failing when tests failed test: fix gene name conversion tests due to external API change fix(tcr.CDR3AAPhyschem): fix when chain is not available fix(tcr.TCRClustering): fix when chain is not available","title":"0.31.3"},{"location":"CHANGELOG/#0312","text":"fix(tcr.CDR3AAPhyschem): use sequence from TRB chain only fix(tcr.TCRClustering): fix for multi-chain TCRs, use TRB only if on_multi is false","title":"0.31.2"},{"location":"CHANGELOG/#0311","text":"enh(scrna.SeuratMap2Ref): check if reference has SCTModel if SCTransform'ed (likely prepared by old Seurat)","title":"0.31.1"},{"location":"CHANGELOG/#0310","text":"deps: bump pipen to 0.15.0","title":"0.31.0"},{"location":"CHANGELOG/#0300","text":"","title":"0.30.0"},{"location":"CHANGELOG/#scrnatcr","text":"BREAKING(scrna): move clustree plots from SeuratClustering/SeuratSubClustering to SeuratClusterStats feat(scrna.CellTypeAnnotation): allow to merge/not to merge (envs.merge) the clusters with the same labels predicted feat(scrna.SeuratPreparing): add scDblFinder to detect doublets feat(scrna.SeuratMap2Ref): add envs.skip_if_normalized option to skip normalization if query is already normalized using the same method as the reference refactor(tcr.Immunarch): source the files for Immunarch scripts for better debugging refactor(scnra.SeuratClustering): refactor the script for better debugging refactor(scnra.SeuratPreparing): refactor the script for better debugging fix(scrna): fix resolution expansion for SeuratClustering and SeuratSubClustering fix(cellranger.CellRangerCount): fix falsy envs.create_bam not working for cellranger v7 fix(scrna): Fix generating PrepSCTFindMarkers command when no previous commands present tests(scrna.ScFGSEA): fix unavailable urls to GMT files chore(scrna.SeuratMap2Ref): optimize memory usage chore(scrna.MetaMarkers): remove plugin_opts.poplog_max chore(tcr.CloneResidency): improve logging when handling subjects","title":"scrna/tcr"},{"location":"CHANGELOG/#other","text":"fix(stats.Mediation): fix when NAs in the data feat(plot): add Scatter for scatter plots tests: use single conda env for tests ci: fix CI due to conda env changes docs(web): update docs of envs.tool for Download/DownloadList feat(web): add GCloudStorageDownloadFile and GCloudStorageDownloadBucket to download files from GCP chore(regulatory.MotifAffinityTest): use template filter source_r to source R files tests(regulatory.MotifAffinityTest): rename regulation to regulatory chore: use template filter source_r to source R files fix(stats): handle case when p-value is 0 for MetaPvalue and MetaPvalue1","title":"other"},{"location":"CHANGELOG/#0292","text":"chore(stats.Mediation): make better logging strategy for various number of cases chore(scrna.SeuratClusterStats): use ident label length to adjust default height for feature plots fix(scrna.MetaMarkers): fix seurat object not updated when expanding cases and run PrepSCTFindMarkers when necessary before calling meta-markers fix(scrna.MarkersFinder): fix fetching command when composing the PrepSCTFindMarkers command fix(scrna_metabolic_landscape): handle null values in for loop in MetabolicFeatures and MetabolicFeaturesIntraSubset for report generation","title":"0.29.2"},{"location":"CHANGELOG/#0291","text":"BREAKING: rename namespace regulation to regulatory choir(plot.Manhattan): default envs.title to None (don't add title to the plot by default) enh(plot.Manhattan): give warnings instead of errors about zooming chromosomes not existing fix(plot.Manhattan): fix envs.ylabel not working feat(stats): add Mediation for mediation analysis feat(plot.QQPlot): add support for custom theoratical values tests(plot.QQPlot): add tests chore(snp.MatrixEQTL): allow pvalue cutoffs to be greater than 1 (but 1 will be used anyway) fix(snp.PlinkIBD): add --keep-allele-order to keep the allele order fix(delim.SampleInfo): fix numbers not split up when each is specified. enh(delim.SampleInfo): make sizes of pie charts proportional to number of samples when each is specified enh(scrna.MarkersFinder): run PrepSCTFindMarkers when necessary before calling FindMarkers feat(scrna.SeuratPreparing): add option to cache Seurat object at different steps feat(scrna.SeuratPreparing): allow doubletfinder to run with a different number of cores chore(scrna.SeuratClustering): record PrepSCTFindMarkers command in sobj@commands tests(scrna.SeuratClusterStats): use less stringent p-value cutoff for DEG/MarkersFinder tests(scrna.SeuratPreparing): add doubletfinder in tests","title":"0.29.1"},{"location":"CHANGELOG/#0290","text":"","title":"0.29.0"},{"location":"CHANGELOG/#depedencies","text":"deps: update pipen-filters to version 0.13 deps: add meme to env_bio.yml for tests deps: bump pipen-board to 0.15.2 ci: update GitHub Actions versions and dependencies","title":"Depedencies"},{"location":"CHANGELOG/#breaking","text":"BREAKING: merge namespace bcftools to vcf","title":"BREAKING"},{"location":"CHANGELOG/#new-featuresprocesses","text":"feat(snp): add PlinkFromVcf to convert VCF files to PLINK format feat(snp): add Plink2GTMat for converting PLINK files to genotype matrix feat(snp): add PlinkIBD analysis for identity by descent feat(snp): add PlinkHWE for Hardy-Weinberg Equilibrium test feat(snp): add PlinkHet for calculating sample heterozygosity feat(snp): add PlinkCallRate for calculating call rate of samples and variants feat(snp): add PlinkFilter process for filtering PLINK files feat(snp): add PlinkFreq process for calculating and filtering by allele frequencies feat(snp): add PlinkUpdateName proc to update variant names in PLINK files feat(gene): add GenePromoters for retrieving gene promoter regions and tests for it feat(bed): add BedtoolsIntersect process for finding the intersection of two BED files feat(regulation): add the namespace and MotifScan to use fimo for motif scanning feat(regulation): add MotifAffinityTest to test the affinity of motifs to the sequences and the affinity change due the mutations. feat(cnv.AneuploidyScore): allow BED and VCF files as in.segfile feat(cnv.TMADScore): allow BED and VCF files as in.segfile feat(cnvkit): allow user home directory (~) to be used in envs.ref in mulitple processes feat(plot): add ManhattanPlot to for support for plotting Manhattan plots feat(plot): add QQPlot proc for generating QQ-plot or PP-plot feat(vcf): add BcftoolsView process for viewing, subsetting, and filtering VCF files feat(vcf): add run_bcftools function for running bcftools with given arguments feat(vcf.BcftoolsSort): allow sorting contigs based on a chrom size file feat(vcf.BcftoolsFilter): allow indexing output file feat(vcf.BcftoolsAnnotate): allow providing annotation file as input file and allow indexing output file feat(stats): add MetaPvalue1 to combine pvalues from the same file feat(stats.MetaPvalue): add envs.keep_single flag to keep the single p-values feat(utils.misc.R): add run_command function for R feat(utils.reference): allow tabix_index to index infile directly feat(snp.MatrixEQTL): add envs.match_samples flag to subset snp, expr and cov data with common samples feat(snp.MatrixEQTL): fix cov data being wrongly transposed feat(snp.MatrixEQTL): allow extra columns when snp and gene position file is BED feat(tests): add lazy loading for reference data download and --local flag for downloading more references locally","title":"New Features/Processes"},{"location":"CHANGELOG/#refactorsimprovements","text":"refactor(utils.gene): redesign gene_name_conversion functions for both python and R refactor(gene.GeneNameConversion): use R for implementation refactor(misc.Shell): save envs.cmd to a file and run it to fix the escaping issues of the command enh(snp.MatrixEQTL): use rtracklayer to read the position files","title":"Refactors/Improvements"},{"location":"CHANGELOG/#minor","text":"choir(misc.Str2File): add default for in.name choir(gene.GeneNameConversion): allow envs.notfound to be ignore or skip when envs.output is append or replace choir(utils.misc.py): flush output for command printing in run_command function choir(utils.misc.py): print command with a new line for run_command function choir(snp.PlinkFromVcf): indicate sex is to be handled choir(cellranger_pipeline): remove unused import choir(bam.CNVpytor): implement cnvnator2vcf directly instead of using cnvnator2vcf.pl","title":"Minor"},{"location":"CHANGELOG/#tests","text":"tests(utils.gene): update tests for gene_name_conversion tests(gene.GeneNameConversion): use right environment for tests tests(snp): add tests for plink related processes tests(snp): disable report generation for plink related tests tests(regulation): specify envs.genome for MotifAffinityTest tests: add python package mygene to conda environment biopipen-r tests: add tests for bcftools processes tests: do not download reference data for hg38 at CI tests: update bioconductor-ggmanh dependency to version 1.9.6 tests: add bcftools to conda environment dependencies for tests","title":"Tests"},{"location":"CHANGELOG/#docs","text":"docs(MatrixEQTL): fix choice items of envs.model docs(cellranger_pipeline): fix types of some items in docs, which should be 'list', instead of 'type=list'","title":"Docs"},{"location":"CHANGELOG/#fixes","text":"fix(utils.reference): avoid index file to be created again for the same infile for tabix_index function fix(utils.reference): pass -f to bgzip or gunzip to overwrite the output if exists fix(vcf): fix passing vcffile as a string in fix_vcffile in VcfFix_utils.py fix(cnvkit_pipeline): fix sex in process channels fix(cnv.AneuploidyScoreSummary): fix when Sample column is already in metafile fix(cnv.TMADScoreSummary): fix when Sample column is already in metafile","title":"Fixes"},{"location":"CHANGELOG/#immunopipe-related","text":"fix(tcr.TCRClusterStats): fix envs.shared_clusters.heatmap_meta being broken by envs.shared_clusters.sample_order choir(scrna.SeuratMap2Ref): present better error message when envs.use or values of envs.MapQuery.refdata not in reference fix(scrna.MarkersFinder): run PrepSCTFindMarkers when needed choir(scrna.SeuratClustering): use FindClusters to run for multiple resolutions choir(scrna.SeuratSubClustering): use FindClusters to run for multiple resolutions feat(scrna.SeuratClustering): add clustree plot feat(scrna.SeuratSubClustering): add clustree plot tests(scrna.SeuratClusterStats): add assertion for clustree plot generation","title":"Immunopipe-related"},{"location":"CHANGELOG/#0281","text":"fix(scrna.CellsDistribution): fix devpars and hm_devpars not working","title":"0.28.1"},{"location":"CHANGELOG/#0280","text":"tests(scrna.CellTypeAnnotation): add tests for CellTypeAnnotation using scCATCH feat(cellranger_pipeline): add docker image building for cellranger pipeline chore(cellranger.CellRangerCount): add envs.create_bam to control whether create bams (supporting cellranger v8) chore(cellranger): add in.id to CellRangerCount and CellRangerVdj to specify sample ids chore(cellranger.CellRangerSummary): set the default value of report_paging to 8","title":"0.28.0"},{"location":"CHANGELOG/#0279","text":"feat(tcr.TCRClusterStats): add sample_order to set sample order on heatmap and cluster_rows to switch row clustering on/off","title":"0.27.9"},{"location":"CHANGELOG/#0278","text":"fix(scrna.SeuratClusterStats): fix selected columns not unique for stats feat(scrna.SeuratMap2Ref): allow non-SCTransform'ed reference feat(scrna.SeuratMap2Ref): allow splitting query object for mapping (pwwang/immunopipe#61) deps: update pipen-board to version 0.15.1","title":"0.27.8"},{"location":"CHANGELOG/#0277","text":"fix(utils.gsea): fix gsea table not being printed for runFGSEA fix(core.filters): fix slugified pathway plot file name in report fix(scrna_metabolic_landscape): fix mutaters not working fix(scrna_metabolic_landscape.MetabolicFeatures/MetabolicFeaturesIntraSubset): skip groups with less than 5 cells in do_one_group and save a warning file under the case fix(utils.gsea): do not switch 1st and 2nd columns when 2nd column is numeric for localizeGmtfile chore: fix typo in class name ExprImpution to ExprImputation choir(tests): remove KEGG_metabolism.gmt for prep_reference.py tests(scrna_metabolic_landscape): fix tests","title":"0.27.7"},{"location":"CHANGELOG/#0276","text":"fix(scrna_metabolic_landscape.MetabolicFeatures): fix return value of groups with less than 5 cells in do_one_group choir(utils.gsea): avoid printing NULL for runFGSEA tests: use the return value of pipen.run() to test the success","title":"0.27.6"},{"location":"CHANGELOG/#0275","text":"fix(scrna.Subset10X/SeuratTo10X): correct the paths to the scripts feat(testing): allow to enable report for testing pipelines feat(scrna.SeuratPreparing): add envs.cell_qc_per_sample to filter cells before merging instead after test: add tests to scrna.SeuratTo10X and scrna.SeuratPreparing fix(scrna.SeuratClusterStats): fix color palette for ridge plots","title":"0.27.5"},{"location":"CHANGELOG/#0274","text":"feat: add plot.ROC choir(delim.SampleInfo): add alpha to the colors of the plots using biopipen color pallete feat: add snp.MatrixEQTL docs(tcr/scrna/scrna_metabolic_landscape): update links of images in docs","title":"0.27.4"},{"location":"CHANGELOG/#0273","text":"deps: temporary fix copier breaks with pyyaml-include v2 (copier-org/copier#1568) deps: bump pipen-poplog to 0.1.2 (quick fix for populating logs when job fails) choir(scrna.ScFGSEA): Skip cases when no cells found (pwwang/immunopipe#50) choir(scrna.MarkersFinder): Skip cases when no cells found (pwwang/immunopipe#50) choir(scrna.MetaMarkers): Skip cases when no cells found (pwwang/immunopipe#50) feat(scrna.SeuratPreparing): support DoubletFinder","title":"0.27.3"},{"location":"CHANGELOG/#0272","text":"fix(utils.misc.py): inherit envs when env passed for run_command() fix(scrna.RadarPlots): fix mutaters not working feat(tcr.CloneResidency): support envs.upset_ymax to set the max value of y axis in upset bar plot. feat(tcr.TCRDock): add process choir(utils.misc.py): update level to DEBUG for python logger (leaving the filtering to pipen-poplog) choir(stats.DiffCoexpr): change log_warn to debug for some verbosal logging messages refactor(snp.PlinkSimulation): make the configuration files as input so multiple simulations could run in parallel easily.","title":"0.27.2"},{"location":"CHANGELOG/#0271","text":"BREAKING(scrna.SeuratMap2Ref): rename envs.name to envs.ident so envs.MapQuery.refdata is not required anymore. It will be inferred from envs.ident and envs.use .","title":"0.27.1"},{"location":"CHANGELOG/#0270","text":"deps: bump pipen to 0.14.5 deps: bump datar to 0.15.6 depr(scrna.MarkersFinder): remove use_presto as it's used by Seurat v5 by default enh(tcr.CloneResidency): support log scale for y axis of upset bar plots enh(scrna.SeuratClusterStats): allow to rotate labels in circos plot (pwwang/immunopipe#48) enh(scrna.SeuratClusterStats): use pal_biopipen for ident colors in circos plot fix(scrna.CellsDistribution): fix the row order of the heatmaps fix(scrna.SeuratClusterStats): fix when split-by is specified feat(scrna.CellsDistribution): support prefix_each feat(scrna.MarkersFinder): allow set max number of genes to plot in dotplots feat(scrna.MarkersFinder): support setting detailed arguments for overlapping plots feat(scrna.MarkersFinder): support prefix_group feat(scrna.ScFGSEA): support prefix_each feat(scrna.RadarPlots): support prefix_each and subset choir(scrna.SeuratClusterStats): use logger instead of print for log messages choir(tcr.TCRClustering): print session info for clustcr script choir(scrna.MarkersFinder): flatten toc when no section and no ident-1 specified choir: extract case expansion pattern (scrna.CellsDistribution, scrna.MarkersFinder, scrna.MetaMarkers, scrna.RadarPlots, scrna.ScFGSEA, scrna.TopExpressingGenes) docs: add more detailed docs for envs.section tests: add assertion for success of the pipelines tests: add tests for utils.misc.R tests: add r-presto to env_r.yml tests: simplify tests for r functions tests: pin scipy to 1.8.0 for clustcr in env_r.yml tests: refactor tests for core.filters using unittest tests: refactor tests for utils.common_docstrs using unittest tests: refactor tests for utils.gene using unittest tests: refactor tests for utils.mutate_helpers using unittest tests: refactor tests for utils.single_cell using unittest","title":"0.27.0"},{"location":"CHANGELOG/#0262","text":"deps: bump datar-pandas to 0.5.5 to dismiss deprecated warnings","title":"0.26.2"},{"location":"CHANGELOG/#0261","text":"deps: bump pipen to 0.14.3 deps: pin ggplot2 to 3.4 due to breaking changes of 3.5 for test fix(utils.misc.R): replace latin and greek characters with closest ascii chars for slugify() feat(scrna.TopExpressingGenes): support subset fix(scrna.CellsDistribution): fix the row order of the heatmaps. enh(tcr.CloneResidency): add legend for multiplets in upset plots. feat(scrna.SeuratClusterStats): add circos plot for cell composition stats (pwwang/immunopipe#46).","title":"0.26.1"},{"location":"CHANGELOG/#0260","text":"deps: bump pipen to 0.14.1 deps: bump pipen-report to 0.18.1 fix(scrna.CellsDistribution): fix multiple cells_by columns and speed up plotting choir(tcr.CloneResidency): mark singletons in Venn diagrams more clear fix(scrna.RadarPlots): fix the order of groups on radar plots choir(scrna.RadarPlots): transpose the count/percentage table to save to files fix(scrna.MarkersFinder): fix generating report json file when no significant genes found choir(scrna.MarkersFinder): Plot maximum 20 genes in dotplots choir(scrna.MarkersFinder): Do not convert dashs in case names to dots feat(utils.misc): add logger for python (which allows pipen-poplog to populate logs to running log) feat: add rnaseq.UnitConversion and tests feat: add rnaseq.Simulation to simulate RNAseq data feat: add snp module and snp.PlinkSimulation process feat: add stats module feat: add stats.ChowTest feat: add stats.LiquidAssoc feat: add stats.DiffCoexpr feat: add stats.MetaPvalue","title":"0.26.0"},{"location":"CHANGELOG/#0254","text":"deps: bump datar to 0.15.4 (support pandas 2.2) fix(utils.single_cell.R): fix immdata_from_expanded missing other data columns fix(tcr.Immunarch): fix mutaters not working when no subset is set fix(scrna.CellsDistribution): fix hm_devpars not working","title":"0.25.4"},{"location":"CHANGELOG/#0253","text":"fix(scrna.CellTypeAnnotation): keep factor meta data when input and output are RDS for celltypist","title":"0.25.3"},{"location":"CHANGELOG/#0252","text":"fix(scrna_metabolic_landscape.MetabolicPathwayHeterogeneity): fix output directory path is not slugified chore(tcr.Immunarch): change case filling log to debug level","title":"0.25.2"},{"location":"CHANGELOG/#0251","text":"scrna.CellTypeAnnotation: leave the meta data as is in celltypist wrapper","title":"0.25.1"},{"location":"CHANGELOG/#0250","text":"deps: bump pipen to 0.13.2 feat: add scrna.AnnData2Seurat and scrna.Seurat2AnnData scrna.MarkersFinder: allow to cache FindAllMarkers results scrna.CellTypeAnnotation: support celltypist (#111) scrna.SeuratSubClustering: add envs_depth = 1 to replace whole envs.cases when new case assigned test: add tests for celltypist of CellTypeAnnotation , AnnData2Seurat and Seurat2AnnData","title":"0.25.0"},{"location":"CHANGELOG/#0242","text":"deps: bump pipen-report to 0.17.3 chore: use internal slugify instead of slugify library scrna.SeuratPreparing: fix displaying filters in report scrna.SeuratPreparing: fix logging Seurat procedure arguments cellranger: add CellRangerSummary cell_ranger: use Iframe in report to have loading indicators cellranger_pipeline: add CellRangerCountPipeline and CellRangerVdjPipeline","title":"0.24.2"},{"location":"CHANGELOG/#0241","text":"tcr.Immunarch: update spectratyping output file extension to png","title":"0.24.1"},{"location":"CHANGELOG/#0240","text":"deps: bump up deps by pipen 0.13 deps: add pipen-poplog to populate job logs to running log deps: bump pipen-poplog to 0.0.2 feat: add utils.caching.R cellranger: fix inferring sample name when fastqs from mulitple lanes scrna.SeuratClustering/SeuratSubClustering: cache Seurat procedures step by step scrna.MetaMarkers: limit log messages to be populated to 15 scrna.SeuratPreparing: log procedure arguments at debug level scrna_metabolic_landscape.MetabolicFeaturesIntraSubset: use logger to log so can be poplutated to running log scrna_metabolic_landscape.MetabolicPathwayActivity: use logger to log so can be poplutated to running log tcr.ImmunarchLoading: add logs for steps tcr.TCRClustering: use logger to log so can be poplutated to running log tcr.TESSA: log command at debug level tcr.Immunarch: add plot_type for divs to support boxplots tcr.TCRClustering: fix log_info not found tcr.Immunarch: make poplog_max 999 to pop all job logs to running log scrna_metabolic_landscape.MetabolicFeaturesIntraSubset: change log level for groups from warning to info","title":"0.24.0"},{"location":"CHANGELOG/#0238","text":"scrna.SeuratPreparing: log Seurat procedure arguments scrna.ScFGSEA: add subset to filter cells","title":"0.23.8"},{"location":"CHANGELOG/#0237","text":"scrna.SeuratPreparing: update log message for transformation/scaling step scrna_metabolic_landscape.MetabolicPathwayHeterogeneity: add utils.gsea script source to support localizeGmtfile","title":"0.23.7"},{"location":"CHANGELOG/#0236","text":"feat: support url for gmtfile wherever GSEA is performed (#113) utils.gsea.R: fix file path in gsea.R tcr.Immunarch: add error message for empty filtered/subset data in diversity scrna.SeuratPreparing: correct description of default assay in docstr scrna.SeuratPreparing: run also the normal normalization procedures when SCTransform is used (useful for visualization purposes on RNA assay) scrna.SeuratClustering: add related issue link to PrepSCTFindMarkers scrna.ModuleScoreCalculator: document the names added by cell cycle score (pwwang/immunopipe#34) scrna.SeuratPreparing: support sample names as reference for IntegrateLayers","title":"0.23.6"},{"location":"CHANGELOG/#0235","text":"scrna.SeuratClusterStats: fix when frac or frac_ofall is true and no group-by nor split-by is specified for stats core.filters: fix when no enriched items found for report component enrichr scrna.MarkersFinder: fix when no enriched items found scrna.MetaMarkers: fix when no enriched items found scrna.TopExpressingGenes: fix when no enriched items found utils.gsea.R: fix when no enriched items found for runEnrichr scrna_metabolic_landscript: fix adding report when ncores > 1","title":"0.23.5"},{"location":"CHANGELOG/#0234","text":"scrna.TopExpressingGenes: fix colnames while pulling average expression scrna.CellsDistribution: fix when cells_by has multiple column names scrna.CellTypeAnnotation: fix the order of the clusters for direct method scrna.SeuratClusterStats: add position options for bar plots for stats scrna.RadarPlots: add colors to set the colors of the loops in radar and bar plots tcr.Immunarch: add split_by and split_order to put subplots together in one single plots","title":"0.23.4"},{"location":"CHANGELOG/#0233","text":"tcr.ImmunarchLoading: change mode from single to paired by default","title":"0.23.3"},{"location":"CHANGELOG/#0232","text":"scrna.RadarPlots: fix test error when not enough observations scrna.RadarPlots: add n and mean to test table","title":"0.23.2"},{"location":"CHANGELOG/#0231","text":"scrna.RadarPlots: fix error when generating report for tests when breakdown is not provided","title":"0.23.1"},{"location":"CHANGELOG/#0230","text":"deps: bump pipen to 0.12.5 deps: bump pipen-report to 0.16.3 deps: Update seurat to 5.0.1 in test env file chore: Add /tmp to .gitignore scrna.MarkersFinder: Add envs.use_presto to use presto to speed up finding markers scrna.MarkersFinder: Fix a bug when subsetting cells scrna.MarkersFinder: Set envs.dbs to KEGG_2021_Human and MSigDB_Hallmark_2020 by default scrna.MarkersFinder: Fix FindAllMarkers/FindMarkers for SCTransform'ed data scrna.SeuratPreparing: Fix handling of empty path in RNAData scrna.SeuratPreparing: Set envs.gene_qc.min_cells to 0 by default (instead of 3) scrna.SeuratPreparing: Add sample integration procedures scrna.SeuratPreparing: Allow to filter genes directly scrna.SeuratClustering: Add options to limit string and numeric output length to have more exact caching signature scrna.SeuratClustering: Set default random.seed to 8525 for FindClusters scrna.SeuratClustering: Allow multiple resolutions for FindClusters scrna.SeuratClustering: Print table of idents in log for found clusters scrna.SeuratClustering: Move integration procedues to SeuratPreparing and do only clustering scrna.SeuratClustering: Update tests scrna.SeuratClustering: Make the cluster labels start with \"c1\" instead of \"0\" scrna.SeuratClustering: Default reduction of RunUMAP and FindNeighbors to pca scrna.SeuratClustering: Fix test scrna.SeuratClustering: Print less verbosal log scrna.SeuratClusterStats: Add ngenes to plot the number of genes expressed scrna.SeuratClusterStats: Add barplot for features and allow aggregation of features scrna.SeuratClusterStats: Fix matching kind for plots of features scrna.SeuratClusterStats: Use new umap for plotting feature and dimplots for sub-clustering scrna.SeuratClusterStats: Use default assay for plotting of number of genes expressed scrna.SeuratClusterStats: Add envs.mutaters to mutate meta data scrna.SeuratClusterStats: Add histograms to plot number of cells against another variable scrna.SeuratClusterStats: Fix reduction for subclustering for dimplots scrna.SeuratClusterStats: Subset seurat object for featureplots when ident is subclusters scrna.SeuratClusterStats: Fix argument layer not excluded for heatmaps in features scrna.SeuratClusterStats: Add frac_ofall and transpose for stats to calculate fraction within group or against all cells, and transpose ident and group, respectively scrna.ModuleScoreCalculator: Fix features not being passed to AddModuleScore as a list scrna.ModuleScoreCalculator: Support calculating diffusion map components scrna.SeuratMap2Ref: Rename envs.alias to `envs.name scrna.SeuratMap2Ref: Set default value of envs.MappingScore.ndim to 30 scrna.SeuratMap2Ref: Add envs.ncores for parallelization scrna.SeuratMap2Ref: Remove preset MapQuery arguments scrna.SeuratMap2Ref: Raise an error when envs.MapQuery.refdata is not provided scrna.SeuratMap2Ref: Default envs.use to the key of envs.MapQuery.refdata with single key scrna.SeuratMap2Ref: Use layer instead of slot in docstring (Seurat v5) scrna.SeuratMap2Ref: Make sure the column of cluster labels is a factor scrna.ScFGSEA: Allow to ignore small group when fgsea fails due to all NAs for pre-ranks scrna.ScFGSEA: Use default assay and use layer instead of slot (Seurat v5) scrna.TopExpressingGenes: Use default assay of Seurat object and fix column names of average expression (Seurat v5) scrna.TopExpressingGenes: Change default enrichment gene sets to KEGG_2021_Human and MSigDB_Hallmark_2020 scrna.MetaMarkers: Change default enrichment gene sets to KEGG_2021_Human and MSigDB_Hallmark_2020 scrna.MetaMarkers: Give better message when tests for genes fail scrna.MetaMarkers: Give error message when not enough cells in case scrna.CellsDistribution: Allow to order clusters by envs.cluster_orderby scrna.CellsDistribution: Add heatmaps scrna.SeuratSubClustering: Add process scrna_metabolic_landscape: Add InlineNotification component to imports for report scrna_metabolic_landscape.MetabolicFeatures: Fix when default assay is SCT scrna_metabolic_landscape.MetabolicFeaturesIntraSubset: Fix when default assay is SCT scrna_metabolic_landscape.MetabolicPathwayActivity: Fix when default assay is SCT scrna_metabolic_landscape.MetabolicPathwayActivity: Use default assay of Seurat object scrna_metabolic_landscape.MetabolicPathwayHeterogenetiy: Fix when default assay is SCT scrna.CellTypeAnnotation: Use layer instead of slot of Seurat object (Seurat v5) for sctype tcr.ImmunarchLoading: Allow empty path in TCRData column in input file tcr.ImmunarchLoading: Do not hide envs.mode anymore in docs tcr.CloneResidency: Fix stringifying the subject in case it is a factor tcr.CloneResidency: Make section works in report tcr.Immunarch: Support paired chain data for VJ conjuction plots tcr.TESSA: Change envs.assay to None to use default assay of Seurat object scrna_basic: remove scrna_basic pipeline, use immunopipe instead scrna.GeneExpressionInvestigation: Remove deprecated code scrna.Write10X: Use layer instead of slot (Seurat v5) scrna.ExprImputation: Use default assay of seurat object scrna.SeuratTo10X: Rename Write10X to SeuratTo10X scrna.SeuratSubClustering: Fix original reduction being poluted by subclustering scrna.SeuratClusterStats: Add avgheatmap to plot more elegant heatmap for average gene expressions scrna.SeuratClusterStats: Fix ident not working for dimplots scrna.SeuratClusterStats: Fix for hists when x is a factor/character vector scrna.SeuratClusterStats: Add cluster_orderby to order clusters for features scrna.SeuratClusterStats: Add na_group to keep NA values in group-by scrna.SeuratClusterStats: Allow avgheatmap to plot features other than gene expressions scrna.SeuratClusterStats: Add mutate_helpers.R source file scrna.SeuratClusterStats: Fix data binding for avgheatmap in features utils.mutate_helpers: Change arguments id_col and compare_col to id and compare, respectively utils.mutate_helpers: Fix that subset can't be an expression for expanded family utils.mutate_helpers: Add top to select top entities (e.g clones) scrna.RadarPlots: Add breakdown and test to break down the cell distribution and run statistic test on the fractions","title":"0.23.0"},{"location":"CHANGELOG/#0228","text":"scrna_metabolic_landscape.MetabolicPathwayActivity: Fix useNames = NA being deprecated in matrixStats v1.2 (more locations) scrna_metabolic_landscape.MetabolicPathwayActivity: Fix heatmap column_split scrna_metabolic_landscape.MetabolicFeaturesIntraSubset: Sort groups when being processed utils.gsea: Fix useNames = NA in rowSds for matrixStats v1.2 utils.mutate_helpers: Fix tests","title":"0.22.8"},{"location":"CHANGELOG/#0227","text":"scrna_metabolic_landscape.MetabolicPathwayActivity: Fix useNames = NA being deprecated in matrixStats v1.2","title":"0.22.7"},{"location":"CHANGELOG/#0226","text":"deps: Bump pipen-board to 0.13.10 (pipen-report to 0.16.2)","title":"0.22.6"},{"location":"CHANGELOG/#0225","text":"docs: Bump pipen-board to 0.13.9 (pipen-report to 0.16.1) cellranger.CellRangerCount: Update iframe height in report cellranger.CellRangerVdj: Update iframe height in report","title":"0.22.5"},{"location":"CHANGELOG/#0224","text":"utils.mutate_helpers: Update docs","title":"0.22.4"},{"location":"CHANGELOG/#0223","text":"utils.mutate_helpers: Return ids only when subset is true and group is not NA for uniq = TRUE in expanded, collapsed, emerged and vanished","title":"0.22.3"},{"location":"CHANGELOG/#0222","text":"docs: Update logo and favicon docs: Update logo height in README.md core.filters: Add exclude argument to dict_to_cli_args filter cellranger: Add CellRangerCount and CellRangerVdj scrna.CellTypeAnnotation: Allow using NA to exclude clusters from output Seurat object scrna.SeuratClusterStats: Fix path of expression table file scrna.MarkersFinder: Use FindAllMarkers if ident.1 is not specified scrna.CellsDistribution: Don't add rownames to the output table file utils.mutate_helpers: Add debug and each to expanded, collapsed, vanished and emerged","title":"0.22.2"},{"location":"CHANGELOG/#0221","text":"scrna.CellsDistribution: Export table with distinct columns scrna.SeuratMetadataMutater: Warn about existing columns in seurat object tcr.ImmunarchLoading: Change metacols to extracols so essential columns get exported tcr.Attach2Seurat: Detach prefix from template in code tcr.CDR3AAPhyschem: Detach prefix from template in code tcr.Immunarch: Use immdata$prefix as prefix by default tcr.TCRClustering: Use immdata$prefix as prefix by default tcr.TESSA: Allow in.immdata to be either an RDS file of immunarch object or a text file of cell-level expanded data","title":"0.22.1"},{"location":"CHANGELOG/#0220","text":"Bump pipen-board to 0.13.8 (pipen-report to 0.16) Use render_job filter to generate report utils: Add biopipen palette scrna.SeuratClusterStats: Add subset for dimplots to scrna.CellsDistribution: Add descr for cases in report scrna.CellsDistribution: Save the table only with necessary columns scrna.MarkersFinder: Add dot plot scrna.MetaMarkers: Use logger to log messages scrna.SeuratClustering: Use logger to log messages scrna.SeuratClustering: Add cache option to cache the clustering results if nothing changed except ncores delim.SampleInfo: Fix handling of null exclude_cols","title":"0.22.0"},{"location":"CHANGELOG/#0212","text":"tcr.Immunarch: Add V-J junction circos plots tcr.Immunarch: Refactor logging statements using r-logger","title":"0.21.2"},{"location":"CHANGELOG/#0211","text":"deps: Update pipen-board and pygments versions docs: Adopt mkdocs-rtd 0.0.10 docs: Fix internal reference in API docs delim.SampleInfo: Refactor data subset logic in SampleInfo class","title":"0.21.1"},{"location":"CHANGELOG/#0210","text":"tcr.Immunarch: Fix empty groups in diversity plot after filtering tcr.Immunarch: Add in.metafile to allow other meta info (i.e. seurat clusters) for future subsetting tcr.Immunarch: Change envs.mutaters now on expanded (cell-level) data tcr.Immunarch: Add subset for cases to do analysis on a subset of data tcr.Immunarch: Add separate_by also works on other diversity plots tcr.Immunarch: Add ymin and ymax to align diversity plots by separate_by tcr.Immunarch: Add ncol to specify # columns in the combined plots scrna.RadarPlots: Fix envs.order not working scrna.MarkersFinder: Add overlap to find overlapping markers between cases (pwwang/immunopipe#24) scrna.MarkersFinder: Add subset for each case to subset cells scrna.MarkersFinder: Add dot plots for cases scrna.CellsDistribution: Allow multiple columns for cells_by scrna.CellsDistribution: Add subset for cases to subset cells cnv.AneuploidyScoreSummary: Ignore .call suffix to get sample name by default cnv.AneuploidyScoreSummary: Fix image path in report while envs.group_cols is a string (not an array) utils.single_cell.R: Add functions to expand, filter and restore immunarch objects utils.common_docstrs: Extract some common docstrings for procs utils.misc.R: Use r-logger for logging for R scripts utils.mutate_helpers.R: Add include_emerged for expanded() and include_vanished for collapsed() utils.mutate_helpers.R: Fix tests tests: Add r-logger to test dependencies","title":"0.21.0"},{"location":"CHANGELOG/#0207","text":"(delim.SampleInfo) Add distinct to case to perform stats on distinct records (scrna_basic) Fix docker image building","title":"0.20.7"},{"location":"CHANGELOG/#0206","text":"\u2b06\ufe0f Bump pipen-board to 0.13.4 \u2728 [scrna.MarkersFinder] Allow to set assay for Seurat::FindMarkers() \u2728 [scrna.CellsDistribution] Add venn/upset plot for overlapping cell groups in different cases","title":"0.20.6"},{"location":"CHANGELOG/#0205","text":"\u2b06\ufe0f Bump pipen-board to 0.13.3 \ud83c\udfd7\ufe0f [tcr.CloneResidency] Rename envs.sample_groups to envs.section to be consistent with other processes \ud83c\udfd7\ufe0f [tcr.CloneResidency] Allow additional metadata from input for more flexible case definition (i.e. analysis for specific seurat clusters) \ud83d\udcdd [scrna.ScFGSEA] Remove the link in the summary of the docstring (since they are not transformed in the report) \ud83c\udfa8 [tcr.CDR3AAPhyschem] Give better error message when wrong group items are given","title":"0.20.5"},{"location":"CHANGELOG/#0204","text":"\ud83d\udc1b [scrna.SeuratClusterStats] Fix toc not saved correct (causing report not including the right sections)","title":"0.20.4"},{"location":"CHANGELOG/#0203","text":"\ud83d\udc1b [scrna.SeuratPreparing] Fix when cell_qc is None \ud83c\udfa8 [scrna.MarkersFinder] Add margins to volcano plot \ud83d\udc1b [scrna.SeuratClusterStats] Fix ident in cases of envs.dimplots not working","title":"0.20.3"},{"location":"CHANGELOG/#0202","text":"\ud83d\ude91 [scrna.SeuratPreparing] Fix % in docstring to crash the pipeline","title":"0.20.2"},{"location":"CHANGELOG/#0201","text":"\ud83d\udcdd [scrna_basic/scrna_metabolic_landscape/scrna/tcr] Update docstring \ud83c\udfa8 [scrna.MarkersFinder] Try include more genes in volcano plot (pwwang/immunopipe#17) \ud83c\udfa8 [scrna.CellsDistribution] Give better error message in CellsDistribution if group value not found (pwwang/immunopipe#16) \ud83d\ude9a [tcr.TCRClusterStats] Rename TCRClusteringStats to TCRClusterStats (pwwang/immunopipe#15)","title":"0.20.1"},{"location":"CHANGELOG/#0200","text":"\u2b06\ufe0f Bump pipen to 0.12","title":"0.20.0"},{"location":"CHANGELOG/#0190","text":"\u2b06\ufe0f Bump pipen-report 0.13.1 (pwwang/immunopipe#9, 2) \u2b06\ufe0f Bump pipen-board to 0.12.5 \ud83d\udc84 [docs] Hide unnecessary items in nav bar \ud83d\udc84 [docs] Get docs, especially API docs, formatted better \ud83d\udc1b [delim.SampleInfo] Fix order in pie charts \ud83c\udfa8 [delim.SampleInfo] Add stricter checker for input file (pwwang/immunopipe#13) \ud83c\udfa8 [scrna.SeuratPreparing] Improve QC plots \ud83d\udcdd [scrna.SeuratPreparing] Fix type annotation for envs.features_defaults.ncol in docstring \ud83d\udc1b [scrna.CellsDistribution] Fix the cluster order in pie charts \ud83d\udc1b [scrna.SeuratClusterStats] Fix the cluster order in pie charts \ud83c\udfa8 [scrna.SeuratClusterStats] Indicate the case name in logs when pie is enable for group-by \u2728 [scrna.SeuratClusterStats] Allow mutiple columns in the file for envs.features_defaults.features \u2728 [scrna.SeuratClustering] Add number of clusters at the end of log \ud83e\ude79 [scrna.ModuleScoreCalculator] Set default assay to RNA in case module scores only caculated using integrated features \ud83d\udcdd [tcr.Immunarch] Fix docstring for envs.div.args \ud83c\udfa8 [tcr.CloneResidency] Allow order to be optional \ud83c\udfa8 [tcr.Immunarch] Allow to skip overlap and gene usage analyses by setting method to none (pwwang/immunopipe#11, pwwang/immunopipe#12) \ud83d\udc1b [tcr.TCRClusteringStats] Don't cluster on heatmap when there are only 2 samples \ud83d\udc1b [scrna_metabolic_landscape.MetabolicFeatures] Import Seurat explictly to avoid satijalab/seurat#2853 \ud83d\udc1b [scrna_metabolic_landscape.MetabolicPathwayActivity] Fix when NA values in data for heatmap \ud83d\udc1b [scrna_metabolic_landscape.MetabolicPathwayHeterogeneity] Fix error when no significant pathways selected","title":"0.19.0"},{"location":"CHANGELOG/#0183","text":"\ud83d\udc1b [scrna.MarkersFinder] Fix when either ident is empty","title":"0.18.3"},{"location":"CHANGELOG/#0182","text":"\ud83d\udc1b [tcr.CDR3AAphyschem.R] Fix a bug when the min length of CDR3 seqs > 12","title":"0.18.2"},{"location":"CHANGELOG/#0181","text":"\u2b06\ufe0f Bump datar to 0.15.3 \ud83c\udfa8 [scrna.MetaMarkers/ScFGSEA/SeuratClusterStats] Remove tidyseurat:: prefix for filter \u2728 [tcr.TESSA] Allow the results to be saved to seurat object \ud83d\udcdd [tcr.TESSA] Fix docs about envs.assay","title":"0.18.1"},{"location":"CHANGELOG/#0180","text":"\ud83d\udd27 Update .gitignore \u2b06\ufe0f Bump pipen to 0.11 \u2b06\ufe0f Bump datar to 0.15.2 \ud83d\udea8 Make line length to 88 for linting \u2728 [core.filters] Add skip argument to r() \ud83d\ude91 [tcr.TESSA] Fix type annotation for envs.max_iter \ud83d\udc1b [delim.SampleInfo] Allow unique: prefix for on in stats cases; fix sample order in plots \u267b\ufe0f [scrna.SeuratClusterStats] Redesign envs \u2728 [scrna.MarkersFinder] Add volcano plot \u2728 [tcr.TESSA] Add envs.assay for seurat object input \ud83d\udc1b [tcr.TESSA] Fix when a V-gene/J-gene is missing \u2705 [gsea.FGSEA] Fix tests \ud83d\udeb8 [scrna.SeuratClustering] Add clear message when k.weight is too large for IntegrateData \u23ce","title":"0.18.0"},{"location":"CHANGELOG/#0177","text":"\u2705 [tests] Allow pass FORCE=true to run local-only tests \u2705 [tests] Fix receiving VERBOSE and FORCE in test script \ud83d\ude91 [tcr.ImmunarchLoading] Fix when Sample is the only column in meta \u2728 [tcr.TESSA] Add process and test","title":"0.17.7"},{"location":"CHANGELOG/#0176","text":"\ud83d\udc77 Fix CI for publishing the package \u2b06\ufe0f Bump pipen-board to 0.11.5 \ud83d\ude91 [scrna.SeuratClusterStats] Adjust default width and height for plots \ud83d\ude91 [scrna.CellTypeAnnotation] Keep order of clusters after hitype annotation","title":"0.17.6"},{"location":"CHANGELOG/#0175","text":"\ud83d\udc77 Do not run CI build for publish job \ud83c\udfa8 [tcr.TCRClustering] Add TCR_Cluster_Size1 in addition to TCR_Cluster_Size in out.clusterfile to represent #cells and #CDR3 seqs \u2b06\ufe0f Bump up dependencies","title":"0.17.5"},{"location":"CHANGELOG/#0174","text":"\u2728 [tcr.TCRClustering] Add TCR_Cluster_Size in out.clusterfile \ud83d\udca5 [scrna.SeuratClusterStats] Rename envs.exprs to envs.features","title":"0.17.4"},{"location":"CHANGELOG/#0173","text":"\u2b06\ufe0f Bump pipen-report to 0.12.8 \ud83d\udcdd [delim.SampleInfo] Show h1 in report only when stats specified \ud83d\udcdd [delim.SampleInfo] Fix parsing excluded_cols in report","title":"0.17.3"},{"location":"CHANGELOG/#0172","text":"\ud83d\udcdd [delim.SampleInfo] Add report template","title":"0.17.2"},{"location":"CHANGELOG/#0171","text":"\ud83c\udfa8 [scrna.CellTypeAnnotation] Change seurat_clusters_old to seurat_clusters_id to save old seurat_clusters \ud83d\udca5 [csv] Rename to delim \ud83d\ude9a [csv.BindRows] Rename to delim.RowsBinder \u2728 [utils.mutate_helpers.R] Add paired() to identify paired records \u2728 [delim.SampleInfo] Add process","title":"0.17.1"},{"location":"CHANGELOG/#0170","text":"\u2b06\ufe0f Bump pipen-board to 0.11.4 \ud83d\udcdd [docs] Update logo \ud83d\udcdd [docs] Add css due to mkdocs-rtd change \ud83d\udca5 [core.filters] Default sortkeys to False for filter r \ud83d\udc1b [scrna.ModuleScoreCalculator] Fix aggregation values of programs \ud83d\udc1b [scrna.SeuratClusterStats] Fix typo for default stats \ud83d\udc1b [scrna.ModuleScoreCalculator] Fix name for cell cycle scores \ud83d\udc1b [scrna.CellsDistribution] Fix when cells_by or group_by is not an identifier \ud83d\ude91 [utils.mutate_helpers.R] Allow accessing metadata using . \u2728 [scrna.ModuleScoreCalculator] Add proc","title":"0.17.0"},{"location":"CHANGELOG/#0167","text":"\ud83d\udd25 [scrna.SeuratMetadataMutater] Remove unnecessary in.mutaters \ud83d\udcdd [docs] Use kmdocs-rtd for documentation \ud83d\udcdd [scrna_basic] Fix docs \ud83d\udcdd [docs] Fix CI when files in docs/ changes \ud83d\udcdd [docs] Fix CI when CI config file changes \ud83d\udcdd [scrna_basic] Update docs for processes \ud83d\udd27 [scrna_basic] Update example config file \ud83d\udcdd [docs] Add logo and favicon \ud83d\udcdd [docs] Fix font-sizes in APIs \ud83d\udcdd [docs] Fix logo size in README","title":"0.16.7"},{"location":"CHANGELOG/#0166","text":"\ud83d\ude91 [scrna] Hotfix for docstring when parsed by argparse help","title":"0.16.6"},{"location":"CHANGELOG/#0165","text":"\ud83d\udca5 [scrna.SeuratMetadataMutater] Move mutaters from in to envs \ud83d\udd25 [scrna.CellsDistribution] Remove unnecessary in.casefile \ud83d\ude91 [scrna.CellTypeAnnotation] Hotfix when envs.hitype_db as a file starts with \"hitypedb_\"","title":"0.16.5"},{"location":"CHANGELOG/#0164","text":"\ud83d\ude91 [scrna.CellTypeAnnotation] Hotfix passing envs.newcol \u2b06\ufe0f Bump pipen-report to 0.12.7","title":"0.16.4"},{"location":"CHANGELOG/#0163","text":"\ud83d\udcdd [scrna_metabolic_landscape] Update docstring \u2728 [tcr.CDR3AAPhyschem] Allow envs.subset_cols to be separated by comma \u2728 [scrna.CellTypeAnnotation] Add envs.newcol to keep original idents","title":"0.16.3"},{"location":"CHANGELOG/#0162","text":"\ud83d\udea8 Add .lintr for R lintr \u2b06\ufe0f Bump pipen-board to 0.11.1 \ud83d\udc84 [report] Separate enrichr_report \ud83d\udc84 [scrna.CellsDistribution] Fix reports \ud83d\udc84 [scrna.CellsDistribution] Reorganize report \ud83d\udc84 [scrna.MarkersFinder] Reorganize report \ud83d\udc84 [scrna.ScFGSEA] Reorganize report \ud83d\udc84 [scrna.TopExpressingGenes] Reorganize report \ud83d\udea8 [scrna.TopExpressingGenes] Fix linting issues in script \ud83d\udd27 [scrna.MarkersFinder] Set envs.prefix_each to True by default \ud83d\udd27 [scrna.TopExpressingGenes] Set envs.prefix_each to True by default \u2728 [scrna.MetaMarkers] Add proc\u23ce","title":"0.16.2"},{"location":"CHANGELOG/#0161","text":"\ud83d\udea8 Fix some linting issues \u2b06\ufe0f Bump pipen-board to 0.11 \ud83c\udfa8 [scrna.CellTypeAnnotation] Rename seurat_clusters.old to seurat_clusters_old to save the old clusters for sctype \ud83d\udc1b [scrna.CellTypeAnnotation] Fix saving annotated cell type to text file for sccatch \ud83c\udfa8 [scrna.CellTypeAnnotation] Save old clustering to seurat_clusters_old for sccatch \ud83c\udfa8 [scrna.CellTypeAnnotation] Save old clustering to seurat_clusters_old for direct method \ud83d\udcdd [scrna.CellTypeAnnotation] Fix links in docs for sccatch \u2728 [scrna.SeuratClusterStats] Allow envs.exprs.genes to be genes directly (separated by \",\") \ud83d\udc84 [docs] Update API doc styles for dark mode \u2728 [tcr.TCRClustering] Save the souce code of GIANA with this package \u2728 [tcr.TCRClusteringStats] Allow multiple cases \ud83d\udcdd [tcr.ImmunarchLoading] Update docstring \u2728 [utils] Add mutate_helpers to identify expanded, collapsed, emerged and vanished clones \ud83d\udc1b [utils/misc.R] Fix list_setdefault and list_update when value is NULL \ud83d\udc1b [scrna.TopExpressionGenes] Fix expanding cases \u2728 [scrna.SeuratClustering] Allow envs.FindIntegrationAnchors.reference to be a string separated by comma \u2728 [scrna.ScFGSEA] Allow multiple cases \u2728 [scrna.MarkersFinder] Allow to use mutate_helpers in envs.mutaters \ud83c\udfa8 [scrna.CellsDistribution] Redesign envs to support multiple cases \ud83d\udc84 [tcr.Immunarch] Fix report generation for rarefraction analysis \ud83d\udd27 [tcr.Immunarch] Change envs to be less error prone \ud83d\udc84 [scrna.CellsDistribution] Fix reports \ud83d\udc84 [scrna.ScFGSEA] Fix reports \u2705 [tests] Fix tests","title":"0.16.1"},{"location":"CHANGELOG/#0160","text":"\u2b06\ufe0f Bump pipen-board to 0.10 \ud83d\udc84 [docs] Update docs styles \ud83d\udea8 [core/testing] Remove unused importings \ud83c\udfa8 [scrna] Rename RNADir to RNAData for input data \ud83d\udc1b [gsea.GSEA] Replace doc.string with doc_string to avoid over parsing by pipen-args \ud83c\udfa8 [tcr.Immunarch] Refactor and split into modules \ud83c\udfa8 [scrna.CellTypeAnnotation] Rename CellTypeAnnotate to CellTypeAnnotation and add hitype \ud83c\udfa8 [tcr.ImmunarchLoading] Make it compatible with immunarch 0.9 \ud83c\udfa8 [scrna.MakersFinder] Support multiple cases \ud83c\udfa8 [scrna.TopExpressionGenes] Support multiple cases \ud83d\udc1b [scrna.RadarPlots] Fix section and devpars not passed to script \ud83d\udc1b [scrna.SeuratClustering] Fix PCA on each sample \ud83c\udfa8 [scrna.ExprImpution] Rename from ExprImpute to ExprImputation \ud83d\udc77 [scrna.CellTypeAnnotation] Add r-hitype to env_r.yml for testing \ud83d\udc1b [scrna.CellTypeAnnotation] Fix typos for hitype script \ud83d\udc1b [scrna.CellTypeAnnotation] Fix startsWith in hitype script \ud83c\udfa8 [scrna_basic] Rename ScrnaBasicAnnotate to ScrnaBasicAnnotation \ud83d\udcdd [scrna_basic] Update docs \ud83d\udc1b [cnvkit_pipeline] Fix docker image building \ud83d\udcdd [cnvkit_pipeline] Fix docs","title":"0.16.0"},{"location":"CHANGELOG/#0152","text":"\u2b06\ufe0f Bump pipen-board to 0.9.1 \u2728 [scrna.RadarPlots] Add process \ud83c\udfa8 [tcr.Immunarch] Separate diversity in script into a different file \u2728 [scrna.TopExpressingGenes] Add process \ud83c\udfa8 [scrna.CellsDistribution] Use a different color palette \ud83c\udfa8 [scrna.SeuratClusterStats] Warn about heatmap without downsampling","title":"0.15.2"},{"location":"CHANGELOG/#0151","text":"\u2b06\ufe0f Bump pipen-board to 0.8.0 \u2b06\ufe0f Bump pipen-report to 0.12.5 (to fix the pydantic error) \ud83c\udfa8 [tcr.CloneResidency] Add indicators during running \ud83c\udfa8 [tcr.CloneResidency] Allow multiple cases add mutaters for metadata \ud83d\udc1b [misc.File2Proc] Check if input file exists \ud83c\udfa8 [tcr.Immunarch] Allow cases for trackings and add mutaters for metadata","title":"0.15.1"},{"location":"CHANGELOG/#0150","text":"\u2b06\ufe0f Bump pipen to 0.10.6 \u2b06\ufe0f Bump pipen-board to 0.7.8 \u2796 Retire cmdy at all places (#54) \u2705 [core.filters] Add run.env to test \u2705 [core.filters] Add test for dashify=True \ud83c\udfa8 [scrna.MarkersFinder] Make envs.sigmarkers case wise for scrna.MarkersFinder (#53)","title":"0.15.0"},{"location":"CHANGELOG/#0143","text":"\u2b06\ufe0f Bump pipen to 0.10.5 \ud83d\udd27 [scrna_metabolic_landscape] Make proc group options for process readonly \ud83c\udfa8 [scrna_metabolic_landscape.MetabolicFeatures] Add indicators during computation","title":"0.14.3"},{"location":"CHANGELOG/#0142","text":"\u2b06\ufe0f Bump pipen-board to 0.7.4 \u2b06\ufe0f Bump pipen-report to 0.12.3 \u26a1\ufe0f Replace do.call with do_call in R scripts to improve performance \ud83d\udc1b [scrna.CellTypeAnnotate] Fix when no cell types is given for direct annotation \ud83d\udc1b [cnv.AneuploidyScore] Fix when envs.cn_tranform is a list of thresholds","title":"0.14.2"},{"location":"CHANGELOG/#0141","text":"\u2b06\ufe0f Bump pipen-board to 0.7.3 \u2b06\ufe0f Bump other dependencies \ud83c\udfa8 [scrna] Add type=int for envs.ncores in docstrings \ud83d\ude91 [tcr.CloneResidency] Dismiss warnings from pivot_wider","title":"0.14.1"},{"location":"CHANGELOG/#0140","text":"\u2b06\ufe0f Bump pipen-board to 0.6.3 \ud83d\udd27 Fix make-examples.sh for docker images for pipelines \ud83d\ude91 [scrna_basic] Fix \"Issued certificate has expired\" in making examples for docker \u2728 [tcr.CDR3AAphyschem] Add process \u2728 [cnv.TMADScore] Add TMADScore and TMADScoreSummary \ud83d\ude91 [cnv.TMADScore] Fix wrong envs.seg_transform received in script \ud83d\udcdd [cnv.TMADScoreSummary] Add report template \u2728 [cnv.TMADScoreSummary] Support grouping by 2 groups hierarchically \ud83d\udca5 [cnv.AneuploidyScore] Change envs.include_sex to envs.excl_chroms so exclusion of chroms is more flexible \ud83d\ude91 [cnv.AneuploidyScoreSummary] Adjust with of CAA plot based on number of samples \u2728 [cnv.AneuploidyScoreSummary] Support grouping by 2 groups hierarchically \u2b06\ufe0f Bump pipen-board to 0.6.3","title":"0.14.0"},{"location":"CHANGELOG/#0130","text":"\u2b06\ufe0f Bump pipen-board to 0.5.8 \u267b\ufe0f [scrna_basic] Change detault tag from dev to master for docker image \ud83d\udcdd [scrna_basic] Change detault tag from dev to master in docs \ud83d\udd27 [scrna_basic] Change detault tag from dev to master in entry.sh \ud83d\udd27 [scrna_basic] Fix make-examples.sh when running indenpendently \ud83d\udd27 [scrna_basic] Add plugin_opts.report_no_collapse in board.html \ud83d\udea7 [cnvkit_pipeline] Init docker building \u2699\ufe0f [cnvkit_pipeline] Make examples \u2699\ufe0f [cnvkit_pipeline] Update example.json for pipen-board \ud83d\udd27 [cnvkit_pipeline] Fix example in docker image \ud83d\udcdd [scrna_metabolic_landscape] Update docstrings to adopt pipen-board \ud83d\udcdd [utils.misc] Add docstring for run_command \ud83d\udc1b [cnvkit.CNVkitGuessBaits] Use a better way to determine python of cnvkit.py","title":"0.13.0"},{"location":"CHANGELOG/#0120","text":"\u2b06\ufe0f Bump pipen to 0.10 \u2b06\ufe0f Bump pipen-runinfo to 0.1.1 \u2b06\ufe0f Bump pipen-report to 0.12 and pipen-runinfo to 0.2 \u2b06\ufe0f Bump pipen-args to 0.10.2 \u2b06\ufe0f Bump pipen-board to 0.5.6 \ud83d\udcdd Use flag instead action=store_true in docstring \u2705 [utils.gene] Fix tests \ud83c\udfa8 [scrna.SeuratMap2Ref] Add envs.MappingScore \u2728 [scrna.SeuratMap2Ref] Add report template \ud83d\udc84 [scrna.SeuratMap2Ref] Make figures in 2 columns in report \u2728 [scrna.CellTypeAnnotate] Add ScCATCH for cell type annotation \ud83c\udfa8 [scrna.CellTypeAnnotate] Warn when no cell types are given \ud83d\udc1b [cnvkit] Fix when some arguments are None \ud83d\udcdd [cnvkit_pipeline] Update docstrings to adopt latest pipen-annotate and pipen-board \ud83d\udcdd [cnv] Update docstring \ud83d\ude91 [cnv.AneuploidyScoreSummary] Fix when envs.group_col is None but in.metafile is given \ud83d\udc77 [scrna_basic] Init docker image building action \ud83d\udc77 [scrna_basic] Fix dockhub credentials \ud83d\udcdd [scrna_basic] Update docstrings to adopt latest pipen-annotate and pipen-board \ud83d\udcdd [scrna_basic] Add documentation \ud83d\udd27 [scrna_basic] Update configuration for docker image building","title":"0.12.0"},{"location":"CHANGELOG/#0110","text":"\u2b06\ufe0f Bump pipen to 0.9 \u2b06\ufe0f Drop support for python3.7 \u2795 Add pipen-board as dependency \u2728 Add board.toml for pipen-board to run \ud83d\udc1b [cnvkit.CNVkitCoverage] Fix error when generating flat reference \ud83c\udfa8 [bed.BedConsensus] Use bedtools genomecov to calculate the consensus regions \ud83d\udc1b [core.filters] Keep list of dict in python as list of list in R \u2728 [scrna_metabolic_landscape] Allow multiple subsettings for the data \u2728 [scrna_basic] Initialize the pipeline \ud83d\udc1b [bed.Bed2Vcf] Fix OrderedDiot not found \ud83c\udfa8 [cnvkit_pipeline] Import cached_property directly \ud83d\udc1b [scrna.SeuratPerparing] Fix when input contains a single sample \ud83c\udfa8 [tests] Use --reuse instead of --former \ud83d\udc1b [vcf.VcfSplitSamples] Fix missing mutations for extract samples \ud83c\udfa8 [scrna_metabolic_landscape.MetabolicPathwayHeterogeneity] Add progress indicator \ud83c\udfa8 [scrna.SeuratClustering] Allow sample names to be assigned for reference for FindIntegrationAnchors \ud83c\udfa8 [scrna_metabolic_landscape.MetabolicPathwayActivity] Add merged heatmaps for subsets \ud83d\udc1b [scrna_metabolic_landscape.MetabolicPathwayIntraSubsets] Fix fetching subsetting_comparison and limit nproc for FGSEA to 1 \ud83c\udfa8 [scrna_metabolic_landscape.MetabolicPathwayFeatures] Ignore NAs in subsets \ud83c\udfa8 [scrna_metabolic_landscape] Adopt pipen-args 0.9.7 \u2728 [scrna.SeuratMap2Ref] Add process \u2796 [utils] Retire cmdy \u2728 [bed.BedtoolsMerge] Add process \ud83c\udfa8 [core.testing] Use --cache to control of reusing previous run \ud83c\udfa8 [csv.BindRows] Allow to add filename \ud83d\udccc [scrna_basic] Adopt pipen-board 0.1.2 \ud83d\udc1b [web.Download] Fix when args is Diot \ud83c\udfa8 [cnvkit.CNVkitCall] Detach cmdy \u2728 [bam.BamSplitChroms] Add process \u2728 [bam.Merge] Add process and test \ud83d\udc1b [core] Fix repr filter in templates for Diot objects \ud83d\udc1b [docs] Add mygene dep for building utils.gene \u2705 [vcf.TruvariBench] Pin truvari to v3.4.0 for tests","title":"0.11.0"},{"location":"CHANGELOG/#0100","text":"\u2b06\ufe0f Adopt pipen-report 0.7 for report templates \u26a1\ufe0f Add todot and sortkeys arguments for filter r \ud83d\udc1b Set default lang for processes using bash \u26a1\ufe0f Update docstrings for processes for pipen-cli-config \u26a1\ufe0f [scrna.ExprImpute] Add progress indicators for alra \ud83d\udc1b [scrna.ExprImpute] Set default assay to RNA for rmagic","title":"0.10.0"},{"location":"CHANGELOG/#090","text":"\u2b06\ufe0f Bump up pipen to 0.6","title":"0.9.0"},{"location":"CHANGELOG/#080","text":"\ud83d\ude80 [vcf.VcfAnno] Add VcfAnno to use vcfanno to annotate VCF files \u2728 [tcgamaf.Maf2Vcf] Add Variant_Classification and Variant_Type to output vcf \u2728 [vcf.VcfFix] Allow gziped vcf as input \ud83e\uddf9 Remove tests for core pipeline (not needed any more)","title":"0.8.0"},{"location":"CHANGELOG/#071","text":"\u2b06\ufe0f Upgrade pipen-filters to 0.2 \ud83d\udc7d\ufe0f Adopt pipen-filters 0.2 in reports \ud83d\udd27 Rename scrna_metabolic namespace to scrna_metabolic_landscape in entry points \u2728 [scrna.MarkersFinder] Add each for cases to run on each value of metadata variable each \u2728 [tcgamaf.Maf2Vcf] Add proc \u2728 [bcftools.BcftoolsSort] Add proc","title":"0.7.1"},{"location":"CHANGELOG/#070","text":"\ud83e\uddd1\u200d\ud83d\udcbb [tcr.Immunarch] Allow separating samples for rarefraction analysis \u2728 [scrna.SeuratClusterStats] Add expression matrix to output \ud83e\uddd1\u200d\ud83d\udcbb [tcr.Immunarch] Allow align_x and log scale for rarefraction analysis \u2728 [cnv.AneuploidyScgitoreSummary] Add heatmaps \ud83e\uddd1\u200d\ud83d\udcbb [tcr.Immunarch] Allow separating samples for rarefraction analysis \u2728 [scrna.SeuratClusterStats] Add expression matrix to output \ud83e\uddd1\u200d\ud83d\udcbb [tcr.Immunarch] Allow align_x and log scale for rarefraction analysis \u2728 [cnv.AneuploidyScoreSummary] Add heatmaps \ud83d\udc1b [cnv.Aneuploidy] Fix when only one arm has signals for a chromosome \u2728 [cnvkit.CNVkitGuessBaits] Add proc \u267b\ufe0f [cnvkit_pipeline] Refactor and add docs \ud83c\udfa8 [cnvkit_pipeline] Use process decorator to define processes \u2728 [scrna.SeuratClusterStats] Allow groupby other metadata column than Sample in cell stats \u2728 [scrna.ExprImput] Add ALRA and set as default \ud83c\udfa8 [scrna.scrna_metabolic_landscape] Move from scrna_metabolic and use Seurat object directly instead of sce \ud83d\udc1b [scrna.SeuratClustering] Fix when there are fewer cells \u2728 [scrna.CellTypeAnnotate] Add proc and tests \u2728 [scrna.SeuratClusterStats] Allow subsetting for cell stats \u2705 [vcf.Vcf2Bed] Fix test \u2705 [tests] Add refgenes for testing \ud83d\udc1b [tests] Fix reference preparing \u2705 [tests] Add sctype db for tests \u2705 [tests] Try not patch using lastest poetry \u2705 [tests] Build test deps and fix tests \ud83d\udc77 [tests] Exclude test_scrna_metabolic_landscape from CI \u2b06\ufe0f Upgrade pipen-cli-run to 0.4.1 \u2b06\ufe0f Upgrade pipen to 0.3.11","title":"0.7.0"},{"location":"CHANGELOG/#062","text":"\ud83c\udfa8 [scripts.utils.vcf] Use format keys for samples \u2728 [vcf.VcfFix] Dedent envs.helpers automatically and allow it to be list of strings \ud83e\uddd1\u200d\ud83d\udcbb [tcr.CloneResidency] Add count table and allow grouping samples in the report \ud83e\uddd1\u200d\ud83d\udcbb [cnvkit.CNVkitCall] Allow not passing threshold \ud83e\uddd1\u200d\ud83d\udcbb [cnvkit.CNVkitCall] Allow setting cutoff to fetch significant genes for enrichment analysis \ud83e\uddd1\u200d\ud83d\udcbb [scrna.SeuratPreparing/SeuratClustering] Do QC in SeuratPreparing only and prepare clustering in SeuratClustering \u2728 [cnvkit_pipeline] Allow customization of colnames in metafile \ud83d\udc9a Fix CI (conda-incubator/setup-miniconda#274)","title":"0.6.2"},{"location":"CHANGELOG/#061","text":"\u2728 [cnvkit_pipeline] Allow purity for each sample \u2728 [tcr.ImmunarchSplitIdents] Add proc \u2728 [vcf.VcfSplitSamples] Add proc \ud83c\udfd7\ufe0f [cnvkit.CNVkitCall] Pass purity as input instead of envs \u2728 [vcf.VcfIntersect] Add proc \u2728 [vcf.VcfSampleSplits] Add envs.private to keep only private sites for each sample \ud83d\udd27 Fix setup.py file type \u2705 Fix tests for utils.gene \ud83d\udea8 Ignore template strings in python scripts for pyright","title":"0.6.1"},{"location":"CHANGELOG/#060","text":"\u2728 [cnv] Add AneuploidyScore and AneuploidyScoreSummary \u2728 [scrna.Write10X] Add Write10X \u2728 [cnv.AneuploidyScore] Add envs.include_sex \ud83d\udc1b [scrna.SeuratSubset] Fix when envs.groupby is not given \u2728 [cnvkit.CNVkitHeatmap] Add envs.order for sample order in the heatmap \u2728 [bam.CNAClinic] Add bam.CNAClinic \u2728 [bam.CNAClinic] Add report \u2728 [cnv.AneuploidyScore] Allow a list of thresholds for envs.cn_transform \u2728 [scrna.SeuratSplit] Add scrna.SeuratSplit \u270f\ufe0f [core] Fix typo in core.proc.Pipeline \ud83d\udc7d\ufe0f Refactor pipeline modules with pipen-cli-run 0.3 \ud83d\udc9a Use mamba in CI","title":"0.6.0"},{"location":"CHANGELOG/#053","text":"\u2728 [scrna.SeuratClusterStats] Allow features to be a file for expression plots \u2728 [tcr.CloneSizeQQPlot] Add process \ud83e\ude79 [tcr.Immunarch] Fix bad characters in the \u201cMotif Analysis\u201d section in report (#43)","title":"0.5.3"},{"location":"CHANGELOG/#052","text":"\u2b06\ufe0f Pump pipen-args to 0.3 \ud83e\ude79 [scrna.CellsDistribution] Filter NA cells.by","title":"0.5.2"},{"location":"CHANGELOG/#051","text":"\ud83d\udc9a Fix CI \ud83d\udea8 Add and fix linting \u2b06\ufe0f Pump pipen-report to 0.4.5","title":"0.5.1"},{"location":"CHANGELOG/#050","text":"\u2705 [vcf.VcfFix] Add chrom size fixes \u2728 [utils.reference] Add bam_index \ud83d\udc1b [bam.CNVpytor] Fix vcf-fix only adds last contig and fix header with snp data \u2728 [vcf.Vcf2Bed] Add process and test \ud83d\udc1b [bed.BedConsensus] Fix final weighting issue \ud83e\ude79 [All] Use %>% instead of |> in all R scripts for backward compatibility \ud83d\udc1b [scrna_metabolic] Don't turn \"Ident\" to \"seurat_clusters\" for grouping.groupby in config \ud83c\udfd7\ufe0f [tests] Add prefix \"biopipen-\" to conda environment names \u2705 [tests] Enable pipen-report only when necessary","title":"0.5.0"},{"location":"CHANGELOG/#049","text":"\ud83d\udc77 [test] Reverse immunarch in env_r \u2728 [bam.CNVpytor] Add filters \u2728 [cnvkit/cnvkit_pipeline] Add processes and pipeline \ud83d\udc1b [bam.cnvkit] Fix filter direction \ud83d\ude91 [scrna_metabolic] Fix nproc for runFGSEA for MetabolicPathwayHeterogeneity","title":"0.4.9"},{"location":"CHANGELOG/#048","text":"\ud83e\ude79 [core] Add default for config.exe.bedtools \ud83e\ude79 [scrna.ScFGSEA] Don't convert sparse matrix to avoid \"problem too large\" error","title":"0.4.8"},{"location":"CHANGELOG/#047","text":"\ud83d\udc1b [scrna.SeuratPreparing] Fix new data preparing when errored","title":"0.4.7"},{"location":"CHANGELOG/#046","text":"\u2728 [vcf.TruvariBench] Allow multimatch to be passed \u2728 [vcf.TruvariConsistency] Add report","title":"0.4.6"},{"location":"CHANGELOG/#045","text":"\u2728 [bam.CNVpytor] Generate and fix VCF file as result \ud83d\udcdd [vcf.TruvariBench] Update docs to show other arguments for truvari bench \u2728 [vcf.TruvariBench] Allow sizemax to be passed \u2728 [bed.BedConsensus] Add process and tests \u2728 [core] Add ref.genome to configurations \u26a1\ufe0f [bed.BedConsensus] Parallelize and speed up \ud83d\udc9a [test] Add bedtools to env bio \ud83d\udc9a [test] Add chromsome sizes to reference \ud83d\udc9a [test] Add r-gsea_r to env r \ud83d\udc9a [scrna.ScFGSEA] Fix tests\u23ce","title":"0.4.5"},{"location":"CHANGELOG/#044","text":"\ud83d\udc1b [scrna.SeuratPreparing] Fix after tidyseurat being used \ud83d\udc1b [scrna.SeuratPreparing] Fix object Sample not found \ud83d\udcdd [Housekeeping] Fix API docs \ud83d\udcdd [Housekeeping] Make apis show neater docs","title":"0.4.4"},{"location":"CHANGELOG/#043","text":"\u2728 [scrna] Add filter for cases in CellsDistribution, MarkersFinder and ScFGSEA \u2728 [utils] Allow gg object for ggs in plot.R \ud83d\udc1b [scrna_metabolic] Fix reports \ud83d\udc1b [scrna_metabolic] Fix multiple cases \ud83d\udc1b [scrna_metabolic] Fix rmagic for normalization \u26a1\ufe0f [scrna.SeuratClusterStats] Add common gene list \u26a1\ufe0f [scrna.MarkersFinder] Add filter2 to filter after mutaters \ud83d\udc1b [tcr.Immunarch] Fix missing library tibble in script \u26a1\ufe0f [scrna.ScFGSEA] Make ident hierarchical","title":"0.4.3"},{"location":"CHANGELOG/#042","text":"\ud83d\udc9a [Housekeeping] Fix CI deploy \u26a1\ufe0f [processes] Use faster do_call() instead of do.call() \ud83d\udcdd [tcr] Fix some docstrings with {{ and }} \u2705 [vcf.TruvariBench] Add ref for test \ud83e\ude79 [tcr.TCRClustering] FIx VGeneScores.txt being generated in current dir \ud83d\udcdd [scrna.SeuratPreparing] Update docstring and refactor script \u2728 [scrna.SeuratClustering] Allow dims to be expanded in arguments \ud83d\udcdd [scrna.MarkersFinder] Adopt reduced case configuration level","title":"0.4.2"},{"location":"CHANGELOG/#041","text":"","title":"0.4.1"},{"location":"CHANGELOG/#general","text":"\ud83d\udc77 [Housekeeping] Add deploy in CI \ud83d\ude9a [Housekeeping] Move tests/test_tcr/TCRClustering to tests/test_tcr/TCRClusteringStats \ud83d\udd27 [Tests] Add r-tidyseurat to env_r.toml","title":"General"},{"location":"CHANGELOG/#processes","text":"\ud83e\ude79 [scrna.CellsDistribution] Reduce envs.cases levels \ud83e\ude79 [scrna.CellsDistribution] Allow acurate sizes to be used in orderby \ud83e\ude79 [scrna.ScFGSEA] Reduce envs.cases levels \u2728 [scrna.ScFGSEA] Allow {ident} or {cluster} as placeholder in cases \u2728 [scrna.SeuratClusterStats] Add dimplots \ud83d\ude91 [scrna.SeuratClusterStats] Limit 20 genes by default \ud83d\udc1b [tcr.ImmunarchLoading] Fix multiple \"Source\" columns in data \ud83e\ude79 [tcr.TCRClustering] Make clusterfile as a meta file that can be used by SeuratMetadataMutater \u2728 [tcr.TCRClusteringStats] Add shared clusters by grouping \ud83d\udcdd [tcr.TCRClusteringStats] Don't show shared TCR clusters between groups if not configured \ud83d\udcdd [gsea.FGSEA] Limit pagesize to 10 in report \u2728 [vcf.TruvariBenchSummary] Add process and test \u2728 [vcf.TruvariBenchSummary] Add default input_data \u270f\ufe0f [bed.Bed2Vcf] Fix typos in doc \u2728 [bed.Bed2Vcf] Allow records to be skipped \u2705 [vcf.TruvariBench] Add ref for test","title":"Processes"},{"location":"CHANGELOG/#040","text":"\u2728 [scrna.CellsDistribution] Add process and test \ud83d\uddd1\ufe0f Remove namespaces (use ns instead)","title":"0.4.0"},{"location":"CHANGELOG/#032","text":"\u2705 Allow tests to run locally only \ud83d\udc9a Add pipen-args for tests \u2705 [plot.Heatmap] Fix test \u2705 [pipeline.scrna_metabolic] Add ARGS in run.env \u2705 [scrna.ScFGSEA] Add test \u2728 [tcr.TCRClusteringStats] Add process \u2705 [tcr.TCRClustering] Use env r for testing \u2705 [tcr.TCRClustering] Add test \u2705 [pipeline.scrna_metabolic] Add test \u2705 [gsea.GSEA] Add tests \u2705 [gsea.FGSEA] Add tests \u2705 [plot.Heatmap] Add tests \u2705 [gene.GeneNameConversion] Add tests \u2705 [utils.gene] Add tests \ud83d\udc9a [bed.Bed2Vcf] Fix test \u2705 [vcf.VcfFix] Add test \u2705 [misc.File2Proc] Use base container for test \u2705 [misc.File2Proc] Fix test \ud83e\ude79 [scrna.ExprImpute] Use if-statement for requirements \u2728 [scrna.SeuratClusterStats] Add process and test","title":"0.3.2"},{"location":"CHANGELOG/#031","text":"\ud83d\uddd1\ufe0f Deprecate biopipen.namespaces , use biopipen.ns instead \u2728 [bed.Bed2Vcf] Add bed.Bed2Vcf \u2728 [vcf.VcfFix] Add vcf.VcfFix \ud83d\udc1b [vcf.vcfFix] Fix when a flag in INFO \u2728 [vcf.TruvariBench] Add vcf.TruvariBench \u2728 [vcf.TruvariConsistency] Add vcf.TruvariConsistency \ud83d\udc1b [utils.reference] Fix typo in tabix_index \ud83d\udc1b [vcf.VcfIndex] Fix vcf.VcfIndex \u2728 [bed.Bed2Vcf] Allow to ignore non-existing contigs and index the output file \u2728 [misc.Shell] Add misc.Shell to run a shell command","title":"0.3.1"},{"location":"CHANGELOG/#030","text":"\u267b\ufe0f Refactor some processes for immunopipe \ud83e\ude79 [scrna.SeuratPreparing] Remove tmp datadir for scrna.SeuratPreparing if exsits \ud83e\ude79 [scrna.SeuratPreparing] Add a TODO comment in scrna.SeuratPreparing (#26) \u2728 [scrna.Subset10X] Add scrna.Subset10X \ud83d\udca5 [tcr.Immunarch] Merge tcr.ImmunarchBasic and tcr.ImmunarchAdvanced into tcr.Immunarch \ud83e\ude79 [tcr.VJUsage] Fix R script being generated at current direct for tcr.VJUsage \u2728 [scrna.SeuratMetadataMutater] Add scrna.SeuratMetadataMutater \ud83d\udc1b [tcr.Immunarch] Fix clonotype tracking not selecting top clones by given top N \u267b\ufe0f [pipeline.scrna_metabolic] Refactor scrna_metabolic \ud83d\udcdd [pipeline.scrna_metabolic] Update docs for scrna_metabolic pipeline \u2728 [pipeline.scrna_metabolic] Allow scrna_metabolic pipeline to handle multiple cases \ud83d\ude91 [scrna.ExprImpute] Fix reticulate not using right python \ud83d\ude91 [scrna.SeuratMetadataMutater] Fix error when input mutaters in None \ud83d\ude91 [scrna_metabolic.MetabolicInputs] Fix diot not imported in script","title":"0.3.0"},{"location":"CHANGELOG/#021","text":"User rtoml over toml","title":"0.2.1"},{"location":"CHANGELOG/#020","text":"\ud83d\udccc Pin deps for docs Don't link non-existing files for misc.Glob2Dir Upgrade datar to 0.8 \u2b06\ufe0f Upgrade pipen to v0.3 \u26a1\ufe0f Load 10X TCR and RNA-seq data files more robustly for scrna.SeuratPreparing and tcr.ImmunarchLoading","title":"0.2.0"},{"location":"CHANGELOG/#019","text":"\ud83d\udc1b Load all_config_annotations.csv if filtered_contig_annotations.csv doesn't exist for tcr.ImmunarchLoad \ud83d\udc1b Calculate diversity for all clones only if filtering by clone sizes failed for tcr.ImmunarchAdvanced \ud83d\ude91 Fix seurat object creating when expressions are named \"Gene Expression\" for scrna.SeuratPreparing \u2728 Add tcr.TCRClustering \u2728 Add raw to immdata for tcr.immunarchLoading \u2728 Add on_raw env to tcr.TCRClustering \u2728 Add bam.ControlFREEC","title":"0.1.9"},{"location":"CHANGELOG/#018","text":"\u2728 Add tcr.Attach2Seurat","title":"0.1.8"},{"location":"CHANGELOG/#017","text":"\u2795 Add datar dep for scrna_metabolic pipeline \ud83d\ude91 Fix scrna_metabolic.MetabolicPathwayActivity \u2728 Add bcftools.BcftoolsFilter \ud83d\udc7d\ufe0f Don't wrap job report in report_jobs report macro (to adopt pipen-report 0.2) \u2728 Add more options for scrna.DimPlots","title":"0.1.7"},{"location":"CHANGELOG/#016","text":"\u2728 Convert CNVpytor results to gff and bed \ud83d\ude91 Make scrna_metabolic pipeline work standalone \u2795 Add datar dep for scrna_metabolic pipeline \ud83d\ude91 Fix scrna_metabolic.MetabolicPathwayActivity \u2728 Add bcftools.BcftoolsFilter","title":"0.1.6"},{"location":"CHANGELOG/#015","text":"\u2728 Add features and fix issues for immunopipe 0.0.4 \u2728 Add some vcf processes","title":"0.1.5"},{"location":"CHANGELOG/#014","text":"\ud83d\udc1b Fix bam.CNVpytor when snpfile is not provided \u2728 Add metabolic pathway analysis for single-cell RNA-seq data","title":"0.1.4"},{"location":"CHANGELOG/#013","text":"Add gsea.GSEA and scrna.SCImpute Add gene name conversions Add gsea.FGSEA Add venn plots and refactor ImmunarchFilter Add plot.Heatmap Reuse plot.Heatmap for scrna.GeneExpressionInvestigation Attach metadata to seurat object in scrna.SeuratPreparing Add envs.group_subset for scrna.GeneExpressionInvestigation Fix typo for scrna.GeneExpressionInvestigation Add docs","title":"0.1.3"},{"location":"CHANGELOG/#012","text":"\u2728 Add envs.qc for scrna.SeuratPreparing","title":"0.1.2"},{"location":"CHANGELOG/#011","text":"Finish processes for immunopipe","title":"0.1.1"},{"location":"CHANGELOG/#010","text":"Adopt pipen 0.2+","title":"0.1.0"},{"location":"api/biopipen.core.config/","text":"module biopipen.core . config </> Provides the envs from configuration files Classes ConfigItems \u2014 Provides the envs from configuration files and defaults thenon-existing values to None. </> class biopipen.core.config . ConfigItems ( *args , **kwargs ) </> Bases diot.diot.Diot dict Provides the envs from configuration files and defaults thenon-existing values to None. Parameters *args \u2014 Anything that can be sent to dict construct **kwargs \u2014 keyword argument that can be sent to dict constructSome diot configurations can also be passed, including: diot_nest: Types to nestly convert values diot_transform: The transforms for keys diot_frozen: Whether to generate a frozen diot. True: freeze the object recursively if there are Diot objects in descendants False: Don'f freeze 'shallow': Only freeze at depth = 1 diot_missing: How to deal with missing keys when accessing them - An exception class or object to raise - A custom function with first argument the key and second the diot object. - Other values will be used as the default value directly Methods __contains__ ( name ) (bool) \u2014 True if the dictionary has the specified key, else False. </> __delitem__ ( name ) \u2014 Delete self[key]. </> __getitem__ ( name ) (Any) \u2014 x. getitem (y) <==> x[y] </> __ior__ ( other ) (Diot) \u2014 Return self|=value. </> __setitem__ ( name , value ) \u2014 Set self[key] to value. </> accessible_keys ( ) (Iterable) \u2014 Get the converted keys </> clear ( ) \u2014 Clear the object </> copy ( ) (Diot) \u2014 Shallow copy the object </> freeze ( frozen ) \u2014 Freeze the diot object </> from_namespace ( namespace , recursive , diot_nest , diot_transform , diot_frozen , diot_missing ) (Diot) \u2014 Get a Diot object from an argparse namespace </> get ( name , value ) (Any) \u2014 Get the value of a key name </> pop ( name , *value ) (Any) \u2014 Pop a key from the object and return the value. If key does notexist, return the given default value </> popitem ( ) (Tuple) \u2014 Pop last item from the object </> setdefault ( name , value ) (Any) \u2014 Set a default value to a key </> thaw ( recursive ) \u2014 A context manager for temporarily change the diot </> to_dict ( ) (Dict) \u2014 Turn the Box and sub Boxes back into a nativepython dictionary. </> to_json ( filename , encoding , errors , **json_kwargs ) (Optional) \u2014 Convert to a json string or save it to json file </> to_toml ( filename , encoding , errors ) (Optional) \u2014 Convert to a toml string or save it to toml file </> to_yaml ( filename , default_flow_style , encoding , errors , **yaml_kwargs ) (Optional) \u2014 Convert to a yaml string or save it to yaml file </> unfreeze ( recursive ) \u2014 Unfreeze the diot object </> update ( *value , **kwargs ) \u2014 Update the object. Shortcut: |= </> update_recursively ( *value , **kwargs ) \u2014 Update the object. Shortcut: |= </> classmethod from_namespace ( namespace , recursive=True , diot_nest=True , diot_transform='safe' , diot_frozen=False , diot_missing=<diot.diot._DiotMissingDefault object at 0x7f84c8bd99c0> ) </> Get a Diot object from an argparse namespace Example >>> from argparse import Namespace >>> Diot . from_namespace ( Namespace ( a = 1 , b = 2 )) Parameters namespace (Namespace) \u2014 The namespace object recursive (bool, optional) \u2014 Do it recursively? diot_nest (Union, optional) \u2014 Types to nestly convert values diot_transform (Union, optional) \u2014 The transforms for keys diot_frozen (Union, optional) \u2014 Whether to generate a frozen diot. - True: freeze the object recursively if there are Diot objects in descendants - False: Don'f freeze - shallow : Only freeze at depth = 1 diot_missing (Any, optional) \u2014 How to deal with missing keys when accessing them - An exception class or object to raise - A custom function with first argument the key and second the diot object. - Other values will be used as the default value directly Returns (Diot) The converted diot object. method __setitem__ ( name , value ) </> Set self[key] to value. method pop ( name , *value ) </> Pop a key from the object and return the value. If key does notexist, return the given default value Parameters name (str) \u2014 The key Returns (Any) The value corresponding to the name or the default value Raises DiotFrozenError \u2014 when try to pop from a frozen diot method popitem ( ) </> Pop last item from the object Returns (Tuple) A tuple of key and value Raises DiotFrozenError \u2014 when try to pop from a frozen diot method update ( *value , **kwargs ) </> Update the object. Shortcut: |= Raises DiotFrozenError \u2014 when try to update a frozen diot method update_recursively ( *value , **kwargs ) </> Update the object. Shortcut: |= Raises DiotFrozenError \u2014 when try to update a frozen diot method __ior__ ( other ) \u2192 Diot </> Return self|=value. method __delitem__ ( name ) </> Delete self[key]. method freeze ( frozen='shallow' ) </> Freeze the diot object Parameters frozen (Union, optional) \u2014 The frozen argument indicating how to freeze:shallow: only freeze at depth=1 True: freeze recursively if there are diot objects in children False: Disable freezing method unfreeze ( recursive=False ) </> Unfreeze the diot object Parameters recursive (bool, optional) \u2014 Whether unfreeze all diot objects recursively generator thaw ( recursive=False ) </> A context manager for temporarily change the diot Parameters recursive (bool, optional) \u2014 Whether unfreeze all diot objects recursively Yields self, the reference to this diot. method setdefault ( name , value ) </> Set a default value to a key Parameters name (str) \u2014 The key name value (Any) \u2014 The default value Returns (Any) The existing value or the value passed in Raises DiotFrozenError \u2014 when try to set default to a frozen diot method accessible_keys ( ) </> Get the converted keys Returns (Iterable) The accessible (transformed) keys method get ( name , value=None ) </> Get the value of a key name Parameters name (str) \u2014 The key name value (Any, optional) \u2014 The value to return if the key does not exist Returns (Any) The corresponding value or the value passed in if the key doesnot exist method __contains__ ( name ) \u2192 bool </> True if the dictionary has the specified key, else False. method clear ( ) </> Clear the object method copy ( ) </> Shallow copy the object Returns (Diot) The copied object method to_dict ( ) </> Turn the Box and sub Boxes back into a nativepython dictionary. Returns (Dict) The converted python dictionary method to_json ( filename=None , encoding='utf-8' , errors='strict' , **json_kwargs ) </> Convert to a json string or save it to json file Parameters filename (Union, optional) \u2014 The filename to save the json to, if not given a jsonstring will be returned encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function **json_kwargs \u2014 Other kwargs for json.dumps Returns (Optional) The json string with filename is not given method to_yaml ( filename=None , default_flow_style=False , encoding='utf-8' , errors='strict' , **yaml_kwargs ) </> Convert to a yaml string or save it to yaml file Parameters filename (Union, optional) \u2014 The filename to save the yaml to, if not given a yamlstring will be returned default_flow_style (bool, optional) \u2014 The default flow style for yaml dumpingSee yaml.dump encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function **yaml_kwargs \u2014 Other kwargs for yaml.dump Returns (Optional) The yaml string with filename is not given method to_toml ( filename=None , encoding='utf-8' , errors='strict' ) </> Convert to a toml string or save it to toml file Parameters filename (Union, optional) \u2014 The filename to save the toml to, if not given a tomlstring will be returned encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function Returns (Optional) The toml string with filename is not given method __getitem__ ( name ) \u2192 Any </> x. getitem (y) <==> x[y]","title":"biopipen.core.config"},{"location":"api/biopipen.core.config/#biopipencoreconfig","text":"</> Provides the envs from configuration files Classes ConfigItems \u2014 Provides the envs from configuration files and defaults thenon-existing values to None. </> class","title":"biopipen.core.config"},{"location":"api/biopipen.core.config/#biopipencoreconfigconfigitems","text":"</> Bases diot.diot.Diot dict Provides the envs from configuration files and defaults thenon-existing values to None. Parameters *args \u2014 Anything that can be sent to dict construct **kwargs \u2014 keyword argument that can be sent to dict constructSome diot configurations can also be passed, including: diot_nest: Types to nestly convert values diot_transform: The transforms for keys diot_frozen: Whether to generate a frozen diot. True: freeze the object recursively if there are Diot objects in descendants False: Don'f freeze 'shallow': Only freeze at depth = 1 diot_missing: How to deal with missing keys when accessing them - An exception class or object to raise - A custom function with first argument the key and second the diot object. - Other values will be used as the default value directly Methods __contains__ ( name ) (bool) \u2014 True if the dictionary has the specified key, else False. </> __delitem__ ( name ) \u2014 Delete self[key]. </> __getitem__ ( name ) (Any) \u2014 x. getitem (y) <==> x[y] </> __ior__ ( other ) (Diot) \u2014 Return self|=value. </> __setitem__ ( name , value ) \u2014 Set self[key] to value. </> accessible_keys ( ) (Iterable) \u2014 Get the converted keys </> clear ( ) \u2014 Clear the object </> copy ( ) (Diot) \u2014 Shallow copy the object </> freeze ( frozen ) \u2014 Freeze the diot object </> from_namespace ( namespace , recursive , diot_nest , diot_transform , diot_frozen , diot_missing ) (Diot) \u2014 Get a Diot object from an argparse namespace </> get ( name , value ) (Any) \u2014 Get the value of a key name </> pop ( name , *value ) (Any) \u2014 Pop a key from the object and return the value. If key does notexist, return the given default value </> popitem ( ) (Tuple) \u2014 Pop last item from the object </> setdefault ( name , value ) (Any) \u2014 Set a default value to a key </> thaw ( recursive ) \u2014 A context manager for temporarily change the diot </> to_dict ( ) (Dict) \u2014 Turn the Box and sub Boxes back into a nativepython dictionary. </> to_json ( filename , encoding , errors , **json_kwargs ) (Optional) \u2014 Convert to a json string or save it to json file </> to_toml ( filename , encoding , errors ) (Optional) \u2014 Convert to a toml string or save it to toml file </> to_yaml ( filename , default_flow_style , encoding , errors , **yaml_kwargs ) (Optional) \u2014 Convert to a yaml string or save it to yaml file </> unfreeze ( recursive ) \u2014 Unfreeze the diot object </> update ( *value , **kwargs ) \u2014 Update the object. Shortcut: |= </> update_recursively ( *value , **kwargs ) \u2014 Update the object. Shortcut: |= </> classmethod","title":"biopipen.core.config.ConfigItems"},{"location":"api/biopipen.core.config/#diotdiotdiotfrom_namespace","text":"</> Get a Diot object from an argparse namespace Example >>> from argparse import Namespace >>> Diot . from_namespace ( Namespace ( a = 1 , b = 2 )) Parameters namespace (Namespace) \u2014 The namespace object recursive (bool, optional) \u2014 Do it recursively? diot_nest (Union, optional) \u2014 Types to nestly convert values diot_transform (Union, optional) \u2014 The transforms for keys diot_frozen (Union, optional) \u2014 Whether to generate a frozen diot. - True: freeze the object recursively if there are Diot objects in descendants - False: Don'f freeze - shallow : Only freeze at depth = 1 diot_missing (Any, optional) \u2014 How to deal with missing keys when accessing them - An exception class or object to raise - A custom function with first argument the key and second the diot object. - Other values will be used as the default value directly Returns (Diot) The converted diot object. method","title":"diot.diot.Diot.from_namespace"},{"location":"api/biopipen.core.config/#diotdiotdiotsetitem","text":"</> Set self[key] to value. method","title":"diot.diot.Diot.setitem"},{"location":"api/biopipen.core.config/#diotdiotdiotpop","text":"</> Pop a key from the object and return the value. If key does notexist, return the given default value Parameters name (str) \u2014 The key Returns (Any) The value corresponding to the name or the default value Raises DiotFrozenError \u2014 when try to pop from a frozen diot method","title":"diot.diot.Diot.pop"},{"location":"api/biopipen.core.config/#diotdiotdiotpopitem","text":"</> Pop last item from the object Returns (Tuple) A tuple of key and value Raises DiotFrozenError \u2014 when try to pop from a frozen diot method","title":"diot.diot.Diot.popitem"},{"location":"api/biopipen.core.config/#diotdiotdiotupdate","text":"</> Update the object. Shortcut: |= Raises DiotFrozenError \u2014 when try to update a frozen diot method","title":"diot.diot.Diot.update"},{"location":"api/biopipen.core.config/#diotdiotdiotupdate_recursively","text":"</> Update the object. Shortcut: |= Raises DiotFrozenError \u2014 when try to update a frozen diot method","title":"diot.diot.Diot.update_recursively"},{"location":"api/biopipen.core.config/#diotdiotdiotior","text":"</> Return self|=value. method","title":"diot.diot.Diot.ior"},{"location":"api/biopipen.core.config/#diotdiotdiotdelitem","text":"</> Delete self[key]. method","title":"diot.diot.Diot.delitem"},{"location":"api/biopipen.core.config/#diotdiotdiotfreeze","text":"</> Freeze the diot object Parameters frozen (Union, optional) \u2014 The frozen argument indicating how to freeze:shallow: only freeze at depth=1 True: freeze recursively if there are diot objects in children False: Disable freezing method","title":"diot.diot.Diot.freeze"},{"location":"api/biopipen.core.config/#diotdiotdiotunfreeze","text":"</> Unfreeze the diot object Parameters recursive (bool, optional) \u2014 Whether unfreeze all diot objects recursively generator","title":"diot.diot.Diot.unfreeze"},{"location":"api/biopipen.core.config/#diotdiotdiotthaw","text":"</> A context manager for temporarily change the diot Parameters recursive (bool, optional) \u2014 Whether unfreeze all diot objects recursively Yields self, the reference to this diot. method","title":"diot.diot.Diot.thaw"},{"location":"api/biopipen.core.config/#diotdiotdiotsetdefault","text":"</> Set a default value to a key Parameters name (str) \u2014 The key name value (Any) \u2014 The default value Returns (Any) The existing value or the value passed in Raises DiotFrozenError \u2014 when try to set default to a frozen diot method","title":"diot.diot.Diot.setdefault"},{"location":"api/biopipen.core.config/#diotdiotdiotaccessible_keys","text":"</> Get the converted keys Returns (Iterable) The accessible (transformed) keys method","title":"diot.diot.Diot.accessible_keys"},{"location":"api/biopipen.core.config/#diotdiotdiotget","text":"</> Get the value of a key name Parameters name (str) \u2014 The key name value (Any, optional) \u2014 The value to return if the key does not exist Returns (Any) The corresponding value or the value passed in if the key doesnot exist method","title":"diot.diot.Diot.get"},{"location":"api/biopipen.core.config/#diotdiotdiotcontains","text":"</> True if the dictionary has the specified key, else False. method","title":"diot.diot.Diot.contains"},{"location":"api/biopipen.core.config/#diotdiotdiotclear","text":"</> Clear the object method","title":"diot.diot.Diot.clear"},{"location":"api/biopipen.core.config/#diotdiotdiotcopy","text":"</> Shallow copy the object Returns (Diot) The copied object method","title":"diot.diot.Diot.copy"},{"location":"api/biopipen.core.config/#diotdiotdiotto_dict","text":"</> Turn the Box and sub Boxes back into a nativepython dictionary. Returns (Dict) The converted python dictionary method","title":"diot.diot.Diot.to_dict"},{"location":"api/biopipen.core.config/#diotdiotdiotto_json","text":"</> Convert to a json string or save it to json file Parameters filename (Union, optional) \u2014 The filename to save the json to, if not given a jsonstring will be returned encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function **json_kwargs \u2014 Other kwargs for json.dumps Returns (Optional) The json string with filename is not given method","title":"diot.diot.Diot.to_json"},{"location":"api/biopipen.core.config/#diotdiotdiotto_yaml","text":"</> Convert to a yaml string or save it to yaml file Parameters filename (Union, optional) \u2014 The filename to save the yaml to, if not given a yamlstring will be returned default_flow_style (bool, optional) \u2014 The default flow style for yaml dumpingSee yaml.dump encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function **yaml_kwargs \u2014 Other kwargs for yaml.dump Returns (Optional) The yaml string with filename is not given method","title":"diot.diot.Diot.to_yaml"},{"location":"api/biopipen.core.config/#diotdiotdiotto_toml","text":"</> Convert to a toml string or save it to toml file Parameters filename (Union, optional) \u2014 The filename to save the toml to, if not given a tomlstring will be returned encoding (str, optional) \u2014 The encoding for saving to file errors (str, optional) \u2014 The errors handling for saveing to fileSee python's open function Returns (Optional) The toml string with filename is not given method","title":"diot.diot.Diot.to_toml"},{"location":"api/biopipen.core.config/#biopipencoreconfigconfigitemsgetitem","text":"</> x. getitem (y) <==> x[y]","title":"biopipen.core.config.ConfigItems.getitem"},{"location":"api/biopipen.core.defaults/","text":"module biopipen.core . defaults </> Provide default variables BIOPIPEN_DIR: the root directory of the biopipen source REPORT_DIR: the root directory of the report SCRIPTS_DIR: the root directory of the scripts","title":"biopipen.core.defaults"},{"location":"api/biopipen.core.defaults/#biopipencoredefaults","text":"</> Provide default variables BIOPIPEN_DIR: the root directory of the biopipen source REPORT_DIR: the root directory of the report SCRIPTS_DIR: the root directory of the scripts","title":"biopipen.core.defaults"},{"location":"api/biopipen.core.filters/","text":"module biopipen.core . filters </> Additional filters for pipen Functions dict_to_cli_args ( dic , exclude , prefix , sep , dup_key , join , start_key , end_key , dashify ) (Union) \u2014 Convert a python dict to a string of CLI arguments </> r ( obj , ignoreintkey , todot , sortkeys , skip , _i ) (str) \u2014 Convert a python object into R repr </> source_r ( path , chdir ) (str) \u2014 Source an R script. </> function biopipen.core.filters . dict_to_cli_args ( dic , exclude=None , prefix=None , sep=' ' , dup_key=True , join=False , start_key='' , end_key='_' , dashify=False ) </> Convert a python dict to a string of CLI arguments Parameters dic (Mapping) \u2014 The dict to convert exclude (Optional, optional) \u2014 The keys to exclude prefix (str | none, optional) \u2014 The prefix of the keys after conversionDefaults to None , mean - for short keys and -- for long keys sep (str | none, optional) \u2014 The separator between key and valueIf None , using \" \" for short keys and \"=\" for long keys dup_key (bool, optional) \u2014 Whether to duplicate the key in cli arguments for list valuesWhen True , {\"a\": [1, 2]} will be converted to \"-a 1 -a 2\" When False , {\"a\": [1, 2]} will be converted to \"-a 1 2\" If sep is None or = , this must be True, otherwise an error will be raised join (bool, optional) \u2014 Whether to join the arguments into a single string start_key (str, optional) \u2014 The key to start the argumentsThis is useful when you want to put some arguments at the beginning of the command line end_key (str, optional) \u2014 The key to end the argumentsThis is useful when you want to put some arguments at the end of the command line dashify (bool, optional) \u2014 Whether to replace _ with - in the keys Returns (Union) The converted string or list of strings function biopipen.core.filters . r ( obj , ignoreintkey=True , todot=None , sortkeys=False , skip=0 , _i=0 ) </> Convert a python object into R repr Examples >>> True -> \"TRUE\" >>> None -> \"NULL\" >>> [ 1 , 2 ] -> c ( 1 , 2 ) >>> { \"a\" : 1 , \"b\" : 2 } -> list ( a = 1 , b = 2 ) Parameters obj (Any) \u2014 The object to convert ignoreintkey (bool, optional) \u2014 When keys of a dict are integers, whether we shouldignore them. For example, when True , {1: 1, 2: 2} will be translated into \"list(1, 2)\" , but \"list( 1 = 1, 2 = 2)\" when False todot (str | none, optional) \u2014 If not None, the string will be converted to a dotFor example, todot=\"-\" will convert \"a-b\" to \"a.b\" Only applies to the keys of obj when it is a dict sortkeys (bool, optional) \u2014 Whether to sort the keys of a dict.True by default, in case the order of keys matters, for example, it could affect whether a job is cached. But sometimes, you want to keep orginal order, for example, arguments passed the dplyr::mutate function. Because the later arguments can refer to the earlier ones. skip (int, optional) \u2014 Levels to skip for todot . For example, skip=1 will skipthe first level of the keys. When todot is \"-\" , skip=1 will convert {\"a-b\": {\"c-d\": 1}} to list(`a-b` = list(`c.d` = 1)) _i (int, optional) \u2014 Current level of the keys. Used internally Returns (str) Then converted string representation of the object function biopipen.core.filters . source_r ( path , chdir=False ) </> Source an R script. In addition to generating source(path) , we also include the mtime for the script to trigger the job not cached when the script is updated. If your process is used in a cloud environment, it is recommended to use the read filter to load the script content instead of sourcing it using the source function in R to void the path issue (path could be different in different environments). Parameters path (str | pathlib.path) \u2014 The path to the R script Returns (str) The R code to source the script","title":"biopipen.core.filters"},{"location":"api/biopipen.core.filters/#biopipencorefilters","text":"</> Additional filters for pipen Functions dict_to_cli_args ( dic , exclude , prefix , sep , dup_key , join , start_key , end_key , dashify ) (Union) \u2014 Convert a python dict to a string of CLI arguments </> r ( obj , ignoreintkey , todot , sortkeys , skip , _i ) (str) \u2014 Convert a python object into R repr </> source_r ( path , chdir ) (str) \u2014 Source an R script. </> function","title":"biopipen.core.filters"},{"location":"api/biopipen.core.filters/#biopipencorefiltersdict_to_cli_args","text":"</> Convert a python dict to a string of CLI arguments Parameters dic (Mapping) \u2014 The dict to convert exclude (Optional, optional) \u2014 The keys to exclude prefix (str | none, optional) \u2014 The prefix of the keys after conversionDefaults to None , mean - for short keys and -- for long keys sep (str | none, optional) \u2014 The separator between key and valueIf None , using \" \" for short keys and \"=\" for long keys dup_key (bool, optional) \u2014 Whether to duplicate the key in cli arguments for list valuesWhen True , {\"a\": [1, 2]} will be converted to \"-a 1 -a 2\" When False , {\"a\": [1, 2]} will be converted to \"-a 1 2\" If sep is None or = , this must be True, otherwise an error will be raised join (bool, optional) \u2014 Whether to join the arguments into a single string start_key (str, optional) \u2014 The key to start the argumentsThis is useful when you want to put some arguments at the beginning of the command line end_key (str, optional) \u2014 The key to end the argumentsThis is useful when you want to put some arguments at the end of the command line dashify (bool, optional) \u2014 Whether to replace _ with - in the keys Returns (Union) The converted string or list of strings function","title":"biopipen.core.filters.dict_to_cli_args"},{"location":"api/biopipen.core.filters/#biopipencorefiltersr","text":"</> Convert a python object into R repr Examples >>> True -> \"TRUE\" >>> None -> \"NULL\" >>> [ 1 , 2 ] -> c ( 1 , 2 ) >>> { \"a\" : 1 , \"b\" : 2 } -> list ( a = 1 , b = 2 ) Parameters obj (Any) \u2014 The object to convert ignoreintkey (bool, optional) \u2014 When keys of a dict are integers, whether we shouldignore them. For example, when True , {1: 1, 2: 2} will be translated into \"list(1, 2)\" , but \"list( 1 = 1, 2 = 2)\" when False todot (str | none, optional) \u2014 If not None, the string will be converted to a dotFor example, todot=\"-\" will convert \"a-b\" to \"a.b\" Only applies to the keys of obj when it is a dict sortkeys (bool, optional) \u2014 Whether to sort the keys of a dict.True by default, in case the order of keys matters, for example, it could affect whether a job is cached. But sometimes, you want to keep orginal order, for example, arguments passed the dplyr::mutate function. Because the later arguments can refer to the earlier ones. skip (int, optional) \u2014 Levels to skip for todot . For example, skip=1 will skipthe first level of the keys. When todot is \"-\" , skip=1 will convert {\"a-b\": {\"c-d\": 1}} to list(`a-b` = list(`c.d` = 1)) _i (int, optional) \u2014 Current level of the keys. Used internally Returns (str) Then converted string representation of the object function","title":"biopipen.core.filters.r"},{"location":"api/biopipen.core.filters/#biopipencorefilterssource_r","text":"</> Source an R script. In addition to generating source(path) , we also include the mtime for the script to trigger the job not cached when the script is updated. If your process is used in a cloud environment, it is recommended to use the read filter to load the script content instead of sourcing it using the source function in R to void the path issue (path could be different in different environments). Parameters path (str | pathlib.path) \u2014 The path to the R script Returns (str) The R code to source the script","title":"biopipen.core.filters.source_r"},{"location":"api/biopipen.core/","text":"package biopipen. core </> module biopipen.core . proc </> Provides a base class for the processes to subclass Classes Proc ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> module biopipen.core . defaults </> Provide default variables BIOPIPEN_DIR: the root directory of the biopipen source REPORT_DIR: the root directory of the report SCRIPTS_DIR: the root directory of the scripts module biopipen.core . config </> Provides the envs from configuration files Classes ConfigItems \u2014 Provides the envs from configuration files and defaults thenon-existing values to None. </> module biopipen.core . filters </> Additional filters for pipen Functions dict_to_cli_args ( dic , exclude , prefix , sep , dup_key , join , start_key , end_key , dashify ) (Union) \u2014 Convert a python dict to a string of CLI arguments </> r ( obj , ignoreintkey , todot , sortkeys , skip , _i ) (str) \u2014 Convert a python object into R repr </> source_r ( path , chdir ) (str) \u2014 Source an R script. </> module biopipen.core . testing </> Provide utilities for testing. Functions get_pipeline ( testfile , loglevel , enable_report , **kwargs ) \u2014 Get a pipeline for a test file </> r_test ( mem ) (callable) \u2014 A decorator to test R code </>","title":"biopipen.core"},{"location":"api/biopipen.core/#biopipencore","text":"</> module","title":"biopipen.core"},{"location":"api/biopipen.core/#biopipencoreproc","text":"</> Provides a base class for the processes to subclass Classes Proc ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> module","title":"biopipen.core.proc"},{"location":"api/biopipen.core/#biopipencoredefaults","text":"</> Provide default variables BIOPIPEN_DIR: the root directory of the biopipen source REPORT_DIR: the root directory of the report SCRIPTS_DIR: the root directory of the scripts module","title":"biopipen.core.defaults"},{"location":"api/biopipen.core/#biopipencoreconfig","text":"</> Provides the envs from configuration files Classes ConfigItems \u2014 Provides the envs from configuration files and defaults thenon-existing values to None. </> module","title":"biopipen.core.config"},{"location":"api/biopipen.core/#biopipencorefilters","text":"</> Additional filters for pipen Functions dict_to_cli_args ( dic , exclude , prefix , sep , dup_key , join , start_key , end_key , dashify ) (Union) \u2014 Convert a python dict to a string of CLI arguments </> r ( obj , ignoreintkey , todot , sortkeys , skip , _i ) (str) \u2014 Convert a python object into R repr </> source_r ( path , chdir ) (str) \u2014 Source an R script. </> module","title":"biopipen.core.filters"},{"location":"api/biopipen.core/#biopipencoretesting","text":"</> Provide utilities for testing. Functions get_pipeline ( testfile , loglevel , enable_report , **kwargs ) \u2014 Get a pipeline for a test file </> r_test ( mem ) (callable) \u2014 A decorator to test R code </>","title":"biopipen.core.testing"},{"location":"api/biopipen.core.proc/","text":"module biopipen.core . proc </> Provides a base class for the processes to subclass Classes Proc ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> class biopipen.core.proc . Proc ( *args , **kwds ) \u2192 Proc </> Bases pipen.proc.Proc Base class for all processes in biopipen to subclass Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.core.proc"},{"location":"api/biopipen.core.proc/#biopipencoreproc","text":"</> Provides a base class for the processes to subclass Classes Proc ( Proc ) \u2014 Base class for all processes in biopipen to subclass </> class","title":"biopipen.core.proc"},{"location":"api/biopipen.core.proc/#biopipencoreprocproc","text":"</> Bases pipen.proc.Proc Base class for all processes in biopipen to subclass Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.core.proc.Proc"},{"location":"api/biopipen.core.proc/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.core.proc/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.core.proc/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.core.proc/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.core.proc/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.core.proc/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.core.proc/#pipenprocprocrun","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.core.testing/","text":"module biopipen.core . testing </> Provide utilities for testing. Functions get_pipeline ( testfile , loglevel , enable_report , **kwargs ) \u2014 Get a pipeline for a test file </> r_test ( mem ) (callable) \u2014 A decorator to test R code </> function biopipen.core.testing . get_pipeline ( testfile , loglevel='debug' , enable_report=False , **kwargs ) </> Get a pipeline for a test file function biopipen.core.testing . r_test ( mem ) \u2192 callable </> A decorator to test R code","title":"biopipen.core.testing"},{"location":"api/biopipen.core.testing/#biopipencoretesting","text":"</> Provide utilities for testing. Functions get_pipeline ( testfile , loglevel , enable_report , **kwargs ) \u2014 Get a pipeline for a test file </> r_test ( mem ) (callable) \u2014 A decorator to test R code </> function","title":"biopipen.core.testing"},{"location":"api/biopipen.core.testing/#biopipencoretestingget_pipeline","text":"</> Get a pipeline for a test file function","title":"biopipen.core.testing.get_pipeline"},{"location":"api/biopipen.core.testing/#biopipencoretestingr_test","text":"</> A decorator to test R code","title":"biopipen.core.testing.r_test"},{"location":"api/biopipen.ns.bam/","text":"module biopipen.ns . bam </> Tools to process sam/bam/cram files Classes CNVpytor ( Proc ) \u2014 Detect CNV using CNVpytor </> ControlFREEC ( Proc ) \u2014 Detect CNVs using Control-FREEC </> CNAClinic ( Proc ) \u2014 Detect CNVs using CNAClinic </> BamSplitChroms ( Proc ) \u2014 Split bam file by chromosomes </> BamMerge ( Proc ) \u2014 Merge bam files </> BamSampling ( Proc ) \u2014 Keeping only a fraction of read pairs from a bam file </> BamSubsetByBed ( Proc ) \u2014 Subset bam file by the regions in a bed file </> BamSort ( Proc ) \u2014 Sort bam file </> SamtoolsView ( Proc ) \u2014 View bam file using samtools, mostly used for filtering </> class biopipen.ns.bam . CNVpytor ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNV using CNVpytor Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam fileWill try to index it if it's not indexed. snpfile \u2014 The snp file Output outdir \u2014 The output directory Envs baf_nomask \u2014 Do not use P mask in BAF histograms binsizes \u2014 The binsizes chrom \u2014 The chromosomes to run on chrsize \u2014 The geome size file to fix missing contigs in VCF header cnvpytor \u2014 Path to cnvpytor filters \u2014 The filters to filter the resultSee - https://github.com/abyzovlab/CNVpytor/blob/master/GettingStarted.md#predicting-cnv-regions genome \u2014 The genome assembly to put in the VCF file mask_snps \u2014 Whether mask 1000 Genome snps ncores \u2014 Number of cores to use ( -j for cnvpytor) refdir \u2014 The directory containing the fasta file for each chromosome samtools \u2014 Path to samtools, used to index bam file in case it's not snp \u2014 How to read snp data Requires cnvpytor \u2014 check: {{proc.envs.cnvpytor}} --version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bam . ControlFREEC ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNVs using Control-FREEC Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file snpfile \u2014 The snp file Output outdir \u2014 The output directory Envs arggs \u2014 Other arguments for Control-FREEC freec \u2014 Path to Control-FREEC executable ncores \u2014 Number of cores to use Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bam . CNAClinic ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNVs using CNAClinic Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The meta file, header included, tab-delimited, includingfollowing columns: - Bam: The path to bam file - Sample: Optional. The sample names, if you don't want filename of bam file to be used - Group: Optional. The group names, either \"Case\" or \"Control\" - Patient: Optional. The patient names. Since CNAClinic only supports paired samples, you need to provide the patient names for each sample. Required if \"Group\" is provided. - Binsizer: Optional. Samples used to estimate the bin size \"Y\", \"Yes\", \"T\", \"True\", will be treated as True If not provided, will use envs.binsizer to get the samples to use. Either this column or envs.binsizer should be provided. Output outdir \u2014 The output directory Envs binsize \u2014 Directly use this binsize for CNAClinic, in bp. binsizer \u2014 The samples used to estimate the bin size, it could be:A list of sample names A float number (0 < x <= 1), the fraction of samples to use A integer number (x > 1), the number of samples to use genome \u2014 The genome assembly ncores \u2014 Number of cores to use plot_args \u2014 The arguments for CNAClinic::plotSampleData plot_multi_args \u2014 The arguments for CNAClinic::plotMultiSampleData run_args \u2014 The arguments for CNAClinic::runSegmentation seed \u2014 The seed for random number generator for choosing samplesfor estimating bin size Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bam . BamSplitChroms ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Split bam file by chromosomes Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file Output outdir \u2014 The output directory with bam files for each chromosome Envs chroms \u2014 The chromosomes to keep, if not provided, will use all index \u2014 Whether to index the output bam files. Requires the input bamfile to be sorted. keep_other_sq \u2014 Keep other chromosomes in \"@SQ\" field in header ncores \u2014 Number of cores to use sambamba \u2014 Path to sambamba executable samtools \u2014 Path to samtools executable tool \u2014 The tool to use, either \"samtools\" or \"sambamba\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bam . BamMerge ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Merge bam files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfiles \u2014 The bam files Output outfile \u2014 The output bam file Envs index \u2014 Whether to index the output bam fileRequires envs.sort to be True merge_args \u2014 The arguments for merging bam files samtools merge or sambamba merge , depending on tool For samtools , these keys are not allowed: -o , -O , --output-fmt , -@ , and --threads , as they are managed by the script For sambamba , these keys are not allowed: -t , and --nthreads , as they are managed by the script ncores \u2014 Number of cores to use sambamba \u2014 Path to sambamba executable samtools \u2014 Path to samtools executable sort \u2014 Whether to sort the output bam file sort_args \u2014 The arguments for sorting bam files samtools sort or sambamba sort , depending on tool For samtools , these keys are not allowed: -o , -@ , and --threads , as they are managed by the script For sambamba , these keys are not allowed: -t , --nthreads , -o and --out , as they are managed by the script tool \u2014 The tool to use, either \"samtools\" or \"sambamba\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bam . BamSampling ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Keeping only a fraction of read pairs from a bam file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file Output outfile \u2014 The output bam file Envs fraction (type=float) \u2014 The fraction of reads to keep.If 0 < fraction <= 1 , it's the fraction of reads to keep. If fraction > 1 , it's the number of reads to keep. Note that when fraction > 1, you may not get the exact number of reads specified but a close number. index \u2014 Whether to index the output bam file ncores \u2014 Number of cores to use samtools \u2014 Path to samtools executable seed \u2014 The seed for random number generator sort \u2014 Whether to sort the output bam file sort_args \u2014 The arguments for sorting bam file using samtools sort .These keys are not allowed: -o , -@ , and --threads , as they are managed by the script. tool \u2014 The tool to use, currently only \"samtools\" is supported Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bam . BamSubsetByBed ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Subset bam file by the regions in a bed file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file bedfile \u2014 The bed file Output outfile \u2014 The output bam file Envs index \u2014 Whether to index the output bam file ncores \u2014 Number of cores to use samtools \u2014 Path to samtools executable tool \u2014 The tool to use, currently only \"samtools\" is supported Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bam . BamSort ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Sort bam file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file Output outfile \u2014 The output bam file Envs \u2014 Other arguments passed to the sorting toolSee samtools sort or sambamba sort byname (flag) \u2014 Whether to sort by read name index (flag) \u2014 Whether to index the output bam fileThe index file will be created in the same directory as the output bam file ncores (type=int) \u2014 Number of cores to use sambamba \u2014 Path to sambamba executable samtools \u2014 Path to samtools executable tmpdir \u2014 The temporary directory to use tool (choice) \u2014 The tool to use. - samtools: Use samtools - sambamba: Use sambamba Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bam . SamtoolsView ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc View bam file using samtools, mostly used for filtering This is a wrapper for samtools view command. It will create a new bam file with the same name as the input bam file. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file Output outfile \u2014 The output bam file Envs \u2014 Other arguments passed to the view toolSee samtools view or sambamba view . index \u2014 Whether to index the output bam fileRequires the input bam file to be sorted. ncores \u2014 Number of cores to use samtools \u2014 Path to samtools executable Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.bam"},{"location":"api/biopipen.ns.bam/#biopipennsbam","text":"</> Tools to process sam/bam/cram files Classes CNVpytor ( Proc ) \u2014 Detect CNV using CNVpytor </> ControlFREEC ( Proc ) \u2014 Detect CNVs using Control-FREEC </> CNAClinic ( Proc ) \u2014 Detect CNVs using CNAClinic </> BamSplitChroms ( Proc ) \u2014 Split bam file by chromosomes </> BamMerge ( Proc ) \u2014 Merge bam files </> BamSampling ( Proc ) \u2014 Keeping only a fraction of read pairs from a bam file </> BamSubsetByBed ( Proc ) \u2014 Subset bam file by the regions in a bed file </> BamSort ( Proc ) \u2014 Sort bam file </> SamtoolsView ( Proc ) \u2014 View bam file using samtools, mostly used for filtering </> class","title":"biopipen.ns.bam"},{"location":"api/biopipen.ns.bam/#biopipennsbamcnvpytor","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNV using CNVpytor Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam fileWill try to index it if it's not indexed. snpfile \u2014 The snp file Output outdir \u2014 The output directory Envs baf_nomask \u2014 Do not use P mask in BAF histograms binsizes \u2014 The binsizes chrom \u2014 The chromosomes to run on chrsize \u2014 The geome size file to fix missing contigs in VCF header cnvpytor \u2014 Path to cnvpytor filters \u2014 The filters to filter the resultSee - https://github.com/abyzovlab/CNVpytor/blob/master/GettingStarted.md#predicting-cnv-regions genome \u2014 The genome assembly to put in the VCF file mask_snps \u2014 Whether mask 1000 Genome snps ncores \u2014 Number of cores to use ( -j for cnvpytor) refdir \u2014 The directory containing the fasta file for each chromosome samtools \u2014 Path to samtools, used to index bam file in case it's not snp \u2014 How to read snp data Requires cnvpytor \u2014 check: {{proc.envs.cnvpytor}} --version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.CNVpytor"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bam/#biopipennsbamcontrolfreec","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNVs using Control-FREEC Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file snpfile \u2014 The snp file Output outdir \u2014 The output directory Envs arggs \u2014 Other arguments for Control-FREEC freec \u2014 Path to Control-FREEC executable ncores \u2014 Number of cores to use Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.ControlFREEC"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bam/#biopipennsbamcnaclinic","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Detect CNVs using CNAClinic Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The meta file, header included, tab-delimited, includingfollowing columns: - Bam: The path to bam file - Sample: Optional. The sample names, if you don't want filename of bam file to be used - Group: Optional. The group names, either \"Case\" or \"Control\" - Patient: Optional. The patient names. Since CNAClinic only supports paired samples, you need to provide the patient names for each sample. Required if \"Group\" is provided. - Binsizer: Optional. Samples used to estimate the bin size \"Y\", \"Yes\", \"T\", \"True\", will be treated as True If not provided, will use envs.binsizer to get the samples to use. Either this column or envs.binsizer should be provided. Output outdir \u2014 The output directory Envs binsize \u2014 Directly use this binsize for CNAClinic, in bp. binsizer \u2014 The samples used to estimate the bin size, it could be:A list of sample names A float number (0 < x <= 1), the fraction of samples to use A integer number (x > 1), the number of samples to use genome \u2014 The genome assembly ncores \u2014 Number of cores to use plot_args \u2014 The arguments for CNAClinic::plotSampleData plot_multi_args \u2014 The arguments for CNAClinic::plotMultiSampleData run_args \u2014 The arguments for CNAClinic::runSegmentation seed \u2014 The seed for random number generator for choosing samplesfor estimating bin size Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.CNAClinic"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bam/#biopipennsbambamsplitchroms","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Split bam file by chromosomes Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file Output outdir \u2014 The output directory with bam files for each chromosome Envs chroms \u2014 The chromosomes to keep, if not provided, will use all index \u2014 Whether to index the output bam files. Requires the input bamfile to be sorted. keep_other_sq \u2014 Keep other chromosomes in \"@SQ\" field in header ncores \u2014 Number of cores to use sambamba \u2014 Path to sambamba executable samtools \u2014 Path to samtools executable tool \u2014 The tool to use, either \"samtools\" or \"sambamba\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.BamSplitChroms"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bam/#biopipennsbambammerge","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Merge bam files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfiles \u2014 The bam files Output outfile \u2014 The output bam file Envs index \u2014 Whether to index the output bam fileRequires envs.sort to be True merge_args \u2014 The arguments for merging bam files samtools merge or sambamba merge , depending on tool For samtools , these keys are not allowed: -o , -O , --output-fmt , -@ , and --threads , as they are managed by the script For sambamba , these keys are not allowed: -t , and --nthreads , as they are managed by the script ncores \u2014 Number of cores to use sambamba \u2014 Path to sambamba executable samtools \u2014 Path to samtools executable sort \u2014 Whether to sort the output bam file sort_args \u2014 The arguments for sorting bam files samtools sort or sambamba sort , depending on tool For samtools , these keys are not allowed: -o , -@ , and --threads , as they are managed by the script For sambamba , these keys are not allowed: -t , --nthreads , -o and --out , as they are managed by the script tool \u2014 The tool to use, either \"samtools\" or \"sambamba\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.BamMerge"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bam/#biopipennsbambamsampling","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Keeping only a fraction of read pairs from a bam file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file Output outfile \u2014 The output bam file Envs fraction (type=float) \u2014 The fraction of reads to keep.If 0 < fraction <= 1 , it's the fraction of reads to keep. If fraction > 1 , it's the number of reads to keep. Note that when fraction > 1, you may not get the exact number of reads specified but a close number. index \u2014 Whether to index the output bam file ncores \u2014 Number of cores to use samtools \u2014 Path to samtools executable seed \u2014 The seed for random number generator sort \u2014 Whether to sort the output bam file sort_args \u2014 The arguments for sorting bam file using samtools sort .These keys are not allowed: -o , -@ , and --threads , as they are managed by the script. tool \u2014 The tool to use, currently only \"samtools\" is supported Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.BamSampling"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun_5","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bam/#biopipennsbambamsubsetbybed","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Subset bam file by the regions in a bed file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file bedfile \u2014 The bed file Output outfile \u2014 The output bam file Envs index \u2014 Whether to index the output bam file ncores \u2014 Number of cores to use samtools \u2014 Path to samtools executable tool \u2014 The tool to use, currently only \"samtools\" is supported Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.BamSubsetByBed"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta_6","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc_6","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass_6","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_6","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc_6","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog_6","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun_6","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bam/#biopipennsbambamsort","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Sort bam file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file Output outfile \u2014 The output bam file Envs \u2014 Other arguments passed to the sorting toolSee samtools sort or sambamba sort byname (flag) \u2014 Whether to sort by read name index (flag) \u2014 Whether to index the output bam fileThe index file will be created in the same directory as the output bam file ncores (type=int) \u2014 Number of cores to use sambamba \u2014 Path to sambamba executable samtools \u2014 Path to samtools executable tmpdir \u2014 The temporary directory to use tool (choice) \u2014 The tool to use. - samtools: Use samtools - sambamba: Use sambamba Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.BamSort"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta_7","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc_7","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass_7","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_7","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc_7","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog_7","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun_7","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bam/#biopipennsbamsamtoolsview","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc View bam file using samtools, mostly used for filtering This is a wrapper for samtools view command. It will create a new bam file with the same name as the input bam file. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bam file Output outfile \u2014 The output bam file Envs \u2014 Other arguments passed to the view toolSee samtools view or sambamba view . index \u2014 Whether to index the output bam fileRequires the input bam file to be sorted. ncores \u2014 Number of cores to use samtools \u2014 Path to samtools executable Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bam.SamtoolsView"},{"location":"api/biopipen.ns.bam/#pipenprocprocmeta_8","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bam/#pipenprocprocfrom_proc_8","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_subclass_8","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bam/#pipenprocprocinit_8","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bam/#pipenprocprocgc_8","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bam/#pipenprocproclog_8","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bam/#pipenprocprocrun_8","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bed/","text":"module biopipen.ns . bed </> Tools to handle BED files Classes BedLiftOver ( Proc ) \u2014 Liftover a BED file using liftOver </> Bed2Vcf ( Proc ) \u2014 Convert a BED file to a valid VCF file with minimal information </> BedConsensus ( Proc ) \u2014 Find consensus regions from multiple BED files. </> BedtoolsMerge ( Proc ) \u2014 Merge overlapping intervals in a BED file, using bedtools merge </> BedtoolsIntersect ( Proc ) \u2014 Find the intersection of two BED files, using bedtools intersect </> BedtoolsMakeWindows ( Proc ) \u2014 Make windows from a BED file or genome size file, using bedtools makewindows . </> class biopipen.ns.bed . BedLiftOver ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Liftover a BED file using liftOver Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input inbed \u2014 The input BED file Output outbed \u2014 The output BED file Envs chain \u2014 The map chain file for liftover liftover \u2014 The path to liftOver Requires liftOver \u2014 check: {{proc.envs.liftover}} 2>&1 | grep \"usage\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bed . Bed2Vcf ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert a BED file to a valid VCF file with minimal information Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input inbed \u2014 The input BED file Output outvcf \u2014 The output VCF file Envs base \u2014 0 or 1, whether the coordinates in BED file are 0- or 1-based converters \u2014 A dict of converters to be used for each INFO or FORMATThe key is the ID of an INFO or FORMAT, and the value is Any converts return None will skip the record formats \u2014 The FORMAT dicts to be added to the VCF fileThe keys 'ID', 'Description', 'Type', and 'Number' are required. genome \u2014 The genome assembly, added as source in header headers \u2014 The header lines to be added to the VCF file helpers \u2014 Raw code to be executed to provide some helper functionssince only lambda functions are supported in converters index \u2014 Sort and index output file infos \u2014 The INFO dicts to be added to the VCF file nonexisting_contigs \u2014 Whether to keep or drop the non-existingcontigs in ref . ref \u2014 The reference fasta file, used to grab the reference allele.To add contigs in header, the fai file is also required at <ref>.fai sample \u2014 The sample name to be used in the VCF fileYou can use a lambda function (in string) to generate the sample name from the stem of input file Requires bcftools \u2014 if: {{proc.envs.index}} check: {{proc.envs.bcftools}} --version cyvcf2 \u2014 check: {{proc.lang}} -c \"import cyvcf2\" pysam \u2014 check: {{proc.lang}} -c \"import pysam\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bed . BedConsensus ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Find consensus regions from multiple BED files. Unlike bedtools merge/cluster , it does not find the union regions nor intersect regions. Instead, it finds the consensus regions using the distributions of the scores of the bins bedtools cluster Bedfile A |----------| 1 Bedfile B |--------| 1 Bedfile C |------| 1 BedConsensus |--------| with cutoff >= 2 bedtools intesect |----| bedtools merge |------------| Distribution |1|2|3333|2|1| (later normalized into 0~1) Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bedfiles \u2014 Input BED files Output outbed \u2014 The output BED file Envs bedtools \u2014 The path to bedtools chrsize \u2014 The chromosome sizes file cutoff \u2014 The cutoff to determine the ends of consensus regionsIf cutoff < 1, it applies to the normalized scores (0~1), which is the percentage of the number of files that cover the region. If cutoff >= 1, it applies to the number of files that cover the region directly. distance \u2014 When the distance between two bins is smaller than this value,they are merged into one bin using bedtools merge -d . 0 means no merging. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bed . BedtoolsMerge ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Merge overlapping intervals in a BED file, using bedtools merge Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input inbed \u2014 The input BED file Output outbed \u2014 The output BED file Envs \u2014 Other options to be passed to bedtools merge See https://bedtools.readthedocs.io/en/latest/content/tools/merge.html bedtools \u2014 The path to bedtools Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bed . BedtoolsIntersect ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Find the intersection of two BED files, using bedtools intersect See https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input afile \u2014 The first BED file bfile \u2014 The second BED file Output outfile \u2014 The output BED file Envs \u2014 Other options to be passed to bedtools intersect bedtools \u2014 The path to bedtools chrsize \u2014 Alias for g in bedtools intersect . postcmd \u2014 The command to be executed for the output file after intersecting.You can use $infile , $outfile , and $outdir to refer to the input, output, and output directory, respectively. sort \u2014 Sort afile and bfile before intersecting.By default, -sorted is used, assuming the input files are sorted. If error occurs, try to set sort to True . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.bed . BedtoolsMakeWindows ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Make windows from a BED file or genome size file, using bedtools makewindows . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input BED file or a genome size fileType will be detected by the number of columns in the file. If it has 3+ columns, it is treated as a BED file, otherwise a genome size file. Output outfile \u2014 The output BED file Envs bedtools \u2014 The path to bedtools name (choice) \u2014 How to name the generated windows/regions - none: Do not add any name - src: Use the source interval's name - winnum: Use the window number - srcwinnum: Use the source interval's name and window number nwin (type=int) \u2014 The number of windows to be generatedExclusive with window and step . Either nwin or window and step should be provided. reverse (flag) \u2014 Reverse numbering of windows in the output step (type=int) \u2014 The step size of the windows window (type=int) \u2014 The size of the windows Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.bed"},{"location":"api/biopipen.ns.bed/#biopipennsbed","text":"</> Tools to handle BED files Classes BedLiftOver ( Proc ) \u2014 Liftover a BED file using liftOver </> Bed2Vcf ( Proc ) \u2014 Convert a BED file to a valid VCF file with minimal information </> BedConsensus ( Proc ) \u2014 Find consensus regions from multiple BED files. </> BedtoolsMerge ( Proc ) \u2014 Merge overlapping intervals in a BED file, using bedtools merge </> BedtoolsIntersect ( Proc ) \u2014 Find the intersection of two BED files, using bedtools intersect </> BedtoolsMakeWindows ( Proc ) \u2014 Make windows from a BED file or genome size file, using bedtools makewindows . </> class","title":"biopipen.ns.bed"},{"location":"api/biopipen.ns.bed/#biopipennsbedbedliftover","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Liftover a BED file using liftOver Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input inbed \u2014 The input BED file Output outbed \u2014 The output BED file Envs chain \u2014 The map chain file for liftover liftover \u2014 The path to liftOver Requires liftOver \u2014 check: {{proc.envs.liftover}} 2>&1 | grep \"usage\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bed.BedLiftOver"},{"location":"api/biopipen.ns.bed/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bed/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bed/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bed/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bed/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bed/#biopipennsbedbed2vcf","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert a BED file to a valid VCF file with minimal information Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input inbed \u2014 The input BED file Output outvcf \u2014 The output VCF file Envs base \u2014 0 or 1, whether the coordinates in BED file are 0- or 1-based converters \u2014 A dict of converters to be used for each INFO or FORMATThe key is the ID of an INFO or FORMAT, and the value is Any converts return None will skip the record formats \u2014 The FORMAT dicts to be added to the VCF fileThe keys 'ID', 'Description', 'Type', and 'Number' are required. genome \u2014 The genome assembly, added as source in header headers \u2014 The header lines to be added to the VCF file helpers \u2014 Raw code to be executed to provide some helper functionssince only lambda functions are supported in converters index \u2014 Sort and index output file infos \u2014 The INFO dicts to be added to the VCF file nonexisting_contigs \u2014 Whether to keep or drop the non-existingcontigs in ref . ref \u2014 The reference fasta file, used to grab the reference allele.To add contigs in header, the fai file is also required at <ref>.fai sample \u2014 The sample name to be used in the VCF fileYou can use a lambda function (in string) to generate the sample name from the stem of input file Requires bcftools \u2014 if: {{proc.envs.index}} check: {{proc.envs.bcftools}} --version cyvcf2 \u2014 check: {{proc.lang}} -c \"import cyvcf2\" pysam \u2014 check: {{proc.lang}} -c \"import pysam\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bed.Bed2Vcf"},{"location":"api/biopipen.ns.bed/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bed/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bed/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bed/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bed/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bed/#biopipennsbedbedconsensus","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Find consensus regions from multiple BED files. Unlike bedtools merge/cluster , it does not find the union regions nor intersect regions. Instead, it finds the consensus regions using the distributions of the scores of the bins bedtools cluster Bedfile A |----------| 1 Bedfile B |--------| 1 Bedfile C |------| 1 BedConsensus |--------| with cutoff >= 2 bedtools intesect |----| bedtools merge |------------| Distribution |1|2|3333|2|1| (later normalized into 0~1) Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bedfiles \u2014 Input BED files Output outbed \u2014 The output BED file Envs bedtools \u2014 The path to bedtools chrsize \u2014 The chromosome sizes file cutoff \u2014 The cutoff to determine the ends of consensus regionsIf cutoff < 1, it applies to the normalized scores (0~1), which is the percentage of the number of files that cover the region. If cutoff >= 1, it applies to the number of files that cover the region directly. distance \u2014 When the distance between two bins is smaller than this value,they are merged into one bin using bedtools merge -d . 0 means no merging. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bed.BedConsensus"},{"location":"api/biopipen.ns.bed/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bed/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bed/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bed/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bed/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bed/#biopipennsbedbedtoolsmerge","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Merge overlapping intervals in a BED file, using bedtools merge Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input inbed \u2014 The input BED file Output outbed \u2014 The output BED file Envs \u2014 Other options to be passed to bedtools merge See https://bedtools.readthedocs.io/en/latest/content/tools/merge.html bedtools \u2014 The path to bedtools Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bed.BedtoolsMerge"},{"location":"api/biopipen.ns.bed/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bed/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bed/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bed/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bed/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bed/#biopipennsbedbedtoolsintersect","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Find the intersection of two BED files, using bedtools intersect See https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input afile \u2014 The first BED file bfile \u2014 The second BED file Output outfile \u2014 The output BED file Envs \u2014 Other options to be passed to bedtools intersect bedtools \u2014 The path to bedtools chrsize \u2014 Alias for g in bedtools intersect . postcmd \u2014 The command to be executed for the output file after intersecting.You can use $infile , $outfile , and $outdir to refer to the input, output, and output directory, respectively. sort \u2014 Sort afile and bfile before intersecting.By default, -sorted is used, assuming the input files are sorted. If error occurs, try to set sort to True . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bed.BedtoolsIntersect"},{"location":"api/biopipen.ns.bed/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bed/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bed/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bed/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bed/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.bed/#biopipennsbedbedtoolsmakewindows","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Make windows from a BED file or genome size file, using bedtools makewindows . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input BED file or a genome size fileType will be detected by the number of columns in the file. If it has 3+ columns, it is treated as a BED file, otherwise a genome size file. Output outfile \u2014 The output BED file Envs bedtools \u2014 The path to bedtools name (choice) \u2014 How to name the generated windows/regions - none: Do not add any name - src: Use the source interval's name - winnum: Use the window number - srcwinnum: Use the source interval's name and window number nwin (type=int) \u2014 The number of windows to be generatedExclusive with window and step . Either nwin or window and step should be provided. reverse (flag) \u2014 Reverse numbering of windows in the output step (type=int) \u2014 The step size of the windows window (type=int) \u2014 The size of the windows Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.bed.BedtoolsMakeWindows"},{"location":"api/biopipen.ns.bed/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.bed/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.bed/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.bed/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.bed/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.bed/#pipenprocprocrun_5","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cellranger/","text":"module biopipen.ns . cellranger </> Cellranger pipeline module for BioPipen Classes CellRangerCount ( Proc ) \u2014 Run cellranger count </> CellRangerVdj ( Proc ) \u2014 Run cellranger vdj </> CellRangerSummary ( Proc ) \u2014 Summarize cellranger metrics </> class biopipen.ns.cellranger . CellRangerCount ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cellranger count to count gene expression and/or feature barcode reads requires cellranger v7+. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input fastqs \u2014 The input fastq filesEither a list of fastq files or a directory containing fastq files If a directory is provided, it should be passed as a list with one element. id \u2014 The id defining output directory. If not provided, it is inferredfrom the fastq files. Note that, unlike the --id argument of cellranger, this will not select the samples from in.fastqs . In stead, it will symlink the fastq files to a temporary directory with this id as prefix and pass that to cellranger. Output outdir \u2014 The output directory Envs \u2014 Other environment variables required by cellranger count See cellranger count --help for more details or https://www.10xgenomics.com/support/software/cell-ranger/advanced/cr-command-line-arguments#count cellranger \u2014 Path to cellranger create_bam (flag) \u2014 Enable or disable BAM file generation.This is required by cellrange v8+. When using cellrange v8-, it will be transformed to --no-bam . include_introns (flag) \u2014 Set to false to exclude intronic reads in count. ncores \u2014 Number of cores to use ref \u2014 Path of folder containing 10x-compatible transcriptome reference tmpdir \u2014 Path to temporary directory, used to save the soft-lined fastq filesto pass to cellranger Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cellranger . CellRangerVdj ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cellranger vdj to perform sequence assembly and paired clonotype calling. requires cellranger v7+. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input fastqs \u2014 The input fastq filesEither a list of fastq files or a directory containing fastq files If a directory is provided, it should be passed as a list with one element. id \u2014 The id determining the output directory. If not provided, it is inferredfrom the fastq files. Output outdir \u2014 The output directory Envs \u2014 Other environment variables required by cellranger vdj See cellranger vdj --help for more details or https://www.10xgenomics.com/support/software/cell-ranger/advanced/cr-command-line-arguments#vdj cellranger \u2014 Path to cellranger ncores \u2014 Number of cores to use ref \u2014 Path of folder containing 10x-compatible transcriptome reference tmpdir \u2014 Path to temporary directory, used to save the soft-lined fastq filesto pass to cellranger Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cellranger . CellRangerSummary ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Summarize cellranger metrics Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indirs \u2014 The directories containing cellranger resultsfrom CellRangerCount / CellRangerVdj . Output outdir \u2014 The output directory Envs group (type=auto) \u2014 The group of the samples for boxplots.If None , don't do boxplots. It can be a dict of group names and sample names, e.g. {\"group1\": [\"sample1\", \"sample2\"], \"group2\": [\"sample3\"]} or a file containing the group information, with the first column being the sample names and the second column being the group names. The file should be tab-delimited with no header. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.cellranger"},{"location":"api/biopipen.ns.cellranger/#biopipennscellranger","text":"</> Cellranger pipeline module for BioPipen Classes CellRangerCount ( Proc ) \u2014 Run cellranger count </> CellRangerVdj ( Proc ) \u2014 Run cellranger vdj </> CellRangerSummary ( Proc ) \u2014 Summarize cellranger metrics </> class","title":"biopipen.ns.cellranger"},{"location":"api/biopipen.ns.cellranger/#biopipennscellrangercellrangercount","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cellranger count to count gene expression and/or feature barcode reads requires cellranger v7+. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input fastqs \u2014 The input fastq filesEither a list of fastq files or a directory containing fastq files If a directory is provided, it should be passed as a list with one element. id \u2014 The id defining output directory. If not provided, it is inferredfrom the fastq files. Note that, unlike the --id argument of cellranger, this will not select the samples from in.fastqs . In stead, it will symlink the fastq files to a temporary directory with this id as prefix and pass that to cellranger. Output outdir \u2014 The output directory Envs \u2014 Other environment variables required by cellranger count See cellranger count --help for more details or https://www.10xgenomics.com/support/software/cell-ranger/advanced/cr-command-line-arguments#count cellranger \u2014 Path to cellranger create_bam (flag) \u2014 Enable or disable BAM file generation.This is required by cellrange v8+. When using cellrange v8-, it will be transformed to --no-bam . include_introns (flag) \u2014 Set to false to exclude intronic reads in count. ncores \u2014 Number of cores to use ref \u2014 Path of folder containing 10x-compatible transcriptome reference tmpdir \u2014 Path to temporary directory, used to save the soft-lined fastq filesto pass to cellranger Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cellranger.CellRangerCount"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cellranger/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cellranger/#biopipennscellrangercellrangervdj","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cellranger vdj to perform sequence assembly and paired clonotype calling. requires cellranger v7+. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input fastqs \u2014 The input fastq filesEither a list of fastq files or a directory containing fastq files If a directory is provided, it should be passed as a list with one element. id \u2014 The id determining the output directory. If not provided, it is inferredfrom the fastq files. Output outdir \u2014 The output directory Envs \u2014 Other environment variables required by cellranger vdj See cellranger vdj --help for more details or https://www.10xgenomics.com/support/software/cell-ranger/advanced/cr-command-line-arguments#vdj cellranger \u2014 Path to cellranger ncores \u2014 Number of cores to use ref \u2014 Path of folder containing 10x-compatible transcriptome reference tmpdir \u2014 Path to temporary directory, used to save the soft-lined fastq filesto pass to cellranger Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cellranger.CellRangerVdj"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cellranger/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cellranger/#biopipennscellrangercellrangersummary","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Summarize cellranger metrics Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indirs \u2014 The directories containing cellranger resultsfrom CellRangerCount / CellRangerVdj . Output outdir \u2014 The output directory Envs group (type=auto) \u2014 The group of the samples for boxplots.If None , don't do boxplots. It can be a dict of group names and sample names, e.g. {\"group1\": [\"sample1\", \"sample2\"], \"group2\": [\"sample3\"]} or a file containing the group information, with the first column being the sample names and the second column being the group names. The file should be tab-delimited with no header. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cellranger.CellRangerSummary"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cellranger/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cellranger/#pipenprocprocrun_2","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cellranger_pipeline/","text":"module biopipen.ns . cellranger_pipeline </> The cellranger pipelines Primarily cellranger process plus summary for summarizing the metrics for multiple samples. Classes CellRangerCountPipeline \u2014 The cellranger count pipeline </> CellRangerVdjPipeline \u2014 The cellranger vdj pipeline </> class biopipen.ns.cellranger_pipeline . CellRangerCountPipeline ( *args , **kwds ) </> Bases pipen_args.procgroup.ProcGroup pipen.procgroup.ProcGroup The cellranger count pipeline Run cellranger count for multiple samples and summarize the metrics. Attributes parser \u2014 Pass arguments to initialize the parser The parser is a singleton and by default initalized at plugin.on_init() hook, which happens usually after the initialization of a process group. </> Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) (Pipen) \u2014 Convert the pipeline to a Pipen instance </> post_init ( ) \u2014 Check if the input is a list of fastq files </> class pipen.procgroup. ProcGropuMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod __init_subclass__ ( ) </> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod add_proc ( self_or_method , proc=None ) </> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method as_pipen ( name=None , desc=None , outdir=None , **kwargs ) </> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | os.pathlike | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns (Pipen) The Pipen instance method post_init ( ) </> Check if the input is a list of fastq files class biopipen.ns.cellranger_pipeline . CellRangerVdjPipeline ( *args , **kwds ) </> Bases pipen_args.procgroup.ProcGroup pipen.procgroup.ProcGroup The cellranger vdj pipeline Run cellranger vdj for multiple samples and summarize the metrics. Attributes parser \u2014 Pass arguments to initialize the parser The parser is a singleton and by default initalized at plugin.on_init() hook, which happens usually after the initialization of a process group. </> Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) (Pipen) \u2014 Convert the pipeline to a Pipen instance </> post_init ( ) \u2014 Check if the input is a list of fastq files </> class pipen.procgroup. ProcGropuMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod __init_subclass__ ( ) </> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod add_proc ( self_or_method , proc=None ) </> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method as_pipen ( name=None , desc=None , outdir=None , **kwargs ) </> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | os.pathlike | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns (Pipen) The Pipen instance method post_init ( ) </> Check if the input is a list of fastq files","title":"biopipen.ns.cellranger_pipeline"},{"location":"api/biopipen.ns.cellranger_pipeline/#biopipennscellranger_pipeline","text":"</> The cellranger pipelines Primarily cellranger process plus summary for summarizing the metrics for multiple samples. Classes CellRangerCountPipeline \u2014 The cellranger count pipeline </> CellRangerVdjPipeline \u2014 The cellranger vdj pipeline </> class","title":"biopipen.ns.cellranger_pipeline"},{"location":"api/biopipen.ns.cellranger_pipeline/#biopipennscellranger_pipelinecellrangercountpipeline","text":"</> Bases pipen_args.procgroup.ProcGroup pipen.procgroup.ProcGroup The cellranger count pipeline Run cellranger count for multiple samples and summarize the metrics. Attributes parser \u2014 Pass arguments to initialize the parser The parser is a singleton and by default initalized at plugin.on_init() hook, which happens usually after the initialization of a process group. </> Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) (Pipen) \u2014 Convert the pipeline to a Pipen instance </> post_init ( ) \u2014 Check if the input is a list of fastq files </> class","title":"biopipen.ns.cellranger_pipeline.CellRangerCountPipeline"},{"location":"api/biopipen.ns.cellranger_pipeline/#pipenprocgroupprocgropumeta","text":"</> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod","title":"pipen.procgroup.ProcGropuMeta"},{"location":"api/biopipen.ns.cellranger_pipeline/#pipenprocgroupprocgroupinit_subclass","text":"</> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod","title":"pipen.procgroup.ProcGroup.init_subclass"},{"location":"api/biopipen.ns.cellranger_pipeline/#pipenprocgroupprocgroupadd_proc","text":"</> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method","title":"pipen.procgroup.ProcGroup.add_proc"},{"location":"api/biopipen.ns.cellranger_pipeline/#pipenprocgroupprocgroupas_pipen","text":"</> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | os.pathlike | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns (Pipen) The Pipen instance method","title":"pipen.procgroup.ProcGroup.as_pipen"},{"location":"api/biopipen.ns.cellranger_pipeline/#biopipennscellranger_pipelinecellrangercountpipelinepost_init","text":"</> Check if the input is a list of fastq files class","title":"biopipen.ns.cellranger_pipeline.CellRangerCountPipeline.post_init"},{"location":"api/biopipen.ns.cellranger_pipeline/#biopipennscellranger_pipelinecellrangervdjpipeline","text":"</> Bases pipen_args.procgroup.ProcGroup pipen.procgroup.ProcGroup The cellranger vdj pipeline Run cellranger vdj for multiple samples and summarize the metrics. Attributes parser \u2014 Pass arguments to initialize the parser The parser is a singleton and by default initalized at plugin.on_init() hook, which happens usually after the initialization of a process group. </> Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) (Pipen) \u2014 Convert the pipeline to a Pipen instance </> post_init ( ) \u2014 Check if the input is a list of fastq files </> class","title":"biopipen.ns.cellranger_pipeline.CellRangerVdjPipeline"},{"location":"api/biopipen.ns.cellranger_pipeline/#pipenprocgroupprocgropumeta_1","text":"</> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod","title":"pipen.procgroup.ProcGropuMeta"},{"location":"api/biopipen.ns.cellranger_pipeline/#pipenprocgroupprocgroupinit_subclass_1","text":"</> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod","title":"pipen.procgroup.ProcGroup.init_subclass"},{"location":"api/biopipen.ns.cellranger_pipeline/#pipenprocgroupprocgroupadd_proc_1","text":"</> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method","title":"pipen.procgroup.ProcGroup.add_proc"},{"location":"api/biopipen.ns.cellranger_pipeline/#pipenprocgroupprocgroupas_pipen_1","text":"</> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | os.pathlike | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns (Pipen) The Pipen instance method","title":"pipen.procgroup.ProcGroup.as_pipen"},{"location":"api/biopipen.ns.cellranger_pipeline/#biopipennscellranger_pipelinecellrangervdjpipelinepost_init","text":"</> Check if the input is a list of fastq files","title":"biopipen.ns.cellranger_pipeline.CellRangerVdjPipeline.post_init"},{"location":"api/biopipen.ns.cnv/","text":"module biopipen.ns . cnv </> CNV/CNA-related processes, mostly tertiary analysis Classes AneuploidyScore ( Proc ) \u2014 Chromosomal arm SCNA/aneuploidy </> AneuploidyScoreSummary ( Proc ) \u2014 Summary table and plots from AneuploidyScore </> TMADScore ( Proc ) \u2014 Trimmed Median Absolute Deviation (TMAD) score for CNV </> TMADScoreSummary ( Proc ) \u2014 Summary table and plots for TMADScore </> class biopipen.ns.cnv . AneuploidyScore ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Chromosomal arm SCNA/aneuploidy The CAAs in this process are calculated using Cohen-Sharir method See https://github.com/quevedor2/aneuploidy_score Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input segfile \u2014 The seg file, generally including chrom, start, end andseg.mean (the log2 ratio). It is typically a tab-delimited file or a BED file. If so, envs.chrom_col, envs.start_col, envs.end_col and envs.seg_col are the 1st, 2nd, 3rd and 5th columns, respectively. It can also be a VCF file. If so, envs.chrom_col and envs.start_col are not required. end_col and envs.seg_col will be a field in the INFO column. VariantAnnotation is required to extract the INFO field. Output outdir \u2014 The output directory containing the CAAs, AS and a histogramplot to show the CAAs for each chromosome arm Envs chrom_col \u2014 The column name for chromosome cn_col \u2014 The column name for copy number cn_transform (type=auto) \u2014 A R function to transform seg.mean intocopy number, or a list of cutoffs to determine the copy number. See https://cnvkit.readthedocs.io/en/stable/pipeline.html#calling-methods. If this is give, cn_col will be ignored. end_col \u2014 The column name for end position excl_chroms (list) \u2014 The chromosomes to be excludedWorks with/without chr prefix. genome \u2014 The genome version, hg19 or hg38 seg_col \u2014 The column name for seg.mean segmean_transform (text) \u2014 A R function to transform seg.mean The transformed value will be used to calculate the CAAs start_col \u2014 The column name for start position threshold (type=float) \u2014 The threshold to determine whether a chromosomearm is gained or lost. wgd_gf (type=float) \u2014 The fraction of the genome that is affected by WGD Requires AneuploidyScore \u2014 check: {{proc.lang}} <(echo \"library(AneuploidyScore)\") ucsc.hg19.cytoband \u2014 if: {{ proc.envs.genome == 'hg19' }} check: {{proc.lang}} <(echo \"library(ucsc.hg19.cytoband)\") ucsc.hg38.cytoband \u2014 if: {{ proc.envs.genome == 'hg38' }} check: {{proc.lang}} <(echo \"library(ucsc.hg38.cytoband)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnv . AneuploidyScoreSummary ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Summary table and plots from AneuploidyScore Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input asdirs \u2014 The output directories from AneuploidyScore metafile \u2014 The metafile containing the sample information Output outdir \u2014 The output directory containing the summary table and plots Envs group_cols (type=auto) \u2014 The column name in the metafile to group thesamples. We also support multiple columns, e.g. [\"group1\", \"group2\"] You can also use group1,group2 to add a secondary grouping based on group2 within each group1 (only works for 2 groups) heatmap_cases (type=json) \u2014 The cases to be included in the heatmapBy default, all arms are included. If specified, keys are the names of the cases and values are the arms, which will be included in the heatmap. The list of arms should be a subset of chr<N>_p and chr<N>_q , where <N> is the chromosome number from 1 to 22, X, Y. You can also use ALL to include all arms. sample_name (text) \u2014 An R function to extract the sample name fromthe file stem (not including .aneuploidy_score part) Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnv . TMADScore ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Trimmed Median Absolute Deviation (TMAD) score for CNV Reference: Mouliere, Chandrananda, Piskorz and Moore et al. Enhanced detection of circulating tumor DNA by fragment size analysis Science Translational Medicine (2018). Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input segfile \u2014 The seg file, two columns are required: * chrom: The chromosome name, used for filtering * seg.mean: The log2 ratio. It is typically a tab-delimited file or a BED file.If so, envs.chrom_col and envs.seg_col are the 1st and 5th columns, respectively. It can also be a VCF file. If so, envs.chrom_col and envs.start_col are not required. end_col and envs.seg_col will be a field in the INFO column. VariantAnnotation is required to extract the INFO field. Output outfile \u2014 The output file containing the TMAD score Envs chrom_col \u2014 The column name for chromosome excl_chroms (list) \u2014 The chromosomes to be excluded seg_col \u2014 The column name for seg.mean segmean_transform \u2014 The transformation function for seg.mean Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnv . TMADScoreSummary ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Summary table and plots for TMADScore Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The metafile containing the sample informationThe first column must be the sample ID tmadfiles \u2014 The output files from TMADScore Output outdir \u2014 The output directory containing the summary table and plots Envs group_cols (type=auto) \u2014 The column name in the metafile to group thesamples Could also be a list of column names If not specified, samples will be plotted individually as a barplot We also support multiple columns, e.g. [\"group1\", \"group2\"] You can also use group1,group2 to add a secondary grouping based on group2 within each group1 (only works for 2 groups) sample_name (text) \u2014 An R function to extract the sample name fromthe file stem (not including .tmad.txt part) Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.cnv"},{"location":"api/biopipen.ns.cnv/#biopipennscnv","text":"</> CNV/CNA-related processes, mostly tertiary analysis Classes AneuploidyScore ( Proc ) \u2014 Chromosomal arm SCNA/aneuploidy </> AneuploidyScoreSummary ( Proc ) \u2014 Summary table and plots from AneuploidyScore </> TMADScore ( Proc ) \u2014 Trimmed Median Absolute Deviation (TMAD) score for CNV </> TMADScoreSummary ( Proc ) \u2014 Summary table and plots for TMADScore </> class","title":"biopipen.ns.cnv"},{"location":"api/biopipen.ns.cnv/#biopipennscnvaneuploidyscore","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Chromosomal arm SCNA/aneuploidy The CAAs in this process are calculated using Cohen-Sharir method See https://github.com/quevedor2/aneuploidy_score Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input segfile \u2014 The seg file, generally including chrom, start, end andseg.mean (the log2 ratio). It is typically a tab-delimited file or a BED file. If so, envs.chrom_col, envs.start_col, envs.end_col and envs.seg_col are the 1st, 2nd, 3rd and 5th columns, respectively. It can also be a VCF file. If so, envs.chrom_col and envs.start_col are not required. end_col and envs.seg_col will be a field in the INFO column. VariantAnnotation is required to extract the INFO field. Output outdir \u2014 The output directory containing the CAAs, AS and a histogramplot to show the CAAs for each chromosome arm Envs chrom_col \u2014 The column name for chromosome cn_col \u2014 The column name for copy number cn_transform (type=auto) \u2014 A R function to transform seg.mean intocopy number, or a list of cutoffs to determine the copy number. See https://cnvkit.readthedocs.io/en/stable/pipeline.html#calling-methods. If this is give, cn_col will be ignored. end_col \u2014 The column name for end position excl_chroms (list) \u2014 The chromosomes to be excludedWorks with/without chr prefix. genome \u2014 The genome version, hg19 or hg38 seg_col \u2014 The column name for seg.mean segmean_transform (text) \u2014 A R function to transform seg.mean The transformed value will be used to calculate the CAAs start_col \u2014 The column name for start position threshold (type=float) \u2014 The threshold to determine whether a chromosomearm is gained or lost. wgd_gf (type=float) \u2014 The fraction of the genome that is affected by WGD Requires AneuploidyScore \u2014 check: {{proc.lang}} <(echo \"library(AneuploidyScore)\") ucsc.hg19.cytoband \u2014 if: {{ proc.envs.genome == 'hg19' }} check: {{proc.lang}} <(echo \"library(ucsc.hg19.cytoband)\") ucsc.hg38.cytoband \u2014 if: {{ proc.envs.genome == 'hg38' }} check: {{proc.lang}} <(echo \"library(ucsc.hg38.cytoband)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnv.AneuploidyScore"},{"location":"api/biopipen.ns.cnv/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnv/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnv/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnv/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnv/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnv/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnv/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnv/#biopipennscnvaneuploidyscoresummary","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Summary table and plots from AneuploidyScore Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input asdirs \u2014 The output directories from AneuploidyScore metafile \u2014 The metafile containing the sample information Output outdir \u2014 The output directory containing the summary table and plots Envs group_cols (type=auto) \u2014 The column name in the metafile to group thesamples. We also support multiple columns, e.g. [\"group1\", \"group2\"] You can also use group1,group2 to add a secondary grouping based on group2 within each group1 (only works for 2 groups) heatmap_cases (type=json) \u2014 The cases to be included in the heatmapBy default, all arms are included. If specified, keys are the names of the cases and values are the arms, which will be included in the heatmap. The list of arms should be a subset of chr<N>_p and chr<N>_q , where <N> is the chromosome number from 1 to 22, X, Y. You can also use ALL to include all arms. sample_name (text) \u2014 An R function to extract the sample name fromthe file stem (not including .aneuploidy_score part) Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnv.AneuploidyScoreSummary"},{"location":"api/biopipen.ns.cnv/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnv/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnv/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnv/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnv/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnv/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnv/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnv/#biopipennscnvtmadscore","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Trimmed Median Absolute Deviation (TMAD) score for CNV Reference: Mouliere, Chandrananda, Piskorz and Moore et al. Enhanced detection of circulating tumor DNA by fragment size analysis Science Translational Medicine (2018). Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input segfile \u2014 The seg file, two columns are required: * chrom: The chromosome name, used for filtering * seg.mean: The log2 ratio. It is typically a tab-delimited file or a BED file.If so, envs.chrom_col and envs.seg_col are the 1st and 5th columns, respectively. It can also be a VCF file. If so, envs.chrom_col and envs.start_col are not required. end_col and envs.seg_col will be a field in the INFO column. VariantAnnotation is required to extract the INFO field. Output outfile \u2014 The output file containing the TMAD score Envs chrom_col \u2014 The column name for chromosome excl_chroms (list) \u2014 The chromosomes to be excluded seg_col \u2014 The column name for seg.mean segmean_transform \u2014 The transformation function for seg.mean Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnv.TMADScore"},{"location":"api/biopipen.ns.cnv/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnv/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnv/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnv/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnv/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnv/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnv/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnv/#biopipennscnvtmadscoresummary","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Summary table and plots for TMADScore Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The metafile containing the sample informationThe first column must be the sample ID tmadfiles \u2014 The output files from TMADScore Output outdir \u2014 The output directory containing the summary table and plots Envs group_cols (type=auto) \u2014 The column name in the metafile to group thesamples Could also be a list of column names If not specified, samples will be plotted individually as a barplot We also support multiple columns, e.g. [\"group1\", \"group2\"] You can also use group1,group2 to add a secondary grouping based on group2 within each group1 (only works for 2 groups) sample_name (text) \u2014 An R function to extract the sample name fromthe file stem (not including .tmad.txt part) Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnv.TMADScoreSummary"},{"location":"api/biopipen.ns.cnv/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnv/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnv/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnv/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnv/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnv/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnv/#pipenprocprocrun_3","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/","text":"module biopipen.ns . cnvkit </> CNVkit commnads Classes CNVkitAccess ( Proc ) \u2014 Calculate the sequence-accessible coordinates in chromosomes from thegiven reference genome using cnvkit.py access </> CNVkitAutobin ( Proc ) \u2014 Quickly estimate read counts or depths in a BAM file to estimatereasonable on- and (if relevant) off-target bin sizes. </> CNVkitCoverage ( Proc ) \u2014 Run cnvkit coverage </> CNVkitReference ( Proc ) \u2014 Run cnvkit reference </> CNVkitFix ( Proc ) \u2014 Run cnvkit.py fix </> CNVkitSegment ( Proc ) \u2014 Run cnvkit.py segment </> CNVkitScatter ( Proc ) \u2014 Run cnvkit.py scatter </> CNVkitDiagram ( Proc ) \u2014 Run cnvkit.py diagram </> CNVkitHeatmap ( Proc ) \u2014 Run cnvkit.py heatmap for multiple cases </> CNVkitCall ( Proc ) \u2014 Run cnvkit.py call </> CNVkitBatch ( Proc ) \u2014 Run cnvkit batch </> CNVkitGuessBaits ( Proc ) \u2014 Guess the bait intervals from the bam files </> class biopipen.ns.cnvkit . CNVkitAccess ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate the sequence-accessible coordinates in chromosomes from thegiven reference genome using cnvkit.py access Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input excfiles \u2014 Additional regions to exclude, in BED format Output outfile \u2014 The output file Envs cnvkit \u2014 Path to cnvkit.py min_gap_size (type=int) \u2014 Minimum gap size between accessible sequenceregions ref \u2014 The reference genome fasta file Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitAutobin ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Quickly estimate read counts or depths in a BAM file to estimatereasonable on- and (if relevant) off-target bin sizes. Using cnvkit.py autobin . If multiple BAMs are given, use the BAM with median file size. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input accfile \u2014 The access file baitfile \u2014 Potentially targeted genomic regions.E.g. all possible exons for the reference genome. Format - BED, interval list, etc. bamfiles \u2014 The bamfiles Output antitarget_file \u2014 The antitarget BED output target_file \u2014 The target BED output Envs annotate \u2014 Use gene models from this file to assign names to the targetregions. Format: UCSC refFlat.txt or ensFlat.txt file (preferred), or BED, interval list, GFF, or similar. antitarget_max_size (type=int) \u2014 Maximum size of antitarget bins. antitarget_min_size (type=int) \u2014 Minimum size of antitarget bins. bp_per_bin (type=int) \u2014 Desired average number of sequencing read basesmapped to each bin. cnvkit \u2014 Path to cnvkit.py method (choice) \u2014 Sequencing protocol. Determines whether and how to useantitarget bins. - hybrid: Hybridization capture - amplicon: Targeted amplicon sequencing - wgs: Whole genome sequencing ref \u2014 The reference genome fasta file short_names (flag) \u2014 Reduce multi-accession bait labels tobe short and consistent. target_max_size (type=int) \u2014 Maximum size of target bins. target_min_size (type=int) \u2014 Minimum size of target bins. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitCoverage ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit coverage Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bamfile target_file \u2014 The target file or anti-target file Output outfile \u2014 The output coverage file Envs cnvkit \u2014 Path to cnvkit.py count (flag) \u2014 Get read depths by counting read midpointswithin each bin. (An alternative algorithm). min_mapq (type=int) \u2014 Minimum mapping quality to include a read. ncores (type=int) \u2014 Number of subprocesses to calculate coveragein parallel ref \u2014 The reference genome fasta file Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitReference ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit reference To genearte a reference file from normal samples, provide the cnn coverage files from the normal samples. To generate a flat reference file, provide the target/antitarget file. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input antitarget_file \u2014 Antitarget intervals (.bed or .list) covfiles \u2014 The coverage files from normal samples sample_sex \u2014 Specify the chromosomal sex of all given samples as male orfemale. Guess each sample from coverage of X and Y chromosomes if not given. target_file \u2014 Target intervals (.bed or .list) Output outfile \u2014 The reference cnn file Envs cluster (flag) \u2014 Calculate and store summary stats forclustered subsets of the normal samples with similar coverage profiles. cnvkit \u2014 Path to cnvkit.py male_reference (flag) \u2014 Create a male reference: shiftfemale samples chrX log-coverage by -1, so the reference chrX average is -1. Otherwise, shift male samples chrX by +1, so the reference chrX average is 0. min_cluster_size (type=int) \u2014 Minimum cluster size to keep in referenceprofiles. no_edge (flag) \u2014 Skip edge-effect correction. no_gc (flag) \u2014 Skip GC correction. no_rmask (flag) \u2014 Skip RepeatMasker correction. ref \u2014 The reference genome fasta file Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitFix ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py fix Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input antitarget_file \u2014 The antitarget file reference \u2014 The refence cnn file sample_id \u2014 Sample ID for target/antitarget files.Otherwise inferred from file names. target_file \u2014 The target file Output outfile \u2014 The fixed coverage files (.cnr) Envs cluster (flag) \u2014 Compare and use cluster-specific valuespresent in the reference profile. (requires envs.cluster=True for CNVkitReference ). cnvkit \u2014 Path to cnvkit.py no_edge (flag) \u2014 Skip edge-effect correction. no_gc (flag) \u2014 Skip GC correction. no_rmask (flag) \u2014 Skip RepeatMasker correction. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitSegment ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py segment For segmentation methods, see https://cnvkit.readthedocs.io/en/stable/pipeline.html#segmentation-methods Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cnrfile \u2014 The fixed coverage files (.cnr) normal_id \u2014 Corresponding normal sample ID in the input VCF.This sample is used to select only germline SNVs to plot b-allele frequencies. sample_id \u2014 Specify the name of the sample in the VCF to use for b-allelefrequency extraction and as the default plot title. vcf \u2014 VCF file name containing variants for segmentationby allele frequencies (optional). Output outfile \u2014 The segmentation file (.cns) Envs cnvkit \u2014 Path to cnvkit.py drop_low_coverage (flag) \u2014 Drop very-low-coverage binsbefore segmentation to avoid false-positive deletions in poor-quality tumor samples. drop_outliers (type=int) \u2014 Drop outlier bins more than this manymultiples of the 95th quantile away from the average within a rolling window. Set to 0 for no outlier filtering. method \u2014 Method to use for segmentation.Candidates - cbs, flasso, haar, none, hmm, hmm-tumor, hmm-germline min_variant_depth (type=int) \u2014 Minimum read depth for a SNV to bedisplayed in the b-allele frequency plot. ncores (type=int) \u2014 Number of subprocesses to segment in parallel.0 or negative for all available cores rscript \u2014 Path to Rscript smooth_cbs (flag) \u2014 Perform an additional smoothing beforeCBS segmentation, which in some cases may increase the sensitivity. Used only for CBS method. threshold \u2014 Significance threshold (p-value or FDR, depending on method)to accept breakpoints during segmentation. For HMM methods, this is the smoothing window size. zygosity_freq (type=float) \u2014 Ignore VCF's genotypes (GT field) andinstead infer zygosity from allele frequencies. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version r-DNAcopy \u2014 check: {{proc.envs.rscript}} <(echo \"library(DNAcopy)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitScatter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py scatter Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cnrfile \u2014 The fixed cnr file (.cnr) cnsfile \u2014 The segmentation file (.cns) normal_id \u2014 Corresponding normal sample ID in the input VCF.This sample is used to select only germline SNVs to plot b-allele frequencies. sample_id \u2014 Specify the name of the sample in the VCF to use for b-allelefrequency extraction and as the default plot title. vcf \u2014 VCF file name containing variants for segmentationby allele frequencies (optional). Output outdir \u2014 Output directory with plots for multiple cases Envs antitarget_marker (flag) \u2014 Plot antitargets using thissymbol when plotting in a selected chromosomal region (-g/--gene or -c/--chromosome). by_bin (flag) \u2014 Plot data x-coordinates by bin indicesinstead of genomic coordinates. All bins will be shown with equal width, no blank regions will be shown, and x-axis values indicate bin number (within chromosome) instead of genomic position. cases (type=json) \u2014 The cases for different plots with keys as case namesand values to overwrite the default args given by envs.<args> , including convert_args , by_bin , chromosome , gene , width antitarget_marker , segment_color , trend , y_max , y_min , min_variant_depth , zygosity_freq and title. By default, an all` case will be created with default arguments if no case specified chromosome \u2014 Chromosome or chromosomal range,e.g. 'chr1' or 'chr1:2333000-2444000', to display. If a range is given, all targeted genes in this range will be shown, unless -g/--gene is also given. cnvkit \u2014 Path to cnvkit.py convert \u2014 Path to convert to convert pdf to png file convert_args (ns) \u2014 The arguments for convert - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - : See convert -help and also: https://linux.die.net/man/1/convert gene \u2014 Name of gene or genes (comma-separated) to display. min_variant_depth (type=int) \u2014 Minimum read depth for a SNV to bedisplayed in the b-allele frequency plot. segment_color \u2014 Plot segment lines in this color. Value can beany string accepted by matplotlib, e.g. 'red' or '#CC0000'. title \u2014 Plot title. Sample ID if not provided. trend (flag) \u2014 Draw a smoothed local trendline on thescatter plot. width (type=int) \u2014 Width of margin to show around the selected gene(s)(-g/--gene) or small chromosomal region (-c/--chromosome). y_max (type=int) \u2014 y-axis upper limit. y_min (tyoe=int) \u2014 y-axis lower limit. zygosity_freq (typ=float) \u2014 Ignore VCF's genotypes (GT field) andinstead infer zygosity from allele frequencies. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version convert \u2014 check: {{proc.envs.convert}} -version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitDiagram ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py diagram Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cnrfile \u2014 The fixed cnr file (.cnr) cnsfile \u2014 The segmentation file (.cns) sample_sex \u2014 Specify the sample's chromosomal sex as male or female.(Otherwise guessed from X and Y coverage). Output outdir \u2014 Output directory with the scatter plots Envs cases (type=json) \u2014 The cases with keys as names and values as differentconfigs, including threshold , min_probes , male_reference , no_shift_xy and title cnvkit \u2014 Path to cnvkit.py convert \u2014 Path to convert to convert pdf to png file convert_args (ns) \u2014 The arguments for convert - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - : See convert -help and also: https://linux.die.net/man/1/convert male_reference (flag) \u2014 Assume inputs were normalized to amale reference (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). min_probes (type=int) \u2014 Minimum number of covered probes to label a gene. no_shift_xy (flag) \u2014 Don't adjust the X and Y chromosomesaccording to sample sex. threshold (type=float) \u2014 Copy number change threshold to label genes. title \u2014 Plot title. Sample ID if not provided. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version convert \u2014 check: {{proc.envs.convert}} -version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitHeatmap ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py heatmap for multiple cases Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sample_sex \u2014 Specify the chromosomal sex of all given samples as maleor female. Separated by comma. (Default: guess each sample from coverage of X and Y chromosomes). segfiles \u2014 Sample coverages as raw probes (.cnr) or segments (.cns). Output outdir \u2014 Output directory with heatmaps of multiple cases Envs by_bin (flag) \u2014 Plot data x-coordinates by bin indicesinstead of genomic coordinates. All bins will be shown with equal width, no blank regions will be shown, and x-axis values indicate bin number (within chromosome) instead of genomic position. cases (type=json) \u2014 The cases for different plots with keys as case namesand values to overwrite the default args given by envs.<args> , including convert_args , by_bin , chromosome , desaturate , male_reference , and, no_shift_xy . By default, an all case will be created with default arguments if no case specified chromosome \u2014 Chromosome (e.g. 'chr1') or chromosomal range(e.g. 'chr1:2333000-2444000') to display. cnvkit \u2014 Path to cnvkit.py convert \u2014 Path to convert to convert pdf to png file convert_args (ns) \u2014 The arguments for convert - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - : See convert -help and also: https://linux.die.net/man/1/convert desaturate (flag) \u2014 Tweak color saturation to focus onsignificant changes. male_reference (flag) \u2014 Assume inputs were normalized toa male reference. (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). no_shift_xy (flag) \u2014 Don't adjust the X and Y chromosomesaccording to sample sex. order \u2014 A file with sample names in the desired order. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version convert \u2014 check: {{proc.envs.convert}} -version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitCall ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py call Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cnrfile \u2014 The fixed cnr file (.cnr), used to generate VCF file cnsfile \u2014 The segmentation file (.cns) normal_id \u2014 Corresponding normal sample ID in the input VCF.This sample is used to select only germline SNVs to plot b-allele frequencies. purity \u2014 Estimated tumor cell fraction, a.k.a. purity or cellularity. sample_id \u2014 Specify the name of the sample in the VCF to use for b-allelefrequency extraction and as the default plot title. sample_sex \u2014 Specify the sample's chromosomal sex as male or female.(Otherwise guessed from X and Y coverage). vcf \u2014 VCF file name containing variants for segmentationby allele frequencies (optional). Output outdir \u2014 The output directory including the call file (.call.cns)bed file, and the vcf file Envs center \u2014 Re-center the log2 ratio values using this estimator ofthe center or average value. center_at (type=float) \u2014 Subtract a constant number from all log2 ratios.For \"manual\" re-centering, in case the --center option gives unsatisfactory results.) cnvkit \u2014 Path to cnvkit.py drop_low_coverage (flag) \u2014 Drop very-low-coverage binsbefore segmentation to avoid false-positive deletions in poor-quality tumor samples. filter \u2014 Merge segments flagged by the specifiedfilter(s) with the adjacent segment(s). male_reference (flag) \u2014 Assume inputs were normalized to amale reference. (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). method (choice) \u2014 Calling method (threshold, clonal or none). - threshold: Using hard thresholds for calling each integer copy number. Use thresholds to set a list of threshold log2 values for each copy number state - clonal: Rescaling and rounding. For a given known tumor cell fraction and normal ploidy, then simple rounding to the nearest integer copy number - none: Do not add a \u201ccn\u201d column or allele copy numbers. But still performs rescaling, re-centering, and extracting b-allele frequencies from a VCF (if requested). min_variant_depth (type=int) \u2014 Minimum read depth for a SNV to bedisplayed in the b-allele frequency plot. ploidy (type=float) \u2014 Ploidy of the sample cells. thresholds \u2014 Hard thresholds for calling each integer copy number,separated by commas. zygosity_freq (type=float) \u2014 Ignore VCF's genotypes (GT field) andinstead infer zygosity from allele frequencies. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitBatch ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit batch If you need in-depth control of the parameters, for example, multiple scatter plots in different regions, or you need to specify sample-sex for different samples, take a look at biopipen.ns.cnvkit_pipeline Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The meta data file containing the sample informationTwo columns BamFile and envs.type_col are required. The tumor samples should be labeled as envs.type_tumor and the normal samples should be labeled as envs.type_normal in the envs.type_col column. If normal samples are not found, a flat reference will be used. The could be other columns in the meta file, but they could be used in biopipen.ns.cnvkit_pipeline . Output outdir \u2014 The output directory Envs access \u2014 Regions of accessible sequence on chromosomes (.bed),as output by the 'access' command. access_excludes \u2014 Exclude these regions from the accessible genomeUsed when envs.access is not specified. access_min_gap_size \u2014 Minimum gap size between accessiblesequence regions if envs.access is not specified. annotate \u2014 Use gene models from this file to assign names to thetarget regions. Format: UCSC refFlat.txt or ensFlat.txt file (preferred), or BED, interval list, GFF, or similar. antitarget_avg_size \u2014 Average size of antitarget bins(results are approximate). antitarget_min_size \u2014 Minimum size of antitarget bins(smaller regions are dropped). antitargets \u2014 Anti-target intervals (.bed or .list) (optional for wgs) cluster \u2014 Calculate and use cluster-specific summary stats in thereference pool to normalize samples. cnvkit \u2014 Path to cnvkit.py count_reads \u2014 Get read depths by counting read midpoints within each bin.(An alternative algorithm). diagram \u2014 Create an ideogram of copy ratios on chromosomes as a PDF. drop_low_coverage \u2014 Drop very-low-coverage bins before segmentation toavoid false-positive deletions in poor-quality tumor samples. male_reference \u2014 Use or assume a male reference (i.e. female sampleswill have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). method \u2014 Sequencing assay type: hybridization capture ('hybrid'),targeted amplicon sequencing ('amplicon'), or whole genome sequencing ('wgs'). Determines whether and how to use antitarget bins. ncores \u2014 Number of subprocesses used to running each of the BAM filesin parallel ref \u2014 Path to a FASTA file containing the reference genome. reference \u2014 Copy number reference file (.cnn) to reuse rscript \u2014 Path to the Rscript excecutable to use for running R code.Use this option to specify a non-default R installation. scatter \u2014 Create a whole-genome copy ratio profile as a PDF scatter plot. segment_method \u2014 cbs,flasso,haar,none,hmm,hmm-tumor,hmm-germlineMethod used in the 'segment' step. short_names \u2014 Reduce multi-accession bait labels to be shortand consistent. target_avg_size \u2014 Average size of split target bins(results are approximate). targets \u2014 Target intervals (.bed or .list) (optional for wgs) type_col \u2014 type_col: The column name in the metafile thatindicates the sample type. type_normal \u2014 The type of normal samples in envs.type_col column of in.metafile type_tumor \u2014 The type of tumor samples in envs.type_col column of in.metafile Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version r-DNAcopy \u2014 check: {{proc.envs.rscript}} <(echo \"library(DNAcopy)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.cnvkit . CNVkitGuessBaits ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Guess the bait intervals from the bam files It runs scripts/guess_baits.py from the cnvkit repo. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input atfile \u2014 The potential target file or access filee.g. all known exons in the reference genome or from cnvkit.py access bamfiles \u2014 The bam files Output targetfile \u2014 The target file Envs cnvkit \u2014 Path to cnvkit.py guided (flag) \u2014 in.atfile is a potential target file when True , otherwise it is an access file. min_depth (type=int) \u2014 Minimum sequencing read depth to accept ascaptured. For guided only. min_gap (type=int) \u2014 Merge regions separated by gaps smaller than this. min_length (type=int) \u2014 Minimum region length to accept as captured. min_gap and min_length are for unguided only. ncores (type=int) \u2014 Number of subprocesses to segment in parallel 0 to use the maximum number of available CPUs. ref \u2014 Path to a FASTA file containing the reference genome. samtools \u2014 Path to samtools executable Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.cnvkit"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkit","text":"</> CNVkit commnads Classes CNVkitAccess ( Proc ) \u2014 Calculate the sequence-accessible coordinates in chromosomes from thegiven reference genome using cnvkit.py access </> CNVkitAutobin ( Proc ) \u2014 Quickly estimate read counts or depths in a BAM file to estimatereasonable on- and (if relevant) off-target bin sizes. </> CNVkitCoverage ( Proc ) \u2014 Run cnvkit coverage </> CNVkitReference ( Proc ) \u2014 Run cnvkit reference </> CNVkitFix ( Proc ) \u2014 Run cnvkit.py fix </> CNVkitSegment ( Proc ) \u2014 Run cnvkit.py segment </> CNVkitScatter ( Proc ) \u2014 Run cnvkit.py scatter </> CNVkitDiagram ( Proc ) \u2014 Run cnvkit.py diagram </> CNVkitHeatmap ( Proc ) \u2014 Run cnvkit.py heatmap for multiple cases </> CNVkitCall ( Proc ) \u2014 Run cnvkit.py call </> CNVkitBatch ( Proc ) \u2014 Run cnvkit batch </> CNVkitGuessBaits ( Proc ) \u2014 Guess the bait intervals from the bam files </> class","title":"biopipen.ns.cnvkit"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitaccess","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate the sequence-accessible coordinates in chromosomes from thegiven reference genome using cnvkit.py access Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input excfiles \u2014 Additional regions to exclude, in BED format Output outfile \u2014 The output file Envs cnvkit \u2014 Path to cnvkit.py min_gap_size (type=int) \u2014 Minimum gap size between accessible sequenceregions ref \u2014 The reference genome fasta file Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitAccess"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitautobin","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Quickly estimate read counts or depths in a BAM file to estimatereasonable on- and (if relevant) off-target bin sizes. Using cnvkit.py autobin . If multiple BAMs are given, use the BAM with median file size. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input accfile \u2014 The access file baitfile \u2014 Potentially targeted genomic regions.E.g. all possible exons for the reference genome. Format - BED, interval list, etc. bamfiles \u2014 The bamfiles Output antitarget_file \u2014 The antitarget BED output target_file \u2014 The target BED output Envs annotate \u2014 Use gene models from this file to assign names to the targetregions. Format: UCSC refFlat.txt or ensFlat.txt file (preferred), or BED, interval list, GFF, or similar. antitarget_max_size (type=int) \u2014 Maximum size of antitarget bins. antitarget_min_size (type=int) \u2014 Minimum size of antitarget bins. bp_per_bin (type=int) \u2014 Desired average number of sequencing read basesmapped to each bin. cnvkit \u2014 Path to cnvkit.py method (choice) \u2014 Sequencing protocol. Determines whether and how to useantitarget bins. - hybrid: Hybridization capture - amplicon: Targeted amplicon sequencing - wgs: Whole genome sequencing ref \u2014 The reference genome fasta file short_names (flag) \u2014 Reduce multi-accession bait labels tobe short and consistent. target_max_size (type=int) \u2014 Maximum size of target bins. target_min_size (type=int) \u2014 Minimum size of target bins. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitAutobin"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitcoverage","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit coverage Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input bamfile \u2014 The bamfile target_file \u2014 The target file or anti-target file Output outfile \u2014 The output coverage file Envs cnvkit \u2014 Path to cnvkit.py count (flag) \u2014 Get read depths by counting read midpointswithin each bin. (An alternative algorithm). min_mapq (type=int) \u2014 Minimum mapping quality to include a read. ncores (type=int) \u2014 Number of subprocesses to calculate coveragein parallel ref \u2014 The reference genome fasta file Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitCoverage"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitreference","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit reference To genearte a reference file from normal samples, provide the cnn coverage files from the normal samples. To generate a flat reference file, provide the target/antitarget file. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input antitarget_file \u2014 Antitarget intervals (.bed or .list) covfiles \u2014 The coverage files from normal samples sample_sex \u2014 Specify the chromosomal sex of all given samples as male orfemale. Guess each sample from coverage of X and Y chromosomes if not given. target_file \u2014 Target intervals (.bed or .list) Output outfile \u2014 The reference cnn file Envs cluster (flag) \u2014 Calculate and store summary stats forclustered subsets of the normal samples with similar coverage profiles. cnvkit \u2014 Path to cnvkit.py male_reference (flag) \u2014 Create a male reference: shiftfemale samples chrX log-coverage by -1, so the reference chrX average is -1. Otherwise, shift male samples chrX by +1, so the reference chrX average is 0. min_cluster_size (type=int) \u2014 Minimum cluster size to keep in referenceprofiles. no_edge (flag) \u2014 Skip edge-effect correction. no_gc (flag) \u2014 Skip GC correction. no_rmask (flag) \u2014 Skip RepeatMasker correction. ref \u2014 The reference genome fasta file Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitReference"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitfix","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py fix Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input antitarget_file \u2014 The antitarget file reference \u2014 The refence cnn file sample_id \u2014 Sample ID for target/antitarget files.Otherwise inferred from file names. target_file \u2014 The target file Output outfile \u2014 The fixed coverage files (.cnr) Envs cluster (flag) \u2014 Compare and use cluster-specific valuespresent in the reference profile. (requires envs.cluster=True for CNVkitReference ). cnvkit \u2014 Path to cnvkit.py no_edge (flag) \u2014 Skip edge-effect correction. no_gc (flag) \u2014 Skip GC correction. no_rmask (flag) \u2014 Skip RepeatMasker correction. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitFix"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitsegment","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py segment For segmentation methods, see https://cnvkit.readthedocs.io/en/stable/pipeline.html#segmentation-methods Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cnrfile \u2014 The fixed coverage files (.cnr) normal_id \u2014 Corresponding normal sample ID in the input VCF.This sample is used to select only germline SNVs to plot b-allele frequencies. sample_id \u2014 Specify the name of the sample in the VCF to use for b-allelefrequency extraction and as the default plot title. vcf \u2014 VCF file name containing variants for segmentationby allele frequencies (optional). Output outfile \u2014 The segmentation file (.cns) Envs cnvkit \u2014 Path to cnvkit.py drop_low_coverage (flag) \u2014 Drop very-low-coverage binsbefore segmentation to avoid false-positive deletions in poor-quality tumor samples. drop_outliers (type=int) \u2014 Drop outlier bins more than this manymultiples of the 95th quantile away from the average within a rolling window. Set to 0 for no outlier filtering. method \u2014 Method to use for segmentation.Candidates - cbs, flasso, haar, none, hmm, hmm-tumor, hmm-germline min_variant_depth (type=int) \u2014 Minimum read depth for a SNV to bedisplayed in the b-allele frequency plot. ncores (type=int) \u2014 Number of subprocesses to segment in parallel.0 or negative for all available cores rscript \u2014 Path to Rscript smooth_cbs (flag) \u2014 Perform an additional smoothing beforeCBS segmentation, which in some cases may increase the sensitivity. Used only for CBS method. threshold \u2014 Significance threshold (p-value or FDR, depending on method)to accept breakpoints during segmentation. For HMM methods, this is the smoothing window size. zygosity_freq (type=float) \u2014 Ignore VCF's genotypes (GT field) andinstead infer zygosity from allele frequencies. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version r-DNAcopy \u2014 check: {{proc.envs.rscript}} <(echo \"library(DNAcopy)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitSegment"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_5","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitscatter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py scatter Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cnrfile \u2014 The fixed cnr file (.cnr) cnsfile \u2014 The segmentation file (.cns) normal_id \u2014 Corresponding normal sample ID in the input VCF.This sample is used to select only germline SNVs to plot b-allele frequencies. sample_id \u2014 Specify the name of the sample in the VCF to use for b-allelefrequency extraction and as the default plot title. vcf \u2014 VCF file name containing variants for segmentationby allele frequencies (optional). Output outdir \u2014 Output directory with plots for multiple cases Envs antitarget_marker (flag) \u2014 Plot antitargets using thissymbol when plotting in a selected chromosomal region (-g/--gene or -c/--chromosome). by_bin (flag) \u2014 Plot data x-coordinates by bin indicesinstead of genomic coordinates. All bins will be shown with equal width, no blank regions will be shown, and x-axis values indicate bin number (within chromosome) instead of genomic position. cases (type=json) \u2014 The cases for different plots with keys as case namesand values to overwrite the default args given by envs.<args> , including convert_args , by_bin , chromosome , gene , width antitarget_marker , segment_color , trend , y_max , y_min , min_variant_depth , zygosity_freq and title. By default, an all` case will be created with default arguments if no case specified chromosome \u2014 Chromosome or chromosomal range,e.g. 'chr1' or 'chr1:2333000-2444000', to display. If a range is given, all targeted genes in this range will be shown, unless -g/--gene is also given. cnvkit \u2014 Path to cnvkit.py convert \u2014 Path to convert to convert pdf to png file convert_args (ns) \u2014 The arguments for convert - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - : See convert -help and also: https://linux.die.net/man/1/convert gene \u2014 Name of gene or genes (comma-separated) to display. min_variant_depth (type=int) \u2014 Minimum read depth for a SNV to bedisplayed in the b-allele frequency plot. segment_color \u2014 Plot segment lines in this color. Value can beany string accepted by matplotlib, e.g. 'red' or '#CC0000'. title \u2014 Plot title. Sample ID if not provided. trend (flag) \u2014 Draw a smoothed local trendline on thescatter plot. width (type=int) \u2014 Width of margin to show around the selected gene(s)(-g/--gene) or small chromosomal region (-c/--chromosome). y_max (type=int) \u2014 y-axis upper limit. y_min (tyoe=int) \u2014 y-axis lower limit. zygosity_freq (typ=float) \u2014 Ignore VCF's genotypes (GT field) andinstead infer zygosity from allele frequencies. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version convert \u2014 check: {{proc.envs.convert}} -version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitScatter"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_6","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_6","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_6","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_6","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_6","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_6","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_6","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitdiagram","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py diagram Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cnrfile \u2014 The fixed cnr file (.cnr) cnsfile \u2014 The segmentation file (.cns) sample_sex \u2014 Specify the sample's chromosomal sex as male or female.(Otherwise guessed from X and Y coverage). Output outdir \u2014 Output directory with the scatter plots Envs cases (type=json) \u2014 The cases with keys as names and values as differentconfigs, including threshold , min_probes , male_reference , no_shift_xy and title cnvkit \u2014 Path to cnvkit.py convert \u2014 Path to convert to convert pdf to png file convert_args (ns) \u2014 The arguments for convert - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - : See convert -help and also: https://linux.die.net/man/1/convert male_reference (flag) \u2014 Assume inputs were normalized to amale reference (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). min_probes (type=int) \u2014 Minimum number of covered probes to label a gene. no_shift_xy (flag) \u2014 Don't adjust the X and Y chromosomesaccording to sample sex. threshold (type=float) \u2014 Copy number change threshold to label genes. title \u2014 Plot title. Sample ID if not provided. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version convert \u2014 check: {{proc.envs.convert}} -version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitDiagram"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_7","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_7","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_7","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_7","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_7","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_7","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_7","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitheatmap","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py heatmap for multiple cases Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sample_sex \u2014 Specify the chromosomal sex of all given samples as maleor female. Separated by comma. (Default: guess each sample from coverage of X and Y chromosomes). segfiles \u2014 Sample coverages as raw probes (.cnr) or segments (.cns). Output outdir \u2014 Output directory with heatmaps of multiple cases Envs by_bin (flag) \u2014 Plot data x-coordinates by bin indicesinstead of genomic coordinates. All bins will be shown with equal width, no blank regions will be shown, and x-axis values indicate bin number (within chromosome) instead of genomic position. cases (type=json) \u2014 The cases for different plots with keys as case namesand values to overwrite the default args given by envs.<args> , including convert_args , by_bin , chromosome , desaturate , male_reference , and, no_shift_xy . By default, an all case will be created with default arguments if no case specified chromosome \u2014 Chromosome (e.g. 'chr1') or chromosomal range(e.g. 'chr1:2333000-2444000') to display. cnvkit \u2014 Path to cnvkit.py convert \u2014 Path to convert to convert pdf to png file convert_args (ns) \u2014 The arguments for convert - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - : See convert -help and also: https://linux.die.net/man/1/convert desaturate (flag) \u2014 Tweak color saturation to focus onsignificant changes. male_reference (flag) \u2014 Assume inputs were normalized toa male reference. (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). no_shift_xy (flag) \u2014 Don't adjust the X and Y chromosomesaccording to sample sex. order \u2014 A file with sample names in the desired order. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version convert \u2014 check: {{proc.envs.convert}} -version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitHeatmap"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_8","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_8","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_8","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_8","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_8","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_8","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_8","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitcall","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit.py call Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cnrfile \u2014 The fixed cnr file (.cnr), used to generate VCF file cnsfile \u2014 The segmentation file (.cns) normal_id \u2014 Corresponding normal sample ID in the input VCF.This sample is used to select only germline SNVs to plot b-allele frequencies. purity \u2014 Estimated tumor cell fraction, a.k.a. purity or cellularity. sample_id \u2014 Specify the name of the sample in the VCF to use for b-allelefrequency extraction and as the default plot title. sample_sex \u2014 Specify the sample's chromosomal sex as male or female.(Otherwise guessed from X and Y coverage). vcf \u2014 VCF file name containing variants for segmentationby allele frequencies (optional). Output outdir \u2014 The output directory including the call file (.call.cns)bed file, and the vcf file Envs center \u2014 Re-center the log2 ratio values using this estimator ofthe center or average value. center_at (type=float) \u2014 Subtract a constant number from all log2 ratios.For \"manual\" re-centering, in case the --center option gives unsatisfactory results.) cnvkit \u2014 Path to cnvkit.py drop_low_coverage (flag) \u2014 Drop very-low-coverage binsbefore segmentation to avoid false-positive deletions in poor-quality tumor samples. filter \u2014 Merge segments flagged by the specifiedfilter(s) with the adjacent segment(s). male_reference (flag) \u2014 Assume inputs were normalized to amale reference. (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). method (choice) \u2014 Calling method (threshold, clonal or none). - threshold: Using hard thresholds for calling each integer copy number. Use thresholds to set a list of threshold log2 values for each copy number state - clonal: Rescaling and rounding. For a given known tumor cell fraction and normal ploidy, then simple rounding to the nearest integer copy number - none: Do not add a \u201ccn\u201d column or allele copy numbers. But still performs rescaling, re-centering, and extracting b-allele frequencies from a VCF (if requested). min_variant_depth (type=int) \u2014 Minimum read depth for a SNV to bedisplayed in the b-allele frequency plot. ploidy (type=float) \u2014 Ploidy of the sample cells. thresholds \u2014 Hard thresholds for calling each integer copy number,separated by commas. zygosity_freq (type=float) \u2014 Ignore VCF's genotypes (GT field) andinstead infer zygosity from allele frequencies. Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitCall"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_9","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_9","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_9","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_9","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_9","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_9","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_9","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitbatch","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run cnvkit batch If you need in-depth control of the parameters, for example, multiple scatter plots in different regions, or you need to specify sample-sex for different samples, take a look at biopipen.ns.cnvkit_pipeline Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The meta data file containing the sample informationTwo columns BamFile and envs.type_col are required. The tumor samples should be labeled as envs.type_tumor and the normal samples should be labeled as envs.type_normal in the envs.type_col column. If normal samples are not found, a flat reference will be used. The could be other columns in the meta file, but they could be used in biopipen.ns.cnvkit_pipeline . Output outdir \u2014 The output directory Envs access \u2014 Regions of accessible sequence on chromosomes (.bed),as output by the 'access' command. access_excludes \u2014 Exclude these regions from the accessible genomeUsed when envs.access is not specified. access_min_gap_size \u2014 Minimum gap size between accessiblesequence regions if envs.access is not specified. annotate \u2014 Use gene models from this file to assign names to thetarget regions. Format: UCSC refFlat.txt or ensFlat.txt file (preferred), or BED, interval list, GFF, or similar. antitarget_avg_size \u2014 Average size of antitarget bins(results are approximate). antitarget_min_size \u2014 Minimum size of antitarget bins(smaller regions are dropped). antitargets \u2014 Anti-target intervals (.bed or .list) (optional for wgs) cluster \u2014 Calculate and use cluster-specific summary stats in thereference pool to normalize samples. cnvkit \u2014 Path to cnvkit.py count_reads \u2014 Get read depths by counting read midpoints within each bin.(An alternative algorithm). diagram \u2014 Create an ideogram of copy ratios on chromosomes as a PDF. drop_low_coverage \u2014 Drop very-low-coverage bins before segmentation toavoid false-positive deletions in poor-quality tumor samples. male_reference \u2014 Use or assume a male reference (i.e. female sampleswill have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). method \u2014 Sequencing assay type: hybridization capture ('hybrid'),targeted amplicon sequencing ('amplicon'), or whole genome sequencing ('wgs'). Determines whether and how to use antitarget bins. ncores \u2014 Number of subprocesses used to running each of the BAM filesin parallel ref \u2014 Path to a FASTA file containing the reference genome. reference \u2014 Copy number reference file (.cnn) to reuse rscript \u2014 Path to the Rscript excecutable to use for running R code.Use this option to specify a non-default R installation. scatter \u2014 Create a whole-genome copy ratio profile as a PDF scatter plot. segment_method \u2014 cbs,flasso,haar,none,hmm,hmm-tumor,hmm-germlineMethod used in the 'segment' step. short_names \u2014 Reduce multi-accession bait labels to be shortand consistent. target_avg_size \u2014 Average size of split target bins(results are approximate). targets \u2014 Target intervals (.bed or .list) (optional for wgs) type_col \u2014 type_col: The column name in the metafile thatindicates the sample type. type_normal \u2014 The type of normal samples in envs.type_col column of in.metafile type_tumor \u2014 The type of tumor samples in envs.type_col column of in.metafile Requires cnvkit \u2014 check: {{proc.envs.cnvkit}} version r-DNAcopy \u2014 check: {{proc.envs.rscript}} <(echo \"library(DNAcopy)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitBatch"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_10","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_10","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_10","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_10","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_10","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_10","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_10","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit/#biopipennscnvkitcnvkitguessbaits","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Guess the bait intervals from the bam files It runs scripts/guess_baits.py from the cnvkit repo. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input atfile \u2014 The potential target file or access filee.g. all known exons in the reference genome or from cnvkit.py access bamfiles \u2014 The bam files Output targetfile \u2014 The target file Envs cnvkit \u2014 Path to cnvkit.py guided (flag) \u2014 in.atfile is a potential target file when True , otherwise it is an access file. min_depth (type=int) \u2014 Minimum sequencing read depth to accept ascaptured. For guided only. min_gap (type=int) \u2014 Merge regions separated by gaps smaller than this. min_length (type=int) \u2014 Minimum region length to accept as captured. min_gap and min_length are for unguided only. ncores (type=int) \u2014 Number of subprocesses to segment in parallel 0 to use the maximum number of available CPUs. ref \u2014 Path to a FASTA file containing the reference genome. samtools \u2014 Path to samtools executable Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.cnvkit.CNVkitGuessBaits"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocmeta_11","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocfrom_proc_11","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_subclass_11","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocinit_11","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocgc_11","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.cnvkit/#pipenprocproclog_11","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.cnvkit/#pipenprocprocrun_11","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.cnvkit_pipeline/","text":"module biopipen.ns . cnvkit_pipeline </> The CNVkit pipeline. Classes CNVkitPipeline \u2014 The CNVkit pipeline </> class biopipen.ns.cnvkit_pipeline . CNVkitPipeline ( *args , **kwds ) </> Bases pipen_args.procgroup.ProcGroup pipen.procgroup.ProcGroup The CNVkit pipeline Unlike cnvkit.py batch , this decouples the steps of the batch command so that we can control the details of each step. Options for different processes can be specified by [CNVkitXXX.envs.xxx] See biopipen.ns.cnvkit.CNVkitXXX for more details. To run this pipeline from command line, with the pipen-run plugin: >>> # In this case, `pipeline.cnvkit_pipeline.metafile` must be provided >>> pipen run cnvkit_pipeline CNVkitPipeline < other pipeline args > To use this as a dependency for other pipelines - >>> from biopipen.ns.cnvkit_pipeline import CNVkitPipeline >>> pipeline = CNVkitPipeline ( < options > ) >>> # pipeline.starts: Start processes of the pipeline >>> # pipeline.ends: End processes of the pipeline >>> # pipeline.procs.<proc>: The process with name <proc> See also the docs for details https://pwwang.github.io/biopipen/pipelines/cnvkit_pipeline/ Attributes parser \u2014 Pass arguments to initialize the parser The parser is a singleton and by default initalized at plugin.on_init() hook, which happens usually after the initialization of a process group. </> Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) (Pipen) \u2014 Convert the pipeline to a Pipen instance </> post_init ( ) \u2014 Post initialization </> class pipen.procgroup. ProcGropuMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod __init_subclass__ ( ) </> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod add_proc ( self_or_method , proc=None ) </> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method as_pipen ( name=None , desc=None , outdir=None , **kwargs ) </> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | os.pathlike | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns (Pipen) The Pipen instance method post_init ( ) </> Post initialization This method is called after arguments are parsed and set to self.opts This method is called before runtime processes are loaded","title":"biopipen.ns.cnvkit_pipeline"},{"location":"api/biopipen.ns.cnvkit_pipeline/#biopipennscnvkit_pipeline","text":"</> The CNVkit pipeline. Classes CNVkitPipeline \u2014 The CNVkit pipeline </> class","title":"biopipen.ns.cnvkit_pipeline"},{"location":"api/biopipen.ns.cnvkit_pipeline/#biopipennscnvkit_pipelinecnvkitpipeline","text":"</> Bases pipen_args.procgroup.ProcGroup pipen.procgroup.ProcGroup The CNVkit pipeline Unlike cnvkit.py batch , this decouples the steps of the batch command so that we can control the details of each step. Options for different processes can be specified by [CNVkitXXX.envs.xxx] See biopipen.ns.cnvkit.CNVkitXXX for more details. To run this pipeline from command line, with the pipen-run plugin: >>> # In this case, `pipeline.cnvkit_pipeline.metafile` must be provided >>> pipen run cnvkit_pipeline CNVkitPipeline < other pipeline args > To use this as a dependency for other pipelines - >>> from biopipen.ns.cnvkit_pipeline import CNVkitPipeline >>> pipeline = CNVkitPipeline ( < options > ) >>> # pipeline.starts: Start processes of the pipeline >>> # pipeline.ends: End processes of the pipeline >>> # pipeline.procs.<proc>: The process with name <proc> See also the docs for details https://pwwang.github.io/biopipen/pipelines/cnvkit_pipeline/ Attributes parser \u2014 Pass arguments to initialize the parser The parser is a singleton and by default initalized at plugin.on_init() hook, which happens usually after the initialization of a process group. </> Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) (Pipen) \u2014 Convert the pipeline to a Pipen instance </> post_init ( ) \u2014 Post initialization </> class","title":"biopipen.ns.cnvkit_pipeline.CNVkitPipeline"},{"location":"api/biopipen.ns.cnvkit_pipeline/#pipenprocgroupprocgropumeta","text":"</> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod","title":"pipen.procgroup.ProcGropuMeta"},{"location":"api/biopipen.ns.cnvkit_pipeline/#pipenprocgroupprocgroupinit_subclass","text":"</> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod","title":"pipen.procgroup.ProcGroup.init_subclass"},{"location":"api/biopipen.ns.cnvkit_pipeline/#pipenprocgroupprocgroupadd_proc","text":"</> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method","title":"pipen.procgroup.ProcGroup.add_proc"},{"location":"api/biopipen.ns.cnvkit_pipeline/#pipenprocgroupprocgroupas_pipen","text":"</> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | os.pathlike | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns (Pipen) The Pipen instance method","title":"pipen.procgroup.ProcGroup.as_pipen"},{"location":"api/biopipen.ns.cnvkit_pipeline/#pipen_argsprocgroupprocgrouppost_init","text":"</> Post initialization This method is called after arguments are parsed and set to self.opts This method is called before runtime processes are loaded","title":"pipen_args.procgroup.ProcGroup.post_init"},{"location":"api/biopipen.ns.delim/","text":"module biopipen.ns . delim </> Tools to deal with csv/tsv files Classes RowsBinder ( Proc ) \u2014 Bind rows of input files </> SampleInfo ( Proc ) \u2014 List sample information and perform statistics </> class biopipen.ns.delim . RowsBinder ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Bind rows of input files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infiles \u2014 The input files to bind.The input files should have the same number of columns, and same delimiter. Output outfile \u2014 The output file with rows bound Envs filenames \u2014 Whether to add filename as the last column.Either a string of an R function that starts with function or a list of names (or string separated by comma) to add for each input file. The R function takes the path of the input file as the only argument and should return a string. The string will be added as the last column of the output file. filenames_col \u2014 The column name for the filenames columns header (flag) \u2014 Whether the input files have header sep \u2014 The separator of the input files Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.delim . SampleInfo ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc List sample information and perform statistics Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file to list sample informationThe input file should be a csv/tsv file with header Output outfile \u2014 The output file with sample information, with mutated columnsif envs.save_mutated is True. The basename of the output file will be the same as the input file. The file name of each plot will be slugified from the case name. Each plot has 3 formats: pdf, png and code.zip, which contains the data and R code to reproduce the plot. Envs defaults (ns) \u2014 The default parameters for envs.stats . - plot_type: The type of the plot. See the supported plot types here: https://pwwang.github.io/plotthis/reference/index.html The plot_type should be lower case and the plot function used in plotthis should be used. The mapping from plot_type to the plot function is like bar -> BarPlot , box -> BoxPlot , etc. - more_formats (list): The additional formats to save the plot. By default, the plot will be saved in png, which is also used to display in the report. You can add more formats to save the plot. For example, more_formats = [\"pdf\", \"svg\"] . - save_code (flag): Whether to save the R code to reproduce the plot. The data used to plot will also be saved. - subset: An expression to subset the data frame before plotting. The expression should be a string of R expression that will be passed to dplyr::filter . For example, subset = \"Sample == 'A'\" . - section: The section name in the report. In case you want to group the plots in the report. - devpars (ns): The device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - descr: The description of the plot, shown in the report. - : You can add more parameters to the defaults. These parameters will be expanded to the envs.stats for each case, and passed to individual plot functions. exclude_cols (auto) \u2014 The columns to exclude in the table in the report.Could be a list or a string separated by comma. mutaters (type=json) \u2014 A dict of mutaters to mutate the data frame.The key is the column name and the value is the R expression to mutate the column. The dict will be transformed to a list in R and passed to dplyr::mutate . You may also use paired() to identify paired samples. The function takes following arguments: * df : The data frame. Use . if the function is called in a dplyr pipe. * id_col : The column name in df for the ids to be returned in the final output. * compare_col : The column name in df to compare the values for each id in id_col . * idents : The values in compare_col to compare. It could be either an an integer or a vector. If it is an integer, the number of values in compare_col must be the same as the integer for the id to be regarded as paired. If it is a vector, the values in compare_col must be the same as the values in idents for the id to be regarded as paired. * uniq : Whether to return unique ids or not. Default is TRUE . If FALSE , you can mutate the meta data frame with the returned ids. Non-paired ids will be NA . save_mutated (flag) \u2014 Whether to save the mutated columns. sep \u2014 The separator of the input file. stats (type=json) \u2014 The statistics to perform.The keys are the case names and the values are the parameters inheirted from envs.defaults . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.delim"},{"location":"api/biopipen.ns.delim/#biopipennsdelim","text":"</> Tools to deal with csv/tsv files Classes RowsBinder ( Proc ) \u2014 Bind rows of input files </> SampleInfo ( Proc ) \u2014 List sample information and perform statistics </> class","title":"biopipen.ns.delim"},{"location":"api/biopipen.ns.delim/#biopipennsdelimrowsbinder","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Bind rows of input files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infiles \u2014 The input files to bind.The input files should have the same number of columns, and same delimiter. Output outfile \u2014 The output file with rows bound Envs filenames \u2014 Whether to add filename as the last column.Either a string of an R function that starts with function or a list of names (or string separated by comma) to add for each input file. The R function takes the path of the input file as the only argument and should return a string. The string will be added as the last column of the output file. filenames_col \u2014 The column name for the filenames columns header (flag) \u2014 Whether the input files have header sep \u2014 The separator of the input files Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.delim.RowsBinder"},{"location":"api/biopipen.ns.delim/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.delim/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.delim/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.delim/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.delim/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.delim/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.delim/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.delim/#biopipennsdelimsampleinfo","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc List sample information and perform statistics Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file to list sample informationThe input file should be a csv/tsv file with header Output outfile \u2014 The output file with sample information, with mutated columnsif envs.save_mutated is True. The basename of the output file will be the same as the input file. The file name of each plot will be slugified from the case name. Each plot has 3 formats: pdf, png and code.zip, which contains the data and R code to reproduce the plot. Envs defaults (ns) \u2014 The default parameters for envs.stats . - plot_type: The type of the plot. See the supported plot types here: https://pwwang.github.io/plotthis/reference/index.html The plot_type should be lower case and the plot function used in plotthis should be used. The mapping from plot_type to the plot function is like bar -> BarPlot , box -> BoxPlot , etc. - more_formats (list): The additional formats to save the plot. By default, the plot will be saved in png, which is also used to display in the report. You can add more formats to save the plot. For example, more_formats = [\"pdf\", \"svg\"] . - save_code (flag): Whether to save the R code to reproduce the plot. The data used to plot will also be saved. - subset: An expression to subset the data frame before plotting. The expression should be a string of R expression that will be passed to dplyr::filter . For example, subset = \"Sample == 'A'\" . - section: The section name in the report. In case you want to group the plots in the report. - devpars (ns): The device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - descr: The description of the plot, shown in the report. - : You can add more parameters to the defaults. These parameters will be expanded to the envs.stats for each case, and passed to individual plot functions. exclude_cols (auto) \u2014 The columns to exclude in the table in the report.Could be a list or a string separated by comma. mutaters (type=json) \u2014 A dict of mutaters to mutate the data frame.The key is the column name and the value is the R expression to mutate the column. The dict will be transformed to a list in R and passed to dplyr::mutate . You may also use paired() to identify paired samples. The function takes following arguments: * df : The data frame. Use . if the function is called in a dplyr pipe. * id_col : The column name in df for the ids to be returned in the final output. * compare_col : The column name in df to compare the values for each id in id_col . * idents : The values in compare_col to compare. It could be either an an integer or a vector. If it is an integer, the number of values in compare_col must be the same as the integer for the id to be regarded as paired. If it is a vector, the values in compare_col must be the same as the values in idents for the id to be regarded as paired. * uniq : Whether to return unique ids or not. Default is TRUE . If FALSE , you can mutate the meta data frame with the returned ids. Non-paired ids will be NA . save_mutated (flag) \u2014 Whether to save the mutated columns. sep \u2014 The separator of the input file. stats (type=json) \u2014 The statistics to perform.The keys are the case names and the values are the parameters inheirted from envs.defaults . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.delim.SampleInfo"},{"location":"api/biopipen.ns.delim/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.delim/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.delim/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.delim/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.delim/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.delim/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.delim/#pipenprocprocrun_1","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.gene/","text":"module biopipen.ns . gene </> Gene related processes Classes GeneNameConversion ( Proc ) \u2014 Convert gene names back and forth using MyGeneInfo </> GenePromoters ( Proc ) \u2014 Get gene promoter regions by specifying the flanking regions of TSS </> class biopipen.ns.gene . GeneNameConversion ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert gene names back and forth using MyGeneInfo Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file with original gene namesIt should be a tab-separated file with header Output outfile \u2014 The output file with converted gene names Envs dup (choice) \u2014 What to do if a conversion results in multiple names. - first: Use the first name, sorted by matching score descendingly (default) - last: Use the last name, sorted by matching score descendingly - combine: Combine all names using ; as separator genecol \u2014 The index (1-based) or name of the column where genes are present infmt \u2014 What's the original gene name formatAvailable fields https://docs.mygene.info/en/latest/doc/query_service.html#available-fields notfound (choice) \u2014 What to do if a conversion cannot be done. - use-query: Ignore the conversion and use the original name - skip: Ignore the conversion and skip the entire row in input file - ignore: Same as skip - error: Report error - na: Use NA outfmt \u2014 What's the target gene name format. Currently only a single formatis supported. output (choice) \u2014 How to output. - append: Add the converted names as new columns at the end using envs.outfmt as the column name. - replace: Drop the original name column, and insert the converted names at the original position. - converted: Only keep the converted names. - with-query: Output 2 columns with original and converted names. species \u2014 Limit gene query to certain species.Supported: human, mouse, rat, fruitfly, nematode, zebrafish, thale-cress, frog and pig Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.gene . GenePromoters ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Get gene promoter regions by specifying the flanking regions of TSS Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file with gene ids/names Output outfile \u2014 The output file with promoter regions in BED format Envs chrsize \u2014 The chromosome size file, from which the chromosome order isused to sort the output down (type=int) \u2014 The downstream distance from TSSIf not specified, the default is envs.up genecol (type=int) \u2014 The index (1-based) of the gene column header (flag) \u2014 Whether the input file has a header match_id (flag) \u2014 Should we match the genes in in.infile by gene_id instead of gene_name in envs.refgene notfound (choice) \u2014 What to do if a gene is not found. - skip: Skip the gene - error: Report error refgene \u2014 The reference gene annotation file in GTF format sort (flag) \u2014 Sort the output by chromosome and start position up (type=int) \u2014 The upstream distance from TSS Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.gene"},{"location":"api/biopipen.ns.gene/#biopipennsgene","text":"</> Gene related processes Classes GeneNameConversion ( Proc ) \u2014 Convert gene names back and forth using MyGeneInfo </> GenePromoters ( Proc ) \u2014 Get gene promoter regions by specifying the flanking regions of TSS </> class","title":"biopipen.ns.gene"},{"location":"api/biopipen.ns.gene/#biopipennsgenegenenameconversion","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert gene names back and forth using MyGeneInfo Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file with original gene namesIt should be a tab-separated file with header Output outfile \u2014 The output file with converted gene names Envs dup (choice) \u2014 What to do if a conversion results in multiple names. - first: Use the first name, sorted by matching score descendingly (default) - last: Use the last name, sorted by matching score descendingly - combine: Combine all names using ; as separator genecol \u2014 The index (1-based) or name of the column where genes are present infmt \u2014 What's the original gene name formatAvailable fields https://docs.mygene.info/en/latest/doc/query_service.html#available-fields notfound (choice) \u2014 What to do if a conversion cannot be done. - use-query: Ignore the conversion and use the original name - skip: Ignore the conversion and skip the entire row in input file - ignore: Same as skip - error: Report error - na: Use NA outfmt \u2014 What's the target gene name format. Currently only a single formatis supported. output (choice) \u2014 How to output. - append: Add the converted names as new columns at the end using envs.outfmt as the column name. - replace: Drop the original name column, and insert the converted names at the original position. - converted: Only keep the converted names. - with-query: Output 2 columns with original and converted names. species \u2014 Limit gene query to certain species.Supported: human, mouse, rat, fruitfly, nematode, zebrafish, thale-cress, frog and pig Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.gene.GeneNameConversion"},{"location":"api/biopipen.ns.gene/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.gene/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.gene/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.gene/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.gene/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.gene/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.gene/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.gene/#biopipennsgenegenepromoters","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Get gene promoter regions by specifying the flanking regions of TSS Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file with gene ids/names Output outfile \u2014 The output file with promoter regions in BED format Envs chrsize \u2014 The chromosome size file, from which the chromosome order isused to sort the output down (type=int) \u2014 The downstream distance from TSSIf not specified, the default is envs.up genecol (type=int) \u2014 The index (1-based) of the gene column header (flag) \u2014 Whether the input file has a header match_id (flag) \u2014 Should we match the genes in in.infile by gene_id instead of gene_name in envs.refgene notfound (choice) \u2014 What to do if a gene is not found. - skip: Skip the gene - error: Report error refgene \u2014 The reference gene annotation file in GTF format sort (flag) \u2014 Sort the output by chromosome and start position up (type=int) \u2014 The upstream distance from TSS Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.gene.GenePromoters"},{"location":"api/biopipen.ns.gene/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.gene/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.gene/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.gene/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.gene/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.gene/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.gene/#pipenprocprocrun_1","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.gsea/","text":"module biopipen.ns . gsea </> Gene set enrichment analysis Classes GSEA ( Proc ) \u2014 Gene set enrichment analysis </> PreRank ( Proc ) \u2014 PreRank the genes for GSEA analysis </> FGSEA ( Proc ) \u2014 Gene set enrichment analysis using fgsea </> Enrichr ( Proc ) \u2014 Gene set enrichment analysis using Enrichr </> class biopipen.ns.gsea . GSEA ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis Need devtools::install_github(\"GSEA-MSigDB/GSEA_R\") Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol doc.string : Documentation string used as a prefix to name result files. If not provided, will use envs['doc.string'] gmtfile \u2014 The GMT file of reference gene sets infile \u2014 The expression file.Either a tab-delimited matrix or an RDS file (on envs.infmt) metafile \u2014 The meta data file, determining the class of the samplesTwo columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples Output outdir \u2014 The output directory Envs clscol \u2014 The column of the metafile determining the classes doc_string \u2014 Documentation string used as a prefix to name result filesOther configs passed to GSEA() directly inopts \u2014 The options for read.table() to read the input fileIf rds will use readRDS() metaopts \u2014 The options for read.table() to read the meta file Requires GSEA-MSigDB/GSEA_R \u2014 check: {{proc.lang}} <(echo \"library(GSEA)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.gsea . PreRank ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc PreRank the genes for GSEA analysis Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol classes : Defines pos and neg labels. If not provided, use will envs.classes . infile \u2014 The expression file.Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile \u2014 The meta data file, determining the class of the samplesTwo columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples Output outfile \u2014 The rank file with 1st column the genes, and the rest theranks for different class pairs provided by envs.classes or in.configfile Envs classes \u2014 The classes to specify the pos and neg labels.It could be a pair of labels (e.g. [\"CASE\", \"CNTRL\"] ), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. [[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]] ) clscol \u2014 The column of metafile specifying the classes of the samples inopts \u2014 Options for read.table() to read in.infile metaopts \u2014 Options for read.table() to read in.metafile method \u2014 The method to do the preranking.Supported: s2n(signal_to_noise) , abs_s2n(abs_signal_to_noise) , t_test , ratio_of_classes , diff_of_classes and log2_ratio_of_classes . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.gsea . FGSEA ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis using fgsea Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The expression file (genes x samples).Either a tab-delimited file. metafile \u2014 The meta data file, determining the class of the samplesTwo columns are required. If column Sample is found, it will be used as the samples; otherwise the first column should be the samples. The other column should be the group/class of the samples, whose name is specified by envs.clscol . Output outdir \u2014 The output directory containing the results, includingthe table and plots. Envs case \u2014 The case label for the positive class. cases (type=json;order=99) \u2014 If you have multiple cases, you can specify them here.The keys are the names of the cases and the values are the above options except mutaters . If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the name GSEA . clscol \u2014 The column of metafile specifying the classes of the samplesWhen in.metafile is not specified, it can also be specified as a list of classes, in the same order as the samples in in.infile . control \u2014 The control label for the negative class.When there are only two classes in in.metafile at column envs.clscol , either case or control can be specified and the other will be automatically set to the other class. eps (type=float) \u2014 This parameter sets the boundary for calculating the p value.See https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html gmtfile \u2014 The pathways in GMT format, with the gene names/ids in the same format as the seurat object.One could also use a URL to a GMT file. For example, from https://download.baderlab.org/EM_Genesets/current_release/Human/symbol/Pathways/ . maxsize (type=int) \u2014 Maximal size of a gene set to test. All pathways above the threshold are excluded. method (choice) \u2014 The method to do the preranking. - signal_to_noise: Signal to noise. The larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \"class marker\". - s2n: Alias of signal_to_noise. - abs_signal_to_noise: The absolute value of signal_to_noise. - abs_s2n: Alias of abs_signal_to_noise. - t_test: T test. Uses the difference of means scaled by the standard deviation and number of samples. - ratio_of_classes: Also referred to as fold change. Uses the ratio of class means to calculate fold change for natural scale data. - diff_of_classes: Difference of class means. Uses the difference of class means to calculate fold change for nature scale data - log2_ratio_of_classes: Log2 ratio of class means. Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. minsize (type=int) \u2014 Minimal size of a gene set to test. All pathways below the threshold are excluded. ncores (type=int) \u2014 Number of cores for parallelizationPassed to nproc of fgseaMultilevel() . rest (type=json;order=98) \u2014 Rest arguments for fgsea() See also https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html top (type=auto) \u2014 Do gsea table and enrich plot for top N pathways.If it is < 1, will apply it to padj , selecting pathways with padj < top . Requires bioconductor-fgsea \u2014 check: {{proc.lang}} -e \"library(fgsea)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.gsea . Enrichr ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis using Enrichr Need devtools::install_github(\"wjawaid/enrichR\") Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The gene list file.You can specify whether this file has header and the index (0-based) of the columns where the genes are present Output outdir \u2014 The output directory Envs dbs \u2014 The databases to enrich against.See https://maayanlab.cloud/Enrichr/#libraries for all available databases/libaries genecol \u2014 Which column has the genes (0-based index or column name) inopts \u2014 Options for read.table() to read in.infile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.gsea"},{"location":"api/biopipen.ns.gsea/#biopipennsgsea","text":"</> Gene set enrichment analysis Classes GSEA ( Proc ) \u2014 Gene set enrichment analysis </> PreRank ( Proc ) \u2014 PreRank the genes for GSEA analysis </> FGSEA ( Proc ) \u2014 Gene set enrichment analysis using fgsea </> Enrichr ( Proc ) \u2014 Gene set enrichment analysis using Enrichr </> class","title":"biopipen.ns.gsea"},{"location":"api/biopipen.ns.gsea/#biopipennsgseagsea","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis Need devtools::install_github(\"GSEA-MSigDB/GSEA_R\") Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol doc.string : Documentation string used as a prefix to name result files. If not provided, will use envs['doc.string'] gmtfile \u2014 The GMT file of reference gene sets infile \u2014 The expression file.Either a tab-delimited matrix or an RDS file (on envs.infmt) metafile \u2014 The meta data file, determining the class of the samplesTwo columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples Output outdir \u2014 The output directory Envs clscol \u2014 The column of the metafile determining the classes doc_string \u2014 Documentation string used as a prefix to name result filesOther configs passed to GSEA() directly inopts \u2014 The options for read.table() to read the input fileIf rds will use readRDS() metaopts \u2014 The options for read.table() to read the meta file Requires GSEA-MSigDB/GSEA_R \u2014 check: {{proc.lang}} <(echo \"library(GSEA)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.gsea.GSEA"},{"location":"api/biopipen.ns.gsea/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.gsea/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.gsea/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.gsea/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.gsea/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.gsea/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.gsea/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.gsea/#biopipennsgseaprerank","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc PreRank the genes for GSEA analysis Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 The configuration file in TOML format to specify some envs. clscol : If not provided, will use envs.clscol classes : Defines pos and neg labels. If not provided, use will envs.classes . infile \u2014 The expression file.Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile \u2014 The meta data file, determining the class of the samplesTwo columns are required Sample: The unique sample id for each sample [Group] : The groups/classes of the samples Output outfile \u2014 The rank file with 1st column the genes, and the rest theranks for different class pairs provided by envs.classes or in.configfile Envs classes \u2014 The classes to specify the pos and neg labels.It could be a pair of labels (e.g. [\"CASE\", \"CNTRL\"] ), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. [[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]] ) clscol \u2014 The column of metafile specifying the classes of the samples inopts \u2014 Options for read.table() to read in.infile metaopts \u2014 Options for read.table() to read in.metafile method \u2014 The method to do the preranking.Supported: s2n(signal_to_noise) , abs_s2n(abs_signal_to_noise) , t_test , ratio_of_classes , diff_of_classes and log2_ratio_of_classes . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.gsea.PreRank"},{"location":"api/biopipen.ns.gsea/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.gsea/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.gsea/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.gsea/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.gsea/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.gsea/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.gsea/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.gsea/#biopipennsgseafgsea","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis using fgsea Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The expression file (genes x samples).Either a tab-delimited file. metafile \u2014 The meta data file, determining the class of the samplesTwo columns are required. If column Sample is found, it will be used as the samples; otherwise the first column should be the samples. The other column should be the group/class of the samples, whose name is specified by envs.clscol . Output outdir \u2014 The output directory containing the results, includingthe table and plots. Envs case \u2014 The case label for the positive class. cases (type=json;order=99) \u2014 If you have multiple cases, you can specify them here.The keys are the names of the cases and the values are the above options except mutaters . If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the name GSEA . clscol \u2014 The column of metafile specifying the classes of the samplesWhen in.metafile is not specified, it can also be specified as a list of classes, in the same order as the samples in in.infile . control \u2014 The control label for the negative class.When there are only two classes in in.metafile at column envs.clscol , either case or control can be specified and the other will be automatically set to the other class. eps (type=float) \u2014 This parameter sets the boundary for calculating the p value.See https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html gmtfile \u2014 The pathways in GMT format, with the gene names/ids in the same format as the seurat object.One could also use a URL to a GMT file. For example, from https://download.baderlab.org/EM_Genesets/current_release/Human/symbol/Pathways/ . maxsize (type=int) \u2014 Maximal size of a gene set to test. All pathways above the threshold are excluded. method (choice) \u2014 The method to do the preranking. - signal_to_noise: Signal to noise. The larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \"class marker\". - s2n: Alias of signal_to_noise. - abs_signal_to_noise: The absolute value of signal_to_noise. - abs_s2n: Alias of abs_signal_to_noise. - t_test: T test. Uses the difference of means scaled by the standard deviation and number of samples. - ratio_of_classes: Also referred to as fold change. Uses the ratio of class means to calculate fold change for natural scale data. - diff_of_classes: Difference of class means. Uses the difference of class means to calculate fold change for nature scale data - log2_ratio_of_classes: Log2 ratio of class means. Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. minsize (type=int) \u2014 Minimal size of a gene set to test. All pathways below the threshold are excluded. ncores (type=int) \u2014 Number of cores for parallelizationPassed to nproc of fgseaMultilevel() . rest (type=json;order=98) \u2014 Rest arguments for fgsea() See also https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html top (type=auto) \u2014 Do gsea table and enrich plot for top N pathways.If it is < 1, will apply it to padj , selecting pathways with padj < top . Requires bioconductor-fgsea \u2014 check: {{proc.lang}} -e \"library(fgsea)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.gsea.FGSEA"},{"location":"api/biopipen.ns.gsea/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.gsea/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.gsea/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.gsea/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.gsea/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.gsea/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.gsea/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.gsea/#biopipennsgseaenrichr","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis using Enrichr Need devtools::install_github(\"wjawaid/enrichR\") Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The gene list file.You can specify whether this file has header and the index (0-based) of the columns where the genes are present Output outdir \u2014 The output directory Envs dbs \u2014 The databases to enrich against.See https://maayanlab.cloud/Enrichr/#libraries for all available databases/libaries genecol \u2014 Which column has the genes (0-based index or column name) inopts \u2014 Options for read.table() to read in.infile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.gsea.Enrichr"},{"location":"api/biopipen.ns.gsea/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.gsea/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.gsea/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.gsea/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.gsea/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.gsea/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.gsea/#pipenprocprocrun_3","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns/","text":"package biopipen. ns </> module biopipen.ns . protein </> Protein-related processes. Classes Prodigy ( Proc ) \u2014 Prediction of binding affinity of protein-protein complexes based onintermolecular contacts using Prodigy. </> ProdigySummary ( Proc ) \u2014 Summary of the output from Prodigy . </> MMCIF2PDB ( Proc ) \u2014 Convert mmCIF or PDBx file to PDB file. </> RMSD ( Proc ) \u2014 Calculate the RMSD between two structures. </> PDB2Fasta ( Proc ) \u2014 Convert PDB file to FASTA file. </> module biopipen.ns . cellranger </> Cellranger pipeline module for BioPipen Classes CellRangerCount ( Proc ) \u2014 Run cellranger count </> CellRangerVdj ( Proc ) \u2014 Run cellranger vdj </> CellRangerSummary ( Proc ) \u2014 Summarize cellranger metrics </> module biopipen.ns . cnvkit </> CNVkit commnads Classes CNVkitAccess ( Proc ) \u2014 Calculate the sequence-accessible coordinates in chromosomes from thegiven reference genome using cnvkit.py access </> CNVkitAutobin ( Proc ) \u2014 Quickly estimate read counts or depths in a BAM file to estimatereasonable on- and (if relevant) off-target bin sizes. </> CNVkitCoverage ( Proc ) \u2014 Run cnvkit coverage </> CNVkitReference ( Proc ) \u2014 Run cnvkit reference </> CNVkitFix ( Proc ) \u2014 Run cnvkit.py fix </> CNVkitSegment ( Proc ) \u2014 Run cnvkit.py segment </> CNVkitScatter ( Proc ) \u2014 Run cnvkit.py scatter </> CNVkitDiagram ( Proc ) \u2014 Run cnvkit.py diagram </> CNVkitHeatmap ( Proc ) \u2014 Run cnvkit.py heatmap for multiple cases </> CNVkitCall ( Proc ) \u2014 Run cnvkit.py call </> CNVkitBatch ( Proc ) \u2014 Run cnvkit batch </> CNVkitGuessBaits ( Proc ) \u2014 Guess the bait intervals from the bam files </> module biopipen.ns . cnv </> CNV/CNA-related processes, mostly tertiary analysis Classes AneuploidyScore ( Proc ) \u2014 Chromosomal arm SCNA/aneuploidy </> AneuploidyScoreSummary ( Proc ) \u2014 Summary table and plots from AneuploidyScore </> TMADScore ( Proc ) \u2014 Trimmed Median Absolute Deviation (TMAD) score for CNV </> TMADScoreSummary ( Proc ) \u2014 Summary table and plots for TMADScore </> module biopipen.ns . web </> Get data from the web Classes Download ( Proc ) \u2014 Download data from URLs </> DownloadList ( Proc ) \u2014 Download data from URLs in a file. </> GCloudStorageDownloadFile ( Proc ) \u2014 Download file from Google Cloud Storage </> GCloudStorageDownloadBucket ( Proc ) \u2014 Download all files from a Google Cloud Storage bucket </> module biopipen.ns . cellranger_pipeline </> The cellranger pipelines Primarily cellranger process plus summary for summarizing the metrics for multiple samples. Classes CellRangerCountPipeline \u2014 The cellranger count pipeline </> CellRangerVdjPipeline \u2014 The cellranger vdj pipeline </> module biopipen.ns . bam </> Tools to process sam/bam/cram files Classes CNVpytor ( Proc ) \u2014 Detect CNV using CNVpytor </> ControlFREEC ( Proc ) \u2014 Detect CNVs using Control-FREEC </> CNAClinic ( Proc ) \u2014 Detect CNVs using CNAClinic </> BamSplitChroms ( Proc ) \u2014 Split bam file by chromosomes </> BamMerge ( Proc ) \u2014 Merge bam files </> BamSampling ( Proc ) \u2014 Keeping only a fraction of read pairs from a bam file </> BamSubsetByBed ( Proc ) \u2014 Subset bam file by the regions in a bed file </> BamSort ( Proc ) \u2014 Sort bam file </> SamtoolsView ( Proc ) \u2014 View bam file using samtools, mostly used for filtering </> module biopipen.ns . scrna </> Tools to analyze single-cell RNA Classes SeuratLoading ( Proc ) \u2014 Seurat - Loading data </> SeuratPreparing ( Proc ) \u2014 Load, prepare and apply QC to data, using Seurat </> SeuratClustering ( Proc ) \u2014 Determine the clusters of cells without reference using Seurat FindClustersprocedure. </> SeuratSubClustering ( Proc ) \u2014 Find clusters of a subset of cells. </> SeuratClusterStats ( Proc ) \u2014 Statistics of the clustering. </> ModuleScoreCalculator ( Proc ) \u2014 Calculate the module scores for each cell </> CellsDistribution ( Proc ) \u2014 Distribution of cells (i.e. in a TCR clone) from different groupsfor each cluster </> SeuratMetadataMutater ( Proc ) \u2014 Mutate the metadata of the seurat object </> DimPlots ( Proc ) \u2014 Seurat - Dimensional reduction plots </> MarkersFinder ( Proc ) \u2014 Find markers between different groups of cells </> TopExpressingGenes ( Proc ) \u2014 Find the top expressing genes in each cluster </> ExprImputation ( Proc ) \u2014 This process imputes the dropout values in scRNA-seq data. </> SCImpute ( Proc ) \u2014 Impute the dropout values in scRNA-seq data. </> SeuratFilter ( Proc ) \u2014 Filtering cells from a seurat object </> SeuratSubset ( Proc ) \u2014 Subset a seurat object into multiple seruat objects </> SeuratSplit ( Proc ) \u2014 Split a seurat object into multiple seruat objects </> Subset10X ( Proc ) \u2014 Subset 10X data, mostly used for testing </> SeuratTo10X ( Proc ) \u2014 Write a Seurat object to 10X format </> ScFGSEA ( Proc ) \u2014 Gene set enrichment analysis for cells in different groups using fgsea </> CellTypeAnnotation ( Proc ) \u2014 Annotate the cell clusters. Currently, four ways are supported: </> SeuratMap2Ref ( Proc ) \u2014 Map the seurat object to reference </> RadarPlots ( Proc ) \u2014 Radar plots for cell proportion in different clusters. </> MetaMarkers ( Proc ) \u2014 Find markers between three or more groups of cells, using one-way ANOVAor Kruskal-Wallis test. </> Seurat2AnnData ( Proc ) \u2014 Convert seurat object to AnnData </> AnnData2Seurat ( Proc ) \u2014 Convert AnnData to seurat object </> ScSimulation ( Proc ) \u2014 Simulate single-cell data using splatter. </> CellCellCommunication ( Proc ) \u2014 Cell-cell communication inference </> CellCellCommunicationPlots ( Proc ) \u2014 Visualization for cell-cell communication inference. </> ScVelo ( Proc ) \u2014 Velocity analysis for single-cell RNA-seq data </> Slingshot ( Proc ) \u2014 Trajectory inference using Slingshot </> LoomTo10X ( Proc ) \u2014 Convert Loom file to 10X format </> PseudoBulkDEG ( Proc ) \u2014 Pseduo-bulk differential gene expression analysis </> module biopipen.ns . tcr </> Tools to analyze single-cell TCR sequencing data Classes ImmunarchLoading ( Proc ) \u2014 Immuarch - Loading data </> ImmunarchFilter ( Proc ) \u2014 Immunarch - Filter data </> Immunarch ( Proc ) \u2014 Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires </> SampleDiversity ( Proc ) \u2014 Sample diversity and rarefaction analysis </> CloneResidency ( Proc ) \u2014 Identification of clone residency </> Immunarch2VDJtools ( Proc ) \u2014 Convert immuarch format into VDJtools input formats. </> ImmunarchSplitIdents ( Proc ) \u2014 Split the data into multiple immunarch datasets by Idents from Seurat </> VJUsage ( Proc ) \u2014 Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions using vdjtools. </> Attach2Seurat ( Proc ) \u2014 Attach the clonal information to a Seurat object as metadata </> TCRClustering ( Proc ) \u2014 Cluster the TCR clones by their CDR3 sequences </> TCRClusterStats ( Proc ) \u2014 Statistics of TCR clusters, generated by TCRClustering . </> CloneSizeQQPlot ( Proc ) \u2014 QQ plot of the clone sizes </> CDR3AAPhyschem ( Proc ) \u2014 CDR3 AA physicochemical feature analysis </> TESSA ( Proc ) \u2014 Tessa is a Bayesian model to integrate T cell receptor (TCR) sequenceprofiling with transcriptomes of T cells. </> TCRDock ( Proc ) \u2014 Using TCRDock to predict the structure of MHC-peptide-TCR complexes </> ScRepLoading ( Proc ) \u2014 Load the single cell TCR/BCR data into a scRepertoire compatible object </> ScRepCombiningExpression ( Proc ) \u2014 Combine the scTCR/BCR data with the expression data </> ClonalStats ( Proc ) \u2014 Visualize the clonal information. </> module biopipen.ns . tcgamaf </> Processes for TCGA MAF files. Classes Maf2Vcf ( Proc ) \u2014 Converts a MAF file to a VCF file. </> MafAddChr ( Proc ) \u2014 Adds the chr prefix to chromosome names in a MAF file if not present. </> module biopipen.ns . vcf </> Tools to handle VCF files Classes VcfLiftOver ( Proc ) \u2014 Liftover a VCF file using GATK </> VcfFilter ( Proc ) \u2014 Filter records in vcf file </> VcfIndex ( Proc ) \u2014 Index VCF files. If they are already index, use the index files </> Vcf2Bed ( Proc ) \u2014 Convert Vcf file to Bed file </> VcfDownSample ( Proc ) \u2014 Down-sample VCF files to keep only a subset of variants in there </> VcfSplitSamples ( Proc ) \u2014 Split a VCF file into multiple VCF files, one for each sample </> VcfIntersect ( Proc ) \u2014 Find variants in both VCF files </> VcfFix ( Proc ) \u2014 Fix some issues with VCF files </> VcfAnno ( Proc ) \u2014 Annotate a VCF file using vcfanno </> TruvariBench ( Proc ) \u2014 Run truvari bench to compare a VCF with CNV calls andbase CNV standards </> TruvariBenchSummary ( Proc ) \u2014 Summarise the statistics from TruvariBench for multiple jobs (VCFs) </> TruvariConsistency ( Proc ) \u2014 Run truvari consistency to check consistency of CNV calls </> BcftoolsAnnotate ( Proc ) \u2014 Add or remove annotations from VCF files </> BcftoolsFilter ( Proc ) \u2014 Apply fixed threshold filters to VCF files </> BcftoolsSort ( Proc ) \u2014 Sort VCF files using bcftools sort . </> BcftoolsMerge ( Proc ) \u2014 Merge multiple VCF files using bcftools merge . </> BcftoolsView ( Proc ) \u2014 View, subset and filter VCF files by position and filtering expression. </> module biopipen.ns . gsea </> Gene set enrichment analysis Classes GSEA ( Proc ) \u2014 Gene set enrichment analysis </> PreRank ( Proc ) \u2014 PreRank the genes for GSEA analysis </> FGSEA ( Proc ) \u2014 Gene set enrichment analysis using fgsea </> Enrichr ( Proc ) \u2014 Gene set enrichment analysis using Enrichr </> module biopipen.ns . scrna_metabolic_landscape </> Metabolic landscape analysis for scRNA-seq data Classes MetabolicPathwayActivity ( Proc ) \u2014 This process calculates the pathway activities in different groups and subsets. </> MetabolicFeatures ( Proc ) \u2014 This process performs enrichment analysis for the metabolic pathwaysfor each group in each subset. </> MetabolicPathwayHeterogeneity ( Proc ) \u2014 Calculate Metabolic Pathway heterogeneity. </> ScrnaMetabolicLandscape \u2014 Metabolic landscape analysis for scRNA-seq data </> module biopipen.ns . delim </> Tools to deal with csv/tsv files Classes RowsBinder ( Proc ) \u2014 Bind rows of input files </> SampleInfo ( Proc ) \u2014 List sample information and perform statistics </> module biopipen.ns . snp </> Plink processes Classes PlinkSimulation ( Proc ) \u2014 Simulate SNPs using PLINK v2 </> MatrixEQTL ( Proc ) \u2014 Run Matrix eQTL </> PlinkFromVcf ( Proc ) \u2014 Convert VCF to PLINK format. </> Plink2GTMat ( Proc ) \u2014 Convert PLINK files to genotype matrix. </> PlinkIBD ( Proc ) \u2014 Run PLINK IBD analysis (identity by descent) </> PlinkHWE ( Proc ) \u2014 Hardy-Weinberg Equilibrium report and filtering </> PlinkHet ( Proc ) \u2014 Calculation of sample heterozygosity. </> PlinkCallRate ( Proc ) \u2014 Calculation of call rate for the samples and variants. </> PlinkFilter ( Proc ) \u2014 Filter samples and variants for PLINK files. </> PlinkFreq ( Proc ) \u2014 Calculate allele frequencies for the variants. </> PlinkUpdateName ( Proc ) \u2014 Update variant names in PLINK files. </> module biopipen.ns . plot </> Plotting data Classes VennDiagram ( Proc ) \u2014 Plot Venn diagram </> Heatmap ( Proc ) \u2014 Plot heatmaps using ComplexHeatmap </> ROC ( Proc ) \u2014 Plot ROC curve using plotROC . </> Manhattan ( Proc ) \u2014 Plot Manhattan plot. </> QQPlot ( Proc ) \u2014 Generate QQ-plot or PP-plot using qqplotr. </> Scatter ( Proc ) \u2014 Generate scatter plot using ggplot2. </> module biopipen.ns . misc </> Misc processes Classes File2Proc ( Proc ) \u2014 Accept a file and pass it down with a symbolic link </> Glob2Dir ( Proc ) \u2014 Create symbolic links in output directory for the files givenby the glob pattern </> Config2File ( Proc ) \u2014 Write a configurationn in string to a configuration file </> Str2File ( Proc ) \u2014 Write the given string to a file </> Shell ( Proc ) \u2014 Run a shell command </> Plot ( Proc ) \u2014 Plot given data using plotthis package in R </> module biopipen.ns . rnaseq </> RNA-seq data analysis Classes UnitConversion ( Proc ) \u2014 Convert expression value units back and forth </> Simulation ( Proc ) \u2014 Simulate RNA-seq data using ESCO/RUVcorr package </> module biopipen.ns . gene </> Gene related processes Classes GeneNameConversion ( Proc ) \u2014 Convert gene names back and forth using MyGeneInfo </> GenePromoters ( Proc ) \u2014 Get gene promoter regions by specifying the flanking regions of TSS </> module biopipen.ns . stats </> Provides processes for statistics. Classes ChowTest ( Proc ) \u2014 Massive Chow tests. </> Mediation ( Proc ) \u2014 Mediation analysis. </> LiquidAssoc ( Proc ) \u2014 Liquid association tests. </> DiffCoexpr ( Proc ) \u2014 Differential co-expression analysis. </> MetaPvalue ( Proc ) \u2014 Calulation of meta p-values. </> MetaPvalue1 ( Proc ) \u2014 Calulation of meta p-values. </> module biopipen.ns . regulatory </> Provides processes for the regulatory related Classes MotifScan ( Proc ) \u2014 Scan the input sequences for binding sites using motifs. </> MotifAffinityTest ( Proc ) \u2014 Test the affinity of motifs to the sequences and the affinity changedue the mutations. </> VariantMotifPlot ( Proc ) \u2014 A plot with a genomic region surrounding a genomic variant, andpotentially disrupted motifs. </> module biopipen.ns . cnvkit_pipeline </> The CNVkit pipeline. Classes CNVkitPipeline \u2014 The CNVkit pipeline </> module biopipen.ns . bed </> Tools to handle BED files Classes BedLiftOver ( Proc ) \u2014 Liftover a BED file using liftOver </> Bed2Vcf ( Proc ) \u2014 Convert a BED file to a valid VCF file with minimal information </> BedConsensus ( Proc ) \u2014 Find consensus regions from multiple BED files. </> BedtoolsMerge ( Proc ) \u2014 Merge overlapping intervals in a BED file, using bedtools merge </> BedtoolsIntersect ( Proc ) \u2014 Find the intersection of two BED files, using bedtools intersect </> BedtoolsMakeWindows ( Proc ) \u2014 Make windows from a BED file or genome size file, using bedtools makewindows . </>","title":"biopipen.ns"},{"location":"api/biopipen.ns/#biopipenns","text":"</> module","title":"biopipen.ns"},{"location":"api/biopipen.ns/#biopipennsprotein","text":"</> Protein-related processes. Classes Prodigy ( Proc ) \u2014 Prediction of binding affinity of protein-protein complexes based onintermolecular contacts using Prodigy. </> ProdigySummary ( Proc ) \u2014 Summary of the output from Prodigy . </> MMCIF2PDB ( Proc ) \u2014 Convert mmCIF or PDBx file to PDB file. </> RMSD ( Proc ) \u2014 Calculate the RMSD between two structures. </> PDB2Fasta ( Proc ) \u2014 Convert PDB file to FASTA file. </> module","title":"biopipen.ns.protein"},{"location":"api/biopipen.ns/#biopipennscellranger","text":"</> Cellranger pipeline module for BioPipen Classes CellRangerCount ( Proc ) \u2014 Run cellranger count </> CellRangerVdj ( Proc ) \u2014 Run cellranger vdj </> CellRangerSummary ( Proc ) \u2014 Summarize cellranger metrics </> module","title":"biopipen.ns.cellranger"},{"location":"api/biopipen.ns/#biopipennscnvkit","text":"</> CNVkit commnads Classes CNVkitAccess ( Proc ) \u2014 Calculate the sequence-accessible coordinates in chromosomes from thegiven reference genome using cnvkit.py access </> CNVkitAutobin ( Proc ) \u2014 Quickly estimate read counts or depths in a BAM file to estimatereasonable on- and (if relevant) off-target bin sizes. </> CNVkitCoverage ( Proc ) \u2014 Run cnvkit coverage </> CNVkitReference ( Proc ) \u2014 Run cnvkit reference </> CNVkitFix ( Proc ) \u2014 Run cnvkit.py fix </> CNVkitSegment ( Proc ) \u2014 Run cnvkit.py segment </> CNVkitScatter ( Proc ) \u2014 Run cnvkit.py scatter </> CNVkitDiagram ( Proc ) \u2014 Run cnvkit.py diagram </> CNVkitHeatmap ( Proc ) \u2014 Run cnvkit.py heatmap for multiple cases </> CNVkitCall ( Proc ) \u2014 Run cnvkit.py call </> CNVkitBatch ( Proc ) \u2014 Run cnvkit batch </> CNVkitGuessBaits ( Proc ) \u2014 Guess the bait intervals from the bam files </> module","title":"biopipen.ns.cnvkit"},{"location":"api/biopipen.ns/#biopipennscnv","text":"</> CNV/CNA-related processes, mostly tertiary analysis Classes AneuploidyScore ( Proc ) \u2014 Chromosomal arm SCNA/aneuploidy </> AneuploidyScoreSummary ( Proc ) \u2014 Summary table and plots from AneuploidyScore </> TMADScore ( Proc ) \u2014 Trimmed Median Absolute Deviation (TMAD) score for CNV </> TMADScoreSummary ( Proc ) \u2014 Summary table and plots for TMADScore </> module","title":"biopipen.ns.cnv"},{"location":"api/biopipen.ns/#biopipennsweb","text":"</> Get data from the web Classes Download ( Proc ) \u2014 Download data from URLs </> DownloadList ( Proc ) \u2014 Download data from URLs in a file. </> GCloudStorageDownloadFile ( Proc ) \u2014 Download file from Google Cloud Storage </> GCloudStorageDownloadBucket ( Proc ) \u2014 Download all files from a Google Cloud Storage bucket </> module","title":"biopipen.ns.web"},{"location":"api/biopipen.ns/#biopipennscellranger_pipeline","text":"</> The cellranger pipelines Primarily cellranger process plus summary for summarizing the metrics for multiple samples. Classes CellRangerCountPipeline \u2014 The cellranger count pipeline </> CellRangerVdjPipeline \u2014 The cellranger vdj pipeline </> module","title":"biopipen.ns.cellranger_pipeline"},{"location":"api/biopipen.ns/#biopipennsbam","text":"</> Tools to process sam/bam/cram files Classes CNVpytor ( Proc ) \u2014 Detect CNV using CNVpytor </> ControlFREEC ( Proc ) \u2014 Detect CNVs using Control-FREEC </> CNAClinic ( Proc ) \u2014 Detect CNVs using CNAClinic </> BamSplitChroms ( Proc ) \u2014 Split bam file by chromosomes </> BamMerge ( Proc ) \u2014 Merge bam files </> BamSampling ( Proc ) \u2014 Keeping only a fraction of read pairs from a bam file </> BamSubsetByBed ( Proc ) \u2014 Subset bam file by the regions in a bed file </> BamSort ( Proc ) \u2014 Sort bam file </> SamtoolsView ( Proc ) \u2014 View bam file using samtools, mostly used for filtering </> module","title":"biopipen.ns.bam"},{"location":"api/biopipen.ns/#biopipennsscrna","text":"</> Tools to analyze single-cell RNA Classes SeuratLoading ( Proc ) \u2014 Seurat - Loading data </> SeuratPreparing ( Proc ) \u2014 Load, prepare and apply QC to data, using Seurat </> SeuratClustering ( Proc ) \u2014 Determine the clusters of cells without reference using Seurat FindClustersprocedure. </> SeuratSubClustering ( Proc ) \u2014 Find clusters of a subset of cells. </> SeuratClusterStats ( Proc ) \u2014 Statistics of the clustering. </> ModuleScoreCalculator ( Proc ) \u2014 Calculate the module scores for each cell </> CellsDistribution ( Proc ) \u2014 Distribution of cells (i.e. in a TCR clone) from different groupsfor each cluster </> SeuratMetadataMutater ( Proc ) \u2014 Mutate the metadata of the seurat object </> DimPlots ( Proc ) \u2014 Seurat - Dimensional reduction plots </> MarkersFinder ( Proc ) \u2014 Find markers between different groups of cells </> TopExpressingGenes ( Proc ) \u2014 Find the top expressing genes in each cluster </> ExprImputation ( Proc ) \u2014 This process imputes the dropout values in scRNA-seq data. </> SCImpute ( Proc ) \u2014 Impute the dropout values in scRNA-seq data. </> SeuratFilter ( Proc ) \u2014 Filtering cells from a seurat object </> SeuratSubset ( Proc ) \u2014 Subset a seurat object into multiple seruat objects </> SeuratSplit ( Proc ) \u2014 Split a seurat object into multiple seruat objects </> Subset10X ( Proc ) \u2014 Subset 10X data, mostly used for testing </> SeuratTo10X ( Proc ) \u2014 Write a Seurat object to 10X format </> ScFGSEA ( Proc ) \u2014 Gene set enrichment analysis for cells in different groups using fgsea </> CellTypeAnnotation ( Proc ) \u2014 Annotate the cell clusters. Currently, four ways are supported: </> SeuratMap2Ref ( Proc ) \u2014 Map the seurat object to reference </> RadarPlots ( Proc ) \u2014 Radar plots for cell proportion in different clusters. </> MetaMarkers ( Proc ) \u2014 Find markers between three or more groups of cells, using one-way ANOVAor Kruskal-Wallis test. </> Seurat2AnnData ( Proc ) \u2014 Convert seurat object to AnnData </> AnnData2Seurat ( Proc ) \u2014 Convert AnnData to seurat object </> ScSimulation ( Proc ) \u2014 Simulate single-cell data using splatter. </> CellCellCommunication ( Proc ) \u2014 Cell-cell communication inference </> CellCellCommunicationPlots ( Proc ) \u2014 Visualization for cell-cell communication inference. </> ScVelo ( Proc ) \u2014 Velocity analysis for single-cell RNA-seq data </> Slingshot ( Proc ) \u2014 Trajectory inference using Slingshot </> LoomTo10X ( Proc ) \u2014 Convert Loom file to 10X format </> PseudoBulkDEG ( Proc ) \u2014 Pseduo-bulk differential gene expression analysis </> module","title":"biopipen.ns.scrna"},{"location":"api/biopipen.ns/#biopipennstcr","text":"</> Tools to analyze single-cell TCR sequencing data Classes ImmunarchLoading ( Proc ) \u2014 Immuarch - Loading data </> ImmunarchFilter ( Proc ) \u2014 Immunarch - Filter data </> Immunarch ( Proc ) \u2014 Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires </> SampleDiversity ( Proc ) \u2014 Sample diversity and rarefaction analysis </> CloneResidency ( Proc ) \u2014 Identification of clone residency </> Immunarch2VDJtools ( Proc ) \u2014 Convert immuarch format into VDJtools input formats. </> ImmunarchSplitIdents ( Proc ) \u2014 Split the data into multiple immunarch datasets by Idents from Seurat </> VJUsage ( Proc ) \u2014 Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions using vdjtools. </> Attach2Seurat ( Proc ) \u2014 Attach the clonal information to a Seurat object as metadata </> TCRClustering ( Proc ) \u2014 Cluster the TCR clones by their CDR3 sequences </> TCRClusterStats ( Proc ) \u2014 Statistics of TCR clusters, generated by TCRClustering . </> CloneSizeQQPlot ( Proc ) \u2014 QQ plot of the clone sizes </> CDR3AAPhyschem ( Proc ) \u2014 CDR3 AA physicochemical feature analysis </> TESSA ( Proc ) \u2014 Tessa is a Bayesian model to integrate T cell receptor (TCR) sequenceprofiling with transcriptomes of T cells. </> TCRDock ( Proc ) \u2014 Using TCRDock to predict the structure of MHC-peptide-TCR complexes </> ScRepLoading ( Proc ) \u2014 Load the single cell TCR/BCR data into a scRepertoire compatible object </> ScRepCombiningExpression ( Proc ) \u2014 Combine the scTCR/BCR data with the expression data </> ClonalStats ( Proc ) \u2014 Visualize the clonal information. </> module","title":"biopipen.ns.tcr"},{"location":"api/biopipen.ns/#biopipennstcgamaf","text":"</> Processes for TCGA MAF files. Classes Maf2Vcf ( Proc ) \u2014 Converts a MAF file to a VCF file. </> MafAddChr ( Proc ) \u2014 Adds the chr prefix to chromosome names in a MAF file if not present. </> module","title":"biopipen.ns.tcgamaf"},{"location":"api/biopipen.ns/#biopipennsvcf","text":"</> Tools to handle VCF files Classes VcfLiftOver ( Proc ) \u2014 Liftover a VCF file using GATK </> VcfFilter ( Proc ) \u2014 Filter records in vcf file </> VcfIndex ( Proc ) \u2014 Index VCF files. If they are already index, use the index files </> Vcf2Bed ( Proc ) \u2014 Convert Vcf file to Bed file </> VcfDownSample ( Proc ) \u2014 Down-sample VCF files to keep only a subset of variants in there </> VcfSplitSamples ( Proc ) \u2014 Split a VCF file into multiple VCF files, one for each sample </> VcfIntersect ( Proc ) \u2014 Find variants in both VCF files </> VcfFix ( Proc ) \u2014 Fix some issues with VCF files </> VcfAnno ( Proc ) \u2014 Annotate a VCF file using vcfanno </> TruvariBench ( Proc ) \u2014 Run truvari bench to compare a VCF with CNV calls andbase CNV standards </> TruvariBenchSummary ( Proc ) \u2014 Summarise the statistics from TruvariBench for multiple jobs (VCFs) </> TruvariConsistency ( Proc ) \u2014 Run truvari consistency to check consistency of CNV calls </> BcftoolsAnnotate ( Proc ) \u2014 Add or remove annotations from VCF files </> BcftoolsFilter ( Proc ) \u2014 Apply fixed threshold filters to VCF files </> BcftoolsSort ( Proc ) \u2014 Sort VCF files using bcftools sort . </> BcftoolsMerge ( Proc ) \u2014 Merge multiple VCF files using bcftools merge . </> BcftoolsView ( Proc ) \u2014 View, subset and filter VCF files by position and filtering expression. </> module","title":"biopipen.ns.vcf"},{"location":"api/biopipen.ns/#biopipennsgsea","text":"</> Gene set enrichment analysis Classes GSEA ( Proc ) \u2014 Gene set enrichment analysis </> PreRank ( Proc ) \u2014 PreRank the genes for GSEA analysis </> FGSEA ( Proc ) \u2014 Gene set enrichment analysis using fgsea </> Enrichr ( Proc ) \u2014 Gene set enrichment analysis using Enrichr </> module","title":"biopipen.ns.gsea"},{"location":"api/biopipen.ns/#biopipennsscrna_metabolic_landscape","text":"</> Metabolic landscape analysis for scRNA-seq data Classes MetabolicPathwayActivity ( Proc ) \u2014 This process calculates the pathway activities in different groups and subsets. </> MetabolicFeatures ( Proc ) \u2014 This process performs enrichment analysis for the metabolic pathwaysfor each group in each subset. </> MetabolicPathwayHeterogeneity ( Proc ) \u2014 Calculate Metabolic Pathway heterogeneity. </> ScrnaMetabolicLandscape \u2014 Metabolic landscape analysis for scRNA-seq data </> module","title":"biopipen.ns.scrna_metabolic_landscape"},{"location":"api/biopipen.ns/#biopipennsdelim","text":"</> Tools to deal with csv/tsv files Classes RowsBinder ( Proc ) \u2014 Bind rows of input files </> SampleInfo ( Proc ) \u2014 List sample information and perform statistics </> module","title":"biopipen.ns.delim"},{"location":"api/biopipen.ns/#biopipennssnp","text":"</> Plink processes Classes PlinkSimulation ( Proc ) \u2014 Simulate SNPs using PLINK v2 </> MatrixEQTL ( Proc ) \u2014 Run Matrix eQTL </> PlinkFromVcf ( Proc ) \u2014 Convert VCF to PLINK format. </> Plink2GTMat ( Proc ) \u2014 Convert PLINK files to genotype matrix. </> PlinkIBD ( Proc ) \u2014 Run PLINK IBD analysis (identity by descent) </> PlinkHWE ( Proc ) \u2014 Hardy-Weinberg Equilibrium report and filtering </> PlinkHet ( Proc ) \u2014 Calculation of sample heterozygosity. </> PlinkCallRate ( Proc ) \u2014 Calculation of call rate for the samples and variants. </> PlinkFilter ( Proc ) \u2014 Filter samples and variants for PLINK files. </> PlinkFreq ( Proc ) \u2014 Calculate allele frequencies for the variants. </> PlinkUpdateName ( Proc ) \u2014 Update variant names in PLINK files. </> module","title":"biopipen.ns.snp"},{"location":"api/biopipen.ns/#biopipennsplot","text":"</> Plotting data Classes VennDiagram ( Proc ) \u2014 Plot Venn diagram </> Heatmap ( Proc ) \u2014 Plot heatmaps using ComplexHeatmap </> ROC ( Proc ) \u2014 Plot ROC curve using plotROC . </> Manhattan ( Proc ) \u2014 Plot Manhattan plot. </> QQPlot ( Proc ) \u2014 Generate QQ-plot or PP-plot using qqplotr. </> Scatter ( Proc ) \u2014 Generate scatter plot using ggplot2. </> module","title":"biopipen.ns.plot"},{"location":"api/biopipen.ns/#biopipennsmisc","text":"</> Misc processes Classes File2Proc ( Proc ) \u2014 Accept a file and pass it down with a symbolic link </> Glob2Dir ( Proc ) \u2014 Create symbolic links in output directory for the files givenby the glob pattern </> Config2File ( Proc ) \u2014 Write a configurationn in string to a configuration file </> Str2File ( Proc ) \u2014 Write the given string to a file </> Shell ( Proc ) \u2014 Run a shell command </> Plot ( Proc ) \u2014 Plot given data using plotthis package in R </> module","title":"biopipen.ns.misc"},{"location":"api/biopipen.ns/#biopipennsrnaseq","text":"</> RNA-seq data analysis Classes UnitConversion ( Proc ) \u2014 Convert expression value units back and forth </> Simulation ( Proc ) \u2014 Simulate RNA-seq data using ESCO/RUVcorr package </> module","title":"biopipen.ns.rnaseq"},{"location":"api/biopipen.ns/#biopipennsgene","text":"</> Gene related processes Classes GeneNameConversion ( Proc ) \u2014 Convert gene names back and forth using MyGeneInfo </> GenePromoters ( Proc ) \u2014 Get gene promoter regions by specifying the flanking regions of TSS </> module","title":"biopipen.ns.gene"},{"location":"api/biopipen.ns/#biopipennsstats","text":"</> Provides processes for statistics. Classes ChowTest ( Proc ) \u2014 Massive Chow tests. </> Mediation ( Proc ) \u2014 Mediation analysis. </> LiquidAssoc ( Proc ) \u2014 Liquid association tests. </> DiffCoexpr ( Proc ) \u2014 Differential co-expression analysis. </> MetaPvalue ( Proc ) \u2014 Calulation of meta p-values. </> MetaPvalue1 ( Proc ) \u2014 Calulation of meta p-values. </> module","title":"biopipen.ns.stats"},{"location":"api/biopipen.ns/#biopipennsregulatory","text":"</> Provides processes for the regulatory related Classes MotifScan ( Proc ) \u2014 Scan the input sequences for binding sites using motifs. </> MotifAffinityTest ( Proc ) \u2014 Test the affinity of motifs to the sequences and the affinity changedue the mutations. </> VariantMotifPlot ( Proc ) \u2014 A plot with a genomic region surrounding a genomic variant, andpotentially disrupted motifs. </> module","title":"biopipen.ns.regulatory"},{"location":"api/biopipen.ns/#biopipennscnvkit_pipeline","text":"</> The CNVkit pipeline. Classes CNVkitPipeline \u2014 The CNVkit pipeline </> module","title":"biopipen.ns.cnvkit_pipeline"},{"location":"api/biopipen.ns/#biopipennsbed","text":"</> Tools to handle BED files Classes BedLiftOver ( Proc ) \u2014 Liftover a BED file using liftOver </> Bed2Vcf ( Proc ) \u2014 Convert a BED file to a valid VCF file with minimal information </> BedConsensus ( Proc ) \u2014 Find consensus regions from multiple BED files. </> BedtoolsMerge ( Proc ) \u2014 Merge overlapping intervals in a BED file, using bedtools merge </> BedtoolsIntersect ( Proc ) \u2014 Find the intersection of two BED files, using bedtools intersect </> BedtoolsMakeWindows ( Proc ) \u2014 Make windows from a BED file or genome size file, using bedtools makewindows . </>","title":"biopipen.ns.bed"},{"location":"api/biopipen.ns.misc/","text":"module biopipen.ns . misc </> Misc processes Classes File2Proc ( Proc ) \u2014 Accept a file and pass it down with a symbolic link </> Glob2Dir ( Proc ) \u2014 Create symbolic links in output directory for the files givenby the glob pattern </> Config2File ( Proc ) \u2014 Write a configurationn in string to a configuration file </> Str2File ( Proc ) \u2014 Write the given string to a file </> Shell ( Proc ) \u2014 Run a shell command </> Plot ( Proc ) \u2014 Plot given data using plotthis package in R </> class biopipen.ns.misc . File2Proc ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Accept a file and pass it down with a symbolic link Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file Output outfile \u2014 The output symbolic link to the input file Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.misc . Glob2Dir ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Create symbolic links in output directory for the files givenby the glob pattern Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.misc . Config2File ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Write a configurationn in string to a configuration file Requires python package rtoml Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input config \u2014 A string representation of configuration name \u2014 The name for output file.Will be config if not given Output outfile \u2014 The output file with the configuration Envs infmt \u2014 The input format. json or toml . outfmt \u2014 The output format. json or toml . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.misc . Str2File ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Write the given string to a file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input name \u2014 The name of the fileIf not given, use envs.name str \u2014 The string to write to file Output outfile \u2014 The output file Envs name \u2014 The name of the output file Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.misc . Shell ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run a shell command Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file Output outfile \u2014 The output file Envs cmd \u2014 The shell command to runUse $infile and $outfile to refer to input and output files outdir \u2014 Whether the out.outfile should be a directory.If so a directory will be created before running the command. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.misc . Plot ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot given data using plotthis package in R Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input datafile \u2014 The input data file in RDS or qs/qs2 format.If it is not in RDS nor qs/qs2 format, read.table will be used to read the data file with the options provided by envs.read_opts . Output plotfile \u2014 The output plot file in PNG format envs: fn: The plot function to use. Required. devpars (ns): The device parameters for the plot. - width: The width of the plot in pixels. - height: The height of the plot in pixels. - res: The resolution of the plot in DPI. more_formats: The additional formats to save the plot in other than PNG. The file will be saved in the same directory as the plotfile. save_code: Whether to save the R code used for plotting. read_opts: Options to read the data file. If the data file is not in RDS nor qs/qs2 format, these options will be passed to read.table . : Additional parameters to the plot function. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.misc"},{"location":"api/biopipen.ns.misc/#biopipennsmisc","text":"</> Misc processes Classes File2Proc ( Proc ) \u2014 Accept a file and pass it down with a symbolic link </> Glob2Dir ( Proc ) \u2014 Create symbolic links in output directory for the files givenby the glob pattern </> Config2File ( Proc ) \u2014 Write a configurationn in string to a configuration file </> Str2File ( Proc ) \u2014 Write the given string to a file </> Shell ( Proc ) \u2014 Run a shell command </> Plot ( Proc ) \u2014 Plot given data using plotthis package in R </> class","title":"biopipen.ns.misc"},{"location":"api/biopipen.ns.misc/#biopipennsmiscfile2proc","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Accept a file and pass it down with a symbolic link Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file Output outfile \u2014 The output symbolic link to the input file Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.misc.File2Proc"},{"location":"api/biopipen.ns.misc/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.misc/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.misc/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.misc/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.misc/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.misc/#biopipennsmiscglob2dir","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Create symbolic links in output directory for the files givenby the glob pattern Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.misc.Glob2Dir"},{"location":"api/biopipen.ns.misc/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.misc/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.misc/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.misc/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.misc/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.misc/#biopipennsmiscconfig2file","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Write a configurationn in string to a configuration file Requires python package rtoml Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input config \u2014 A string representation of configuration name \u2014 The name for output file.Will be config if not given Output outfile \u2014 The output file with the configuration Envs infmt \u2014 The input format. json or toml . outfmt \u2014 The output format. json or toml . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.misc.Config2File"},{"location":"api/biopipen.ns.misc/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.misc/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.misc/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.misc/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.misc/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.misc/#biopipennsmiscstr2file","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Write the given string to a file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input name \u2014 The name of the fileIf not given, use envs.name str \u2014 The string to write to file Output outfile \u2014 The output file Envs name \u2014 The name of the output file Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.misc.Str2File"},{"location":"api/biopipen.ns.misc/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.misc/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.misc/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.misc/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.misc/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.misc/#biopipennsmiscshell","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run a shell command Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file Output outfile \u2014 The output file Envs cmd \u2014 The shell command to runUse $infile and $outfile to refer to input and output files outdir \u2014 Whether the out.outfile should be a directory.If so a directory will be created before running the command. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.misc.Shell"},{"location":"api/biopipen.ns.misc/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.misc/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.misc/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.misc/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.misc/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.misc/#biopipennsmiscplot","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot given data using plotthis package in R Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input datafile \u2014 The input data file in RDS or qs/qs2 format.If it is not in RDS nor qs/qs2 format, read.table will be used to read the data file with the options provided by envs.read_opts . Output plotfile \u2014 The output plot file in PNG format envs: fn: The plot function to use. Required. devpars (ns): The device parameters for the plot. - width: The width of the plot in pixels. - height: The height of the plot in pixels. - res: The resolution of the plot in DPI. more_formats: The additional formats to save the plot in other than PNG. The file will be saved in the same directory as the plotfile. save_code: Whether to save the R code used for plotting. read_opts: Options to read the data file. If the data file is not in RDS nor qs/qs2 format, these options will be passed to read.table . : Additional parameters to the plot function. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.misc.Plot"},{"location":"api/biopipen.ns.misc/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.misc/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.misc/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.misc/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.misc/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.misc/#pipenprocprocrun_5","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.plot/","text":"module biopipen.ns . plot </> Plotting data Classes VennDiagram ( Proc ) \u2014 Plot Venn diagram </> Heatmap ( Proc ) \u2014 Plot heatmaps using ComplexHeatmap </> ROC ( Proc ) \u2014 Plot ROC curve using plotROC . </> Manhattan ( Proc ) \u2014 Plot Manhattan plot. </> QQPlot ( Proc ) \u2014 Generate QQ-plot or PP-plot using qqplotr. </> Scatter ( Proc ) \u2014 Generate scatter plot using ggplot2. </> class biopipen.ns.plot . VennDiagram ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot Venn diagram Needs ggVennDiagram Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for dataIf envs.intype is raw, it should be a data frame with row names as categories and only column as elements separated by comma ( , ) If it is computed , it should be a data frame with row names the elements and columns the categories. The data should be binary indicator ( 0, 1 ) indicating whether the elements are present in the categories. Output outfile \u2014 The output figure file Envs args \u2014 Additional arguments for ggVennDiagram() devpars \u2014 The parameters for png() ggs \u2014 Additional ggplot expression to adjust the plot inopts \u2014 The options for read.table() to read in.infile intype \u2014 raw or computed . See in.infile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.plot . Heatmap ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot heatmaps using ComplexHeatmap Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples >>> pipen run plot Heatmap >>> -- in . infile data . txt >>> -- in . annofiles anno . txt >>> -- envs . args . row_names_gp 'r:fontsize5' >>> -- envs . args . column_names_gp 'r:fontsize5' >>> -- envs . args . clustering_distance_rows pearson >>> -- envs . args . clustering_distance_columns pearson >>> -- envs . args . show_row_names false >>> -- envs . args . row_split 3 >>> -- args . devpars . width 5000 >>> -- args . devpars . height 5000 >>> -- args . draw . merge_legends >>> -- envs . args . heatmap_legend_param . title AUC >>> -- envs . args . row_dend_reorder >>> -- envs . args . column_dend_reorder >>> -- envs . args . top_annotation >>> 'r:HeatmapAnnotation( >>> Mutation = as.matrix(annos[,(length(groups)+1):ncol(annos)]) >>> )' >>> -- envs . args . right_annotation >>> 'r:rowAnnotation( >>> AUC = anno_boxplot(as.matrix(data), outline = F) >>> )' >>> -- args . globals >>> 'fontsize8 = gpar(fontsize = 12); >>> fontsize5 = gpar(fontsize = 8); >>> groups = c (\"Group1\", \"Group2\", \"Group3\")' >>> -- args . seed 8525 Input annofiles \u2014 The files for annotation data infile \u2014 The data matrix file Output outdir \u2014 Other data of the heatmapIncluding RDS file of the heatmap, row clusters and col clusters. outfile \u2014 The heatmap plot Envs anopts \u2014 Options for read.table() to read in.annofiles args \u2014 Arguments for ComplexHeatmap::Heatmap() devpars \u2014 The parameters for device. draw \u2014 Options for ComplexHeatmap::draw() globals \u2014 Some globals for the expression in args to be evaluated inopts \u2014 Options for read.table() to read in.infile seed \u2014 The seed Requires bioconductor-complexheatmap \u2014 check: {{proc.lang}} <(echo \"library(ComplexHeatmap)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.plot . ROC ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot ROC curve using plotROC . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for data, tab-separated.The first column should be ids of the records (this is optional if envs.noids is True). The second column should be the labels of the records (1 for positive, 0 for negative). If they are not binary, you can specify the positive label by envs.pos_label . From the third column, it should be the scores of the different models. Output outfile \u2014 The output figure file Envs args \u2014 Additional arguments for geom_roc() or geom_rocci() if envs.ci is True. ci \u2014 Whether to use geom_rocci() instead of geom_roc() . devpars \u2014 The parameters for png() noids \u2014 Whether the input file has ids (first column) or not. pos_label \u2014 The positive label. style_roc \u2014 Arguments for style_roc() Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.plot . Manhattan ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot Manhattan plot. Using the ggmanh package. Requires ggmanh v1.9.6 or later. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for dataIt should contain at least three columns, the chromosome, the position and the p-value of the SNPs. Header is required. Output outfile \u2014 The output figure file Envs args (ns) \u2014 Additional arguments for manhattan_plot() .See https://rdrr.io/github/leejs-abv/ggmanh/man/manhattan_plot.html . Note that - will be replaced by . in the argument names. - : Additional arguments for manhattan_plot() chrom_col \u2014 The column for chromosomeAn integer (1-based) or a string indicating the column name. chroms (auto) \u2014 The chromosomes and order to plotA hyphen ( - ) can be used to indicate a range. For example chr1-22,chrX,chrY,chrM will plot all autosomes, X, Y and M. if auto , only the chromosomes in the data will be plotted in the order they appear in the data. devpars (ns) \u2014 The parameters for png() - res (type=int): The resolution - width (type=int): The width - height (type=int): The height hicolors (auto) \u2014 The colors for significant and non-significant SNPsIf a single color is given, the non-significant SNPs will be in grey. Set it to None to disable the highlighting. label_col \u2014 The column for label.Once specified, the significant SNPs will be labeled on the plot. pos_col \u2014 The column for positionAn integer (1-based) or a string indicating the column name. pval_col \u2014 The column for p-valueAn integer (1-based) or a string indicating the column name. rescale (flag) \u2014 Whether to rescale the p-values rescale_ratio_threshold (type=float) \u2014 Threshold of that triggers the rescale signif (auto) \u2014 A single value or a list of values to indicate the significance levelsMultiple values should be also separated by comma ( , ). The minimum value will be used as the cutoff to determine if the SNPs are significant. thin_bins (type=int) \u2014 Number of bins to partition the data. thin_n (type=int) \u2014 Number of max points per horizontal partitions of the plot. 0 or None to disable thinning. title \u2014 The title of the plot ylabel \u2014 The y-axis label zoom (auto) \u2014 Chromosomes to zoom inEach chromosome should be separated by comma ( , ) or in a list. Single chromosome is also accepted. Ranges are also accepted, see envs.chroms . Each chromosome will be saved in a separate file. zoom_devpars (ns) \u2014 The parameters for the zoomed plot - width (type=int): The width - height (type=int): The height, inherited from devpars by default - res (type=int): The resolution, inherited from devpars by default Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.plot . QQPlot ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Generate QQ-plot or PP-plot using qqplotr. See https://cran.r-project.org/web/packages/qqplotr/vignettes/introduction.html . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for dataIt should contain at least one column of p-values or the values to be plotted. Header is required. theorfile \u2014 The file for theoretical values (optional)This file should contain at least one column of theoretical values. The values will be passed to envs.theor_qfunc to calculate the theoretical quantiles. Header is required. Output outfile \u2014 The output figure file Envs args (ns) \u2014 The common arguments for envs.band , envs.line and envs.point . - distribution: The distribution of the theoretical quantiles When custom is used, the envs.theor_col should be provided and values will be added to dparams automatically. - dparams (type=json): The parameters for the distribution - : Other shared arguments between stat_*_band , stat_*_line and stat_*_point . band (ns) \u2014 The arguments for stat_qq_band() or stat_pp_band() .See https://rdrr.io/cran/qqplotr/man/stat_qq_band.html and https://rdrr.io/cran/qqplotr/man/stat_pp_band.html . Set to None or band.disabled to True to disable the band. - disabled (flag): Disable the band - distribution: The distribution of the theoretical quantiles When custom is used, the envs.theor_col should be provided and values will be added to dparams automatically. - dparams (type=json): The parameters for the distribution - : Additional arguments for stat_qq_band() or stat_pp_band() devpars (ns) \u2014 The parameters for png() - res (type=int): The resolution - width (type=int): The width - height (type=int): The height ggs (list) \u2014 Additional ggplot expression to adjust the plot. kind (choice) \u2014 The kind of the plot, qq or pp - qq: QQ-plot - pp: PP-plot line (ns) \u2014 The arguments for stat_qq_line() or stat_pp_line() .See https://rdrr.io/cran/qqplot/man/stat_qq_line.html and https://rdrr.io/cran/qqplot/man/stat_pp_line.html . Set to None or line.disabled to True to disable the line. - disabled (flag): Disable the line - distribution: The distribution of the theoretical quantiles When custom is used, the envs.theor_col should be provided and values will be added to dparams automatically. - dparams (type=json): The parameters for the distribution - : Additional arguments for stat_qq_line() or stat_pp_line() point (ns) \u2014 The arguments for geom_qq_point() or geom_pp_point() .See https://rdrr.io/cran/qqplot/man/stat_qq_point.html and https://rdrr.io/cran/qqplot/man/stat_pp_point.html . Set to None or point.disabled to True to disable the point. - disabled (flag): Disable the point - distribution: The distribution of the theoretical quantiles When custom is used, the envs.theor_col should be provided and values will be added to dparams automatically. - dparams (type=json): The parameters for the distribution - : Additional arguments for geom_qq_point() or geom_pp_point() theor_col \u2014 The column for theoretical values in in.theorfile if provided,otherwise in in.infile . An integer (1-based) or a string indicating the column name. If distribution of band , line , or point is custom , this column must be provided. theor_funs (ns) \u2014 The R functions to generate density, quantile and deviatesof the theoretical distribution base on the theoretical values if distribution of band , line , or point is custom . - dcustom: The density function, used by band - qcustom: The quantile function, used by point - rcustom: The deviates function, used by line theor_trans \u2014 The transformation of the theoretical values.The theor_funs have default functions to take the theoretical values. This transformation will be applied to the theoretical values before passing to the theor_funs . title \u2014 The title of the plot trans \u2014 The transformation of the valuesYou can use -log10 to transform the values to -log10(values) . Otherwise you can a direct R function or a custom R function. For example function(x) -log10(x) . val_col \u2014 The column for values to be plottedAn integer (1-based) or a string indicating the column name. xlabel \u2014 The x-axis label ylabel \u2014 The y-axis label Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.plot . Scatter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Generate scatter plot using ggplot2. ggpmisc is used for the stats and labels. See also https://cran.r-project.org/web/packages/ggpmisc/vignettes/model-based-annotations.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for dataIt should contain at least two columns for x and y values. Header is required. Output outfile \u2014 The output figure file Envs args (ns) \u2014 Additional arguments for geom_point() See https://ggplot2.tidyverse.org/reference/geom_point.html . - : Additional arguments for geom_point() devpars (ns) \u2014 The parameters for png() - res (type=int): The resolution - width (type=int): The width - height (type=int): The height formula \u2014 The formula for the model ggs (list) \u2014 Additional ggplot expression to adjust the plot. mapping \u2014 Extra mapping for all geoms, including stats .Should be aes(color = group) but all these are valid: color = group or (color = group) . stats (type=json) \u2014 The stats to add to the plot.A dict with keys available stats in ggpmisc (without stat_ ). See https://cran.r-project.org/web/packages/ggpmisc/vignettes/model-based-annotations.html#statistics . The values should be the arguments for the stats. If you want a stat to be added multiple times, add a suffix #x to the key. For example, poly_line#1 and poly_line#2 will add two polynomial lines. x_col \u2014 The column for x valuesAn integer (1-based) or a string indicating the column name. y_col \u2014 The column for y valuesAn integer (1-based) or a string indicating the column name. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.plot"},{"location":"api/biopipen.ns.plot/#biopipennsplot","text":"</> Plotting data Classes VennDiagram ( Proc ) \u2014 Plot Venn diagram </> Heatmap ( Proc ) \u2014 Plot heatmaps using ComplexHeatmap </> ROC ( Proc ) \u2014 Plot ROC curve using plotROC . </> Manhattan ( Proc ) \u2014 Plot Manhattan plot. </> QQPlot ( Proc ) \u2014 Generate QQ-plot or PP-plot using qqplotr. </> Scatter ( Proc ) \u2014 Generate scatter plot using ggplot2. </> class","title":"biopipen.ns.plot"},{"location":"api/biopipen.ns.plot/#biopipennsplotvenndiagram","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot Venn diagram Needs ggVennDiagram Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for dataIf envs.intype is raw, it should be a data frame with row names as categories and only column as elements separated by comma ( , ) If it is computed , it should be a data frame with row names the elements and columns the categories. The data should be binary indicator ( 0, 1 ) indicating whether the elements are present in the categories. Output outfile \u2014 The output figure file Envs args \u2014 Additional arguments for ggVennDiagram() devpars \u2014 The parameters for png() ggs \u2014 Additional ggplot expression to adjust the plot inopts \u2014 The options for read.table() to read in.infile intype \u2014 raw or computed . See in.infile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.plot.VennDiagram"},{"location":"api/biopipen.ns.plot/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.plot/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.plot/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.plot/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.plot/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.plot/#biopipennsplotheatmap","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot heatmaps using ComplexHeatmap Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples >>> pipen run plot Heatmap >>> -- in . infile data . txt >>> -- in . annofiles anno . txt >>> -- envs . args . row_names_gp 'r:fontsize5' >>> -- envs . args . column_names_gp 'r:fontsize5' >>> -- envs . args . clustering_distance_rows pearson >>> -- envs . args . clustering_distance_columns pearson >>> -- envs . args . show_row_names false >>> -- envs . args . row_split 3 >>> -- args . devpars . width 5000 >>> -- args . devpars . height 5000 >>> -- args . draw . merge_legends >>> -- envs . args . heatmap_legend_param . title AUC >>> -- envs . args . row_dend_reorder >>> -- envs . args . column_dend_reorder >>> -- envs . args . top_annotation >>> 'r:HeatmapAnnotation( >>> Mutation = as.matrix(annos[,(length(groups)+1):ncol(annos)]) >>> )' >>> -- envs . args . right_annotation >>> 'r:rowAnnotation( >>> AUC = anno_boxplot(as.matrix(data), outline = F) >>> )' >>> -- args . globals >>> 'fontsize8 = gpar(fontsize = 12); >>> fontsize5 = gpar(fontsize = 8); >>> groups = c (\"Group1\", \"Group2\", \"Group3\")' >>> -- args . seed 8525 Input annofiles \u2014 The files for annotation data infile \u2014 The data matrix file Output outdir \u2014 Other data of the heatmapIncluding RDS file of the heatmap, row clusters and col clusters. outfile \u2014 The heatmap plot Envs anopts \u2014 Options for read.table() to read in.annofiles args \u2014 Arguments for ComplexHeatmap::Heatmap() devpars \u2014 The parameters for device. draw \u2014 Options for ComplexHeatmap::draw() globals \u2014 Some globals for the expression in args to be evaluated inopts \u2014 Options for read.table() to read in.infile seed \u2014 The seed Requires bioconductor-complexheatmap \u2014 check: {{proc.lang}} <(echo \"library(ComplexHeatmap)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.plot.Heatmap"},{"location":"api/biopipen.ns.plot/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.plot/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.plot/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.plot/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.plot/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.plot/#biopipennsplotroc","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot ROC curve using plotROC . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for data, tab-separated.The first column should be ids of the records (this is optional if envs.noids is True). The second column should be the labels of the records (1 for positive, 0 for negative). If they are not binary, you can specify the positive label by envs.pos_label . From the third column, it should be the scores of the different models. Output outfile \u2014 The output figure file Envs args \u2014 Additional arguments for geom_roc() or geom_rocci() if envs.ci is True. ci \u2014 Whether to use geom_rocci() instead of geom_roc() . devpars \u2014 The parameters for png() noids \u2014 Whether the input file has ids (first column) or not. pos_label \u2014 The positive label. style_roc \u2014 Arguments for style_roc() Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.plot.ROC"},{"location":"api/biopipen.ns.plot/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.plot/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.plot/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.plot/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.plot/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.plot/#biopipennsplotmanhattan","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Plot Manhattan plot. Using the ggmanh package. Requires ggmanh v1.9.6 or later. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for dataIt should contain at least three columns, the chromosome, the position and the p-value of the SNPs. Header is required. Output outfile \u2014 The output figure file Envs args (ns) \u2014 Additional arguments for manhattan_plot() .See https://rdrr.io/github/leejs-abv/ggmanh/man/manhattan_plot.html . Note that - will be replaced by . in the argument names. - : Additional arguments for manhattan_plot() chrom_col \u2014 The column for chromosomeAn integer (1-based) or a string indicating the column name. chroms (auto) \u2014 The chromosomes and order to plotA hyphen ( - ) can be used to indicate a range. For example chr1-22,chrX,chrY,chrM will plot all autosomes, X, Y and M. if auto , only the chromosomes in the data will be plotted in the order they appear in the data. devpars (ns) \u2014 The parameters for png() - res (type=int): The resolution - width (type=int): The width - height (type=int): The height hicolors (auto) \u2014 The colors for significant and non-significant SNPsIf a single color is given, the non-significant SNPs will be in grey. Set it to None to disable the highlighting. label_col \u2014 The column for label.Once specified, the significant SNPs will be labeled on the plot. pos_col \u2014 The column for positionAn integer (1-based) or a string indicating the column name. pval_col \u2014 The column for p-valueAn integer (1-based) or a string indicating the column name. rescale (flag) \u2014 Whether to rescale the p-values rescale_ratio_threshold (type=float) \u2014 Threshold of that triggers the rescale signif (auto) \u2014 A single value or a list of values to indicate the significance levelsMultiple values should be also separated by comma ( , ). The minimum value will be used as the cutoff to determine if the SNPs are significant. thin_bins (type=int) \u2014 Number of bins to partition the data. thin_n (type=int) \u2014 Number of max points per horizontal partitions of the plot. 0 or None to disable thinning. title \u2014 The title of the plot ylabel \u2014 The y-axis label zoom (auto) \u2014 Chromosomes to zoom inEach chromosome should be separated by comma ( , ) or in a list. Single chromosome is also accepted. Ranges are also accepted, see envs.chroms . Each chromosome will be saved in a separate file. zoom_devpars (ns) \u2014 The parameters for the zoomed plot - width (type=int): The width - height (type=int): The height, inherited from devpars by default - res (type=int): The resolution, inherited from devpars by default Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.plot.Manhattan"},{"location":"api/biopipen.ns.plot/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.plot/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.plot/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.plot/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.plot/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.plot/#biopipennsplotqqplot","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Generate QQ-plot or PP-plot using qqplotr. See https://cran.r-project.org/web/packages/qqplotr/vignettes/introduction.html . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for dataIt should contain at least one column of p-values or the values to be plotted. Header is required. theorfile \u2014 The file for theoretical values (optional)This file should contain at least one column of theoretical values. The values will be passed to envs.theor_qfunc to calculate the theoretical quantiles. Header is required. Output outfile \u2014 The output figure file Envs args (ns) \u2014 The common arguments for envs.band , envs.line and envs.point . - distribution: The distribution of the theoretical quantiles When custom is used, the envs.theor_col should be provided and values will be added to dparams automatically. - dparams (type=json): The parameters for the distribution - : Other shared arguments between stat_*_band , stat_*_line and stat_*_point . band (ns) \u2014 The arguments for stat_qq_band() or stat_pp_band() .See https://rdrr.io/cran/qqplotr/man/stat_qq_band.html and https://rdrr.io/cran/qqplotr/man/stat_pp_band.html . Set to None or band.disabled to True to disable the band. - disabled (flag): Disable the band - distribution: The distribution of the theoretical quantiles When custom is used, the envs.theor_col should be provided and values will be added to dparams automatically. - dparams (type=json): The parameters for the distribution - : Additional arguments for stat_qq_band() or stat_pp_band() devpars (ns) \u2014 The parameters for png() - res (type=int): The resolution - width (type=int): The width - height (type=int): The height ggs (list) \u2014 Additional ggplot expression to adjust the plot. kind (choice) \u2014 The kind of the plot, qq or pp - qq: QQ-plot - pp: PP-plot line (ns) \u2014 The arguments for stat_qq_line() or stat_pp_line() .See https://rdrr.io/cran/qqplot/man/stat_qq_line.html and https://rdrr.io/cran/qqplot/man/stat_pp_line.html . Set to None or line.disabled to True to disable the line. - disabled (flag): Disable the line - distribution: The distribution of the theoretical quantiles When custom is used, the envs.theor_col should be provided and values will be added to dparams automatically. - dparams (type=json): The parameters for the distribution - : Additional arguments for stat_qq_line() or stat_pp_line() point (ns) \u2014 The arguments for geom_qq_point() or geom_pp_point() .See https://rdrr.io/cran/qqplot/man/stat_qq_point.html and https://rdrr.io/cran/qqplot/man/stat_pp_point.html . Set to None or point.disabled to True to disable the point. - disabled (flag): Disable the point - distribution: The distribution of the theoretical quantiles When custom is used, the envs.theor_col should be provided and values will be added to dparams automatically. - dparams (type=json): The parameters for the distribution - : Additional arguments for geom_qq_point() or geom_pp_point() theor_col \u2014 The column for theoretical values in in.theorfile if provided,otherwise in in.infile . An integer (1-based) or a string indicating the column name. If distribution of band , line , or point is custom , this column must be provided. theor_funs (ns) \u2014 The R functions to generate density, quantile and deviatesof the theoretical distribution base on the theoretical values if distribution of band , line , or point is custom . - dcustom: The density function, used by band - qcustom: The quantile function, used by point - rcustom: The deviates function, used by line theor_trans \u2014 The transformation of the theoretical values.The theor_funs have default functions to take the theoretical values. This transformation will be applied to the theoretical values before passing to the theor_funs . title \u2014 The title of the plot trans \u2014 The transformation of the valuesYou can use -log10 to transform the values to -log10(values) . Otherwise you can a direct R function or a custom R function. For example function(x) -log10(x) . val_col \u2014 The column for values to be plottedAn integer (1-based) or a string indicating the column name. xlabel \u2014 The x-axis label ylabel \u2014 The y-axis label Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.plot.QQPlot"},{"location":"api/biopipen.ns.plot/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.plot/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.plot/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.plot/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.plot/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.plot/#biopipennsplotscatter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Generate scatter plot using ggplot2. ggpmisc is used for the stats and labels. See also https://cran.r-project.org/web/packages/ggpmisc/vignettes/model-based-annotations.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file for dataIt should contain at least two columns for x and y values. Header is required. Output outfile \u2014 The output figure file Envs args (ns) \u2014 Additional arguments for geom_point() See https://ggplot2.tidyverse.org/reference/geom_point.html . - : Additional arguments for geom_point() devpars (ns) \u2014 The parameters for png() - res (type=int): The resolution - width (type=int): The width - height (type=int): The height formula \u2014 The formula for the model ggs (list) \u2014 Additional ggplot expression to adjust the plot. mapping \u2014 Extra mapping for all geoms, including stats .Should be aes(color = group) but all these are valid: color = group or (color = group) . stats (type=json) \u2014 The stats to add to the plot.A dict with keys available stats in ggpmisc (without stat_ ). See https://cran.r-project.org/web/packages/ggpmisc/vignettes/model-based-annotations.html#statistics . The values should be the arguments for the stats. If you want a stat to be added multiple times, add a suffix #x to the key. For example, poly_line#1 and poly_line#2 will add two polynomial lines. x_col \u2014 The column for x valuesAn integer (1-based) or a string indicating the column name. y_col \u2014 The column for y valuesAn integer (1-based) or a string indicating the column name. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.plot.Scatter"},{"location":"api/biopipen.ns.plot/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.plot/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.plot/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.plot/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.plot/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.plot/#pipenprocprocrun_5","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.protein/","text":"module biopipen.ns . protein </> Protein-related processes. Classes Prodigy ( Proc ) \u2014 Prediction of binding affinity of protein-protein complexes based onintermolecular contacts using Prodigy. </> ProdigySummary ( Proc ) \u2014 Summary of the output from Prodigy . </> MMCIF2PDB ( Proc ) \u2014 Convert mmCIF or PDBx file to PDB file. </> RMSD ( Proc ) \u2014 Calculate the RMSD between two structures. </> PDB2Fasta ( Proc ) \u2014 Convert PDB file to FASTA file. </> class biopipen.ns.protein . Prodigy ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Prediction of binding affinity of protein-protein complexes based onintermolecular contacts using Prodigy. See https://rascar.science.uu.nl/prodigy/ and https://github.com/haddocking/prodigy . prodigy-prot must be installed under the given python of proc.lang . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The structure file in PDB or mmCIF format. Output outdir \u2014 The output directory containing all output files. outfile \u2014 The output file generated by Prodigy. Envs acc_threshold (type=float) \u2014 The accessibility threshold for BSA analysis. contact_list (flag) \u2014 Whether to generate contact list. distance_cutoff (type=float) \u2014 The distance cutoff to calculate intermolecularcontacts. outtype (choice) \u2014 Set the format of the output file ( out.outfile ).All three files will be generated. This option only determines which is assigned to out.outfile . - raw: The raw output file from prodigy. - json: The output file in JSON format. - tsv: The output file in CSV format. pymol_selection (flag) \u2014 Whether output a script to highlight the interfaceresidues in PyMOL. selection (list) \u2014 The selection of the chains to analyze. ['A', 'B'] will analyze chains A and B. ['A,B', 'C'] will analyze chain A and C; and B and C. ['A', 'B', 'C'] will analyze all combinations of A, B, and C. temperature (type=float) \u2014 The temperature (C) for Kd prediction. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.protein . ProdigySummary ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Summary of the output from Prodigy . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infiles \u2014 The output json file generated by Prodigy . Output outdir \u2014 The directory of summary files generated by ProdigySummary . Envs group (type=auto) \u2014 The group of the samples for boxplots.If None , don't do boxplots. It can be a dict of group names and sample names, e.g. {\"group1\": [\"sample1\", \"sample2\"], \"group2\": [\"sample3\"]} or a file containing the group information, with the first column being the sample names and the second column being the group names. The file should be tab-delimited with no header. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.protein . MMCIF2PDB ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert mmCIF or PDBx file to PDB file. Using BeEM Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input mmCIF or PDBx file. Output outfile \u2014 The output PDB file.The \"outfmt\" set to 3 to always output a single PDB file. Envs \u2014 Other options for MAXIT/BeEM.For BeEM, \"outfmt\" will not be used as it is set to 3. beem \u2014 The path to the BeEM executable. maxit \u2014 The path to the MAXIT executable. tool (choice) \u2014 The tool to use for conversion. - maxit: Use MAXIT. - beem: Use BeEM. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.protein . RMSD ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate the RMSD between two structures. See also https://github.com/charnley/rmsd. If the input is in mmCIF format, convert it to PDB first. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile1 \u2014 The first structure file. infile2 \u2014 The second structure file. Output outfile \u2014 The output file containing the RMSD value. Envs \u2014 Other options for calculate_rmsd. beem \u2014 The path to the BeEM executable. ca_only (flag) \u2014 Whether to calculate RMSD using only C-alpha atoms. calculate_rmsd \u2014 The path to the calculate_rmsd executable. conv_tool (choice) \u2014 The tool to use for conversion. - maxit: Use MAXIT. - beem: Use BeEM. duel (choice) \u2014 How to handle the duel atoms. Default is \"keep\". - keep: Keep both atoms. - keep_first: Keep the first atom. - keep_last: Keep the last atom. - average: Average the coordinates. reorder (flag) \u2014 Whether to reorder the atoms in the structures. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.protein . PDB2Fasta ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert PDB file to FASTA file. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input PDB file. Output outfile \u2014 The output FASTA file. Envs chains (auto) \u2014 The chains to extract. A list of chain IDs or separated bycommas. If None, extract all chains. wrap (type=int) \u2014 The number of residues per line in the output FASTAfile. Set to 0 to disable wrapping. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.protein"},{"location":"api/biopipen.ns.protein/#biopipennsprotein","text":"</> Protein-related processes. Classes Prodigy ( Proc ) \u2014 Prediction of binding affinity of protein-protein complexes based onintermolecular contacts using Prodigy. </> ProdigySummary ( Proc ) \u2014 Summary of the output from Prodigy . </> MMCIF2PDB ( Proc ) \u2014 Convert mmCIF or PDBx file to PDB file. </> RMSD ( Proc ) \u2014 Calculate the RMSD between two structures. </> PDB2Fasta ( Proc ) \u2014 Convert PDB file to FASTA file. </> class","title":"biopipen.ns.protein"},{"location":"api/biopipen.ns.protein/#biopipennsproteinprodigy","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Prediction of binding affinity of protein-protein complexes based onintermolecular contacts using Prodigy. See https://rascar.science.uu.nl/prodigy/ and https://github.com/haddocking/prodigy . prodigy-prot must be installed under the given python of proc.lang . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The structure file in PDB or mmCIF format. Output outdir \u2014 The output directory containing all output files. outfile \u2014 The output file generated by Prodigy. Envs acc_threshold (type=float) \u2014 The accessibility threshold for BSA analysis. contact_list (flag) \u2014 Whether to generate contact list. distance_cutoff (type=float) \u2014 The distance cutoff to calculate intermolecularcontacts. outtype (choice) \u2014 Set the format of the output file ( out.outfile ).All three files will be generated. This option only determines which is assigned to out.outfile . - raw: The raw output file from prodigy. - json: The output file in JSON format. - tsv: The output file in CSV format. pymol_selection (flag) \u2014 Whether output a script to highlight the interfaceresidues in PyMOL. selection (list) \u2014 The selection of the chains to analyze. ['A', 'B'] will analyze chains A and B. ['A,B', 'C'] will analyze chain A and C; and B and C. ['A', 'B', 'C'] will analyze all combinations of A, B, and C. temperature (type=float) \u2014 The temperature (C) for Kd prediction. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.protein.Prodigy"},{"location":"api/biopipen.ns.protein/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.protein/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.protein/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.protein/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.protein/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.protein/#biopipennsproteinprodigysummary","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Summary of the output from Prodigy . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infiles \u2014 The output json file generated by Prodigy . Output outdir \u2014 The directory of summary files generated by ProdigySummary . Envs group (type=auto) \u2014 The group of the samples for boxplots.If None , don't do boxplots. It can be a dict of group names and sample names, e.g. {\"group1\": [\"sample1\", \"sample2\"], \"group2\": [\"sample3\"]} or a file containing the group information, with the first column being the sample names and the second column being the group names. The file should be tab-delimited with no header. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.protein.ProdigySummary"},{"location":"api/biopipen.ns.protein/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.protein/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.protein/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.protein/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.protein/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.protein/#biopipennsproteinmmcif2pdb","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert mmCIF or PDBx file to PDB file. Using BeEM Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input mmCIF or PDBx file. Output outfile \u2014 The output PDB file.The \"outfmt\" set to 3 to always output a single PDB file. Envs \u2014 Other options for MAXIT/BeEM.For BeEM, \"outfmt\" will not be used as it is set to 3. beem \u2014 The path to the BeEM executable. maxit \u2014 The path to the MAXIT executable. tool (choice) \u2014 The tool to use for conversion. - maxit: Use MAXIT. - beem: Use BeEM. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.protein.MMCIF2PDB"},{"location":"api/biopipen.ns.protein/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.protein/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.protein/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.protein/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.protein/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.protein/#biopipennsproteinrmsd","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate the RMSD between two structures. See also https://github.com/charnley/rmsd. If the input is in mmCIF format, convert it to PDB first. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile1 \u2014 The first structure file. infile2 \u2014 The second structure file. Output outfile \u2014 The output file containing the RMSD value. Envs \u2014 Other options for calculate_rmsd. beem \u2014 The path to the BeEM executable. ca_only (flag) \u2014 Whether to calculate RMSD using only C-alpha atoms. calculate_rmsd \u2014 The path to the calculate_rmsd executable. conv_tool (choice) \u2014 The tool to use for conversion. - maxit: Use MAXIT. - beem: Use BeEM. duel (choice) \u2014 How to handle the duel atoms. Default is \"keep\". - keep: Keep both atoms. - keep_first: Keep the first atom. - keep_last: Keep the last atom. - average: Average the coordinates. reorder (flag) \u2014 Whether to reorder the atoms in the structures. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.protein.RMSD"},{"location":"api/biopipen.ns.protein/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.protein/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.protein/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.protein/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.protein/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.protein/#biopipennsproteinpdb2fasta","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert PDB file to FASTA file. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input PDB file. Output outfile \u2014 The output FASTA file. Envs chains (auto) \u2014 The chains to extract. A list of chain IDs or separated bycommas. If None, extract all chains. wrap (type=int) \u2014 The number of residues per line in the output FASTAfile. Set to 0 to disable wrapping. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.protein.PDB2Fasta"},{"location":"api/biopipen.ns.protein/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.protein/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.protein/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.protein/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.protein/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.protein/#pipenprocprocrun_4","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.regulatory/","text":"module biopipen.ns . regulatory </> Provides processes for the regulatory related Classes MotifScan ( Proc ) \u2014 Scan the input sequences for binding sites using motifs. </> MotifAffinityTest ( Proc ) \u2014 Test the affinity of motifs to the sequences and the affinity changedue the mutations. </> VariantMotifPlot ( Proc ) \u2014 A plot with a genomic region surrounding a genomic variant, andpotentially disrupted motifs. </> class biopipen.ns.regulatory . MotifScan ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Scan the input sequences for binding sites using motifs. Currently only fimo from MEME suite is supported, based on the research/comparisons done by the following reference. Reference: - Evaluating tools for transcription factor binding site prediction Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input motiffile \u2014 File containing motif names.The file contains the motif and regulator names. The motif names should match the names in the motif database. This file must have a header. If multiple columns are present, it should be delimited by tab. seqfile \u2014 File containing sequences in FASTA format. Output outdir \u2014 Directory containing the results.Especially fimo_output.txt extending from fimo.tsv , which contains: 1. the results with the regulator information if envs.regulator_col is provided, otherwise, the regulator columns will be filled with the motif names. 2. the original sequence from the fasta file (in.seqfile) 3. corrected genomic coordinates if the genomic coordinates are included in the sequence names. See also the Output section of https://meme-suite.org/meme/doc/fimo.html . Note that --no-pgc is passed to fimo to not parse the genomic coordinates from the sequence names by fimo. When fimo parses the genomic coordinates, DDX11L1 in >DDX11L1::chr1:11869-14412 will be lost. The purpose of this is to keep the sequence names as they are in the output. If the sequence names are in the format of >NAME::chr1:START-END , we will correct the coordinates in the output. Also note that it requires meme/fimo v5.5.5+ to do this (where the --no-pgc option is available). Envs args (ns) \u2014 Additional arguments to pass to the tool. - : Additional arguments for fimo. See: https://meme-suite.org/meme/doc/fimo.html cutoff (type=float) \u2014 The cutoff for p-value to write the results.When envs.q_cutoff is set, this is applied to the q-value. This is passed to --thresh in fimo. fimo \u2014 The path to fimo binary. motif_col \u2014 The column name in the motif file containing the motif names. motifdb \u2014 The path to the motif database. This is required.It should be in the format of MEME motif database. Databases can be downloaded here: https://meme-suite.org/meme/doc/download.html . See also introduction to the databases: https://meme-suite.org/meme/db/motifs . notfound (choice) \u2014 What to do if a motif is not found in the database. - error: Report error and stop the process. - ignore: Ignore the motif and continue. q (flag) \u2014 Calculate q-value.When False , --no-qvalue is passed to fimo. The q-value calculation is that of Benjamini and Hochberg (BH) (1995). q_cutoff (flag) \u2014 Apply envs.cutoff to q-value. regulator_col \u2014 The column name in the motif file containing the regulator names.Both motif_col and regulator_col should be the direct column names or the index (1-based) of the columns. If no regulator_col is provided, no regulator information is written in the output. tool (choice) \u2014 The tool to use for scanning.Currently only fimo is supported. - fimo: Use fimo from MEME suite. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.regulatory . MotifAffinityTest ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Test the affinity of motifs to the sequences and the affinity changedue the mutations. See also https://simon-coetzee.github.io/motifBreakR and https://www.bioconductor.org/packages/release/bioc/vignettes/atSNP/inst/doc/atsnp-vignette.html When using atSNP, motifBreakR is also required to plot the variants and motifs. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input motiffile \u2014 File containing motif names.The file contains the motif and regulator names. The motif names should match the names in the motif database. This file must have a header. If multiple columns are present, it should be delimited by tab. varfile \u2014 File containing the variants.It could be a VCF file or a BED-like file. If it is a VCF file, it does not need to be indexed. Only records with PASS in the FILTER column are used. If it is a BED-like file, it should contain the following columns, chrom , start , end , name , score , strand , ref , alt . Output outdir \u2014 Directory containing the results.For motifBreakR, motifbreakr.txt will be created. Records with effect strong / weak are written ( neutral is not). For atSNP, atsnp.txt will be created. Records with p-value ( envs.atsnp_args.p ) < envs.cutoff are written. Envs atsnp_args (ns) \u2014 Additional arguments to pass to atSNP. - padj_cutoff (flag): The envs.cutoff will be applied to the adjusted p-value. Only works for atSNP . - padj (choice): The method to adjust the p-values. Only works for atSNP - holm: Holm's method - hochberg: Hochberg's method - hommel: Hommel's method - bonferroni: Bonferroni method - BH: Benjamini & Hochberg's method - BY: Benjamini & Yekutieli's method - fdr: False discovery rate - none: No adjustment - p (choice): Which p-value to use for adjustment and cutoff. - pval_ref: p-value for the reference allele affinity score. - pval_snp: p-value for the SNP allele affinity score. - pval_cond_ref: and - pval_cond_snp: conditional p-values for the affinity scores of the reference and SNP alleles. - pval_diff: p-value for the affinity score change between the two alleles. - pval_rank: p-value for the rank test between the two alleles. bcftools \u2014 The path to bcftools binary.Used to convert the VCF file to the BED file when the input is a VCF file. cutoff (type=float) \u2014 The cutoff for p-value to write the results. devpars (ns) \u2014 The default device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. genome \u2014 The genome assembly.Used to fetch the sequences around the variants by package, for example, BSgenome.Hsapiens.UCSC.hg19 is required if hg19 . If it is an organism other than human, please specify the full name of the package, for example, BSgenome.Mmusculus.UCSC.mm10 . motif_col \u2014 The column name in the motif file containing the motif names.If this is not provided, envs.regulator_col and envs.regmotifs are required, which are used to infer the motif names from the regulator names. motifbreakr_args (ns) \u2014 Additional arguments to pass to motifBreakR. - method (choice): The method to use. See details of https://rdrr.io/bioc/motifbreakR/man/motifbreakR.html and https://simon-coetzee.github.io/motifBreakR/#methods . - default: Use the default method. - log: Use the standard summation of log probabilities - ic: Use information content - notrans: Use the default method without transformation motifdb \u2014 The path to the motif database. This is required.It should be in the format of MEME motif database. Databases can be downloaded here: https://meme-suite.org/meme/doc/download.html . See also introduction to the databases: https://meme-suite.org/meme/db/motifs . universalmotif is required to read the motif database. ncores (type=int) \u2014 The number of cores to use. notfound (choice) \u2014 What to do if a motif is not found in the database,or a regulator is not found in the regulator-motif mapping (envs.regmotifs) file. - error: Report error and stop the process. - ignore: Ignore the motif and continue. plot_nvars (type=int) \u2014 Number of variants to plot.Plot top <plot_nvars> variants with the largest abs(alleleDiff) (motifBreakR) or smallest p-values (atSNP). plots (type=json) \u2014 Specify the details for the plots.When specified, plot_nvars is ignored. The keys are the variant names and the values are the details for the plots, including: devpars: The device parameters for the plot to override the default (envs.devpars). which: An expression passed to subset(results, subset = ...) to get the motifs for the variant to plot. Or an integer to get the top which motifs. For example, effect == \"strong\" to get the motifs with strong effect in motifBreakR result. regmotifs \u2014 The path to the regulator-motif mapping file.It must have header and the columns Motif or Model for motif names and TF , Regulator or Transcription factor for regulator names. regulator_col \u2014 The column name in the motif file containing the regulator names.Both motif_col and regulator_col should be the direct column names or the index (1-based) of the columns. If no regulator_col is provided, no regulator information is written in the output. Otherwise, the regulator information is written in the output in the Regulator column. tool (choice) \u2014 The tool to use for the test. - motifbreakr: Use motifBreakR. - motifBreakR: Use motifBreakR. - atsnp: Use atSNP. - atSNP: Use atSNP. var_col \u2014 The column names in the in.motiffile containing the variant information.It has to be matching the names in the in.varfile . This is helpful when we only need to test the pairs of variants and motifs in the in.motiffile . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.regulatory . VariantMotifPlot ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc A plot with a genomic region surrounding a genomic variant, andpotentially disrupted motifs. Currently only SNVs are supported. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 File containing the variants and motifs.It is a TAB-delimited file with the following columns: - chrom: The chromosome of the SNV. Alias: chr, seqnames. - start: The start position of the SNV, no matter 0- or 1-based. - end: The end position of the SNV, which will be used as the position of the SNV. - strand: Indicating the direction of the surrounding sequence matching the motif. - SNP_id: The name of the SNV. - REF: The reference allele of the SNV. - ALT: The alternative allele of the SNV. - providerId: The motif id. It can be specified by envs.motif_col . - providerName: The name of the motif provider. Optional. - Regulator: The regulator name. Optional, can be specified by envs.regulator_col . - motifPos: The position of the motif, relative to the position of the SNV. For example, '-8, 4' means the motif is 8 bp upstream and 4 bp downstream of the SNV. Envs devpars (ns) \u2014 The default device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. genome \u2014 The genome assembly.Used to fetch the sequences around the variants by package, for example, BSgenome.Hsapiens.UCSC.hg19 is required if hg19 . If it is an organism other than human, please specify the full name of the package, for example, BSgenome.Mmusculus.UCSC.mm10 . motif_col \u2014 The column name in the motif file containing the motif names.If this is not provided, envs.regulator_col and envs.regmotifs are required, which are used to infer the motif names from the regulator names. motifdb \u2014 The path to the motif database. This is required.It should be in the format of MEME motif database. Databases can be downloaded here: https://meme-suite.org/meme/doc/download.html . See also introduction to the databases: https://meme-suite.org/meme/db/motifs . universalmotif is required to read the motif database. notfound (choice) \u2014 What to do if a motif is not found in the database,or a regulator is not found in the regulator-motif mapping (envs.regmotifs) file. - error: Report error and stop the process. - ignore: Ignore the motif and continue. plot_vars (type=auto) \u2014 The variants (SNP_id) to plot.A list of variant names to plot or a string with the variant names separated by comma. When not specified, all variants are plotted. regmotifs \u2014 The path to the regulator-motif mapping file.It must have header and the columns Motif or Model for motif names and TF , Regulator or Transcription factor for regulator names. regulator_col \u2014 The column name in the motif file containing the regulator names.Both motif_col and regulator_col should be the direct column names or the index (1-based) of the columns. If no regulator_col is provided, no regulator information is written in the output. Otherwise, the regulator information is written in the output in the Regulator column. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.regulatory"},{"location":"api/biopipen.ns.regulatory/#biopipennsregulatory","text":"</> Provides processes for the regulatory related Classes MotifScan ( Proc ) \u2014 Scan the input sequences for binding sites using motifs. </> MotifAffinityTest ( Proc ) \u2014 Test the affinity of motifs to the sequences and the affinity changedue the mutations. </> VariantMotifPlot ( Proc ) \u2014 A plot with a genomic region surrounding a genomic variant, andpotentially disrupted motifs. </> class","title":"biopipen.ns.regulatory"},{"location":"api/biopipen.ns.regulatory/#biopipennsregulatorymotifscan","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Scan the input sequences for binding sites using motifs. Currently only fimo from MEME suite is supported, based on the research/comparisons done by the following reference. Reference: - Evaluating tools for transcription factor binding site prediction Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input motiffile \u2014 File containing motif names.The file contains the motif and regulator names. The motif names should match the names in the motif database. This file must have a header. If multiple columns are present, it should be delimited by tab. seqfile \u2014 File containing sequences in FASTA format. Output outdir \u2014 Directory containing the results.Especially fimo_output.txt extending from fimo.tsv , which contains: 1. the results with the regulator information if envs.regulator_col is provided, otherwise, the regulator columns will be filled with the motif names. 2. the original sequence from the fasta file (in.seqfile) 3. corrected genomic coordinates if the genomic coordinates are included in the sequence names. See also the Output section of https://meme-suite.org/meme/doc/fimo.html . Note that --no-pgc is passed to fimo to not parse the genomic coordinates from the sequence names by fimo. When fimo parses the genomic coordinates, DDX11L1 in >DDX11L1::chr1:11869-14412 will be lost. The purpose of this is to keep the sequence names as they are in the output. If the sequence names are in the format of >NAME::chr1:START-END , we will correct the coordinates in the output. Also note that it requires meme/fimo v5.5.5+ to do this (where the --no-pgc option is available). Envs args (ns) \u2014 Additional arguments to pass to the tool. - : Additional arguments for fimo. See: https://meme-suite.org/meme/doc/fimo.html cutoff (type=float) \u2014 The cutoff for p-value to write the results.When envs.q_cutoff is set, this is applied to the q-value. This is passed to --thresh in fimo. fimo \u2014 The path to fimo binary. motif_col \u2014 The column name in the motif file containing the motif names. motifdb \u2014 The path to the motif database. This is required.It should be in the format of MEME motif database. Databases can be downloaded here: https://meme-suite.org/meme/doc/download.html . See also introduction to the databases: https://meme-suite.org/meme/db/motifs . notfound (choice) \u2014 What to do if a motif is not found in the database. - error: Report error and stop the process. - ignore: Ignore the motif and continue. q (flag) \u2014 Calculate q-value.When False , --no-qvalue is passed to fimo. The q-value calculation is that of Benjamini and Hochberg (BH) (1995). q_cutoff (flag) \u2014 Apply envs.cutoff to q-value. regulator_col \u2014 The column name in the motif file containing the regulator names.Both motif_col and regulator_col should be the direct column names or the index (1-based) of the columns. If no regulator_col is provided, no regulator information is written in the output. tool (choice) \u2014 The tool to use for scanning.Currently only fimo is supported. - fimo: Use fimo from MEME suite. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.regulatory.MotifScan"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.regulatory/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.regulatory/#biopipennsregulatorymotifaffinitytest","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Test the affinity of motifs to the sequences and the affinity changedue the mutations. See also https://simon-coetzee.github.io/motifBreakR and https://www.bioconductor.org/packages/release/bioc/vignettes/atSNP/inst/doc/atsnp-vignette.html When using atSNP, motifBreakR is also required to plot the variants and motifs. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input motiffile \u2014 File containing motif names.The file contains the motif and regulator names. The motif names should match the names in the motif database. This file must have a header. If multiple columns are present, it should be delimited by tab. varfile \u2014 File containing the variants.It could be a VCF file or a BED-like file. If it is a VCF file, it does not need to be indexed. Only records with PASS in the FILTER column are used. If it is a BED-like file, it should contain the following columns, chrom , start , end , name , score , strand , ref , alt . Output outdir \u2014 Directory containing the results.For motifBreakR, motifbreakr.txt will be created. Records with effect strong / weak are written ( neutral is not). For atSNP, atsnp.txt will be created. Records with p-value ( envs.atsnp_args.p ) < envs.cutoff are written. Envs atsnp_args (ns) \u2014 Additional arguments to pass to atSNP. - padj_cutoff (flag): The envs.cutoff will be applied to the adjusted p-value. Only works for atSNP . - padj (choice): The method to adjust the p-values. Only works for atSNP - holm: Holm's method - hochberg: Hochberg's method - hommel: Hommel's method - bonferroni: Bonferroni method - BH: Benjamini & Hochberg's method - BY: Benjamini & Yekutieli's method - fdr: False discovery rate - none: No adjustment - p (choice): Which p-value to use for adjustment and cutoff. - pval_ref: p-value for the reference allele affinity score. - pval_snp: p-value for the SNP allele affinity score. - pval_cond_ref: and - pval_cond_snp: conditional p-values for the affinity scores of the reference and SNP alleles. - pval_diff: p-value for the affinity score change between the two alleles. - pval_rank: p-value for the rank test between the two alleles. bcftools \u2014 The path to bcftools binary.Used to convert the VCF file to the BED file when the input is a VCF file. cutoff (type=float) \u2014 The cutoff for p-value to write the results. devpars (ns) \u2014 The default device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. genome \u2014 The genome assembly.Used to fetch the sequences around the variants by package, for example, BSgenome.Hsapiens.UCSC.hg19 is required if hg19 . If it is an organism other than human, please specify the full name of the package, for example, BSgenome.Mmusculus.UCSC.mm10 . motif_col \u2014 The column name in the motif file containing the motif names.If this is not provided, envs.regulator_col and envs.regmotifs are required, which are used to infer the motif names from the regulator names. motifbreakr_args (ns) \u2014 Additional arguments to pass to motifBreakR. - method (choice): The method to use. See details of https://rdrr.io/bioc/motifbreakR/man/motifbreakR.html and https://simon-coetzee.github.io/motifBreakR/#methods . - default: Use the default method. - log: Use the standard summation of log probabilities - ic: Use information content - notrans: Use the default method without transformation motifdb \u2014 The path to the motif database. This is required.It should be in the format of MEME motif database. Databases can be downloaded here: https://meme-suite.org/meme/doc/download.html . See also introduction to the databases: https://meme-suite.org/meme/db/motifs . universalmotif is required to read the motif database. ncores (type=int) \u2014 The number of cores to use. notfound (choice) \u2014 What to do if a motif is not found in the database,or a regulator is not found in the regulator-motif mapping (envs.regmotifs) file. - error: Report error and stop the process. - ignore: Ignore the motif and continue. plot_nvars (type=int) \u2014 Number of variants to plot.Plot top <plot_nvars> variants with the largest abs(alleleDiff) (motifBreakR) or smallest p-values (atSNP). plots (type=json) \u2014 Specify the details for the plots.When specified, plot_nvars is ignored. The keys are the variant names and the values are the details for the plots, including: devpars: The device parameters for the plot to override the default (envs.devpars). which: An expression passed to subset(results, subset = ...) to get the motifs for the variant to plot. Or an integer to get the top which motifs. For example, effect == \"strong\" to get the motifs with strong effect in motifBreakR result. regmotifs \u2014 The path to the regulator-motif mapping file.It must have header and the columns Motif or Model for motif names and TF , Regulator or Transcription factor for regulator names. regulator_col \u2014 The column name in the motif file containing the regulator names.Both motif_col and regulator_col should be the direct column names or the index (1-based) of the columns. If no regulator_col is provided, no regulator information is written in the output. Otherwise, the regulator information is written in the output in the Regulator column. tool (choice) \u2014 The tool to use for the test. - motifbreakr: Use motifBreakR. - motifBreakR: Use motifBreakR. - atsnp: Use atSNP. - atSNP: Use atSNP. var_col \u2014 The column names in the in.motiffile containing the variant information.It has to be matching the names in the in.varfile . This is helpful when we only need to test the pairs of variants and motifs in the in.motiffile . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.regulatory.MotifAffinityTest"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.regulatory/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.regulatory/#biopipennsregulatoryvariantmotifplot","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc A plot with a genomic region surrounding a genomic variant, andpotentially disrupted motifs. Currently only SNVs are supported. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 File containing the variants and motifs.It is a TAB-delimited file with the following columns: - chrom: The chromosome of the SNV. Alias: chr, seqnames. - start: The start position of the SNV, no matter 0- or 1-based. - end: The end position of the SNV, which will be used as the position of the SNV. - strand: Indicating the direction of the surrounding sequence matching the motif. - SNP_id: The name of the SNV. - REF: The reference allele of the SNV. - ALT: The alternative allele of the SNV. - providerId: The motif id. It can be specified by envs.motif_col . - providerName: The name of the motif provider. Optional. - Regulator: The regulator name. Optional, can be specified by envs.regulator_col . - motifPos: The position of the motif, relative to the position of the SNV. For example, '-8, 4' means the motif is 8 bp upstream and 4 bp downstream of the SNV. Envs devpars (ns) \u2014 The default device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. genome \u2014 The genome assembly.Used to fetch the sequences around the variants by package, for example, BSgenome.Hsapiens.UCSC.hg19 is required if hg19 . If it is an organism other than human, please specify the full name of the package, for example, BSgenome.Mmusculus.UCSC.mm10 . motif_col \u2014 The column name in the motif file containing the motif names.If this is not provided, envs.regulator_col and envs.regmotifs are required, which are used to infer the motif names from the regulator names. motifdb \u2014 The path to the motif database. This is required.It should be in the format of MEME motif database. Databases can be downloaded here: https://meme-suite.org/meme/doc/download.html . See also introduction to the databases: https://meme-suite.org/meme/db/motifs . universalmotif is required to read the motif database. notfound (choice) \u2014 What to do if a motif is not found in the database,or a regulator is not found in the regulator-motif mapping (envs.regmotifs) file. - error: Report error and stop the process. - ignore: Ignore the motif and continue. plot_vars (type=auto) \u2014 The variants (SNP_id) to plot.A list of variant names to plot or a string with the variant names separated by comma. When not specified, all variants are plotted. regmotifs \u2014 The path to the regulator-motif mapping file.It must have header and the columns Motif or Model for motif names and TF , Regulator or Transcription factor for regulator names. regulator_col \u2014 The column name in the motif file containing the regulator names.Both motif_col and regulator_col should be the direct column names or the index (1-based) of the columns. If no regulator_col is provided, no regulator information is written in the output. Otherwise, the regulator information is written in the output in the Regulator column. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.regulatory.VariantMotifPlot"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.regulatory/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.regulatory/#pipenprocprocrun_2","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.rnaseq/","text":"module biopipen.ns . rnaseq </> RNA-seq data analysis Classes UnitConversion ( Proc ) \u2014 Convert expression value units back and forth </> Simulation ( Proc ) \u2014 Simulate RNA-seq data using ESCO/RUVcorr package </> class biopipen.ns.rnaseq . UnitConversion ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert expression value units back and forth See https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/ and https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/#fpkm . Following converstions are supported - * count -> cpm, fpkm/rpkm, fpkmuq/rpkmrq, tpm, tmm * fpkm/rpkm -> count, tpm, cpm * tpm -> count, fpkm/rpkm, cpm * cpm -> count, fpkm/rpkm, tpm NOTE that during some conversions, sum(counts/effLen) is approximated to sum(counts)/sum(effLen) * length(effLen)) You can also use this process to just transform the expression values, e.g., take log2 of the expression values. In this case, you can set inunit and outunit to count and log2(count + 1) respectively. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 Input file containing expression valuesThe file should be a matrix with rows representing genes and columns representing samples. It could be an RDS file containing a data frame or a matrix, or a text file containing a matrix with tab as the delimiter. The text file can be gzipped. Output outfile \u2014 Output file containing the converted expression valuesThe file will be a matrix with rows representing genes and columns representing samples. Envs inunit \u2014 The input unit of the expression values.You can also use an expression to indicate the input unit, e.g., log2(counts + 1) . The expression should be like A * fn(B*X + C) + D , where A , B , C and D are constants, fn is a function, and X is the input unit. Currently only expr , sqrt , log2 , log10 and log are supported as functions. Supported input units are: * counts/count/rawcounts/rawcount: raw counts. * cpm: counts per million. * fpkm/rpkm: fragments per kilobase of transcript per million. * fpkmuq/rpkmuq: upper quartile normalized FPKM/RPKM. * tpm: transcripts per million. * tmm: trimmed mean of M-values. meanfl (type=auto) \u2014 A file containing the mean fragment length for each sampleby rows (samples as rowname), without header. Or a fixed universal estimated number (1 used by TCGA). nreads (type=auto) \u2014 The estimatied total number of reads for each sample.or you can pass a file with the number for each sample by rows (samples as rowname), without header. When converting fpkm/rpkm -> count , it should be total reads of that sample. When converting cpm -> count : it should be total reads of that sample. When converting tpm -> count : it should be total reads of that sample. When converting tpm -> cpm : it should be total reads of that sample. When converting tpm -> fpkm/rpkm : it should be sum(fpkm) of that sample. It is not used when converting count -> cpm, fpkm/rpkm, tpm . outunit \u2014 The output unit of the expression values. An expression can also beused for transformation (e.g. log2(tpm + 1) ). If inunit is count , then this means we are converting raw counts to tpm, and transforming it to log2(tpm + 1) as the output. Any expression supported by R can be used. Same units as inunit are supported. refexon \u2014 Path to the reference exon gff file. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.rnaseq . Simulation ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Simulate RNA-seq data using ESCO/RUVcorr package Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input ngenes \u2014 Number of genes to simulate nsamples \u2014 Number of samples to simulateIf you want to force the process to re-simulate for the same ngenes and nsamples , you can set a different value for envs.seed . Note that the samples will be shown as cells in the output (since the simulation is designed for single-cell RNA-seq data). Output outdir \u2014 Output directory containing the simulated data sim.rds and True.rds will be generated. For ESCO , sim.rds contains the simulated data in a SingleCellExperiment object, and True.rds contains the matrix of true counts. For RUVcorr , sim.rds contains the simulated data in list with Truth , A matrix containing the values of X\u03b2; Y A matrix containing the values in Y ; Noise A matrix containing the values in W\u03b1 ; Sigma A matrix containing the true gene-gene correlations, as defined by X\u03b2; and Info A matrix containing some of the general information about the simulation. For all matrices, rows represent genes and columns represent samples. outfile \u2014 Output file containing the simulated data with rows representinggenes and columns representing samples. Envs esco_args (ns) \u2014 Additional arguments to pass to the simulation function. - save (choice): Which type of data to save to out.outfile . - simulated-truth : saves the simulated true counts. - zero-inflated : saves the zero-inflated counts. - down-sampled : saves the down-sampled counts. - type (choice): Which type of heterogenounity to use. - single: produces a single population. - group: produces distinct groups. - tree: produces distinct groups but admits a tree structure. - traj: produces distinct groups but admits a smooth trajectory structure. - : See https://rdrr.io/github/JINJINT/ESCO/man/escoParams.html . index_start (type=int) \u2014 The index to start from when naming the samples.Affects the sample names in out.outfile only. ncores (type=int) \u2014 Number of cores to use. ruvcorr_args (ns) \u2014 Additional arguments to pass to the simulationfunction. - : See https://rdrr.io/bioc/RUVcorr/man/simulateGEdata.html . seed (type=int) \u2014 Random seed.If not set, seed will not be set. tool (choice) \u2014 Which tool to use for simulation. - ESCO: uses the ESCO package. - RUVcorr: uses the RUVcorr package. transpose_output (flag) \u2014 If set, the output will be transposed. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.rnaseq"},{"location":"api/biopipen.ns.rnaseq/#biopipennsrnaseq","text":"</> RNA-seq data analysis Classes UnitConversion ( Proc ) \u2014 Convert expression value units back and forth </> Simulation ( Proc ) \u2014 Simulate RNA-seq data using ESCO/RUVcorr package </> class","title":"biopipen.ns.rnaseq"},{"location":"api/biopipen.ns.rnaseq/#biopipennsrnasequnitconversion","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert expression value units back and forth See https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/ and https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/#fpkm . Following converstions are supported - * count -> cpm, fpkm/rpkm, fpkmuq/rpkmrq, tpm, tmm * fpkm/rpkm -> count, tpm, cpm * tpm -> count, fpkm/rpkm, cpm * cpm -> count, fpkm/rpkm, tpm NOTE that during some conversions, sum(counts/effLen) is approximated to sum(counts)/sum(effLen) * length(effLen)) You can also use this process to just transform the expression values, e.g., take log2 of the expression values. In this case, you can set inunit and outunit to count and log2(count + 1) respectively. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 Input file containing expression valuesThe file should be a matrix with rows representing genes and columns representing samples. It could be an RDS file containing a data frame or a matrix, or a text file containing a matrix with tab as the delimiter. The text file can be gzipped. Output outfile \u2014 Output file containing the converted expression valuesThe file will be a matrix with rows representing genes and columns representing samples. Envs inunit \u2014 The input unit of the expression values.You can also use an expression to indicate the input unit, e.g., log2(counts + 1) . The expression should be like A * fn(B*X + C) + D , where A , B , C and D are constants, fn is a function, and X is the input unit. Currently only expr , sqrt , log2 , log10 and log are supported as functions. Supported input units are: * counts/count/rawcounts/rawcount: raw counts. * cpm: counts per million. * fpkm/rpkm: fragments per kilobase of transcript per million. * fpkmuq/rpkmuq: upper quartile normalized FPKM/RPKM. * tpm: transcripts per million. * tmm: trimmed mean of M-values. meanfl (type=auto) \u2014 A file containing the mean fragment length for each sampleby rows (samples as rowname), without header. Or a fixed universal estimated number (1 used by TCGA). nreads (type=auto) \u2014 The estimatied total number of reads for each sample.or you can pass a file with the number for each sample by rows (samples as rowname), without header. When converting fpkm/rpkm -> count , it should be total reads of that sample. When converting cpm -> count : it should be total reads of that sample. When converting tpm -> count : it should be total reads of that sample. When converting tpm -> cpm : it should be total reads of that sample. When converting tpm -> fpkm/rpkm : it should be sum(fpkm) of that sample. It is not used when converting count -> cpm, fpkm/rpkm, tpm . outunit \u2014 The output unit of the expression values. An expression can also beused for transformation (e.g. log2(tpm + 1) ). If inunit is count , then this means we are converting raw counts to tpm, and transforming it to log2(tpm + 1) as the output. Any expression supported by R can be used. Same units as inunit are supported. refexon \u2014 Path to the reference exon gff file. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.rnaseq.UnitConversion"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.rnaseq/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.rnaseq/#biopipennsrnaseqsimulation","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Simulate RNA-seq data using ESCO/RUVcorr package Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input ngenes \u2014 Number of genes to simulate nsamples \u2014 Number of samples to simulateIf you want to force the process to re-simulate for the same ngenes and nsamples , you can set a different value for envs.seed . Note that the samples will be shown as cells in the output (since the simulation is designed for single-cell RNA-seq data). Output outdir \u2014 Output directory containing the simulated data sim.rds and True.rds will be generated. For ESCO , sim.rds contains the simulated data in a SingleCellExperiment object, and True.rds contains the matrix of true counts. For RUVcorr , sim.rds contains the simulated data in list with Truth , A matrix containing the values of X\u03b2; Y A matrix containing the values in Y ; Noise A matrix containing the values in W\u03b1 ; Sigma A matrix containing the true gene-gene correlations, as defined by X\u03b2; and Info A matrix containing some of the general information about the simulation. For all matrices, rows represent genes and columns represent samples. outfile \u2014 Output file containing the simulated data with rows representinggenes and columns representing samples. Envs esco_args (ns) \u2014 Additional arguments to pass to the simulation function. - save (choice): Which type of data to save to out.outfile . - simulated-truth : saves the simulated true counts. - zero-inflated : saves the zero-inflated counts. - down-sampled : saves the down-sampled counts. - type (choice): Which type of heterogenounity to use. - single: produces a single population. - group: produces distinct groups. - tree: produces distinct groups but admits a tree structure. - traj: produces distinct groups but admits a smooth trajectory structure. - : See https://rdrr.io/github/JINJINT/ESCO/man/escoParams.html . index_start (type=int) \u2014 The index to start from when naming the samples.Affects the sample names in out.outfile only. ncores (type=int) \u2014 Number of cores to use. ruvcorr_args (ns) \u2014 Additional arguments to pass to the simulationfunction. - : See https://rdrr.io/bioc/RUVcorr/man/simulateGEdata.html . seed (type=int) \u2014 Random seed.If not set, seed will not be set. tool (choice) \u2014 Which tool to use for simulation. - ESCO: uses the ESCO package. - RUVcorr: uses the RUVcorr package. transpose_output (flag) \u2014 If set, the output will be transposed. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.rnaseq.Simulation"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.rnaseq/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.rnaseq/#pipenprocprocrun_1","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/","text":"module biopipen.ns . scrna </> Tools to analyze single-cell RNA Classes SeuratLoading ( Proc ) \u2014 Seurat - Loading data </> SeuratPreparing ( Proc ) \u2014 Load, prepare and apply QC to data, using Seurat </> SeuratClustering ( Proc ) \u2014 Determine the clusters of cells without reference using Seurat FindClustersprocedure. </> SeuratSubClustering ( Proc ) \u2014 Find clusters of a subset of cells. </> SeuratClusterStats ( Proc ) \u2014 Statistics of the clustering. </> ModuleScoreCalculator ( Proc ) \u2014 Calculate the module scores for each cell </> CellsDistribution ( Proc ) \u2014 Distribution of cells (i.e. in a TCR clone) from different groupsfor each cluster </> SeuratMetadataMutater ( Proc ) \u2014 Mutate the metadata of the seurat object </> DimPlots ( Proc ) \u2014 Seurat - Dimensional reduction plots </> MarkersFinder ( Proc ) \u2014 Find markers between different groups of cells </> TopExpressingGenes ( Proc ) \u2014 Find the top expressing genes in each cluster </> ExprImputation ( Proc ) \u2014 This process imputes the dropout values in scRNA-seq data. </> SCImpute ( Proc ) \u2014 Impute the dropout values in scRNA-seq data. </> SeuratFilter ( Proc ) \u2014 Filtering cells from a seurat object </> SeuratSubset ( Proc ) \u2014 Subset a seurat object into multiple seruat objects </> SeuratSplit ( Proc ) \u2014 Split a seurat object into multiple seruat objects </> Subset10X ( Proc ) \u2014 Subset 10X data, mostly used for testing </> SeuratTo10X ( Proc ) \u2014 Write a Seurat object to 10X format </> ScFGSEA ( Proc ) \u2014 Gene set enrichment analysis for cells in different groups using fgsea </> CellTypeAnnotation ( Proc ) \u2014 Annotate the cell clusters. Currently, four ways are supported: </> SeuratMap2Ref ( Proc ) \u2014 Map the seurat object to reference </> RadarPlots ( Proc ) \u2014 Radar plots for cell proportion in different clusters. </> MetaMarkers ( Proc ) \u2014 Find markers between three or more groups of cells, using one-way ANOVAor Kruskal-Wallis test. </> Seurat2AnnData ( Proc ) \u2014 Convert seurat object to AnnData </> AnnData2Seurat ( Proc ) \u2014 Convert AnnData to seurat object </> ScSimulation ( Proc ) \u2014 Simulate single-cell data using splatter. </> CellCellCommunication ( Proc ) \u2014 Cell-cell communication inference </> CellCellCommunicationPlots ( Proc ) \u2014 Visualization for cell-cell communication inference. </> ScVelo ( Proc ) \u2014 Velocity analysis for single-cell RNA-seq data </> Slingshot ( Proc ) \u2014 Trajectory inference using Slingshot </> LoomTo10X ( Proc ) \u2014 Convert Loom file to 10X format </> PseudoBulkDEG ( Proc ) \u2014 Pseduo-bulk differential gene expression analysis </> class biopipen.ns.scrna . SeuratLoading ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Loading data Deprecated, should be superseded by SeuratPreparing Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The metadata of the samplesA tab-delimited file Two columns are required: - Sample to specify the sample names. - RNAData to assign the path of the data to the samples The path will be read by Read10X() from Seurat Output rdsfile \u2014 The RDS file with a list of Seurat object Envs qc \u2014 The QC filter for each sample.This will be passed to subset(obj, subset=<qc>) . For example nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5 Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratPreparing ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Load, prepare and apply QC to data, using Seurat This process will - - Prepare the seurat object - Apply QC to the data - Integrate the data from different samples See also - https://satijalab.org/seurat/articles/pbmc3k_tutorial.html#standard-pre-processing-workflow-1) - https://satijalab.org/seurat/articles/integration_introduction This process will read the scRNA-seq data, based on the information provided by SampleInfo , specifically, the paths specified by the RNAData column. Those paths should be either paths to directoies containing matrix.mtx , barcodes.tsv and features.tsv files that can be loaded by Seurat::Read10X() , or paths of loom files that can be loaded by SeuratDisk::LoadLoom() , or paths to h5 files that can be loaded by Seurat::Read10X_h5() . Each sample will be loaded individually and then merged into one Seurat object, and then perform QC. In order to perform QC, some additional columns are added to the meta data of the Seurat object. They are: precent.mt : The percentage of mitochondrial genes. percent.ribo : The percentage of ribosomal genes. precent.hb : The percentage of hemoglobin genes. percent.plat : The percentage of platelet genes. For integration, two routes are available: Performing integration on datasets normalized with SCTransform Using NormalizeData and FindIntegrationAnchors /// Note When using SCTransform , the default Assay will be set to SCT in output, rather than RNA . If you are using cca or rpca interation, the default assay will be integrated . /// /// Note From biopipen v0.23.0, this requires Seurat v5.0.0 or higher. /// Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The metadata of the samplesA tab-delimited file Two columns are required: Sample to specify the sample names. RNAData to assign the path of the data to the samples The path will be read by Read10X() from Seurat , or the path to the h5 file that can be read by Read10X_h5() from Seurat . It can also be an RDS or qs2 file containing a Seurat object. Note that it must has a column named Sample in the meta.data to specify the sample names. Output outfile \u2014 The qs2 file with the Seurat object with all samples integrated.Note that the cell ids are prefixied with sample names. Envs DoubletFinder (ns) \u2014 Arguments to run DoubletFinder .See also https://demultiplexing-doublet-detecting-docs.readthedocs.io/en/latest/DoubletFinder.html . - PCs (type=int): Number of PCs to use for 'doubletFinder' function. - doublets (type=float): Number of expected doublets as a proportion of the pool size. - pN (type=float): Number of doublets to simulate as a proportion of the pool size. - ncores (type=int): Number of cores to use for DoubletFinder::paramSweep . Set to None to use envs.ncores . Since parallelization of the function usually exhausts memory, if big envs.ncores does not work for DoubletFinder , set this to a smaller number. FindVariableFeatures (ns) \u2014 Arguments for FindVariableFeatures() . object is specified internally, and - in the key will be replaced with . . - : See https://satijalab.org/seurat/reference/findvariablefeatures IntegrateLayers (ns) \u2014 Arguments for IntegrateLayers() . object is specified internally, and - in the key will be replaced with . . When use_sct is True , normalization-method defaults to SCT . - method (choice): The method to use for integration. - CCAIntegration: Use Seurat::CCAIntegration . - CCA: Same as CCAIntegration . - cca: Same as CCAIntegration . - RPCAIntegration: Use Seurat::RPCAIntegration . - RPCA: Same as RPCAIntegration . - rpca: Same as RPCAIntegration . - HarmonyIntegration: Use Seurat::HarmonyIntegration . - Harmony: Same as HarmonyIntegration . - harmony: Same as HarmonyIntegration . - FastMNNIntegration: Use Seurat::FastMNNIntegration . - FastMNN: Same as FastMNNIntegration . - fastmnn: Same as FastMNNIntegration . - scVIIntegration: Use Seurat::scVIIntegration . - scVI: Same as scVIIntegration . - scvi: Same as scVIIntegration . - : See https://satijalab.org/seurat/reference/integratelayers NormalizeData (ns) \u2014 Arguments for NormalizeData() . object is specified internally, and - in the key will be replaced with . . - : See https://satijalab.org/seurat/reference/normalizedata RunPCA (ns) \u2014 Arguments for RunPCA() . object and features is specified internally, and - in the key will be replaced with . . - npcs (type=int): The number of PCs to compute. For each sample, npcs will be no larger than the number of columns - 1. - : See https://satijalab.org/seurat/reference/runpca SCTransform (ns) \u2014 Arguments for SCTransform() . object is specified internally, and - in the key will be replaced with . . - return-only-var-genes: Whether to return only variable genes. - min_cells: The minimum number of cells that a gene must be expressed in to be kept. A hidden argument of SCTransform to filter genes. If you try to keep all genes in the RNA assay, you can set min_cells to 0 and return-only-var-genes to False . See https://github.com/satijalab/seurat/issues/3598#issuecomment-715505537 - : See https://satijalab.org/seurat/reference/sctransform ScaleData (ns) \u2014 Arguments for ScaleData() . object and features is specified internally, and - in the key will be replaced with . . - : See https://satijalab.org/seurat/reference/scaledata cache (type=auto) \u2014 Whether to cache the information at different steps.If True , the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. The cached seurat object will be saved as <signature>.<kind>.RDS file, where <signature> is the signature determined by the input and envs of the process. See https://github.com/satijalab/seurat/issues/7849 , https://github.com/satijalab/seurat/issues/5358 and https://github.com/satijalab/seurat/issues/6748 for more details also about reproducibility issues. To not use the cached seurat object, you can either set cache to False or delete the cached file at <signature>.RDS in the cache directory. cell_qc \u2014 Filter expression to filter cells, using tidyrseurat::filter() . It can also be a dictionary of expressions, where the names of the list are sample names. You can have a default expression in the list with the name \"DEFAULT\" for the samples that are not listed. Available QC keys include nFeature_RNA , nCount_RNA , percent.mt , percent.ribo , percent.hb , and percent.plat . /// Tip | Example Including the columns added above, all available QC keys include nFeature_RNA , nCount_RNA , percent.mt , percent.ribo , percent.hb , and percent.plat . For example: [SeuratPreparing.envs] cell_qc = \"nFeature_RNA > 200 & percent.mt < 5\" will keep cells with more than 200 genes and less than 5%% mitochondrial genes. /// doublet_detector (choice) \u2014 The doublet detector to use. - none: Do not use any doublet detector. - DoubletFinder: Use DoubletFinder to detect doublets. - doubletfinder: Same as DoubletFinder . - scDblFinder: Use scDblFinder to detect doublets. - scdblfinder: Same as scDblFinder . gene_qc (ns) \u2014 Filter genes. gene_qc is applied after cell_qc . - min_cells: The minimum number of cells that a gene must be expressed in to be kept. - excludes: The genes to exclude. Multiple genes can be specified by comma separated values, or as a list. /// Tip | Example [SeuratPreparing.envs] gene_qc = { min_cells = 3 } will keep genes that are expressed in at least 3 cells. /// min_cells (type=int) \u2014 The minimum number of cells that a gene must beexpressed in to be kept. This is used in Seurat::CreateSeuratObject() . Futher QC ( envs.cell_qc , envs.gene_qc ) will be performed after this. It doesn't work when data is loaded from loom files or RDS/qs2 files. min_features (type=int) \u2014 The minimum number of features that a cell mustexpress to be kept. This is used in Seurat::CreateSeuratObject() . Futher QC ( envs.cell_qc , envs.gene_qc ) will be performed after this. It doesn't work when data is loaded from loom files or RDS/qs2 files. mutaters (type=json) \u2014 The mutaters to mutate the metadata to the cells.These new columns will be added to the metadata of the Seurat object and will be saved in the output file. ncores (type=int) \u2014 Number of cores to use.Used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. no_integration (flag) \u2014 Whether to skip integration or not. qc_plots (type=json) \u2014 The plots for QC metrics.It should be a json (or python dict) with the keys as the names of the plots and the values also as dicts with the following keys: * kind: The kind of QC. Either gene or cell (default). * devpars: The device parameters for the plot. A dict with res , height , and width . * more_formats: The formats to save the plots other than png . * save_code: Whether to save the code to reproduce the plot. * other arguments passed to biopipen.utils::VizSeuratCellQC when kind is cell or biopipen.utils::VizSeuratGeneQC when kind is gene . scDblFinder (ns) \u2014 Arguments to run scDblFinder . - dbr (type=float): The expected doublet rate. - ncores (type=int): Number of cores to use for scDblFinder . Set to None to use envs.ncores . - : See https://rdrr.io/bioc/scDblFinder/man/scDblFinder.html . use_sct (flag) \u2014 Whether use SCTransform routine to integrate samples or not.Before the following procedures, the RNA layer will be split by samples. If False , following procedures will be performed in the order: * NormalizeData . * FindVariableFeatures . * ScaleData . See https://satijalab.org/seurat/articles/seurat5_integration#layers-in-the-seurat-v5-object and https://satijalab.org/seurat/articles/pbmc3k_tutorial.html If True , following procedures will be performed in the order: * SCTransform . See https://satijalab.org/seurat/articles/seurat5_integration#perform-streamlined-one-line-integrative-analysis Requires r-bracer \u2014 check: {{proc.lang}} <(echo \"library(bracer)\") r-future \u2014 check: {{proc.lang}} <(echo \"library(future)\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library(Seurat)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratClustering ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Determine the clusters of cells without reference using Seurat FindClustersprocedure. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object loaded by SeuratPreparing Output outfile \u2014 The seurat object with cluster information at seurat_clusters . Envs FindClusters (ns) \u2014 Arguments for FindClusters() . object is specified internally, and - in the key will be replaced with . . The cluster labels will be saved in seurat_clusters and prefixed with \"c\". The first cluster will be \"c1\", instead of \"c0\". - resolution (type=auto): The resolution of the clustering. You can have multiple resolutions as a list or as a string separated by comma. Ranges are also supported, for example: 0.1:0.5:0.1 will generate 0.1, 0.2, 0.3, 0.4, 0.5 . The step can be omitted, defaulting to 0.1. The results will be saved in seurat_clusters_<resolution> . The final resolution will be used to define the clusters at seurat_clusters . - : See https://satijalab.org/seurat/reference/findclusters FindNeighbors (ns) \u2014 Arguments for FindNeighbors() . object is specified internally, and - in the key will be replaced with . . - reduction: The reduction to use. If not provided, sobj@misc$integrated_new_reduction will be used. - : See https://satijalab.org/seurat/reference/findneighbors RunPCA (ns) \u2014 Arguments for RunPCA() . RunUMAP (ns) \u2014 Arguments for RunUMAP() . object is specified internally, and - in the key will be replaced with . . dims=N will be expanded to dims=1:N ; The maximal value of N will be the minimum of N and the number of columns - 1 for each sample. - dims (type=int): The number of PCs to use - reduction: The reduction to use for UMAP. If not provided, sobj@misc$integrated_new_reduction will be used. - : See https://satijalab.org/seurat/reference/runumap cache (type=auto) \u2014 Where to cache the information at different steps.If True , the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. Set to False to not cache the results. ncores (type=int;order=-100) \u2014 Number of cores to use.Used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. See also: https://satijalab.org/seurat/articles/future_vignette.html Requires r-dplyr \u2014 check: {{proc.lang}} <(echo \"library(dplyr)\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library(Seurat)\") r-tidyr \u2014 check: {{proc.lang}} <(echo \"library(tidyr)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratSubClustering ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Find clusters of a subset of cells. It's unlike [ Seurat::FindSubCluster ], which only finds subclusters of a single cluster. Instead, it will perform the whole clustering procedure on the subset of cells. One can use metadata to specify the subset of cells to perform clustering on. For the subset of cells, the reductions will be re-performed on the subset of cells, and then the clustering will be performed on the subset of cells. The reduction will be saved in object@reduction$<casename>.<reduction> of the original object and the clustering will be saved in the metadata of the original object using the casename as the column name. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS or qs/qs2 format. Output outfile \u2014 The seurat object with the subclustering information in qs/qs2 format. Envs FindClusters (ns) \u2014 Arguments for FindClusters() . object is specified internally, and - in the key will be replaced with . . The cluster labels will be prefixed with \"s\". The first cluster will be \"s1\", instead of \"s0\". - resolution (type=auto): The resolution of the clustering. You can have multiple resolutions as a list or as a string separated by comma. Ranges are also supported, for example: 0.1:0.5:0.1 will generate 0.1, 0.2, 0.3, 0.4, 0.5 . The step can be omitted, defaulting to 0.1. The results will be saved in <casename>_<resolution> . The final resolution will be used to define the clusters at <casename> . - : See https://satijalab.org/seurat/reference/findclusters FindNeighbors (ns) \u2014 Arguments for FindNeighbors() . object is specified internally, and - in the key will be replaced with . . - reduction: The reduction to use. If not provided, object@misc$integrated_new_reduction will be used. - : See https://satijalab.org/seurat/reference/findneighbors RunPCA (ns) \u2014 Arguments for RunPCA() . object is specified internally as the subset object, and - in the key will be replaced with . . - : See https://satijalab.org/seurat/reference/runpca RunUMAP (ns) \u2014 Arguments for RunUMAP() . object is specified internally as the subset object, and - in the key will be replaced with . . dims=N will be expanded to dims=1:N ; The maximal value of N will be the minimum of N and the number of columns - 1 for each sample. - dims (type=int): The number of PCs to use - reduction: The reduction to use for UMAP. If not provided, sobj@misc$integrated_new_reduction will be used. - : See https://satijalab.org/seurat/reference/runumap cache (type=auto) \u2014 Whether to cache the results.If True , the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. Set to False to not cache the results. cases (type=json) \u2014 The cases to perform subclustering.Keys are the names of the cases and values are the dicts inherited from envs except mutaters and cache . If empty, a case with name subcluster will be created with default parameters. The case name will be passed to biopipen.utils::SeuratSubCluster() as name . It will be used as the prefix for the reduction name, keys and cluster names. For reduction keys, it will be toupper(<name>) + \"PC_\" and toupper(<name>) + \"UMAP_\". For cluster names, it will be <name> + \".\" + resolution. And the final cluster name will be <name> . Note that the name should be alphanumeric and anything other than alphanumeric will be removed. mutaters (type=json) \u2014 The mutaters to mutate the metadata to subset the cells.The mutaters will be applied in the order specified. ncores (type=int;order=-100) \u2014 Number of cores to use.Used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. subset \u2014 An expression to subset the cells, will be passed to tidyseurat::filter() . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratClusterStats ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Statistics of the clustering. Including the number/fraction of cells in each cluster, the gene expression values and dimension reduction plots. It's also possible to perform stats on TCR clones/clusters or other metadata for each T-cell cluster. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples Number of cells in each cluster [SeuratClusterStats.envs.stats] # suppose you have nothing set in `envs.stats_defaults` # otherwise, the settings will be inherited here nCells_All = { } {: width=\"80%\" } Number of cells in each cluster by groups [SeuratClusterStats.envs.stats] nCells_Sample = { group_by = \"Sample\" } {: width=\"80%\" } Violin plots for the gene expressions [SeuratClusterStats.envs.features] features = \"CD4,CD8A\" # Remove the dots in the violin plots vlnplots = { pt-size = 0 , kind = \"vln\" } # Don't use the default genes vlnplots_1 = { features = [ \"FOXP3\" , \"IL2RA\" ], pt-size = 0 , kind = \"vln\" } {: width=\"80%\" } {: width=\"80%\" } Dimension reduction plot with labels [SeuratClusterStats.envs.dimplots.Idents] label = true {: width=\"80%\" } Input srtobj \u2014 The seurat object loaded by SeuratClustering Output outdir \u2014 The output directory.Different types of plots will be saved in different subdirectories. For example, clustree plots will be saved in clustrees subdirectory. For each case in envs.clustrees , both the png and pdf files will be saved. Envs cache (type=auto) \u2014 Whether to cache the plots.Currently only plots for features are supported, since creating the those plots can be time consuming. If True , the plots will be cached in the job output directory, which will be not cleaned up when job is rerunning. clustrees (type=json) \u2014 The cases for clustree plots.Keys are the names of the plots and values are the dicts inherited from env.clustrees_defaults except prefix . There is no default case for clustrees . clustrees_defaults (ns) \u2014 The parameters for the clustree plots. - devpars (ns): The device parameters for the clustree plot. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - more_formats (type=list): The formats to save the plots other than png . - save_code (flag): Whether to save the code to reproduce the plot. - prefix (type=auto): string indicating columns containing clustering information. The trailing dot is not necessary and will be added automatically. When TRUE , clustrees will be plotted when there is FindClusters or FindClusters.* in the obj@commands . The latter is generated by SeuratSubClustering . This will be ignored when envs.clustrees is specified (the prefix of each case must be specified separately). - : Other arguments passed to scplotter::ClustreePlot . See https://pwwang.github.io/scplotter/reference/ClustreePlot.html dimplots (type=json) \u2014 The dimensional reduction plots.Keys are the titles of the plots and values are the dicts inherited from env.dimplots_defaults . It can also have other parameters from scplotter::CellDimPlot . dimplots_defaults (ns) \u2014 The default parameters for dimplots . - group_by: The identity to use. If it is from subclustering (reduction sub_umap_<ident> exists), this reduction will be used if reduction is set to dim or auto . - split_by: The column name in metadata to split the cells into different plots. - subset: An expression to subset the cells, will be passed to tidyrseurat::filter() . - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - reduction (choice): Which dimensionality reduction to use. - dim: Use Seurat::DimPlot . First searches for umap , then tsne , then pca . If ident is from subclustering, sub_umap_<ident> will be used. - auto: Same as dim - umap: Use Seurat::UMAPPlot . - tsne: Use Seurat::TSNEPlot . - pca: Use Seurat::PCAPlot . - : See https://pwwang.github.io/scplotter/reference/CellDimPlot.html features (type=json) \u2014 The plots for features, include gene expressions, and columns from metadata.Keys are the titles of the cases and values are the dicts inherited from env.features_defaults . features_defaults (ns) \u2014 The default parameters for features . - features (type=auto): The features to plot. It can be either a string with comma separated features, a list of features, a file path with file:// prefix with features (one per line), or an integer to use the top N features from VariantFeatures(srtobj) . It can also be a dict with the keys as the feature group names and the values as the features, which is used for heatmap to group the features. - order_by (type=auto): The order of the clusters to show on the plot. An expression passed to dplyr::arrange() on the grouped meta data frame (by ident ). For example, you can order the clusters by the activation score of the cluster: desc(mean(ActivationScore, na.rm = TRUE)) , suppose you have a column ActivationScore in the metadata. You may also specify the literal order of the clusters by a list of strings (at least two). - subset: An expression to subset the cells, will be passed to tidyrseurat::filter() . - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - descr: The description of the plot, showing in the report. - more_formats (type=list): The formats to save the plots other than png . - save_code (flag): Whether to save the code to reproduce the plot. - save_data (flag): Whether to save the data used to generate the plot. - : Other arguments passed to scplotter::FeatureStatPlot . See https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html mutaters (type=json) \u2014 The mutaters to mutate the metadata to subset the cells.The mutaters will be applied in the order specified. You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . ngenes (type=json) \u2014 The number of genes expressed in each cell.Keys are the names of the plots and values are the dicts inherited from env.ngenes_defaults . ngenes_defaults (ns) \u2014 The default parameters for ngenes .The default parameters to plot the number of genes expressed in each cell. - more_formats (type=list): The formats to save the plots other than png . - subset: An expression to subset the cells, will be passed to tidyrseurat::filter() . - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. stats (type=json) \u2014 The number/fraction of cells to plot.Keys are the names of the plots and values are the dicts inherited from env.stats_defaults . Here are some examples - { \"nCells_All\": {}, \"nCells_Sample\": {\"group_by\": \"Sample\"}, \"fracCells_Sample\": {\"scale_y\": True, \"group_by\": \"Sample\", plot_type = \"pie\"}, } stats_defaults (ns) \u2014 The default parameters for stats .This is to do some basic statistics on the clusters/cells. For more comprehensive analysis, see https://pwwang.github.io/scplotter/reference/CellStatPlot.html . The parameters from the cases can overwrite the default parameters. - subset: An expression to subset the cells, will be passed to tidyrseurat::filter() . - devpars (ns): The device parameters for the clustree plot. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - descr: The description of the plot, showing in the report. - more_formats (type=list): The formats to save the plots other than png . - save_code (flag): Whether to save the code to reproduce the plot. - save_data (flag): Whether to save the data used to generate the plot. - : Other arguments passed to scplotter::CellStatPlot . See https://pwwang.github.io/scplotter/reference/CellStatPlot.html . Requires r-seurat \u2014 check: {{proc.lang}} -e \"library(Seurat)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . ModuleScoreCalculator ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate the module scores for each cell The module scores are calculated by Seurat::AddModuleScore() or Seurat::CellCycleScoring() for cell cycle scores. The module scores are calculated as the average expression levels of each program on single cell level, subtracted by the aggregated expression of control feature sets. All analyzed features are binned based on averaged expression, and the control features are randomly selected from each bin. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object loaded by SeuratClustering Output rdsfile \u2014 The seurat object with module scores added to the metadata. Envs defaults (ns) \u2014 The default parameters for modules . - features: The features to calculate the scores. Multiple features should be separated by comma. You can also specify cc.genes or cc.genes.updated.2019 to use the cell cycle genes to calculate cell cycle scores. If so, three columns will be added to the metadata, including S.Score , G2M.Score and Phase . Only one type of cell cycle scores can be calculated at a time. - nbin (type=int): Number of bins of aggregate expression levels for all analyzed features. - ctrl (type=int): Number of control features selected from the same bin per analyzed feature. - k (flag): Use feature clusters returned from DoKMeans . - assay: The assay to use. - seed (type=int): Set a random seed. - search (flag): Search for symbol synonyms for features in features that don't match features in object? - keep (flag): Keep the scores for each feature? Only works for non-cell cycle scores. - agg (choice): The aggregation function to use. Only works for non-cell cycle scores. - mean: The mean of the expression levels - median: The median of the expression levels - sum: The sum of the expression levels - max: The max of the expression levels - min: The min of the expression levels - var: The variance of the expression levels - sd: The standard deviation of the expression levels modules (type=json) \u2014 The modules to calculate the scores.Keys are the names of the expression programs and values are the dicts inherited from env.defaults . Here are some examples - { \"CellCycle\": {\"features\": \"cc.genes.updated.2019\"}, \"Exhaustion\": {\"features\": \"HAVCR2,ENTPD1,LAYN,LAG3\"}, \"Activation\": {\"features\": \"IFNG\"}, \"Proliferation\": {\"features\": \"STMN1,TUBB\"} } For CellCycle , the columns S.Score , G2M.Score and Phase will be added to the metadata. S.Score and G2M.Score are the cell cycle scores for each cell, and Phase is the cell cycle phase for each cell. You can also add Diffusion Components (DC) to the modules {\"DC\": {\"features\": 2, \"kind\": \"diffmap\"}} will perform diffusion map as a reduction and add the first 2 components as DC_1 and DC_2 to the metadata. diffmap is a shortcut for diffusion_map . Other key-value pairs will pass to destiny::DiffusionMap() . You can later plot the diffusion map by using reduction = \"DC\" in env.dimplots in SeuratClusterStats . This requires SingleCellExperiment and destiny R packages. post_mutaters (type=json) \u2014 The mutaters to mutate the metadata aftercalculating the module scores. The mutaters will be applied in the order specified. This is useful when you want to create new scores based on the calculated module scores. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . CellsDistribution ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Distribution of cells (i.e. in a TCR clone) from different groupsfor each cluster This generates a set of pie charts with proportion of cells in each cluster Rows are the cells identities (i.e. TCR clones or TCR clusters), columns are groups (i.e. clinic groups). Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples [CellsDistribution.envs.mutaters] # Add Patient1_Tumor_Expanded column with CDR3.aa that # expands in Tumor of patient 1 Patient1_Tumor_Expanded = ''' expanded(., region, \"Tumor\", subset = patient == \"Lung1\", uniq = FALSE) ''' [CellsDistribution.envs.cases.Patient1_Tumor_Expanded] cells_by = \"Patient1_Tumor_Expanded\" cells_orderby = \"desc(CloneSize)\" group_by = \"region\" group_order = [ \"Tumor\" , \"Normal\" ] Input srtobj \u2014 The seurat object in RDS format Output outdir \u2014 The output directory.The results for each case will be saved in a subdirectory. Envs cases (type=json;order=99) \u2014 If you have multiple cases, you can specify them here.Keys are the names of the cases and values are the options above except mutaters . If some options are not specified, the options in envs will be used. If no cases are specified, a default case will be used with case name DEFAULT . cells_by \u2014 The column name in metadata to group the cells for the rows of the plot.If your cell groups have overlapping cells, you can also use multiple columns, separated by comma ( , ). These columns will be concatenated to form the cell groups. For the overlapping cells, they will be counted multiple times for different groups. So make sure the cell group names in different columns are unique. cells_n (type=int) \u2014 The max number of groups to show for each cell group identity (row).Ignored if cells_order is specified. cells_order (list) \u2014 The order of the cells (rows) to show on the plot cells_orderby \u2014 An expression passed to dplyr::arrange() to order the cells (rows) of the plot.Only works when cells-order is not specified. The data frame passed to dplyr::arrange() is grouped by cells_by before ordering. You can have multiple expressions separated by semicolon ( ; ). The expessions will be parsed by rlang::parse_exprs() . 4 extra columns were added to the metadata for ordering the rows in the plot: * CloneSize : The size (number of cells) of clones (identified by cells_by ) * CloneGroupSize : The clone size in each group (identified by group_by ) * CloneClusterSize : The clone size in each cluster (identified by seurat_clusters ) * CloneGroupClusterSize : The clone size in each group and cluster (identified by group_by and seurat_clusters ) cluster_orderby \u2014 The order of the clusters to show on the plot.An expression passed to dplyr::summarise() on the grouped data frame (by seurat_clusters ). The summary stat will be passed to dplyr::arrange() to order the clusters. It's applied on the whole meta.data before grouping and subsetting. For example, you can order the clusters by the activation score of the cluster: desc(mean(ActivationScore, na.rm = TRUE)) , suppose you have a column ActivationScore in the metadata. descr \u2014 The description of the case, will be shown in the report. devpars (ns) \u2014 The device parameters for the plots of pie charts. - res (type=int): The resolution of the plots - height (type=int): The height of the plots - width (type=int): The width of the plots each \u2014 The column name in metadata to separate the cells into different plots. group_by \u2014 The column name in metadata to group the cells for the columns of the plot. group_order (list) \u2014 The order of the groups (columns) to show on the plot hm_devpars (ns) \u2014 The device parameters for the heatmaps. - res (type=int): The resolution of the heatmaps. - height (type=int): The height of the heatmaps. - width (type=int): The width of the heatmaps. mutaters (type=json) \u2014 The mutaters to mutate the metadataKeys are the names of the mutaters and values are the R expressions passed by dplyr::mutate() to mutate the metadata. overlap (list) \u2014 Plot the overlap of cell groups (values of cells_by ) in different casesunder the same section. The section must have at least 2 cases, each case should have a single cells_by column. prefix_each (flag) \u2014 Whether to prefix the each column name to thevalue as the case/section name. section \u2014 The section to show in the report. This allows different cases to be put in the same section in report.Only works when each is not specified. subset \u2014 An expression to subset the cells, will be passed to dplyr::filter() on metadata.This will be applied prior to each . Requires r-dplyr \u2014 check: {{proc.lang}} -e \"library(dplyr)\" r-seurat \u2014 check: {{proc.lang}} -e \"library(Seurat)\" r-tidyr \u2014 check: {{proc.lang}} -e \"library(tidyr)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratMetadataMutater ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Mutate the metadata of the seurat object Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 Additional metadataA tab-delimited file with columns as meta columns and rows as cells. srtobj \u2014 The seurat object loaded by SeuratPreparing Output outfile \u2014 The seurat object with the additional metadata Envs mutaters (type=json) \u2014 The mutaters to mutate the metadata.The key-value pairs will be passed the dplyr::mutate() to mutate the metadata. Requires r-dplyr \u2014 check: {{proc.lang}} <(echo \"library(dplyr)\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library(Seurat)\") r-tibble \u2014 check: {{proc.lang}} <(echo \"library(tibble)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . DimPlots ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Dimensional reduction plots Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 A toml configuration file with \"cases\"If this is given, envs.cases will be overriden name \u2014 The name of the job, used in report srtobj \u2014 The seruat object in RDS format Output outdir \u2014 The output directory Envs cases \u2014 The cases for the dim plotsKeys are the names and values are the arguments to Seurat::Dimplots Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . MarkersFinder ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Find markers between different groups of cells When only group_by is specified as \"seurat_clusters\" in envs.cases , the markers will be found for all the clusters. You can also find the differentially expressed genes between any two groups of cells by setting group_by to a different column name in metadata. Follow envs.cases for more details. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object loaded by SeuratPreparing If you have your Seurat object prepared by yourself, you can also use it here, but you should make sure that the object has been processed by PrepSCTFindMarkers if data is not normalized using SCTransform . Output outdir \u2014 The output directory for the markers and plots Envs allenrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from allenrich_plots_defaults . The cases under envs.cases can inherit this options. allenrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . allmarker_plots (type=json) \u2014 All marker plot cases.The keys are the names of the cases and the values are the dicts inherited from allmarker_plots_defaults . allmarker_plots_defaults (ns) \u2014 Default options for the plots for all markers when ident-1 is not specified. - plot_type: The type of the plot. See https://pwwang.github.io/biopipen.utils.R/reference/VizDEGs.html . Available types are violin , box , bar , ridge , dim , heatmap and dot . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : Other arguments passed to biopipen.utils::VizDEGs() . assay \u2014 The assay to use. cache (type=auto) \u2014 Where to cache the results.If True , cache to outdir of the job. If False , don't cache. Otherwise, specify the directory to cache to. cases (type=json) \u2014 If you have multiple cases for marker discovery, you can specify themhere. The keys are the names of the cases and the values are the above options. If some options are not specified, the default values specified above (under envs ) will be used. If no cases are specified, the default case will be added with the default values under envs with the name Marker Discovery . dbs (list) \u2014 The dbs to do enrichment analysis for significantmarkers See below for all libraries. https://maayanlab.cloud/Enrichr/#libraries each \u2014 The column name in metadata to separate the cells into differentcases. When this is specified, the case will be expanded for each value of the column in metadata. For example, when you have envs.cases.\"Cluster Markers\".each = \"Sample\" , then the case will be expanded as envs.cases.\"Cluster Markers - Sample1\" , envs.cases.\"Cluster Markers - Sample2\" , etc. You can specify allmarker_plots and overlaps to plot the markers for all cases in the same plot and plot the overlaps of the markers between different cases by values in this column. enrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from enrich_plots_defaults . The cases under envs.cases can inherit this options. enrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . Available types are bar , dot , lollipop , network , enrichmap and wordcloud . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . enrich_style (choice) \u2014 The style of the enrichment analysis.The enrichment analysis will be done by EnrichIt() from enrichit . Two styles are available: - enrichr: enrichr style enrichment analysis (fisher's exact test will be used). - clusterprofiler: clusterProfiler style enrichment analysis (hypergeometric test will be used). - clusterProfiler: alias for clusterprofiler error (flag) \u2014 Error out if no/not enough markers are found or no pathways are enriched.If False , empty results will be returned. group_by \u2014 The column name in metadata to group the cells.If only group_by is specified, and ident-1 and ident-2 are not specified, markers will be found for all groups in this column in the manner of \"group vs rest\" comparison. NA group will be ignored. If None , Seurat::Idents(srtobj) will be used, which is usually \"seurat_clusters\" after unsupervised clustering. ident_1 \u2014 The first group of cells to compareWhen this is empty, the comparisons will be expanded to each group v.s. the rest of the cells in group_by . ident_2 \u2014 The second group of cells to compareIf not provided, the rest of the cells are used for ident-2 . marker_plots (type=json) \u2014 Cases of the plots to generate for the markers.Plot cases. The keys are the names of the cases and the values are the dicts inherited from marker_plots_defaults . The cases under envs.cases can inherit this options. marker_plots_defaults (ns) \u2014 Default options for the plots to generate for the markers. - plot_type: The type of the plot. See https://pwwang.github.io/biopipen.utils.R/reference/VizDEGs.html . Available types are violin , box , bar , ridge , dim , heatmap and dot . There are two additional types available - volcano_pct and volcano_log2fc . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : Other arguments passed to biopipen.utils::VizDEGs() . If plot_type is volcano_pct or volcano_log2fc , they will be passed to scplotter::VolcanoPlot() . mutaters (type=json) \u2014 The mutaters to mutate the metadata.You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . ncores (type=int) \u2014 Number of cores to use for parallel computing for some Seurat procedures. * Used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. * See also: https://satijalab.org/seurat/articles/future_vignette.html overlaps (type=json) \u2014 Cases for investigating the overlapping of significant markers between different cases or comparisons.The keys are the names of the cases and the values are the dicts inherited from overlaps_defaults . There are two situations that we can perform overlaps: 1. If ident-1 is not specified, the overlaps can be performed between different comparisons. 2. If each is specified, the overlaps can be performed between different cases, where in each case, ident-1 must be specified. overlaps_defaults (ns) \u2014 Default options for investigating the overlapping of significant markers between different cases or comparisons.This means either ident-1 should be empty, so that they can be expanded to multiple comparisons. - sigmarkers: The expression to filter the significant markers for each case. If not provided, envs.sigmarkers will be used. - plot_type (choice): The type of the plot to generate for the overlaps. - venn: Use plotthis::VennDiagram() . - upset: Use plotthis::UpsetPlot() . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : More arguments pased to plotthis::VennDiagram() ( https://pwwang.github.io/plotthis/reference/venndiagram1.html ) or plotthis::UpsetPlot() ( https://pwwang.github.io/plotthis/reference/upsetplot1.html ) rest (ns) \u2014 Rest arguments for Seurat::FindMarkers() .Use - to replace . in the argument name. For example, use min-pct instead of min.pct . - : See https://satijalab.org/seurat/reference/findmarkers sigmarkers \u2014 An expression passed to dplyr::filter() to filter thesignificant markers for enrichment analysis. Available variables are p_val , avg_log2FC , pct.1 , pct.2 and p_val_adj . For example, \"p_val_adj < 0.05 & abs(avg_log2FC) > 1\" to select markers with adjusted p-value < 0.05 and absolute log2 fold change > 1. subset \u2014 An expression to subset the cells for each case. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . TopExpressingGenes ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Find the top expressing genes in each cluster Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS or qs/qs2 format Output outdir \u2014 The output directory for the tables and plots Envs cases (type=json) \u2014 If you have multiple cases, you can specify themhere. The keys are the names of the cases and the values are the above options except mutaters . If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the default values under envs with the name Top Expressing Genes . dbs (list) \u2014 The dbs to do enrichment analysis for significantmarkers See below for all libraries. https://maayanlab.cloud/Enrichr/#libraries each \u2014 The column name in metadata to separate the cells into differentcases. enrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from enrich_plots_defaults . The cases under envs.cases can inherit this options. enrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . Available types are bar , dot , lollipop , network , enrichmap and wordcloud . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.htmll . enrich_style (choice) \u2014 The style of the enrichment analysis.The enrichment analysis will be done by EnrichIt() from enrichit . Two styles are available: - enrichr: enrichr style enrichment analysis (fisher's exact test will be used). - clusterprofiler: clusterProfiler style enrichment analysis (hypergeometric test will be used). - clusterProfiler: alias for clusterprofiler group_by \u2014 The column name in metadata to group the cells. ident \u2014 The group of cells to find the top expressing genes.The cells will be selected by the group_by column with this ident value in metadata. If not provided, the top expressing genes will be found for all groups of cells in the group_by column. mutaters (type=json) \u2014 The mutaters to mutate the metadata.You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . n (type=int) \u2014 The number of top expressing genes to find. subset \u2014 An expression to subset the cells for each case. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . ExprImputation ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc This process imputes the dropout values in scRNA-seq data. It takes the Seurat object as input and outputs the Seurat object with imputed expression data. Reference: - Linderman, George C., Jun Zhao, and Yuval Kluger. \"Zero-preserving imputation of scRNA-seq data using low-rank approximation.\" BioRxiv (2018): 397588. - Li, Wei Vivian, and Jingyi Jessica Li. \"An accurate and robust imputation method scImpute for single-cell RNA-seq data.\" Nature communications 9.1 (2018): 997. - Dijk, David van, et al. \"MAGIC: A diffusion-based imputation method reveals gene-gene interactions in single-cell RNA-sequencing data.\" BioRxiv (2017): 111591. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file in RDS/qs format of Seurat object Output outfile \u2014 The output file in RDS format of Seurat objectNote that with rmagic and alra, the original default assay will be renamed to RAW and the imputed RNA assay will be renamed to RNA and set as default assay. Envs alra_args (type=json) \u2014 The arguments for RunALRA() rmagic_args (ns) \u2014 The arguments for rmagic - python: The python path where magic-impute is installed. - threshold (type=float): The threshold for magic imputation. Only the genes with dropout rates greater than this threshold (No. of cells with non-zero expression / total number of cells) will be imputed. scimpute_args (ns) \u2014 The arguments for scimpute - drop_thre (type=float): The dropout threshold - kcluster (type=int): Number of clusters to use - ncores (type=int): Number of cores to use - refgene: The reference gene file tool (choice) \u2014 Either alra, scimpute or rmagic - alra: Use RunALRA() from Seurat - scimpute: Use scImpute() from scimpute - rmagic: Use magic() from Rmagic Requires magic-impute \u2014 if: {{proc.envs.tool == \"rmagic\"}} check: {{proc.envs.rmagic_args.python}} -c \"import magic\") r-dplyr \u2014 if: {{proc.envs.tool == \"scimpute\"}} check: {{proc.lang}} <(echo \"library(dplyr)\") r-rmagic \u2014 if: {{proc.envs.tool == \"rmagic\"}} check: | {{proc.lang}} <( echo \" tryCatch( { setwd(dirname(Sys.getenv('CONDA_PREFIX'))) }, error = function(e) NULL ); library(Rmagic) \" ) r-scimpute \u2014 if: {{proc.envs.tool == \"scimpute\"}} check: {{proc.lang}} <(echo \"library(scImpute)\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library(Seurat)\") r-seuratwrappers \u2014 if: {{proc.envs.tool == \"alra\"}} check: {{proc.lang}} <(echo \"library(SeuratWrappers)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SCImpute ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Impute the dropout values in scRNA-seq data. Deprecated. Use ExprImputation instead. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input groupfile \u2014 The file to subset the matrix or label the cellsCould be an output from ImmunarchFilter infile \u2014 The input file for imputationEither a SeuratObject or a matrix of count/TPM Output outfile \u2014 The output matrix Envs infmt \u2014 The input format.Either seurat or matrix Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Filtering cells from a seurat object Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input filters \u2014 The filters to apply. Could be a file or string in TOML, ora python dictionary, with following keys: - mutaters: Create new columns in the metadata - filter: A R expression that will pass to subset(sobj, subset = ...) to filter the cells srtobj \u2014 The seurat object in RDS Output outfile \u2014 The filtered seurat object in RDS Envs invert \u2014 Invert the selection? Requires r-dplyr \u2014 check: {{proc.lang}} <(echo \"library('dplyr')\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library('Seurat')\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratSubset ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Subset a seurat object into multiple seruat objects Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS subsets \u2014 The subsettings to apply. Could be a file or string in TOML, ora python dictionary, with following keys: - : Name of the case mutaters: Create new columns in the metadata subset: A R expression that will pass to subset(sobj, subset = ...) groupby: The column to group by, each value will be a case If groupby is given, subset will be ignored, each value of the groupby column will be a case Output outdir \u2014 The output directory with the subset seurat objects Envs ignore_nas \u2014 Ignore NA values? Requires r-dplyr \u2014 check: {{proc.lang}} <(echo \"library('dplyr')\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library('Seurat')\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratSplit ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Split a seurat object into multiple seruat objects Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input by \u2014 The metadata column to split by srtobj \u2014 The seurat object in RDS Output outdir \u2014 The output directory with the subset seurat objects Envs by \u2014 The metadata column to split byIgnored if by is given in the input recell \u2014 Rename the cell ids using the by columnA string of R function taking the original cell ids and by Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . Subset10X ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Subset 10X data, mostly used for testing Requires r-matrix to load matrix.mtx.gz Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 The input directory Output outdir \u2014 The output directory Envs feats_to_keep \u2014 The features/genes to keep.The final features list will be feats_to_keep + nfeats ncells \u2014 The number of cells to keep.If <=1 then it will be the percentage of cells to keep nfeats \u2014 The number of features to keep.If <=1 then it will be the percentage of features to keep seed \u2014 The seed for random number generator Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratTo10X ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Write a Seurat object to 10X format using write10xCounts from DropletUtils Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS Output outdir \u2014 The output directory.When envs.split_by is specified, the subdirectories will be created for each distinct value of the column. Otherwise, the matrices will be written to the output directory. Envs version \u2014 The version of 10X format Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . ScFGSEA ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis for cells in different groups using fgsea This process allows us to do Gene Set Enrichment Analysis (GSEA) on the expression data, but based on variaties of grouping, including the from the meta data and the scTCR-seq data as well. The GSEA is done using the fgsea package, which allows to quickly and accurately calculate arbitrarily low GSEA P-values for a collection of gene sets. The fgsea package is based on the fast algorithm for preranked GSEA described in Subramanian et al. 2005 . For each case, the process will generate a table with the enrichment scores for each gene set, and GSEA plots for the top gene sets. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS format Output outdir \u2014 The output directory for the results and plots Envs alleach_plots (type=json) \u2014 Cases of the plots to generate for all pathways.The keys are the names of the cases and the values are the dicts inherited from alleach_plots_defaults . alleach_plots_defaults (ns) \u2014 Default options for the plots to generate for all pathways. - plot_type: The type of the plot, currently either dot or heatmap (default) - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html . cases (type=json;order=99) \u2014 If you have multiple cases, you can specify them here.The keys are the names of the cases and the values are the above options except mutaters . If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the name GSEA . each \u2014 The column name in metadata to separate the cells into different subsets to do the analysis. eps (type=float) \u2014 This parameter sets the boundary for calculating the p value.See https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html gmtfile \u2014 The pathways in GMT format, with the gene names/ids in the same format as the seurat object.One could also use a URL to a GMT file. For example, from https://download.baderlab.org/EM_Genesets/current_release/Human/symbol/Pathways/ . group_by \u2014 The column name in metadata to group the cells. ident_1 \u2014 The first group of cells to compare ident_2 \u2014 The second group of cells to compare, if not provided, the rest of the cells that are not NA s in group_by column are used for ident-2 . maxsize (type=int) \u2014 Maximal size of a gene set to test. All pathways above the threshold are excluded. method (choice) \u2014 The method to do the preranking. - signal_to_noise: Signal to noise. The larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \"class marker\". - s2n: Alias of signal_to_noise. - abs_signal_to_noise: The absolute value of signal_to_noise. - abs_s2n: Alias of abs_signal_to_noise. - t_test: T test. Uses the difference of means scaled by the standard deviation and number of samples. - ratio_of_classes: Also referred to as fold change. Uses the ratio of class means to calculate fold change for natural scale data. - diff_of_classes: Difference of class means. Uses the difference of class means to calculate fold change for nature scale data - log2_ratio_of_classes: Log2 ratio of class means. Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. minsize (type=int) \u2014 Minimal size of a gene set to test. All pathways below the threshold are excluded. mutaters (type=json) \u2014 The mutaters to mutate the metadata.The key-value pairs will be passed the dplyr::mutate() to mutate the metadata. You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . ncores (type=int) \u2014 Number of cores for parallelizationPassed to nproc of fgseaMultilevel() . rest (type=json;order=98) \u2014 Rest arguments for fgsea() See also https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html subset \u2014 An expression to subset the cells. top (type=auto) \u2014 Do gsea table and enrich plot for top N pathways.If it is < 1, will apply it to padj , selecting pathways with padj < top . Requires bioconductor-fgsea \u2014 check: {{proc.lang}} -e \"library(fgsea)\" r-seurat \u2014 check: {{proc.lang}} -e \"library(seurat)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . CellTypeAnnotation ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Annotate the cell clusters. Currently, four ways are supported: Pass the cell type annotation directly Use ScType Use scCATCH Use hitype The annotated cell types will replace the original seurat_clusters column in the metadata, so that the downstream processes will use the annotated cell types. The old seurat_clusters column will be renamed to seurat_clusters_id . If you are using ScType , scCATCH , or hitype , a text file containing the mapping from the old seurat_clusters to the new cell types will be generated and saved to cluster2celltype.tsv under <workdir>/<pipline_name>/CellTypeAnnotation/0/output/ . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples [CellTypeAnnotation.envs] tool = \"direct\" cell_types = [ \"CellType1\" , \"CellType2\" , \"-\" , \"CellType4\" ] The cell types will be assigned as: 0 -> CellType1 1 -> CellType2 2 -> 2 3 -> CellType4 Input sobjfile \u2014 The single-cell object in RDS/qs/qs2/h5ad format. Output outfile \u2014 The rds/qs/qs2/h5ad file of seurat object with cell type annotated.A text file containing the mapping from the old seurat_clusters to the new cell types will be generated and saved to cluster2celltype.tsv under the job output directory. Envs cell_types (list) \u2014 The cell types to use for direct annotation.You can use \"-\" or \"\" as the placeholder for the clusters that you want to keep the original cell types ( seurat_clusters ). If the length of cell_types is shorter than the number of clusters, the remaining clusters will be kept as the original cell types. You can also use NA to remove the clusters from downstream analysis. This only works when envs.newcol is not specified. /// Note If tool is direct and cell_types is not specified or an empty list, the original cell types will be kept and nothing will be changed. /// celltypist_args (ns) \u2014 The arguments for celltypist::celltypist() if tool is celltypist . - model: The path to model file. - python: The python path where celltypist is installed. - majority_voting: When true, it refines cell identities within local subclusters after an over-clustering approach at the cost of increased runtime. - over_clustering (type=auto): The column name in metadata to use as clusters for majority voting. Set to False to disable over-clustering. When in.sobjfile is rds/qs/qs2 (supposing we have a Seurat object), the default ident is used by default. Otherwise, it is False by default. - assay: When converting a Seurat object to AnnData, the assay to use. If input is h5seurat, this defaults to RNA. If input is Seurat object in RDS, this defaults to the default assay. hitype_db \u2014 The database to use for hitype.Compatible with sctype_db . See also https://pwwang.github.io/hitype/articles/prepare-gene-sets.html You can also use built-in databases, including hitypedb_short , hitypedb_full , and hitypedb_pbmc3k . hitype_tissue \u2014 The tissue to use for hitype .Avaiable tissues should be the first column ( tissueType ) of hitype_db . If not specified, all rows in hitype_db will be used. merge (flag) \u2014 Whether to merge the clusters with the same cell types.Otherwise, a suffix will be added to the cell types (ie. .1 , .2 , etc). more_cell_types (type=json) \u2014 The additional cell type annotations to add to the metadata.The keys are the new column names and the values are the cell types lists. The cell type lists work the same as cell_types above. This is useful when you want to keep multiple annotations of cell types. newcol \u2014 The new column name to store the cell types.If not specified, the seurat_clusters column will be overwritten. If specified, the original seurat_clusters column will be kept and Idents will be kept as the original seurat_clusters . outtype (choice) \u2014 The output file type. Currently only works for celltypist .An RDS file will be generated for other tools. - input: Use the same file type as the input. - rds: Use RDS file. - qs: Use qs2 file. - qs2: Use qs2 file. - h5ad: Use AnnData file. sccatch_args (ns) \u2014 The arguments for scCATCH::findmarkergene() if tool is sccatch . - species: The specie of cells. - cancer: If the sample is from cancer tissue, then the cancer type may be defined. - tissue: Tissue origin of cells must be defined. - marker: The marker genes for cell type identification. - if_use_custom_marker (flag): Whether to use custom marker genes. If True , no species , cancer , and tissue are needed. - : Other arguments for scCATCH::findmarkergene() . You can pass an RDS file to sccatch_args.marker to work as custom marker. If so, if_use_custom_marker will be set to TRUE automatically. sctype_db \u2014 The database to use for sctype.Check examples at https://github.com/IanevskiAleksandr/sc-type/blob/master/ScTypeDB_full.xlsx sctype_tissue \u2014 The tissue to use for sctype .Avaiable tissues should be the first column ( tissueType ) of sctype_db . If not specified, all rows in sctype_db will be used. tool (choice) \u2014 The tool to use for cell type annotation. - sctype: Use scType to annotate cell types. See https://github.com/IanevskiAleksandr/sc-type - hitype: Use hitype to annotate cell types. See https://github.com/pwwang/hitype - sccatch: Use scCATCH to annotate cell types. See https://github.com/ZJUFanLab/scCATCH - celltypist: Use celltypist to annotate cell types. See https://github.com/Teichlab/celltypist - direct: Directly assign cell types Requires r-HGNChelper \u2014 if: {{proc.envs.tool == 'sctype'}} check: {{proc.lang}} -e \"library(HGNChelper)\" r-dplyr \u2014 if: {{proc.envs.tool == 'sctype'}} check: {{proc.lang}} -e \"library(dplyr)\" r-openxlsx \u2014 if: {{proc.envs.tool == 'sctype'}} check: {{proc.lang}} -e \"library(openxlsx)\" r-seurat \u2014 if: {{proc.envs.tool == 'sctype'}} check: {{proc.lang}} -e \"library(Seurat)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . SeuratMap2Ref ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Map the seurat object to reference See: https://satijalab.org/seurat/articles/integration_mapping.html and https://satijalab.org/seurat/articles/multimodal_reference_mapping.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object Output outfile \u2014 The rds file of seurat object with cell type annotated.Note that the reduction name will be ref.umap for the mapping. To visualize the mapping, you should use ref.umap as the reduction name. Envs FindTransferAnchors (ns) \u2014 Arguments for FindTransferAnchors() - normalization-method (choice): Name of normalization method used. - LogNormalize: Log-normalize the data matrix - SCT: Scale data using the SCTransform method - auto: Automatically detect the normalization method. See envs.refnorm . - reference-reduction: Name of dimensional reduction to use from the reference if running the pcaproject workflow. Optionally enables reuse of precomputed reference dimensional reduction. - : See https://satijalab.org/seurat/reference/findtransferanchors . Note that the hyphen ( - ) will be transformed into . for the keys. MapQuery (ns) \u2014 Arguments for MapQuery() - reference-reduction: Name of reduction to use from the reference for neighbor finding - reduction-model: DimReduc object that contains the umap model. - refdata (type=json): Extra data to transfer from the reference to the query. - : See https://satijalab.org/seurat/reference/mapquery . Note that the hyphen ( - ) will be transformed into . for the keys. NormalizeData (ns) \u2014 Arguments for NormalizeData() - normalization-method: Normalization method. - : See https://satijalab.org/seurat/reference/normalizedata . Note that the hyphen ( - ) will be transformed into . for the keys. SCTransform (ns) \u2014 Arguments for SCTransform() - do-correct-umi (flag): Place corrected UMI matrix in assay counts layer? - do-scale (flag): Whether to scale residuals to have unit variance? - do-center (flag): Whether to center residuals to have mean zero? - : See https://satijalab.org/seurat/reference/sctransform . Note that the hyphen ( - ) will be transformed into . for the keys. cache (type=auto) \u2014 Whether to cache the information at different steps.If True , the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. The cached seurat object will be saved as <signature>.<kind>.RDS file, where <signature> is the signature determined by the input and envs of the process. See https://github.com/satijalab/seurat/issues/7849 , https://github.com/satijalab/seurat/issues/5358 and https://github.com/satijalab/seurat/issues/6748 for more details also about reproducibility issues. To not use the cached seurat object, you can either set cache to False or delete the cached file at <signature>.RDS in the cache directory. ident \u2014 The name of the ident for query transferred from envs.use of the reference. mutaters (type=json) \u2014 The mutaters to mutate the metadata.This is helpful when we want to create new columns for split_by . ncores (type=int;order=-100) \u2014 Number of cores to use.When split_by is used, this will be the number of cores for each object to map to the reference. When split_by is not used, this is used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. See also: https://satijalab.org/seurat/archive/v3.0/future_vignette.html plots (type=json) \u2014 The plots to generate.The keys are the names of the plots and the values are the arguments for the plot. The arguments will be passed to biopipen.utils::VizSeuratMap2Ref() to generate the plots. The plots will be saved to the output directory. See https://pwwang.github.io/biopipen.utils.R/reference/VizSeuratMap2Ref.html . ref \u2014 The reference seurat object file.Either an RDS file or a h5seurat file that can be loaded by Seurat::LoadH5Seurat() . The file type is determined by the extension. .rds or .RDS for RDS file, .h5seurat or .h5 for h5seurat file. refnorm (choice) \u2014 Normalization method the reference used. The same method will be used for the query. - LogNormalize: Using NormalizeData . - SCTransform: Using SCTransform . - SCT: Alias of SCTransform. - auto: Automatically detect the normalization method. If the default assay of reference is SCT , then SCTransform will be used. skip_if_normalized \u2014 Skip normalization if the query is already normalized.Since the object is supposed to be generated by SeuratPreparing , it is already normalized. However, a different normalization method may be used. If the reference is normalized by the same method as the query, the normalization can be skipped. Otherwise, the normalization cannot be skipped. The normalization method used for the query set is determined by the default assay. If SCT , then SCTransform is used; otherwise, NormalizeData is used. You can set this to False to force re-normalization (with or without the arguments previously used). split_by \u2014 The column name in metadata to split the query into multiple objects.This helps when the original query is too large to process. use \u2014 A column name of metadata from the reference(e.g. celltype.l1 , celltype.l2 ) to transfer to the query as the cell types (ident) for downstream analysis. This field is required. If you want to transfer multiple columns, you can use envs.MapQuery.refdata . Requires r-seurat \u2014 check: {{proc.lang}} -e \"library(Seurat)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . RadarPlots ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Radar plots for cell proportion in different clusters. This process generates the radar plots for the clusters of T cells. It explores the proportion of cells in different groups (e.g. Tumor vs Blood) in different T-cell clusters. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples Let's say we have a metadata like this: Cell Source Timepoint seurat_clusters A Blood Pre 0 B Blood Pre 0 C Blood Post 1 D Blood Post 1 E Tumor Pre 2 F Tumor Pre 2 G Tumor Post 3 H Tumor Post 3 With configurations: [RadarPlots.envs] by = \"Source\" Then we will have a radar plots like this: We can use each to separate the cells into different cases: [RadarPlots.envs] by = \"Source\" each = \"Timepoint\" Then we will have two radar plots, one for Pre and one for Post : Using cluster_order to change the order of the clusters and show only the first 3 clusters: [RadarPlots.envs] by = \"Source\" cluster_order = [ \"2\" , \"0\" , \"1\" ] breaks = [ 0 , 50 , 100 ] # also change the breaks n. / Input srtobj \u2014 The seurat object in RDS or qs/qs2 format Output outdir \u2014 The output directory for the plots Envs bar_devpars (ns) \u2014 The parameters for png() for the barplot - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot breakdown \u2014 An additional column with groups to break down the cellsdistribution in each cluster. For example, if you want to see the distribution of the cells in each cluster in different samples. In this case, you should have multiple values in each by . These values won't be plotted in the radar plot, but a barplot will be generated with the mean value of each group and the error bar. breaks (list;itype=int) \u2014 breaks of the radar plots, from 0 to 100.If not given, the breaks will be calculated automatically. by \u2014 Which column to use to separate the cells in different groups. NA s will be ignored. For example, If you have a column named Source that marks the source of the cells, and you want to separate the cells into Tumor and Blood groups, you can set by to Source . The there will be two curves in the radar plot, one for Tumor and one for Blood . cases (type=json) \u2014 The cases for the multiple radar plots.Keys are the names of the cases and values are the arguments for the plots ( each , by , order , breaks , direction , ident , cluster_order and devpars ). If not cases are given, a default case will be used, with the key DEFAULT . The keys must be valid string as part of the file name. cluster_order (list) \u2014 The order of the clusters.You may also use it to filter the clusters. If not given, all clusters will be used. If the cluster names are integers, use them directly for the order, even though a prefix Cluster is added on the plot. colors \u2014 The colors for the groups in by . If not specified,the default colors will be used. Multiple colors can be separated by comma ( , ). You can specify biopipen to use the biopipen palette. devpars (ns) \u2014 The parameters for png() - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot direction (choice) \u2014 Direction to calculate the percentages. - inter-cluster: the percentage of the cells in all groups in each cluster (percentage adds up to 1 for each cluster). - intra-cluster: the percentage of the cells in all clusters. (percentage adds up to 1 for each group). each \u2014 A column with values to separate all cells in different casesWhen specified, the case will be expanded to multiple cases for each value in the column. If specified, section will be ignored, and the case name will be used as the section name. ident \u2014 The column name of the cluster information. mutaters (type=json) \u2014 Mutaters to mutate the metadata of theseurat object. Keys are the column names and values are the expressions to mutate the columns. These new columns will be used to define your cases. order (list) \u2014 The order of the values in by . You can also limit(filter) the values we have in by . For example, if column Source has values Tumor , Blood , Spleen , and you only want to plot Tumor and Blood , you can set order to [\"Tumor\", \"Blood\"] . This will also have Tumor as the first item in the legend and Blood as the second item. prefix_each (flag) \u2014 Whether to prefix the each column name to the values as thecase/section name. section \u2014 If you want to put multiple cases into a same sectionin the report, you can set this option to the name of the section. Only used in the report. subset \u2014 The subset of the cells to do the analysis. test (choice) \u2014 The test to use to calculate the p values.If there are more than 2 groups in by , the p values will be calculated pairwise group by group. Only works when breakdown is specified and by has 2 groups or more. - wilcox: Wilcoxon rank sum test - t: T test - none: No test will be performed Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . MetaMarkers ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Find markers between three or more groups of cells, using one-way ANOVAor Kruskal-Wallis test. Sometimes, you may want to find the markers for cells from more than 2 groups. In this case, you can use this process to find the markers for the groups and do enrichment analysis for the markers. Each marker is examined using either one-way ANOVA or Kruskal-Wallis test. The p values are adjusted using the specified method. The significant markers are then used for enrichment analysis using enrichr api. Other than the markers and the enrichment analysis as outputs, this process also generates violin plots for the top 10 markers. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object loaded by SeuratPreparing Output outdir \u2014 The output directory for the markers Envs cases (type=json) \u2014 If you have multiple cases, you can specify themhere. The keys are the names of the cases and the values are the above options except ncores and mutaters . If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the default values under envs with the name DEFAULT . dbs (list) \u2014 The dbs to do enrichment analysis for significantmarkers See below for all libraries. https://maayanlab.cloud/Enrichr/#libraries each \u2014 The column name in metadata to separate the cells into different cases. group-by \u2014 The column name in metadata to group the cells.If only group-by is specified, and idents are not specified, markers will be found for all groups in this column. NA group will be ignored. idents \u2014 The groups of cells to compare, values should be in the group-by column. method (choice) \u2014 The method for the test. - anova: One-way ANOVA - kruskal: Kruskal-Wallis test mutaters (type=json) \u2014 The mutaters to mutate the metadataThe key-value pairs will be passed the dplyr::mutate() to mutate the metadata. ncores (type=int) \u2014 Number of cores to use to parallelize for genes p_adjust (choice) \u2014 The method to adjust the p values, which can be used to filter the significant markers.See also https://rdrr.io/r/stats/p.adjust.html - holm: Holm-Bonferroni method - hochberg: Hochberg method - hommel: Hommel method - bonferroni: Bonferroni method - BH: Benjamini-Hochberg method - BY: Benjamini-Yekutieli method - fdr: FDR method of Benjamini-Hochberg - none: No adjustment prefix_each (flag) \u2014 Whether to add the each value as prefix to the case name. section \u2014 The section name for the report.Worked only when each is not specified. Otherwise, the section name will be constructed from each and group-by . If DEFAULT , and it's the only section, it not included in the case/section names. sigmarkers \u2014 An expression passed to dplyr::filter() to filter thesignificant markers for enrichment analysis. The default is p.value < 0.05 . If method = 'anova' , the variables that can be used for filtering are: sumsq , meansq , statistic , p.value and p_adjust . If method = 'kruskal' , the variables that can be used for filtering are: statistic , p.value and p_adjust . subset \u2014 The subset of the cells to do the analysis.An expression passed to dplyr::filter() . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . Seurat2AnnData ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert seurat object to AnnData Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file, in RDS or qs/qs2 format Output outfile \u2014 The AnnData file Envs assay \u2014 The assay to use for AnnData.If not specified, the default assay will be used. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . AnnData2Seurat ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert AnnData to seurat object Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input adfile \u2014 The AnnData .h5ad file Output outfile \u2014 The seurat object file in RDS or qs/qs2 format Envs assay \u2014 The assay to use to convert to seurat object. dotplot_check (type=auto) \u2014 Whether to do a check with a dot plot.( scplotter::FeatureStatPlot(plot_type = \"dot\", ..) will be used) to see if the conversion is successful. Set to False to disable the check. If True , top 10 variable genes will be used for the check. You can give a list of genes or a string of genes with comma ( , ) separated to use for the check. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . ScSimulation ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Simulate single-cell data using splatter. See https://www.bioconductor.org/packages/devel/bioc/vignettes/splatter/inst/doc/splatter.html#2_Quickstart Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input seed \u2014 The seed for the simulationYou could also use string as the seed, and the seed will be generated by digest::digest2int() . So this could also work as a unique identifier for the simulation (ie. Sample ID). Output outfile \u2014 The output Seurat object/SingleCellExperiment in qs/qs2 format Envs method (choice) \u2014 which simulation method to use. Options are: - single: produces a single population - groups: produces distinct groups (eg. cell types), or - paths: selects cells from continuous trajectories (eg. differentiation processes) ncells (type=int) \u2014 The number of cells to simulate ngenes (type=int) \u2014 The number of genes to simulate nspikes (type=int) \u2014 The number of spike-ins to simulateWhen ngenes , ncells , and nspikes are not specified, the default params from mockSCE() will be used. By default, ngenes = 2000 , ncells = 200 , and nspikes = 100 . outtype (choice) \u2014 The output file type. - seurat: Seurat object - singlecellexperiment: SingleCellExperiment object - sce: alias for singlecellexperiment params (ns) \u2014 Other parameters for simulation.The parameters are initialized splitEstimate(mockSCE()) and then updated with the given parameters. See https://rdrr.io/bioc/splatter/man/SplatParams.html . Hyphens ( - ) will be transformed into dots ( . ) for the keys. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . CellCellCommunication ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Cell-cell communication inference This is implemented based on LIANA , which is a Python package for cell-cell communication inference and provides a list of existing methods including CellPhoneDB , Connectome , log2FC, NATMI , SingleCellSignalR , Rank_Aggregate, Geometric Mean, scSeqComm , and CellChat . You can also try python -c 'import liana; liana.mt.show_methods()' to see the methods available. Note that this process does not do any visualization. You can use CellCellCommunicationPlots to visualize the results. Reference: - Review . - LIANA . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file in RDS or h5seurat format or AnnData file. Output outfile \u2014 The output file with the 'liana_res' data frame.Stats are provided for both ligand and receptor entities, more specifically: ligand and receptor are the two entities that potentially interact. As a reminder, CCC events are not limited to secreted signalling, but we refer to them as ligand and receptor for simplicity. Also, in the case of heteromeric complexes, the ligand and receptor columns represent the subunit with minimum expression, while * complex corresponds to the actual complex, with subunits being separated by . source and target columns represent the source/sender and target/receiver cell identity for each interaction, respectively * *_props : represents the proportion of cells that express the entity. By default, any interactions in which either entity is not expressed in above 10%% of cells per cell type is considered as a false positive, under the assumption that since CCC occurs between cell types, a sufficient proportion of cells within should express the genes. * *_means : entity expression mean per cell type. * lr_means : mean ligand-receptor expression, as a measure of ligand-receptor interaction magnitude. * cellphone_pvals : permutation-based p-values, as a measure of interaction specificity. Envs \u2014 Other arguments for the method.The arguments are passed to the method directly. See the method documentation for more details and also help(liana.mt.<method>.__call__) in Python. assay \u2014 The assay to use for the analysis.Only works for Seurat object. expr_prop (type=float) \u2014 Minimum expression proportion for the ligands andreceptors (+ their subunits) in the corresponding cell identities. Set to 0 to return unfiltered results. groupby \u2014 The column name in metadata to group the cells.Typically, this column should be the cluster id. method (choice) \u2014 The method to use for cell-cell communication inference. - CellPhoneDB: Use CellPhoneDB method. Magnitude Score: lr_means; Specificity Score: cellphone_pvals. - Connectome: Use Connectome method. - log2FC: Use log2FC method. - NATMI: Use NATMI method. - SingleCellSignalR: Use SingleCellSignalR method. - Rank_Aggregate: Use Rank_Aggregate method. - Geometric_Mean: Use Geometric Mean method. - scSeqComm: Use scSeqComm method. - CellChat: Use CellChat method. - cellphonedb: alias for CellPhoneDB - connectome: alias for Connectome - log2fc: alias for log2FC - natmi: alias for NATMI - singlesignaler: alias for SingleCellSignalR - rank_aggregate: alias for Rank_Aggregate - geometric_mean: alias for Geometric_Mean - scseqcomm: alias for scSeqComm - cellchat: alias for CellChat min_cells (type=int) \u2014 Minimum cells (per cell identity if grouped by groupby )to be considered for downstream analysis. n_perms (type=int) \u2014 Number of permutations for the permutation test.Relevant only for permutation-based methods (e.g., CellPhoneDB ). If 0 is passed, no permutation testing is performed. ncores (type=int) \u2014 The number of cores to use. rscript \u2014 The path to the Rscript executable used to convert RDS file to AnnData.if in.sobjfile is an RDS file, it will be converted to AnnData file (h5ad). You need Seurat , SeuratDisk and digest installed. seed (type=int) \u2014 The seed for the random number generator. species (choice) \u2014 The species of the cells. - human: Human cells, the 'consensus' resource will be used. - mouse: Mouse cells, the 'mouseconsensus' resource will be used. split_by \u2014 The column name in metadata to split the cells to run the method separately.The results will be combined together with this column in the final output. subset \u2014 An expression in string to subset the cells.When a .rds or .h5seurat file is provided for in.sobjfile , you can provide an expression in R , which will be passed to base::subset() in R to subset the cells. But you can always pass an expression in python to subset the cells. See https://anndata.readthedocs.io/en/latest/tutorials/notebooks/getting-started.html#subsetting-using-metadata . You should use adata to refer to the AnnData object. For example, adata.obs.groups == \"g1\" will subset the cells with groups equal to g1 . subset_using \u2014 The method to subset the cells. - auto: Automatically detect the method to use. Note that this is not always accurate. We simply check if [ is in the expression. If so, we use python to subset the cells; otherwise, we use R . - python: Use python to subset the cells. - r: Use R to subset the cells. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . CellCellCommunicationPlots ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Visualization for cell-cell communication inference. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cccfile \u2014 The output file from CellCellCommunication Output outdir \u2014 The output directory for the plots. Envs \u2014 Other arguments passed to scplotter::CCCPlot cases (type=json) \u2014 The cases for the plots.The keys are the names of the cases and the values are the arguments for the plots. The arguments include the ones inherited from envs . You can have a special plot_type \"table\" to generate a table for the ccc data to save as a text file and show in the report. If no cases are given, a default case will be used, with the key Cell-Cell Communication . descr \u2014 The description of the plot. devpars (ns) \u2014 The parameters for the plot. - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot magnitude \u2014 The column name in the data to use as the magnitude of thecommunication. By default, the second last column will be used. See li.mt.show_methods() for the available methods in LIANA. or https://liana-py.readthedocs.io/en/latest/notebooks/basic_usage.html#Tileplot more_formats (type=list) \u2014 The additional formats to save the plots. specificity \u2014 The column name in the data to use as the specificity of the communication.By default, the last column will be used. If the method doesn't have a specificity, set it to None. subset \u2014 An expression to pass to dplyr::filter() to subset the ccc data. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . ScVelo ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Velocity analysis for single-cell RNA-seq data This process is implemented based on the Python package scvelo (v0.3.3). Note that it doesn't work with numpy>=2 . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file in RDS or h5seurat format or AnnData file. Output outfile \u2014 The output object with the velocity embeddings and information.In either RDS, h5seurat or h5ad format, depending on the envs.outtype . There will be also plots generated in the output directory (parent directory of outfile ). Note that these plots will not be used in the report, but can be used as supplementary information for the velocity analysis. To visualize the velocity embeddings, you can use the SeuratClusterStats process with v_reduction provided to one of the envs.dimplots . Envs calculate_velocity_genes (flag) \u2014 Whether to calculate the velocity genes. denoise (flag) \u2014 Whether to denoise the data. denoise_topn (type=int) \u2014 Number of genes with highest likelihood selected toinfer velocity directions. fitting_by (choice) \u2014 The mode to use for fitting the velocities. - stochastic: Stochastic mode - deterministic: Deterministic mode group_by \u2014 The column name in metadata to group the cells.Typically, this column should be the cluster id. kinetics (flag) \u2014 Whether to compute the RNA velocity kinetics. kinetics_topn (type=int) \u2014 Number of genes with highest likelihood selected toinfer velocity directions. min_shared_counts (type=int) \u2014 Minimum number of counts(both unspliced and spliced) required for a gene. mode (type=list) \u2014 The mode to use for the velocity analysis.It should be a subset of ['deterministic', 'stochastic', 'dynamical'] , meaning that we can perform the velocity analysis in multiple modes. n_neighbors (type=int) \u2014 The number of neighbors to use for the velocity graph. n_pcs (type=int) \u2014 The number of PCs to use for the velocity graph. ncores (type=int) \u2014 Number of cores to use. outtype (choice) \u2014 The output file type. - : The same as the input file type. - h5seurat: h5seurat file - h5ad: h5ad file - qs: qs/qs2 file - qs2: qs2 file - rds: RDS file rscript \u2014 The path to the Rscript executable used to convert RDS file to AnnData.if in.sobjfile is an RDS file, it will be converted to AnnData file (h5ad). You need Seurat , SeuratDisk and digest installed. top_n (type=int) \u2014 The number of top features to plot. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . Slingshot ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Trajectory inference using Slingshot This process is implemented based on the R package slingshot . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file in RDS or qs format. Output outfile \u2014 The output object with the trajectory information.The lineages are stored in the metadata of the seurat object at columns LineageX , where X is the lineage number. The BranchID column contains the branch id for each cell. One can use scplotter::CellDimPlot(object, lineages = c(\"Lineage1\", \"Lineage2\", ...)) to visualize the trajectories. Envs align_start (flag) \u2014 Whether to align the starting pseudotime values at the maximum pseudotime. dims (type=auto) \u2014 The dimensions to use for the analysis.A list or a string with comma separated values. Consecutive numbers can be specified with a colon ( : ) or a dash ( - ). end \u2014 The ending group for the Slingshot analysis. group_by \u2014 The column name in metadata to group the cells.Typically, this column should be the cluster id. prefix \u2014 The prefix to add to the column names of the resulting pseudotime variable. reduction \u2014 The nonlinear reduction to use for the trajectory analysis. reverse (flag) \u2014 Logical value indicating whether to reverse the pseudotime variable. seed (type=int) \u2014 The seed for the random number generator. start \u2014 The starting group for the Slingshot analysis. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . LoomTo10X ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert Loom file to 10X format Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input loomfile \u2014 The Loom file Output outdir \u2014 The output directory for the 10X format files,including the matrix.mtx.gz , barcodes.tsv.gz and features.tsv.gz files. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna . PseudoBulkDEG ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Pseduo-bulk differential gene expression analysis This process performs differential gene expression analysis, instead of on single-cell level, on the pseudo-bulk data, aggregated from the single-cell data. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file in RDS or qs/qs2 format. Output outdir \u2014 The output containing the results of the differential gene expressionanalysis. Envs aggregate_by \u2014 The column names in metadata to aggregate the cells. allenrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from allenrich_plots_defaults . The cases under envs.cases can inherit this options. allenrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . allmarker_plots (type=json) \u2014 All marker plot cases.The keys are the names of the cases and the values are the dicts inherited from allmarker_plots_defaults . allmarker_plots_defaults (ns) \u2014 Default options for the plots for all markers when ident-1 is not specified. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html . Available types are violin , box , bar , ridge , dim , heatmap and dot . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - order_by: an expression to order the markers, passed by dplyr::arrange() . - genes: The number of top genes to show or an expression passed to dplyr::filter() to filter the genes. - : Other arguments passed to scplotter::FeatureStatPlot() . assay \u2014 The assay to pull and aggregate the data. cache (type=auto) \u2014 Where to cache the results.If True , cache to outdir of the job. If False , don't cache. Otherwise, specify the directory to cache to. cases (type=json) \u2014 The cases for the analysis.The keys are the names of the cases and the values are the arguments for the analysis. The arguments include the ones inherited from envs . If no cases are specified, a default case will be added with the name DEG Analysis and the default values specified above. dbs (list) \u2014 The databases to use for enrichment analysis.The databases are passed to biopipen.utils::Enrichr() to do the enrichment analysis. The default databases are KEGG_2021_Human and MSigDB_Hallmark_2020 . See https://maayanlab.cloud/Enrichr/#libraries for the available libraries. each \u2014 The column name in metadata to separate the cells into different cases.When specified, the case will be expanded to multiple cases for each value in the column. enrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from enrich_plots_defaults . The cases under envs.cases can inherit this options. enrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . Available types are bar , dot , lollipop , network , enrichmap and wordcloud . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.htmll . enrich_style (choice) \u2014 The style of the enrichment analysis. - enrichr: Use enrichr -style for the enrichment analysis. - clusterProfiler: Use clusterProfiler -style for the enrichment analysis. error (flag) \u2014 Error out if no/not enough markers are found or no pathways are enriched.If False , empty results will be returned. group_by \u2014 The column name in metadata to group the cells. ident_1 \u2014 The first identity to compare. ident_2 \u2014 The second identity to compare.If not specified, the rest of the identities will be compared with ident_1 . layer \u2014 The layer to pull and aggregate the data. marker_plots (type=json) \u2014 Cases of the plots to generate for the markers.Plot cases. The keys are the names of the cases and the values are the dicts inherited from marker_plots_defaults . The cases under envs.cases can inherit this options. marker_plots_defaults (ns) \u2014 Default options for the plots to generate for the markers. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html . Available types are violin , box , bar , ridge , dim , heatmap and dot . There are two additional types available - volcano_pct and volcano_log2fc . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - order_by: an expression to order the markers, passed by dplyr::arrange() . - genes: The number of top genes to show or an expression passed to dplyr::filter() to filter the genes. - : Other arguments passed to scplotter::FeatureStatPlot() . If plot_type is volcano_pct or volcano_log2fc , they will be passed to scplotter::VolcanoPlot() . mutaters (type=json) \u2014 Mutaters to mutate the metadata of theseurat object. Keys are the new column names and values are the expressions to mutate the columns. These new columns can be used to define your cases. You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . ncores (type=int) \u2014 Number of cores to use for parallelization. overlaps (type=json) \u2014 Cases for investigating the overlapping of significant markers between different cases or comparisons.The keys are the names of the cases and the values are the dicts inherited from overlaps_defaults . There are two situations that we can perform overlaps: 1. If ident-1 is not specified, the overlaps can be performed between different comparisons. 2. If each is specified, the overlaps can be performed between different cases, where in each case, ident-1 must be specified. overlaps_defaults (ns) \u2014 Default options for investigating the overlapping of significant markers between different cases or comparisons.This means either ident-1 should be empty, so that they can be expanded to multiple comparisons. - sigmarkers: The expression to filter the significant markers for each case. If not provided, envs.sigmarkers will be used. - plot_type (choice): The type of the plot to generate for the overlaps. - venn: Use plotthis::VennDiagram() . - upset: Use plotthis::UpsetPlot() . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : More arguments pased to plotthis::VennDiagram() ( https://pwwang.github.io/plotthis/reference/venndiagram1.html ) or plotthis::UpsetPlot() ( https://pwwang.github.io/plotthis/reference/upsetplot1.html ) paired_by \u2014 The column name in metadata to mark the paired samples.For example, subject. If specified, the paired test will be performed. plots (type=json) \u2014 The parameters for the plots.The keys are the names of the plots and the values are the parameters for the plots. The parameters will override the defaults in plots_defaults . If not specified, no plots will be generated. plots_defaults (ns) \u2014 The default parameters for the plots. - : Parameters passed to biopipen.utils::VizBulkDEGs() . See: https://pwwang.github.io/biopipen.utils.R/reference/VizBulkDEGs.html sigmarkers \u2014 An expression passed to dplyr::filter() to filter thesignificant markers for enrichment analysis. The default is p_val_adj < 0.05 . If tool = 'DESeq2' , the variables that can be used for filtering are: baseMean , log2FC , lfcSE , stat , p_val , p_val_adj . If tool = 'edgeR' , the variables that can be used for filtering are: logCPM , log2FC , LR , p_val , p_val_adj . subset \u2014 An expression in string to subset the cells. tool (choice) \u2014 The method to use for the differential expression analysis. - DESeq2: Use DESeq2 for the analysis. - edgeR: Use edgeR for the analysis. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.scrna"},{"location":"api/biopipen.ns.scrna/#biopipennsscrna","text":"</> Tools to analyze single-cell RNA Classes SeuratLoading ( Proc ) \u2014 Seurat - Loading data </> SeuratPreparing ( Proc ) \u2014 Load, prepare and apply QC to data, using Seurat </> SeuratClustering ( Proc ) \u2014 Determine the clusters of cells without reference using Seurat FindClustersprocedure. </> SeuratSubClustering ( Proc ) \u2014 Find clusters of a subset of cells. </> SeuratClusterStats ( Proc ) \u2014 Statistics of the clustering. </> ModuleScoreCalculator ( Proc ) \u2014 Calculate the module scores for each cell </> CellsDistribution ( Proc ) \u2014 Distribution of cells (i.e. in a TCR clone) from different groupsfor each cluster </> SeuratMetadataMutater ( Proc ) \u2014 Mutate the metadata of the seurat object </> DimPlots ( Proc ) \u2014 Seurat - Dimensional reduction plots </> MarkersFinder ( Proc ) \u2014 Find markers between different groups of cells </> TopExpressingGenes ( Proc ) \u2014 Find the top expressing genes in each cluster </> ExprImputation ( Proc ) \u2014 This process imputes the dropout values in scRNA-seq data. </> SCImpute ( Proc ) \u2014 Impute the dropout values in scRNA-seq data. </> SeuratFilter ( Proc ) \u2014 Filtering cells from a seurat object </> SeuratSubset ( Proc ) \u2014 Subset a seurat object into multiple seruat objects </> SeuratSplit ( Proc ) \u2014 Split a seurat object into multiple seruat objects </> Subset10X ( Proc ) \u2014 Subset 10X data, mostly used for testing </> SeuratTo10X ( Proc ) \u2014 Write a Seurat object to 10X format </> ScFGSEA ( Proc ) \u2014 Gene set enrichment analysis for cells in different groups using fgsea </> CellTypeAnnotation ( Proc ) \u2014 Annotate the cell clusters. Currently, four ways are supported: </> SeuratMap2Ref ( Proc ) \u2014 Map the seurat object to reference </> RadarPlots ( Proc ) \u2014 Radar plots for cell proportion in different clusters. </> MetaMarkers ( Proc ) \u2014 Find markers between three or more groups of cells, using one-way ANOVAor Kruskal-Wallis test. </> Seurat2AnnData ( Proc ) \u2014 Convert seurat object to AnnData </> AnnData2Seurat ( Proc ) \u2014 Convert AnnData to seurat object </> ScSimulation ( Proc ) \u2014 Simulate single-cell data using splatter. </> CellCellCommunication ( Proc ) \u2014 Cell-cell communication inference </> CellCellCommunicationPlots ( Proc ) \u2014 Visualization for cell-cell communication inference. </> ScVelo ( Proc ) \u2014 Velocity analysis for single-cell RNA-seq data </> Slingshot ( Proc ) \u2014 Trajectory inference using Slingshot </> LoomTo10X ( Proc ) \u2014 Convert Loom file to 10X format </> PseudoBulkDEG ( Proc ) \u2014 Pseduo-bulk differential gene expression analysis </> class","title":"biopipen.ns.scrna"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratloading","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Loading data Deprecated, should be superseded by SeuratPreparing Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The metadata of the samplesA tab-delimited file Two columns are required: - Sample to specify the sample names. - RNAData to assign the path of the data to the samples The path will be read by Read10X() from Seurat Output rdsfile \u2014 The RDS file with a list of Seurat object Envs qc \u2014 The QC filter for each sample.This will be passed to subset(obj, subset=<qc>) . For example nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5 Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratLoading"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratpreparing","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Load, prepare and apply QC to data, using Seurat This process will - - Prepare the seurat object - Apply QC to the data - Integrate the data from different samples See also - https://satijalab.org/seurat/articles/pbmc3k_tutorial.html#standard-pre-processing-workflow-1) - https://satijalab.org/seurat/articles/integration_introduction This process will read the scRNA-seq data, based on the information provided by SampleInfo , specifically, the paths specified by the RNAData column. Those paths should be either paths to directoies containing matrix.mtx , barcodes.tsv and features.tsv files that can be loaded by Seurat::Read10X() , or paths of loom files that can be loaded by SeuratDisk::LoadLoom() , or paths to h5 files that can be loaded by Seurat::Read10X_h5() . Each sample will be loaded individually and then merged into one Seurat object, and then perform QC. In order to perform QC, some additional columns are added to the meta data of the Seurat object. They are: precent.mt : The percentage of mitochondrial genes. percent.ribo : The percentage of ribosomal genes. precent.hb : The percentage of hemoglobin genes. percent.plat : The percentage of platelet genes. For integration, two routes are available: Performing integration on datasets normalized with SCTransform Using NormalizeData and FindIntegrationAnchors /// Note When using SCTransform , the default Assay will be set to SCT in output, rather than RNA . If you are using cca or rpca interation, the default assay will be integrated . /// /// Note From biopipen v0.23.0, this requires Seurat v5.0.0 or higher. /// Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The metadata of the samplesA tab-delimited file Two columns are required: Sample to specify the sample names. RNAData to assign the path of the data to the samples The path will be read by Read10X() from Seurat , or the path to the h5 file that can be read by Read10X_h5() from Seurat . It can also be an RDS or qs2 file containing a Seurat object. Note that it must has a column named Sample in the meta.data to specify the sample names. Output outfile \u2014 The qs2 file with the Seurat object with all samples integrated.Note that the cell ids are prefixied with sample names. Envs DoubletFinder (ns) \u2014 Arguments to run DoubletFinder .See also https://demultiplexing-doublet-detecting-docs.readthedocs.io/en/latest/DoubletFinder.html . - PCs (type=int): Number of PCs to use for 'doubletFinder' function. - doublets (type=float): Number of expected doublets as a proportion of the pool size. - pN (type=float): Number of doublets to simulate as a proportion of the pool size. - ncores (type=int): Number of cores to use for DoubletFinder::paramSweep . Set to None to use envs.ncores . Since parallelization of the function usually exhausts memory, if big envs.ncores does not work for DoubletFinder , set this to a smaller number. FindVariableFeatures (ns) \u2014 Arguments for FindVariableFeatures() . object is specified internally, and - in the key will be replaced with . . - : See https://satijalab.org/seurat/reference/findvariablefeatures IntegrateLayers (ns) \u2014 Arguments for IntegrateLayers() . object is specified internally, and - in the key will be replaced with . . When use_sct is True , normalization-method defaults to SCT . - method (choice): The method to use for integration. - CCAIntegration: Use Seurat::CCAIntegration . - CCA: Same as CCAIntegration . - cca: Same as CCAIntegration . - RPCAIntegration: Use Seurat::RPCAIntegration . - RPCA: Same as RPCAIntegration . - rpca: Same as RPCAIntegration . - HarmonyIntegration: Use Seurat::HarmonyIntegration . - Harmony: Same as HarmonyIntegration . - harmony: Same as HarmonyIntegration . - FastMNNIntegration: Use Seurat::FastMNNIntegration . - FastMNN: Same as FastMNNIntegration . - fastmnn: Same as FastMNNIntegration . - scVIIntegration: Use Seurat::scVIIntegration . - scVI: Same as scVIIntegration . - scvi: Same as scVIIntegration . - : See https://satijalab.org/seurat/reference/integratelayers NormalizeData (ns) \u2014 Arguments for NormalizeData() . object is specified internally, and - in the key will be replaced with . . - : See https://satijalab.org/seurat/reference/normalizedata RunPCA (ns) \u2014 Arguments for RunPCA() . object and features is specified internally, and - in the key will be replaced with . . - npcs (type=int): The number of PCs to compute. For each sample, npcs will be no larger than the number of columns - 1. - : See https://satijalab.org/seurat/reference/runpca SCTransform (ns) \u2014 Arguments for SCTransform() . object is specified internally, and - in the key will be replaced with . . - return-only-var-genes: Whether to return only variable genes. - min_cells: The minimum number of cells that a gene must be expressed in to be kept. A hidden argument of SCTransform to filter genes. If you try to keep all genes in the RNA assay, you can set min_cells to 0 and return-only-var-genes to False . See https://github.com/satijalab/seurat/issues/3598#issuecomment-715505537 - : See https://satijalab.org/seurat/reference/sctransform ScaleData (ns) \u2014 Arguments for ScaleData() . object and features is specified internally, and - in the key will be replaced with . . - : See https://satijalab.org/seurat/reference/scaledata cache (type=auto) \u2014 Whether to cache the information at different steps.If True , the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. The cached seurat object will be saved as <signature>.<kind>.RDS file, where <signature> is the signature determined by the input and envs of the process. See https://github.com/satijalab/seurat/issues/7849 , https://github.com/satijalab/seurat/issues/5358 and https://github.com/satijalab/seurat/issues/6748 for more details also about reproducibility issues. To not use the cached seurat object, you can either set cache to False or delete the cached file at <signature>.RDS in the cache directory. cell_qc \u2014 Filter expression to filter cells, using tidyrseurat::filter() . It can also be a dictionary of expressions, where the names of the list are sample names. You can have a default expression in the list with the name \"DEFAULT\" for the samples that are not listed. Available QC keys include nFeature_RNA , nCount_RNA , percent.mt , percent.ribo , percent.hb , and percent.plat . /// Tip | Example Including the columns added above, all available QC keys include nFeature_RNA , nCount_RNA , percent.mt , percent.ribo , percent.hb , and percent.plat . For example: [SeuratPreparing.envs] cell_qc = \"nFeature_RNA > 200 & percent.mt < 5\" will keep cells with more than 200 genes and less than 5%% mitochondrial genes. /// doublet_detector (choice) \u2014 The doublet detector to use. - none: Do not use any doublet detector. - DoubletFinder: Use DoubletFinder to detect doublets. - doubletfinder: Same as DoubletFinder . - scDblFinder: Use scDblFinder to detect doublets. - scdblfinder: Same as scDblFinder . gene_qc (ns) \u2014 Filter genes. gene_qc is applied after cell_qc . - min_cells: The minimum number of cells that a gene must be expressed in to be kept. - excludes: The genes to exclude. Multiple genes can be specified by comma separated values, or as a list. /// Tip | Example [SeuratPreparing.envs] gene_qc = { min_cells = 3 } will keep genes that are expressed in at least 3 cells. /// min_cells (type=int) \u2014 The minimum number of cells that a gene must beexpressed in to be kept. This is used in Seurat::CreateSeuratObject() . Futher QC ( envs.cell_qc , envs.gene_qc ) will be performed after this. It doesn't work when data is loaded from loom files or RDS/qs2 files. min_features (type=int) \u2014 The minimum number of features that a cell mustexpress to be kept. This is used in Seurat::CreateSeuratObject() . Futher QC ( envs.cell_qc , envs.gene_qc ) will be performed after this. It doesn't work when data is loaded from loom files or RDS/qs2 files. mutaters (type=json) \u2014 The mutaters to mutate the metadata to the cells.These new columns will be added to the metadata of the Seurat object and will be saved in the output file. ncores (type=int) \u2014 Number of cores to use.Used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. no_integration (flag) \u2014 Whether to skip integration or not. qc_plots (type=json) \u2014 The plots for QC metrics.It should be a json (or python dict) with the keys as the names of the plots and the values also as dicts with the following keys: * kind: The kind of QC. Either gene or cell (default). * devpars: The device parameters for the plot. A dict with res , height , and width . * more_formats: The formats to save the plots other than png . * save_code: Whether to save the code to reproduce the plot. * other arguments passed to biopipen.utils::VizSeuratCellQC when kind is cell or biopipen.utils::VizSeuratGeneQC when kind is gene . scDblFinder (ns) \u2014 Arguments to run scDblFinder . - dbr (type=float): The expected doublet rate. - ncores (type=int): Number of cores to use for scDblFinder . Set to None to use envs.ncores . - : See https://rdrr.io/bioc/scDblFinder/man/scDblFinder.html . use_sct (flag) \u2014 Whether use SCTransform routine to integrate samples or not.Before the following procedures, the RNA layer will be split by samples. If False , following procedures will be performed in the order: * NormalizeData . * FindVariableFeatures . * ScaleData . See https://satijalab.org/seurat/articles/seurat5_integration#layers-in-the-seurat-v5-object and https://satijalab.org/seurat/articles/pbmc3k_tutorial.html If True , following procedures will be performed in the order: * SCTransform . See https://satijalab.org/seurat/articles/seurat5_integration#perform-streamlined-one-line-integrative-analysis Requires r-bracer \u2014 check: {{proc.lang}} <(echo \"library(bracer)\") r-future \u2014 check: {{proc.lang}} <(echo \"library(future)\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library(Seurat)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratPreparing"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratclustering","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Determine the clusters of cells without reference using Seurat FindClustersprocedure. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object loaded by SeuratPreparing Output outfile \u2014 The seurat object with cluster information at seurat_clusters . Envs FindClusters (ns) \u2014 Arguments for FindClusters() . object is specified internally, and - in the key will be replaced with . . The cluster labels will be saved in seurat_clusters and prefixed with \"c\". The first cluster will be \"c1\", instead of \"c0\". - resolution (type=auto): The resolution of the clustering. You can have multiple resolutions as a list or as a string separated by comma. Ranges are also supported, for example: 0.1:0.5:0.1 will generate 0.1, 0.2, 0.3, 0.4, 0.5 . The step can be omitted, defaulting to 0.1. The results will be saved in seurat_clusters_<resolution> . The final resolution will be used to define the clusters at seurat_clusters . - : See https://satijalab.org/seurat/reference/findclusters FindNeighbors (ns) \u2014 Arguments for FindNeighbors() . object is specified internally, and - in the key will be replaced with . . - reduction: The reduction to use. If not provided, sobj@misc$integrated_new_reduction will be used. - : See https://satijalab.org/seurat/reference/findneighbors RunPCA (ns) \u2014 Arguments for RunPCA() . RunUMAP (ns) \u2014 Arguments for RunUMAP() . object is specified internally, and - in the key will be replaced with . . dims=N will be expanded to dims=1:N ; The maximal value of N will be the minimum of N and the number of columns - 1 for each sample. - dims (type=int): The number of PCs to use - reduction: The reduction to use for UMAP. If not provided, sobj@misc$integrated_new_reduction will be used. - : See https://satijalab.org/seurat/reference/runumap cache (type=auto) \u2014 Where to cache the information at different steps.If True , the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. Set to False to not cache the results. ncores (type=int;order=-100) \u2014 Number of cores to use.Used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. See also: https://satijalab.org/seurat/articles/future_vignette.html Requires r-dplyr \u2014 check: {{proc.lang}} <(echo \"library(dplyr)\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library(Seurat)\") r-tidyr \u2014 check: {{proc.lang}} <(echo \"library(tidyr)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratClustering"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratsubclustering","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Find clusters of a subset of cells. It's unlike [ Seurat::FindSubCluster ], which only finds subclusters of a single cluster. Instead, it will perform the whole clustering procedure on the subset of cells. One can use metadata to specify the subset of cells to perform clustering on. For the subset of cells, the reductions will be re-performed on the subset of cells, and then the clustering will be performed on the subset of cells. The reduction will be saved in object@reduction$<casename>.<reduction> of the original object and the clustering will be saved in the metadata of the original object using the casename as the column name. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS or qs/qs2 format. Output outfile \u2014 The seurat object with the subclustering information in qs/qs2 format. Envs FindClusters (ns) \u2014 Arguments for FindClusters() . object is specified internally, and - in the key will be replaced with . . The cluster labels will be prefixed with \"s\". The first cluster will be \"s1\", instead of \"s0\". - resolution (type=auto): The resolution of the clustering. You can have multiple resolutions as a list or as a string separated by comma. Ranges are also supported, for example: 0.1:0.5:0.1 will generate 0.1, 0.2, 0.3, 0.4, 0.5 . The step can be omitted, defaulting to 0.1. The results will be saved in <casename>_<resolution> . The final resolution will be used to define the clusters at <casename> . - : See https://satijalab.org/seurat/reference/findclusters FindNeighbors (ns) \u2014 Arguments for FindNeighbors() . object is specified internally, and - in the key will be replaced with . . - reduction: The reduction to use. If not provided, object@misc$integrated_new_reduction will be used. - : See https://satijalab.org/seurat/reference/findneighbors RunPCA (ns) \u2014 Arguments for RunPCA() . object is specified internally as the subset object, and - in the key will be replaced with . . - : See https://satijalab.org/seurat/reference/runpca RunUMAP (ns) \u2014 Arguments for RunUMAP() . object is specified internally as the subset object, and - in the key will be replaced with . . dims=N will be expanded to dims=1:N ; The maximal value of N will be the minimum of N and the number of columns - 1 for each sample. - dims (type=int): The number of PCs to use - reduction: The reduction to use for UMAP. If not provided, sobj@misc$integrated_new_reduction will be used. - : See https://satijalab.org/seurat/reference/runumap cache (type=auto) \u2014 Whether to cache the results.If True , the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. Set to False to not cache the results. cases (type=json) \u2014 The cases to perform subclustering.Keys are the names of the cases and values are the dicts inherited from envs except mutaters and cache . If empty, a case with name subcluster will be created with default parameters. The case name will be passed to biopipen.utils::SeuratSubCluster() as name . It will be used as the prefix for the reduction name, keys and cluster names. For reduction keys, it will be toupper(<name>) + \"PC_\" and toupper(<name>) + \"UMAP_\". For cluster names, it will be <name> + \".\" + resolution. And the final cluster name will be <name> . Note that the name should be alphanumeric and anything other than alphanumeric will be removed. mutaters (type=json) \u2014 The mutaters to mutate the metadata to subset the cells.The mutaters will be applied in the order specified. ncores (type=int;order=-100) \u2014 Number of cores to use.Used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. subset \u2014 An expression to subset the cells, will be passed to tidyseurat::filter() . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratSubClustering"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratclusterstats","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Statistics of the clustering. Including the number/fraction of cells in each cluster, the gene expression values and dimension reduction plots. It's also possible to perform stats on TCR clones/clusters or other metadata for each T-cell cluster. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples","title":"biopipen.ns.scrna.SeuratClusterStats"},{"location":"api/biopipen.ns.scrna/#number-of-cells-in-each-cluster","text":"[SeuratClusterStats.envs.stats] # suppose you have nothing set in `envs.stats_defaults` # otherwise, the settings will be inherited here nCells_All = { } {: width=\"80%\" }","title":"Number of cells in each cluster"},{"location":"api/biopipen.ns.scrna/#number-of-cells-in-each-cluster-by-groups","text":"[SeuratClusterStats.envs.stats] nCells_Sample = { group_by = \"Sample\" } {: width=\"80%\" }","title":"Number of cells in each cluster by groups"},{"location":"api/biopipen.ns.scrna/#violin-plots-for-the-gene-expressions","text":"[SeuratClusterStats.envs.features] features = \"CD4,CD8A\" # Remove the dots in the violin plots vlnplots = { pt-size = 0 , kind = \"vln\" } # Don't use the default genes vlnplots_1 = { features = [ \"FOXP3\" , \"IL2RA\" ], pt-size = 0 , kind = \"vln\" } {: width=\"80%\" } {: width=\"80%\" }","title":"Violin plots for the gene expressions"},{"location":"api/biopipen.ns.scrna/#dimension-reduction-plot-with-labels","text":"[SeuratClusterStats.envs.dimplots.Idents] label = true {: width=\"80%\" } Input srtobj \u2014 The seurat object loaded by SeuratClustering Output outdir \u2014 The output directory.Different types of plots will be saved in different subdirectories. For example, clustree plots will be saved in clustrees subdirectory. For each case in envs.clustrees , both the png and pdf files will be saved. Envs cache (type=auto) \u2014 Whether to cache the plots.Currently only plots for features are supported, since creating the those plots can be time consuming. If True , the plots will be cached in the job output directory, which will be not cleaned up when job is rerunning. clustrees (type=json) \u2014 The cases for clustree plots.Keys are the names of the plots and values are the dicts inherited from env.clustrees_defaults except prefix . There is no default case for clustrees . clustrees_defaults (ns) \u2014 The parameters for the clustree plots. - devpars (ns): The device parameters for the clustree plot. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - more_formats (type=list): The formats to save the plots other than png . - save_code (flag): Whether to save the code to reproduce the plot. - prefix (type=auto): string indicating columns containing clustering information. The trailing dot is not necessary and will be added automatically. When TRUE , clustrees will be plotted when there is FindClusters or FindClusters.* in the obj@commands . The latter is generated by SeuratSubClustering . This will be ignored when envs.clustrees is specified (the prefix of each case must be specified separately). - : Other arguments passed to scplotter::ClustreePlot . See https://pwwang.github.io/scplotter/reference/ClustreePlot.html dimplots (type=json) \u2014 The dimensional reduction plots.Keys are the titles of the plots and values are the dicts inherited from env.dimplots_defaults . It can also have other parameters from scplotter::CellDimPlot . dimplots_defaults (ns) \u2014 The default parameters for dimplots . - group_by: The identity to use. If it is from subclustering (reduction sub_umap_<ident> exists), this reduction will be used if reduction is set to dim or auto . - split_by: The column name in metadata to split the cells into different plots. - subset: An expression to subset the cells, will be passed to tidyrseurat::filter() . - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - reduction (choice): Which dimensionality reduction to use. - dim: Use Seurat::DimPlot . First searches for umap , then tsne , then pca . If ident is from subclustering, sub_umap_<ident> will be used. - auto: Same as dim - umap: Use Seurat::UMAPPlot . - tsne: Use Seurat::TSNEPlot . - pca: Use Seurat::PCAPlot . - : See https://pwwang.github.io/scplotter/reference/CellDimPlot.html features (type=json) \u2014 The plots for features, include gene expressions, and columns from metadata.Keys are the titles of the cases and values are the dicts inherited from env.features_defaults . features_defaults (ns) \u2014 The default parameters for features . - features (type=auto): The features to plot. It can be either a string with comma separated features, a list of features, a file path with file:// prefix with features (one per line), or an integer to use the top N features from VariantFeatures(srtobj) . It can also be a dict with the keys as the feature group names and the values as the features, which is used for heatmap to group the features. - order_by (type=auto): The order of the clusters to show on the plot. An expression passed to dplyr::arrange() on the grouped meta data frame (by ident ). For example, you can order the clusters by the activation score of the cluster: desc(mean(ActivationScore, na.rm = TRUE)) , suppose you have a column ActivationScore in the metadata. You may also specify the literal order of the clusters by a list of strings (at least two). - subset: An expression to subset the cells, will be passed to tidyrseurat::filter() . - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - descr: The description of the plot, showing in the report. - more_formats (type=list): The formats to save the plots other than png . - save_code (flag): Whether to save the code to reproduce the plot. - save_data (flag): Whether to save the data used to generate the plot. - : Other arguments passed to scplotter::FeatureStatPlot . See https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html mutaters (type=json) \u2014 The mutaters to mutate the metadata to subset the cells.The mutaters will be applied in the order specified. You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . ngenes (type=json) \u2014 The number of genes expressed in each cell.Keys are the names of the plots and values are the dicts inherited from env.ngenes_defaults . ngenes_defaults (ns) \u2014 The default parameters for ngenes .The default parameters to plot the number of genes expressed in each cell. - more_formats (type=list): The formats to save the plots other than png . - subset: An expression to subset the cells, will be passed to tidyrseurat::filter() . - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. stats (type=json) \u2014 The number/fraction of cells to plot.Keys are the names of the plots and values are the dicts inherited from env.stats_defaults . Here are some examples - { \"nCells_All\": {}, \"nCells_Sample\": {\"group_by\": \"Sample\"}, \"fracCells_Sample\": {\"scale_y\": True, \"group_by\": \"Sample\", plot_type = \"pie\"}, } stats_defaults (ns) \u2014 The default parameters for stats .This is to do some basic statistics on the clusters/cells. For more comprehensive analysis, see https://pwwang.github.io/scplotter/reference/CellStatPlot.html . The parameters from the cases can overwrite the default parameters. - subset: An expression to subset the cells, will be passed to tidyrseurat::filter() . - devpars (ns): The device parameters for the clustree plot. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - descr: The description of the plot, showing in the report. - more_formats (type=list): The formats to save the plots other than png . - save_code (flag): Whether to save the code to reproduce the plot. - save_data (flag): Whether to save the data used to generate the plot. - : Other arguments passed to scplotter::CellStatPlot . See https://pwwang.github.io/scplotter/reference/CellStatPlot.html . Requires r-seurat \u2014 check: {{proc.lang}} -e \"library(Seurat)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"Dimension reduction plot with labels"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnamodulescorecalculator","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate the module scores for each cell The module scores are calculated by Seurat::AddModuleScore() or Seurat::CellCycleScoring() for cell cycle scores. The module scores are calculated as the average expression levels of each program on single cell level, subtracted by the aggregated expression of control feature sets. All analyzed features are binned based on averaged expression, and the control features are randomly selected from each bin. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object loaded by SeuratClustering Output rdsfile \u2014 The seurat object with module scores added to the metadata. Envs defaults (ns) \u2014 The default parameters for modules . - features: The features to calculate the scores. Multiple features should be separated by comma. You can also specify cc.genes or cc.genes.updated.2019 to use the cell cycle genes to calculate cell cycle scores. If so, three columns will be added to the metadata, including S.Score , G2M.Score and Phase . Only one type of cell cycle scores can be calculated at a time. - nbin (type=int): Number of bins of aggregate expression levels for all analyzed features. - ctrl (type=int): Number of control features selected from the same bin per analyzed feature. - k (flag): Use feature clusters returned from DoKMeans . - assay: The assay to use. - seed (type=int): Set a random seed. - search (flag): Search for symbol synonyms for features in features that don't match features in object? - keep (flag): Keep the scores for each feature? Only works for non-cell cycle scores. - agg (choice): The aggregation function to use. Only works for non-cell cycle scores. - mean: The mean of the expression levels - median: The median of the expression levels - sum: The sum of the expression levels - max: The max of the expression levels - min: The min of the expression levels - var: The variance of the expression levels - sd: The standard deviation of the expression levels modules (type=json) \u2014 The modules to calculate the scores.Keys are the names of the expression programs and values are the dicts inherited from env.defaults . Here are some examples - { \"CellCycle\": {\"features\": \"cc.genes.updated.2019\"}, \"Exhaustion\": {\"features\": \"HAVCR2,ENTPD1,LAYN,LAG3\"}, \"Activation\": {\"features\": \"IFNG\"}, \"Proliferation\": {\"features\": \"STMN1,TUBB\"} } For CellCycle , the columns S.Score , G2M.Score and Phase will be added to the metadata. S.Score and G2M.Score are the cell cycle scores for each cell, and Phase is the cell cycle phase for each cell. You can also add Diffusion Components (DC) to the modules {\"DC\": {\"features\": 2, \"kind\": \"diffmap\"}} will perform diffusion map as a reduction and add the first 2 components as DC_1 and DC_2 to the metadata. diffmap is a shortcut for diffusion_map . Other key-value pairs will pass to destiny::DiffusionMap() . You can later plot the diffusion map by using reduction = \"DC\" in env.dimplots in SeuratClusterStats . This requires SingleCellExperiment and destiny R packages. post_mutaters (type=json) \u2014 The mutaters to mutate the metadata aftercalculating the module scores. The mutaters will be applied in the order specified. This is useful when you want to create new scores based on the calculated module scores. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.ModuleScoreCalculator"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_5","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnacellsdistribution","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Distribution of cells (i.e. in a TCR clone) from different groupsfor each cluster This generates a set of pie charts with proportion of cells in each cluster Rows are the cells identities (i.e. TCR clones or TCR clusters), columns are groups (i.e. clinic groups). Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples [CellsDistribution.envs.mutaters] # Add Patient1_Tumor_Expanded column with CDR3.aa that # expands in Tumor of patient 1 Patient1_Tumor_Expanded = ''' expanded(., region, \"Tumor\", subset = patient == \"Lung1\", uniq = FALSE) ''' [CellsDistribution.envs.cases.Patient1_Tumor_Expanded] cells_by = \"Patient1_Tumor_Expanded\" cells_orderby = \"desc(CloneSize)\" group_by = \"region\" group_order = [ \"Tumor\" , \"Normal\" ] Input srtobj \u2014 The seurat object in RDS format Output outdir \u2014 The output directory.The results for each case will be saved in a subdirectory. Envs cases (type=json;order=99) \u2014 If you have multiple cases, you can specify them here.Keys are the names of the cases and values are the options above except mutaters . If some options are not specified, the options in envs will be used. If no cases are specified, a default case will be used with case name DEFAULT . cells_by \u2014 The column name in metadata to group the cells for the rows of the plot.If your cell groups have overlapping cells, you can also use multiple columns, separated by comma ( , ). These columns will be concatenated to form the cell groups. For the overlapping cells, they will be counted multiple times for different groups. So make sure the cell group names in different columns are unique. cells_n (type=int) \u2014 The max number of groups to show for each cell group identity (row).Ignored if cells_order is specified. cells_order (list) \u2014 The order of the cells (rows) to show on the plot cells_orderby \u2014 An expression passed to dplyr::arrange() to order the cells (rows) of the plot.Only works when cells-order is not specified. The data frame passed to dplyr::arrange() is grouped by cells_by before ordering. You can have multiple expressions separated by semicolon ( ; ). The expessions will be parsed by rlang::parse_exprs() . 4 extra columns were added to the metadata for ordering the rows in the plot: * CloneSize : The size (number of cells) of clones (identified by cells_by ) * CloneGroupSize : The clone size in each group (identified by group_by ) * CloneClusterSize : The clone size in each cluster (identified by seurat_clusters ) * CloneGroupClusterSize : The clone size in each group and cluster (identified by group_by and seurat_clusters ) cluster_orderby \u2014 The order of the clusters to show on the plot.An expression passed to dplyr::summarise() on the grouped data frame (by seurat_clusters ). The summary stat will be passed to dplyr::arrange() to order the clusters. It's applied on the whole meta.data before grouping and subsetting. For example, you can order the clusters by the activation score of the cluster: desc(mean(ActivationScore, na.rm = TRUE)) , suppose you have a column ActivationScore in the metadata. descr \u2014 The description of the case, will be shown in the report. devpars (ns) \u2014 The device parameters for the plots of pie charts. - res (type=int): The resolution of the plots - height (type=int): The height of the plots - width (type=int): The width of the plots each \u2014 The column name in metadata to separate the cells into different plots. group_by \u2014 The column name in metadata to group the cells for the columns of the plot. group_order (list) \u2014 The order of the groups (columns) to show on the plot hm_devpars (ns) \u2014 The device parameters for the heatmaps. - res (type=int): The resolution of the heatmaps. - height (type=int): The height of the heatmaps. - width (type=int): The width of the heatmaps. mutaters (type=json) \u2014 The mutaters to mutate the metadataKeys are the names of the mutaters and values are the R expressions passed by dplyr::mutate() to mutate the metadata. overlap (list) \u2014 Plot the overlap of cell groups (values of cells_by ) in different casesunder the same section. The section must have at least 2 cases, each case should have a single cells_by column. prefix_each (flag) \u2014 Whether to prefix the each column name to thevalue as the case/section name. section \u2014 The section to show in the report. This allows different cases to be put in the same section in report.Only works when each is not specified. subset \u2014 An expression to subset the cells, will be passed to dplyr::filter() on metadata.This will be applied prior to each . Requires r-dplyr \u2014 check: {{proc.lang}} -e \"library(dplyr)\" r-seurat \u2014 check: {{proc.lang}} -e \"library(Seurat)\" r-tidyr \u2014 check: {{proc.lang}} -e \"library(tidyr)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.CellsDistribution"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_6","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_6","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_6","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_6","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_6","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_6","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_6","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratmetadatamutater","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Mutate the metadata of the seurat object Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 Additional metadataA tab-delimited file with columns as meta columns and rows as cells. srtobj \u2014 The seurat object loaded by SeuratPreparing Output outfile \u2014 The seurat object with the additional metadata Envs mutaters (type=json) \u2014 The mutaters to mutate the metadata.The key-value pairs will be passed the dplyr::mutate() to mutate the metadata. Requires r-dplyr \u2014 check: {{proc.lang}} <(echo \"library(dplyr)\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library(Seurat)\") r-tibble \u2014 check: {{proc.lang}} <(echo \"library(tibble)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratMetadataMutater"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_7","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_7","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_7","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_7","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_7","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_7","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_7","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnadimplots","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Seurat - Dimensional reduction plots Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 A toml configuration file with \"cases\"If this is given, envs.cases will be overriden name \u2014 The name of the job, used in report srtobj \u2014 The seruat object in RDS format Output outdir \u2014 The output directory Envs cases \u2014 The cases for the dim plotsKeys are the names and values are the arguments to Seurat::Dimplots Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.DimPlots"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_8","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_8","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_8","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_8","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_8","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_8","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_8","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnamarkersfinder","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Find markers between different groups of cells When only group_by is specified as \"seurat_clusters\" in envs.cases , the markers will be found for all the clusters. You can also find the differentially expressed genes between any two groups of cells by setting group_by to a different column name in metadata. Follow envs.cases for more details. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object loaded by SeuratPreparing If you have your Seurat object prepared by yourself, you can also use it here, but you should make sure that the object has been processed by PrepSCTFindMarkers if data is not normalized using SCTransform . Output outdir \u2014 The output directory for the markers and plots Envs allenrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from allenrich_plots_defaults . The cases under envs.cases can inherit this options. allenrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . allmarker_plots (type=json) \u2014 All marker plot cases.The keys are the names of the cases and the values are the dicts inherited from allmarker_plots_defaults . allmarker_plots_defaults (ns) \u2014 Default options for the plots for all markers when ident-1 is not specified. - plot_type: The type of the plot. See https://pwwang.github.io/biopipen.utils.R/reference/VizDEGs.html . Available types are violin , box , bar , ridge , dim , heatmap and dot . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : Other arguments passed to biopipen.utils::VizDEGs() . assay \u2014 The assay to use. cache (type=auto) \u2014 Where to cache the results.If True , cache to outdir of the job. If False , don't cache. Otherwise, specify the directory to cache to. cases (type=json) \u2014 If you have multiple cases for marker discovery, you can specify themhere. The keys are the names of the cases and the values are the above options. If some options are not specified, the default values specified above (under envs ) will be used. If no cases are specified, the default case will be added with the default values under envs with the name Marker Discovery . dbs (list) \u2014 The dbs to do enrichment analysis for significantmarkers See below for all libraries. https://maayanlab.cloud/Enrichr/#libraries each \u2014 The column name in metadata to separate the cells into differentcases. When this is specified, the case will be expanded for each value of the column in metadata. For example, when you have envs.cases.\"Cluster Markers\".each = \"Sample\" , then the case will be expanded as envs.cases.\"Cluster Markers - Sample1\" , envs.cases.\"Cluster Markers - Sample2\" , etc. You can specify allmarker_plots and overlaps to plot the markers for all cases in the same plot and plot the overlaps of the markers between different cases by values in this column. enrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from enrich_plots_defaults . The cases under envs.cases can inherit this options. enrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . Available types are bar , dot , lollipop , network , enrichmap and wordcloud . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . enrich_style (choice) \u2014 The style of the enrichment analysis.The enrichment analysis will be done by EnrichIt() from enrichit . Two styles are available: - enrichr: enrichr style enrichment analysis (fisher's exact test will be used). - clusterprofiler: clusterProfiler style enrichment analysis (hypergeometric test will be used). - clusterProfiler: alias for clusterprofiler error (flag) \u2014 Error out if no/not enough markers are found or no pathways are enriched.If False , empty results will be returned. group_by \u2014 The column name in metadata to group the cells.If only group_by is specified, and ident-1 and ident-2 are not specified, markers will be found for all groups in this column in the manner of \"group vs rest\" comparison. NA group will be ignored. If None , Seurat::Idents(srtobj) will be used, which is usually \"seurat_clusters\" after unsupervised clustering. ident_1 \u2014 The first group of cells to compareWhen this is empty, the comparisons will be expanded to each group v.s. the rest of the cells in group_by . ident_2 \u2014 The second group of cells to compareIf not provided, the rest of the cells are used for ident-2 . marker_plots (type=json) \u2014 Cases of the plots to generate for the markers.Plot cases. The keys are the names of the cases and the values are the dicts inherited from marker_plots_defaults . The cases under envs.cases can inherit this options. marker_plots_defaults (ns) \u2014 Default options for the plots to generate for the markers. - plot_type: The type of the plot. See https://pwwang.github.io/biopipen.utils.R/reference/VizDEGs.html . Available types are violin , box , bar , ridge , dim , heatmap and dot . There are two additional types available - volcano_pct and volcano_log2fc . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : Other arguments passed to biopipen.utils::VizDEGs() . If plot_type is volcano_pct or volcano_log2fc , they will be passed to scplotter::VolcanoPlot() . mutaters (type=json) \u2014 The mutaters to mutate the metadata.You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . ncores (type=int) \u2014 Number of cores to use for parallel computing for some Seurat procedures. * Used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. * See also: https://satijalab.org/seurat/articles/future_vignette.html overlaps (type=json) \u2014 Cases for investigating the overlapping of significant markers between different cases or comparisons.The keys are the names of the cases and the values are the dicts inherited from overlaps_defaults . There are two situations that we can perform overlaps: 1. If ident-1 is not specified, the overlaps can be performed between different comparisons. 2. If each is specified, the overlaps can be performed between different cases, where in each case, ident-1 must be specified. overlaps_defaults (ns) \u2014 Default options for investigating the overlapping of significant markers between different cases or comparisons.This means either ident-1 should be empty, so that they can be expanded to multiple comparisons. - sigmarkers: The expression to filter the significant markers for each case. If not provided, envs.sigmarkers will be used. - plot_type (choice): The type of the plot to generate for the overlaps. - venn: Use plotthis::VennDiagram() . - upset: Use plotthis::UpsetPlot() . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : More arguments pased to plotthis::VennDiagram() ( https://pwwang.github.io/plotthis/reference/venndiagram1.html ) or plotthis::UpsetPlot() ( https://pwwang.github.io/plotthis/reference/upsetplot1.html ) rest (ns) \u2014 Rest arguments for Seurat::FindMarkers() .Use - to replace . in the argument name. For example, use min-pct instead of min.pct . - : See https://satijalab.org/seurat/reference/findmarkers sigmarkers \u2014 An expression passed to dplyr::filter() to filter thesignificant markers for enrichment analysis. Available variables are p_val , avg_log2FC , pct.1 , pct.2 and p_val_adj . For example, \"p_val_adj < 0.05 & abs(avg_log2FC) > 1\" to select markers with adjusted p-value < 0.05 and absolute log2 fold change > 1. subset \u2014 An expression to subset the cells for each case. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.MarkersFinder"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_9","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_9","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_9","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_9","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_9","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_9","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_9","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnatopexpressinggenes","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Find the top expressing genes in each cluster Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS or qs/qs2 format Output outdir \u2014 The output directory for the tables and plots Envs cases (type=json) \u2014 If you have multiple cases, you can specify themhere. The keys are the names of the cases and the values are the above options except mutaters . If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the default values under envs with the name Top Expressing Genes . dbs (list) \u2014 The dbs to do enrichment analysis for significantmarkers See below for all libraries. https://maayanlab.cloud/Enrichr/#libraries each \u2014 The column name in metadata to separate the cells into differentcases. enrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from enrich_plots_defaults . The cases under envs.cases can inherit this options. enrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . Available types are bar , dot , lollipop , network , enrichmap and wordcloud . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.htmll . enrich_style (choice) \u2014 The style of the enrichment analysis.The enrichment analysis will be done by EnrichIt() from enrichit . Two styles are available: - enrichr: enrichr style enrichment analysis (fisher's exact test will be used). - clusterprofiler: clusterProfiler style enrichment analysis (hypergeometric test will be used). - clusterProfiler: alias for clusterprofiler group_by \u2014 The column name in metadata to group the cells. ident \u2014 The group of cells to find the top expressing genes.The cells will be selected by the group_by column with this ident value in metadata. If not provided, the top expressing genes will be found for all groups of cells in the group_by column. mutaters (type=json) \u2014 The mutaters to mutate the metadata.You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . n (type=int) \u2014 The number of top expressing genes to find. subset \u2014 An expression to subset the cells for each case. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.TopExpressingGenes"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_10","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_10","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_10","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_10","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_10","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_10","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_10","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaexprimputation","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc This process imputes the dropout values in scRNA-seq data. It takes the Seurat object as input and outputs the Seurat object with imputed expression data. Reference: - Linderman, George C., Jun Zhao, and Yuval Kluger. \"Zero-preserving imputation of scRNA-seq data using low-rank approximation.\" BioRxiv (2018): 397588. - Li, Wei Vivian, and Jingyi Jessica Li. \"An accurate and robust imputation method scImpute for single-cell RNA-seq data.\" Nature communications 9.1 (2018): 997. - Dijk, David van, et al. \"MAGIC: A diffusion-based imputation method reveals gene-gene interactions in single-cell RNA-sequencing data.\" BioRxiv (2017): 111591. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file in RDS/qs format of Seurat object Output outfile \u2014 The output file in RDS format of Seurat objectNote that with rmagic and alra, the original default assay will be renamed to RAW and the imputed RNA assay will be renamed to RNA and set as default assay. Envs alra_args (type=json) \u2014 The arguments for RunALRA() rmagic_args (ns) \u2014 The arguments for rmagic - python: The python path where magic-impute is installed. - threshold (type=float): The threshold for magic imputation. Only the genes with dropout rates greater than this threshold (No. of cells with non-zero expression / total number of cells) will be imputed. scimpute_args (ns) \u2014 The arguments for scimpute - drop_thre (type=float): The dropout threshold - kcluster (type=int): Number of clusters to use - ncores (type=int): Number of cores to use - refgene: The reference gene file tool (choice) \u2014 Either alra, scimpute or rmagic - alra: Use RunALRA() from Seurat - scimpute: Use scImpute() from scimpute - rmagic: Use magic() from Rmagic Requires magic-impute \u2014 if: {{proc.envs.tool == \"rmagic\"}} check: {{proc.envs.rmagic_args.python}} -c \"import magic\") r-dplyr \u2014 if: {{proc.envs.tool == \"scimpute\"}} check: {{proc.lang}} <(echo \"library(dplyr)\") r-rmagic \u2014 if: {{proc.envs.tool == \"rmagic\"}} check: | {{proc.lang}} <( echo \" tryCatch( { setwd(dirname(Sys.getenv('CONDA_PREFIX'))) }, error = function(e) NULL ); library(Rmagic) \" ) r-scimpute \u2014 if: {{proc.envs.tool == \"scimpute\"}} check: {{proc.lang}} <(echo \"library(scImpute)\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library(Seurat)\") r-seuratwrappers \u2014 if: {{proc.envs.tool == \"alra\"}} check: {{proc.lang}} <(echo \"library(SeuratWrappers)\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.ExprImputation"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_11","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_11","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_11","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_11","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_11","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_11","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_11","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnascimpute","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Impute the dropout values in scRNA-seq data. Deprecated. Use ExprImputation instead. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input groupfile \u2014 The file to subset the matrix or label the cellsCould be an output from ImmunarchFilter infile \u2014 The input file for imputationEither a SeuratObject or a matrix of count/TPM Output outfile \u2014 The output matrix Envs infmt \u2014 The input format.Either seurat or matrix Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SCImpute"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_12","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_12","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_12","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_12","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_12","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_12","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_12","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratfilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Filtering cells from a seurat object Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input filters \u2014 The filters to apply. Could be a file or string in TOML, ora python dictionary, with following keys: - mutaters: Create new columns in the metadata - filter: A R expression that will pass to subset(sobj, subset = ...) to filter the cells srtobj \u2014 The seurat object in RDS Output outfile \u2014 The filtered seurat object in RDS Envs invert \u2014 Invert the selection? Requires r-dplyr \u2014 check: {{proc.lang}} <(echo \"library('dplyr')\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library('Seurat')\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratFilter"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_13","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_13","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_13","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_13","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_13","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_13","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_13","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratsubset","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Subset a seurat object into multiple seruat objects Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS subsets \u2014 The subsettings to apply. Could be a file or string in TOML, ora python dictionary, with following keys: - : Name of the case mutaters: Create new columns in the metadata subset: A R expression that will pass to subset(sobj, subset = ...) groupby: The column to group by, each value will be a case If groupby is given, subset will be ignored, each value of the groupby column will be a case Output outdir \u2014 The output directory with the subset seurat objects Envs ignore_nas \u2014 Ignore NA values? Requires r-dplyr \u2014 check: {{proc.lang}} <(echo \"library('dplyr')\") r-seurat \u2014 check: {{proc.lang}} <(echo \"library('Seurat')\") Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratSubset"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_14","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_14","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_14","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_14","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_14","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_14","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_14","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratsplit","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Split a seurat object into multiple seruat objects Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input by \u2014 The metadata column to split by srtobj \u2014 The seurat object in RDS Output outdir \u2014 The output directory with the subset seurat objects Envs by \u2014 The metadata column to split byIgnored if by is given in the input recell \u2014 Rename the cell ids using the by columnA string of R function taking the original cell ids and by Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratSplit"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_15","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_15","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_15","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_15","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_15","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_15","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_15","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnasubset10x","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Subset 10X data, mostly used for testing Requires r-matrix to load matrix.mtx.gz Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 The input directory Output outdir \u2014 The output directory Envs feats_to_keep \u2014 The features/genes to keep.The final features list will be feats_to_keep + nfeats ncells \u2014 The number of cells to keep.If <=1 then it will be the percentage of cells to keep nfeats \u2014 The number of features to keep.If <=1 then it will be the percentage of features to keep seed \u2014 The seed for random number generator Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.Subset10X"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_16","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_16","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_16","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_16","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_16","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_16","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_16","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratto10x","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Write a Seurat object to 10X format using write10xCounts from DropletUtils Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS Output outdir \u2014 The output directory.When envs.split_by is specified, the subdirectories will be created for each distinct value of the column. Otherwise, the matrices will be written to the output directory. Envs version \u2014 The version of 10X format Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratTo10X"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_17","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_17","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_17","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_17","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_17","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_17","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_17","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnascfgsea","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Gene set enrichment analysis for cells in different groups using fgsea This process allows us to do Gene Set Enrichment Analysis (GSEA) on the expression data, but based on variaties of grouping, including the from the meta data and the scTCR-seq data as well. The GSEA is done using the fgsea package, which allows to quickly and accurately calculate arbitrarily low GSEA P-values for a collection of gene sets. The fgsea package is based on the fast algorithm for preranked GSEA described in Subramanian et al. 2005 . For each case, the process will generate a table with the enrichment scores for each gene set, and GSEA plots for the top gene sets. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object in RDS format Output outdir \u2014 The output directory for the results and plots Envs alleach_plots (type=json) \u2014 Cases of the plots to generate for all pathways.The keys are the names of the cases and the values are the dicts inherited from alleach_plots_defaults . alleach_plots_defaults (ns) \u2014 Default options for the plots to generate for all pathways. - plot_type: The type of the plot, currently either dot or heatmap (default) - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html . cases (type=json;order=99) \u2014 If you have multiple cases, you can specify them here.The keys are the names of the cases and the values are the above options except mutaters . If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the name GSEA . each \u2014 The column name in metadata to separate the cells into different subsets to do the analysis. eps (type=float) \u2014 This parameter sets the boundary for calculating the p value.See https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html gmtfile \u2014 The pathways in GMT format, with the gene names/ids in the same format as the seurat object.One could also use a URL to a GMT file. For example, from https://download.baderlab.org/EM_Genesets/current_release/Human/symbol/Pathways/ . group_by \u2014 The column name in metadata to group the cells. ident_1 \u2014 The first group of cells to compare ident_2 \u2014 The second group of cells to compare, if not provided, the rest of the cells that are not NA s in group_by column are used for ident-2 . maxsize (type=int) \u2014 Maximal size of a gene set to test. All pathways above the threshold are excluded. method (choice) \u2014 The method to do the preranking. - signal_to_noise: Signal to noise. The larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \"class marker\". - s2n: Alias of signal_to_noise. - abs_signal_to_noise: The absolute value of signal_to_noise. - abs_s2n: Alias of abs_signal_to_noise. - t_test: T test. Uses the difference of means scaled by the standard deviation and number of samples. - ratio_of_classes: Also referred to as fold change. Uses the ratio of class means to calculate fold change for natural scale data. - diff_of_classes: Difference of class means. Uses the difference of class means to calculate fold change for nature scale data - log2_ratio_of_classes: Log2 ratio of class means. Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. minsize (type=int) \u2014 Minimal size of a gene set to test. All pathways below the threshold are excluded. mutaters (type=json) \u2014 The mutaters to mutate the metadata.The key-value pairs will be passed the dplyr::mutate() to mutate the metadata. You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . ncores (type=int) \u2014 Number of cores for parallelizationPassed to nproc of fgseaMultilevel() . rest (type=json;order=98) \u2014 Rest arguments for fgsea() See also https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html subset \u2014 An expression to subset the cells. top (type=auto) \u2014 Do gsea table and enrich plot for top N pathways.If it is < 1, will apply it to padj , selecting pathways with padj < top . Requires bioconductor-fgsea \u2014 check: {{proc.lang}} -e \"library(fgsea)\" r-seurat \u2014 check: {{proc.lang}} -e \"library(seurat)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.ScFGSEA"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_18","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_18","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_18","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_18","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_18","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_18","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_18","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnacelltypeannotation","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Annotate the cell clusters. Currently, four ways are supported: Pass the cell type annotation directly Use ScType Use scCATCH Use hitype The annotated cell types will replace the original seurat_clusters column in the metadata, so that the downstream processes will use the annotated cell types. The old seurat_clusters column will be renamed to seurat_clusters_id . If you are using ScType , scCATCH , or hitype , a text file containing the mapping from the old seurat_clusters to the new cell types will be generated and saved to cluster2celltype.tsv under <workdir>/<pipline_name>/CellTypeAnnotation/0/output/ . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples [CellTypeAnnotation.envs] tool = \"direct\" cell_types = [ \"CellType1\" , \"CellType2\" , \"-\" , \"CellType4\" ] The cell types will be assigned as: 0 -> CellType1 1 -> CellType2 2 -> 2 3 -> CellType4 Input sobjfile \u2014 The single-cell object in RDS/qs/qs2/h5ad format. Output outfile \u2014 The rds/qs/qs2/h5ad file of seurat object with cell type annotated.A text file containing the mapping from the old seurat_clusters to the new cell types will be generated and saved to cluster2celltype.tsv under the job output directory. Envs cell_types (list) \u2014 The cell types to use for direct annotation.You can use \"-\" or \"\" as the placeholder for the clusters that you want to keep the original cell types ( seurat_clusters ). If the length of cell_types is shorter than the number of clusters, the remaining clusters will be kept as the original cell types. You can also use NA to remove the clusters from downstream analysis. This only works when envs.newcol is not specified. /// Note If tool is direct and cell_types is not specified or an empty list, the original cell types will be kept and nothing will be changed. /// celltypist_args (ns) \u2014 The arguments for celltypist::celltypist() if tool is celltypist . - model: The path to model file. - python: The python path where celltypist is installed. - majority_voting: When true, it refines cell identities within local subclusters after an over-clustering approach at the cost of increased runtime. - over_clustering (type=auto): The column name in metadata to use as clusters for majority voting. Set to False to disable over-clustering. When in.sobjfile is rds/qs/qs2 (supposing we have a Seurat object), the default ident is used by default. Otherwise, it is False by default. - assay: When converting a Seurat object to AnnData, the assay to use. If input is h5seurat, this defaults to RNA. If input is Seurat object in RDS, this defaults to the default assay. hitype_db \u2014 The database to use for hitype.Compatible with sctype_db . See also https://pwwang.github.io/hitype/articles/prepare-gene-sets.html You can also use built-in databases, including hitypedb_short , hitypedb_full , and hitypedb_pbmc3k . hitype_tissue \u2014 The tissue to use for hitype .Avaiable tissues should be the first column ( tissueType ) of hitype_db . If not specified, all rows in hitype_db will be used. merge (flag) \u2014 Whether to merge the clusters with the same cell types.Otherwise, a suffix will be added to the cell types (ie. .1 , .2 , etc). more_cell_types (type=json) \u2014 The additional cell type annotations to add to the metadata.The keys are the new column names and the values are the cell types lists. The cell type lists work the same as cell_types above. This is useful when you want to keep multiple annotations of cell types. newcol \u2014 The new column name to store the cell types.If not specified, the seurat_clusters column will be overwritten. If specified, the original seurat_clusters column will be kept and Idents will be kept as the original seurat_clusters . outtype (choice) \u2014 The output file type. Currently only works for celltypist .An RDS file will be generated for other tools. - input: Use the same file type as the input. - rds: Use RDS file. - qs: Use qs2 file. - qs2: Use qs2 file. - h5ad: Use AnnData file. sccatch_args (ns) \u2014 The arguments for scCATCH::findmarkergene() if tool is sccatch . - species: The specie of cells. - cancer: If the sample is from cancer tissue, then the cancer type may be defined. - tissue: Tissue origin of cells must be defined. - marker: The marker genes for cell type identification. - if_use_custom_marker (flag): Whether to use custom marker genes. If True , no species , cancer , and tissue are needed. - : Other arguments for scCATCH::findmarkergene() . You can pass an RDS file to sccatch_args.marker to work as custom marker. If so, if_use_custom_marker will be set to TRUE automatically. sctype_db \u2014 The database to use for sctype.Check examples at https://github.com/IanevskiAleksandr/sc-type/blob/master/ScTypeDB_full.xlsx sctype_tissue \u2014 The tissue to use for sctype .Avaiable tissues should be the first column ( tissueType ) of sctype_db . If not specified, all rows in sctype_db will be used. tool (choice) \u2014 The tool to use for cell type annotation. - sctype: Use scType to annotate cell types. See https://github.com/IanevskiAleksandr/sc-type - hitype: Use hitype to annotate cell types. See https://github.com/pwwang/hitype - sccatch: Use scCATCH to annotate cell types. See https://github.com/ZJUFanLab/scCATCH - celltypist: Use celltypist to annotate cell types. See https://github.com/Teichlab/celltypist - direct: Directly assign cell types Requires r-HGNChelper \u2014 if: {{proc.envs.tool == 'sctype'}} check: {{proc.lang}} -e \"library(HGNChelper)\" r-dplyr \u2014 if: {{proc.envs.tool == 'sctype'}} check: {{proc.lang}} -e \"library(dplyr)\" r-openxlsx \u2014 if: {{proc.envs.tool == 'sctype'}} check: {{proc.lang}} -e \"library(openxlsx)\" r-seurat \u2014 if: {{proc.envs.tool == 'sctype'}} check: {{proc.lang}} -e \"library(Seurat)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.CellTypeAnnotation"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_19","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_19","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_19","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_19","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_19","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_19","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_19","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseuratmap2ref","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Map the seurat object to reference See: https://satijalab.org/seurat/articles/integration_mapping.html and https://satijalab.org/seurat/articles/multimodal_reference_mapping.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object Output outfile \u2014 The rds file of seurat object with cell type annotated.Note that the reduction name will be ref.umap for the mapping. To visualize the mapping, you should use ref.umap as the reduction name. Envs FindTransferAnchors (ns) \u2014 Arguments for FindTransferAnchors() - normalization-method (choice): Name of normalization method used. - LogNormalize: Log-normalize the data matrix - SCT: Scale data using the SCTransform method - auto: Automatically detect the normalization method. See envs.refnorm . - reference-reduction: Name of dimensional reduction to use from the reference if running the pcaproject workflow. Optionally enables reuse of precomputed reference dimensional reduction. - : See https://satijalab.org/seurat/reference/findtransferanchors . Note that the hyphen ( - ) will be transformed into . for the keys. MapQuery (ns) \u2014 Arguments for MapQuery() - reference-reduction: Name of reduction to use from the reference for neighbor finding - reduction-model: DimReduc object that contains the umap model. - refdata (type=json): Extra data to transfer from the reference to the query. - : See https://satijalab.org/seurat/reference/mapquery . Note that the hyphen ( - ) will be transformed into . for the keys. NormalizeData (ns) \u2014 Arguments for NormalizeData() - normalization-method: Normalization method. - : See https://satijalab.org/seurat/reference/normalizedata . Note that the hyphen ( - ) will be transformed into . for the keys. SCTransform (ns) \u2014 Arguments for SCTransform() - do-correct-umi (flag): Place corrected UMI matrix in assay counts layer? - do-scale (flag): Whether to scale residuals to have unit variance? - do-center (flag): Whether to center residuals to have mean zero? - : See https://satijalab.org/seurat/reference/sctransform . Note that the hyphen ( - ) will be transformed into . for the keys. cache (type=auto) \u2014 Whether to cache the information at different steps.If True , the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. The cached seurat object will be saved as <signature>.<kind>.RDS file, where <signature> is the signature determined by the input and envs of the process. See https://github.com/satijalab/seurat/issues/7849 , https://github.com/satijalab/seurat/issues/5358 and https://github.com/satijalab/seurat/issues/6748 for more details also about reproducibility issues. To not use the cached seurat object, you can either set cache to False or delete the cached file at <signature>.RDS in the cache directory. ident \u2014 The name of the ident for query transferred from envs.use of the reference. mutaters (type=json) \u2014 The mutaters to mutate the metadata.This is helpful when we want to create new columns for split_by . ncores (type=int;order=-100) \u2014 Number of cores to use.When split_by is used, this will be the number of cores for each object to map to the reference. When split_by is not used, this is used in future::plan(strategy = \"multicore\", workers = <ncores>) to parallelize some Seurat procedures. See also: https://satijalab.org/seurat/archive/v3.0/future_vignette.html plots (type=json) \u2014 The plots to generate.The keys are the names of the plots and the values are the arguments for the plot. The arguments will be passed to biopipen.utils::VizSeuratMap2Ref() to generate the plots. The plots will be saved to the output directory. See https://pwwang.github.io/biopipen.utils.R/reference/VizSeuratMap2Ref.html . ref \u2014 The reference seurat object file.Either an RDS file or a h5seurat file that can be loaded by Seurat::LoadH5Seurat() . The file type is determined by the extension. .rds or .RDS for RDS file, .h5seurat or .h5 for h5seurat file. refnorm (choice) \u2014 Normalization method the reference used. The same method will be used for the query. - LogNormalize: Using NormalizeData . - SCTransform: Using SCTransform . - SCT: Alias of SCTransform. - auto: Automatically detect the normalization method. If the default assay of reference is SCT , then SCTransform will be used. skip_if_normalized \u2014 Skip normalization if the query is already normalized.Since the object is supposed to be generated by SeuratPreparing , it is already normalized. However, a different normalization method may be used. If the reference is normalized by the same method as the query, the normalization can be skipped. Otherwise, the normalization cannot be skipped. The normalization method used for the query set is determined by the default assay. If SCT , then SCTransform is used; otherwise, NormalizeData is used. You can set this to False to force re-normalization (with or without the arguments previously used). split_by \u2014 The column name in metadata to split the query into multiple objects.This helps when the original query is too large to process. use \u2014 A column name of metadata from the reference(e.g. celltype.l1 , celltype.l2 ) to transfer to the query as the cell types (ident) for downstream analysis. This field is required. If you want to transfer multiple columns, you can use envs.MapQuery.refdata . Requires r-seurat \u2014 check: {{proc.lang}} -e \"library(Seurat)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.SeuratMap2Ref"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_20","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_20","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_20","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_20","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_20","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_20","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_20","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaradarplots","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Radar plots for cell proportion in different clusters. This process generates the radar plots for the clusters of T cells. It explores the proportion of cells in different groups (e.g. Tumor vs Blood) in different T-cell clusters. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples Let's say we have a metadata like this: Cell Source Timepoint seurat_clusters A Blood Pre 0 B Blood Pre 0 C Blood Post 1 D Blood Post 1 E Tumor Pre 2 F Tumor Pre 2 G Tumor Post 3 H Tumor Post 3 With configurations: [RadarPlots.envs] by = \"Source\" Then we will have a radar plots like this: We can use each to separate the cells into different cases: [RadarPlots.envs] by = \"Source\" each = \"Timepoint\" Then we will have two radar plots, one for Pre and one for Post : Using cluster_order to change the order of the clusters and show only the first 3 clusters: [RadarPlots.envs] by = \"Source\" cluster_order = [ \"2\" , \"0\" , \"1\" ] breaks = [ 0 , 50 , 100 ] # also change the breaks n. / Input srtobj \u2014 The seurat object in RDS or qs/qs2 format Output outdir \u2014 The output directory for the plots Envs bar_devpars (ns) \u2014 The parameters for png() for the barplot - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot breakdown \u2014 An additional column with groups to break down the cellsdistribution in each cluster. For example, if you want to see the distribution of the cells in each cluster in different samples. In this case, you should have multiple values in each by . These values won't be plotted in the radar plot, but a barplot will be generated with the mean value of each group and the error bar. breaks (list;itype=int) \u2014 breaks of the radar plots, from 0 to 100.If not given, the breaks will be calculated automatically. by \u2014 Which column to use to separate the cells in different groups. NA s will be ignored. For example, If you have a column named Source that marks the source of the cells, and you want to separate the cells into Tumor and Blood groups, you can set by to Source . The there will be two curves in the radar plot, one for Tumor and one for Blood . cases (type=json) \u2014 The cases for the multiple radar plots.Keys are the names of the cases and values are the arguments for the plots ( each , by , order , breaks , direction , ident , cluster_order and devpars ). If not cases are given, a default case will be used, with the key DEFAULT . The keys must be valid string as part of the file name. cluster_order (list) \u2014 The order of the clusters.You may also use it to filter the clusters. If not given, all clusters will be used. If the cluster names are integers, use them directly for the order, even though a prefix Cluster is added on the plot. colors \u2014 The colors for the groups in by . If not specified,the default colors will be used. Multiple colors can be separated by comma ( , ). You can specify biopipen to use the biopipen palette. devpars (ns) \u2014 The parameters for png() - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot direction (choice) \u2014 Direction to calculate the percentages. - inter-cluster: the percentage of the cells in all groups in each cluster (percentage adds up to 1 for each cluster). - intra-cluster: the percentage of the cells in all clusters. (percentage adds up to 1 for each group). each \u2014 A column with values to separate all cells in different casesWhen specified, the case will be expanded to multiple cases for each value in the column. If specified, section will be ignored, and the case name will be used as the section name. ident \u2014 The column name of the cluster information. mutaters (type=json) \u2014 Mutaters to mutate the metadata of theseurat object. Keys are the column names and values are the expressions to mutate the columns. These new columns will be used to define your cases. order (list) \u2014 The order of the values in by . You can also limit(filter) the values we have in by . For example, if column Source has values Tumor , Blood , Spleen , and you only want to plot Tumor and Blood , you can set order to [\"Tumor\", \"Blood\"] . This will also have Tumor as the first item in the legend and Blood as the second item. prefix_each (flag) \u2014 Whether to prefix the each column name to the values as thecase/section name. section \u2014 If you want to put multiple cases into a same sectionin the report, you can set this option to the name of the section. Only used in the report. subset \u2014 The subset of the cells to do the analysis. test (choice) \u2014 The test to use to calculate the p values.If there are more than 2 groups in by , the p values will be calculated pairwise group by group. Only works when breakdown is specified and by has 2 groups or more. - wilcox: Wilcoxon rank sum test - t: T test - none: No test will be performed Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.RadarPlots"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_21","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_21","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_21","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_21","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_21","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_21","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_21","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnametamarkers","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Find markers between three or more groups of cells, using one-way ANOVAor Kruskal-Wallis test. Sometimes, you may want to find the markers for cells from more than 2 groups. In this case, you can use this process to find the markers for the groups and do enrichment analysis for the markers. Each marker is examined using either one-way ANOVA or Kruskal-Wallis test. The p values are adjusted using the specified method. The significant markers are then used for enrichment analysis using enrichr api. Other than the markers and the enrichment analysis as outputs, this process also generates violin plots for the top 10 markers. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input srtobj \u2014 The seurat object loaded by SeuratPreparing Output outdir \u2014 The output directory for the markers Envs cases (type=json) \u2014 If you have multiple cases, you can specify themhere. The keys are the names of the cases and the values are the above options except ncores and mutaters . If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the default values under envs with the name DEFAULT . dbs (list) \u2014 The dbs to do enrichment analysis for significantmarkers See below for all libraries. https://maayanlab.cloud/Enrichr/#libraries each \u2014 The column name in metadata to separate the cells into different cases. group-by \u2014 The column name in metadata to group the cells.If only group-by is specified, and idents are not specified, markers will be found for all groups in this column. NA group will be ignored. idents \u2014 The groups of cells to compare, values should be in the group-by column. method (choice) \u2014 The method for the test. - anova: One-way ANOVA - kruskal: Kruskal-Wallis test mutaters (type=json) \u2014 The mutaters to mutate the metadataThe key-value pairs will be passed the dplyr::mutate() to mutate the metadata. ncores (type=int) \u2014 Number of cores to use to parallelize for genes p_adjust (choice) \u2014 The method to adjust the p values, which can be used to filter the significant markers.See also https://rdrr.io/r/stats/p.adjust.html - holm: Holm-Bonferroni method - hochberg: Hochberg method - hommel: Hommel method - bonferroni: Bonferroni method - BH: Benjamini-Hochberg method - BY: Benjamini-Yekutieli method - fdr: FDR method of Benjamini-Hochberg - none: No adjustment prefix_each (flag) \u2014 Whether to add the each value as prefix to the case name. section \u2014 The section name for the report.Worked only when each is not specified. Otherwise, the section name will be constructed from each and group-by . If DEFAULT , and it's the only section, it not included in the case/section names. sigmarkers \u2014 An expression passed to dplyr::filter() to filter thesignificant markers for enrichment analysis. The default is p.value < 0.05 . If method = 'anova' , the variables that can be used for filtering are: sumsq , meansq , statistic , p.value and p_adjust . If method = 'kruskal' , the variables that can be used for filtering are: statistic , p.value and p_adjust . subset \u2014 The subset of the cells to do the analysis.An expression passed to dplyr::filter() . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.MetaMarkers"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_22","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_22","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_22","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_22","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_22","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_22","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_22","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaseurat2anndata","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert seurat object to AnnData Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file, in RDS or qs/qs2 format Output outfile \u2014 The AnnData file Envs assay \u2014 The assay to use for AnnData.If not specified, the default assay will be used. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.Seurat2AnnData"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_23","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_23","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_23","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_23","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_23","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_23","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_23","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaanndata2seurat","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert AnnData to seurat object Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input adfile \u2014 The AnnData .h5ad file Output outfile \u2014 The seurat object file in RDS or qs/qs2 format Envs assay \u2014 The assay to use to convert to seurat object. dotplot_check (type=auto) \u2014 Whether to do a check with a dot plot.( scplotter::FeatureStatPlot(plot_type = \"dot\", ..) will be used) to see if the conversion is successful. Set to False to disable the check. If True , top 10 variable genes will be used for the check. You can give a list of genes or a string of genes with comma ( , ) separated to use for the check. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.AnnData2Seurat"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_24","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_24","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_24","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_24","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_24","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_24","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_24","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnascsimulation","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Simulate single-cell data using splatter. See https://www.bioconductor.org/packages/devel/bioc/vignettes/splatter/inst/doc/splatter.html#2_Quickstart Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input seed \u2014 The seed for the simulationYou could also use string as the seed, and the seed will be generated by digest::digest2int() . So this could also work as a unique identifier for the simulation (ie. Sample ID). Output outfile \u2014 The output Seurat object/SingleCellExperiment in qs/qs2 format Envs method (choice) \u2014 which simulation method to use. Options are: - single: produces a single population - groups: produces distinct groups (eg. cell types), or - paths: selects cells from continuous trajectories (eg. differentiation processes) ncells (type=int) \u2014 The number of cells to simulate ngenes (type=int) \u2014 The number of genes to simulate nspikes (type=int) \u2014 The number of spike-ins to simulateWhen ngenes , ncells , and nspikes are not specified, the default params from mockSCE() will be used. By default, ngenes = 2000 , ncells = 200 , and nspikes = 100 . outtype (choice) \u2014 The output file type. - seurat: Seurat object - singlecellexperiment: SingleCellExperiment object - sce: alias for singlecellexperiment params (ns) \u2014 Other parameters for simulation.The parameters are initialized splitEstimate(mockSCE()) and then updated with the given parameters. See https://rdrr.io/bioc/splatter/man/SplatParams.html . Hyphens ( - ) will be transformed into dots ( . ) for the keys. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.ScSimulation"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_25","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_25","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_25","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_25","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_25","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_25","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_25","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnacellcellcommunication","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Cell-cell communication inference This is implemented based on LIANA , which is a Python package for cell-cell communication inference and provides a list of existing methods including CellPhoneDB , Connectome , log2FC, NATMI , SingleCellSignalR , Rank_Aggregate, Geometric Mean, scSeqComm , and CellChat . You can also try python -c 'import liana; liana.mt.show_methods()' to see the methods available. Note that this process does not do any visualization. You can use CellCellCommunicationPlots to visualize the results. Reference: - Review . - LIANA . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file in RDS or h5seurat format or AnnData file. Output outfile \u2014 The output file with the 'liana_res' data frame.Stats are provided for both ligand and receptor entities, more specifically: ligand and receptor are the two entities that potentially interact. As a reminder, CCC events are not limited to secreted signalling, but we refer to them as ligand and receptor for simplicity. Also, in the case of heteromeric complexes, the ligand and receptor columns represent the subunit with minimum expression, while * complex corresponds to the actual complex, with subunits being separated by . source and target columns represent the source/sender and target/receiver cell identity for each interaction, respectively * *_props : represents the proportion of cells that express the entity. By default, any interactions in which either entity is not expressed in above 10%% of cells per cell type is considered as a false positive, under the assumption that since CCC occurs between cell types, a sufficient proportion of cells within should express the genes. * *_means : entity expression mean per cell type. * lr_means : mean ligand-receptor expression, as a measure of ligand-receptor interaction magnitude. * cellphone_pvals : permutation-based p-values, as a measure of interaction specificity. Envs \u2014 Other arguments for the method.The arguments are passed to the method directly. See the method documentation for more details and also help(liana.mt.<method>.__call__) in Python. assay \u2014 The assay to use for the analysis.Only works for Seurat object. expr_prop (type=float) \u2014 Minimum expression proportion for the ligands andreceptors (+ their subunits) in the corresponding cell identities. Set to 0 to return unfiltered results. groupby \u2014 The column name in metadata to group the cells.Typically, this column should be the cluster id. method (choice) \u2014 The method to use for cell-cell communication inference. - CellPhoneDB: Use CellPhoneDB method. Magnitude Score: lr_means; Specificity Score: cellphone_pvals. - Connectome: Use Connectome method. - log2FC: Use log2FC method. - NATMI: Use NATMI method. - SingleCellSignalR: Use SingleCellSignalR method. - Rank_Aggregate: Use Rank_Aggregate method. - Geometric_Mean: Use Geometric Mean method. - scSeqComm: Use scSeqComm method. - CellChat: Use CellChat method. - cellphonedb: alias for CellPhoneDB - connectome: alias for Connectome - log2fc: alias for log2FC - natmi: alias for NATMI - singlesignaler: alias for SingleCellSignalR - rank_aggregate: alias for Rank_Aggregate - geometric_mean: alias for Geometric_Mean - scseqcomm: alias for scSeqComm - cellchat: alias for CellChat min_cells (type=int) \u2014 Minimum cells (per cell identity if grouped by groupby )to be considered for downstream analysis. n_perms (type=int) \u2014 Number of permutations for the permutation test.Relevant only for permutation-based methods (e.g., CellPhoneDB ). If 0 is passed, no permutation testing is performed. ncores (type=int) \u2014 The number of cores to use. rscript \u2014 The path to the Rscript executable used to convert RDS file to AnnData.if in.sobjfile is an RDS file, it will be converted to AnnData file (h5ad). You need Seurat , SeuratDisk and digest installed. seed (type=int) \u2014 The seed for the random number generator. species (choice) \u2014 The species of the cells. - human: Human cells, the 'consensus' resource will be used. - mouse: Mouse cells, the 'mouseconsensus' resource will be used. split_by \u2014 The column name in metadata to split the cells to run the method separately.The results will be combined together with this column in the final output. subset \u2014 An expression in string to subset the cells.When a .rds or .h5seurat file is provided for in.sobjfile , you can provide an expression in R , which will be passed to base::subset() in R to subset the cells. But you can always pass an expression in python to subset the cells. See https://anndata.readthedocs.io/en/latest/tutorials/notebooks/getting-started.html#subsetting-using-metadata . You should use adata to refer to the AnnData object. For example, adata.obs.groups == \"g1\" will subset the cells with groups equal to g1 . subset_using \u2014 The method to subset the cells. - auto: Automatically detect the method to use. Note that this is not always accurate. We simply check if [ is in the expression. If so, we use python to subset the cells; otherwise, we use R . - python: Use python to subset the cells. - r: Use R to subset the cells. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.CellCellCommunication"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_26","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_26","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_26","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_26","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_26","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_26","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_26","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnacellcellcommunicationplots","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Visualization for cell-cell communication inference. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cccfile \u2014 The output file from CellCellCommunication Output outdir \u2014 The output directory for the plots. Envs \u2014 Other arguments passed to scplotter::CCCPlot cases (type=json) \u2014 The cases for the plots.The keys are the names of the cases and the values are the arguments for the plots. The arguments include the ones inherited from envs . You can have a special plot_type \"table\" to generate a table for the ccc data to save as a text file and show in the report. If no cases are given, a default case will be used, with the key Cell-Cell Communication . descr \u2014 The description of the plot. devpars (ns) \u2014 The parameters for the plot. - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot magnitude \u2014 The column name in the data to use as the magnitude of thecommunication. By default, the second last column will be used. See li.mt.show_methods() for the available methods in LIANA. or https://liana-py.readthedocs.io/en/latest/notebooks/basic_usage.html#Tileplot more_formats (type=list) \u2014 The additional formats to save the plots. specificity \u2014 The column name in the data to use as the specificity of the communication.By default, the last column will be used. If the method doesn't have a specificity, set it to None. subset \u2014 An expression to pass to dplyr::filter() to subset the ccc data. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.CellCellCommunicationPlots"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_27","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_27","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_27","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_27","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_27","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_27","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_27","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnascvelo","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Velocity analysis for single-cell RNA-seq data This process is implemented based on the Python package scvelo (v0.3.3). Note that it doesn't work with numpy>=2 . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file in RDS or h5seurat format or AnnData file. Output outfile \u2014 The output object with the velocity embeddings and information.In either RDS, h5seurat or h5ad format, depending on the envs.outtype . There will be also plots generated in the output directory (parent directory of outfile ). Note that these plots will not be used in the report, but can be used as supplementary information for the velocity analysis. To visualize the velocity embeddings, you can use the SeuratClusterStats process with v_reduction provided to one of the envs.dimplots . Envs calculate_velocity_genes (flag) \u2014 Whether to calculate the velocity genes. denoise (flag) \u2014 Whether to denoise the data. denoise_topn (type=int) \u2014 Number of genes with highest likelihood selected toinfer velocity directions. fitting_by (choice) \u2014 The mode to use for fitting the velocities. - stochastic: Stochastic mode - deterministic: Deterministic mode group_by \u2014 The column name in metadata to group the cells.Typically, this column should be the cluster id. kinetics (flag) \u2014 Whether to compute the RNA velocity kinetics. kinetics_topn (type=int) \u2014 Number of genes with highest likelihood selected toinfer velocity directions. min_shared_counts (type=int) \u2014 Minimum number of counts(both unspliced and spliced) required for a gene. mode (type=list) \u2014 The mode to use for the velocity analysis.It should be a subset of ['deterministic', 'stochastic', 'dynamical'] , meaning that we can perform the velocity analysis in multiple modes. n_neighbors (type=int) \u2014 The number of neighbors to use for the velocity graph. n_pcs (type=int) \u2014 The number of PCs to use for the velocity graph. ncores (type=int) \u2014 Number of cores to use. outtype (choice) \u2014 The output file type. - : The same as the input file type. - h5seurat: h5seurat file - h5ad: h5ad file - qs: qs/qs2 file - qs2: qs2 file - rds: RDS file rscript \u2014 The path to the Rscript executable used to convert RDS file to AnnData.if in.sobjfile is an RDS file, it will be converted to AnnData file (h5ad). You need Seurat , SeuratDisk and digest installed. top_n (type=int) \u2014 The number of top features to plot. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.ScVelo"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_28","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_28","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_28","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_28","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_28","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_28","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_28","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaslingshot","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Trajectory inference using Slingshot This process is implemented based on the R package slingshot . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file in RDS or qs format. Output outfile \u2014 The output object with the trajectory information.The lineages are stored in the metadata of the seurat object at columns LineageX , where X is the lineage number. The BranchID column contains the branch id for each cell. One can use scplotter::CellDimPlot(object, lineages = c(\"Lineage1\", \"Lineage2\", ...)) to visualize the trajectories. Envs align_start (flag) \u2014 Whether to align the starting pseudotime values at the maximum pseudotime. dims (type=auto) \u2014 The dimensions to use for the analysis.A list or a string with comma separated values. Consecutive numbers can be specified with a colon ( : ) or a dash ( - ). end \u2014 The ending group for the Slingshot analysis. group_by \u2014 The column name in metadata to group the cells.Typically, this column should be the cluster id. prefix \u2014 The prefix to add to the column names of the resulting pseudotime variable. reduction \u2014 The nonlinear reduction to use for the trajectory analysis. reverse (flag) \u2014 Logical value indicating whether to reverse the pseudotime variable. seed (type=int) \u2014 The seed for the random number generator. start \u2014 The starting group for the Slingshot analysis. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.Slingshot"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_29","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_29","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_29","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_29","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_29","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_29","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_29","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnaloomto10x","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert Loom file to 10X format Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input loomfile \u2014 The Loom file Output outdir \u2014 The output directory for the 10X format files,including the matrix.mtx.gz , barcodes.tsv.gz and features.tsv.gz files. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.LoomTo10X"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_30","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_30","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_30","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_30","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_30","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_30","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_30","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna/#biopipennsscrnapseudobulkdeg","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Pseduo-bulk differential gene expression analysis This process performs differential gene expression analysis, instead of on single-cell level, on the pseudo-bulk data, aggregated from the single-cell data. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The seurat object file in RDS or qs/qs2 format. Output outdir \u2014 The output containing the results of the differential gene expressionanalysis. Envs aggregate_by \u2014 The column names in metadata to aggregate the cells. allenrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from allenrich_plots_defaults . The cases under envs.cases can inherit this options. allenrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . allmarker_plots (type=json) \u2014 All marker plot cases.The keys are the names of the cases and the values are the dicts inherited from allmarker_plots_defaults . allmarker_plots_defaults (ns) \u2014 Default options for the plots for all markers when ident-1 is not specified. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html . Available types are violin , box , bar , ridge , dim , heatmap and dot . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - order_by: an expression to order the markers, passed by dplyr::arrange() . - genes: The number of top genes to show or an expression passed to dplyr::filter() to filter the genes. - : Other arguments passed to scplotter::FeatureStatPlot() . assay \u2014 The assay to pull and aggregate the data. cache (type=auto) \u2014 Where to cache the results.If True , cache to outdir of the job. If False , don't cache. Otherwise, specify the directory to cache to. cases (type=json) \u2014 The cases for the analysis.The keys are the names of the cases and the values are the arguments for the analysis. The arguments include the ones inherited from envs . If no cases are specified, a default case will be added with the name DEG Analysis and the default values specified above. dbs (list) \u2014 The databases to use for enrichment analysis.The databases are passed to biopipen.utils::Enrichr() to do the enrichment analysis. The default databases are KEGG_2021_Human and MSigDB_Hallmark_2020 . See https://maayanlab.cloud/Enrichr/#libraries for the available libraries. each \u2014 The column name in metadata to separate the cells into different cases.When specified, the case will be expanded to multiple cases for each value in the column. enrich_plots (type=json) \u2014 Cases of the plots to generate for the enrichment analysis.The keys are the names of the cases and the values are the dicts inherited from enrich_plots_defaults . The cases under envs.cases can inherit this options. enrich_plots_defaults (ns) \u2014 Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html . Available types are bar , dot , lollipop , network , enrichmap and wordcloud . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : See https://pwwang.github.io/scplotter/reference/EnrichmentPlot.htmll . enrich_style (choice) \u2014 The style of the enrichment analysis. - enrichr: Use enrichr -style for the enrichment analysis. - clusterProfiler: Use clusterProfiler -style for the enrichment analysis. error (flag) \u2014 Error out if no/not enough markers are found or no pathways are enriched.If False , empty results will be returned. group_by \u2014 The column name in metadata to group the cells. ident_1 \u2014 The first identity to compare. ident_2 \u2014 The second identity to compare.If not specified, the rest of the identities will be compared with ident_1 . layer \u2014 The layer to pull and aggregate the data. marker_plots (type=json) \u2014 Cases of the plots to generate for the markers.Plot cases. The keys are the names of the cases and the values are the dicts inherited from marker_plots_defaults . The cases under envs.cases can inherit this options. marker_plots_defaults (ns) \u2014 Default options for the plots to generate for the markers. - plot_type: The type of the plot. See https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html . Available types are violin , box , bar , ridge , dim , heatmap and dot . There are two additional types available - volcano_pct and volcano_log2fc . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - order_by: an expression to order the markers, passed by dplyr::arrange() . - genes: The number of top genes to show or an expression passed to dplyr::filter() to filter the genes. - : Other arguments passed to scplotter::FeatureStatPlot() . If plot_type is volcano_pct or volcano_log2fc , they will be passed to scplotter::VolcanoPlot() . mutaters (type=json) \u2014 Mutaters to mutate the metadata of theseurat object. Keys are the new column names and values are the expressions to mutate the columns. These new columns can be used to define your cases. You can also use the clone selectors to select the TCR clones/clusters. See https://pwwang.github.io/scplotter/reference/clone_selectors.html . ncores (type=int) \u2014 Number of cores to use for parallelization. overlaps (type=json) \u2014 Cases for investigating the overlapping of significant markers between different cases or comparisons.The keys are the names of the cases and the values are the dicts inherited from overlaps_defaults . There are two situations that we can perform overlaps: 1. If ident-1 is not specified, the overlaps can be performed between different comparisons. 2. If each is specified, the overlaps can be performed between different cases, where in each case, ident-1 must be specified. overlaps_defaults (ns) \u2014 Default options for investigating the overlapping of significant markers between different cases or comparisons.This means either ident-1 should be empty, so that they can be expanded to multiple comparisons. - sigmarkers: The expression to filter the significant markers for each case. If not provided, envs.sigmarkers will be used. - plot_type (choice): The type of the plot to generate for the overlaps. - venn: Use plotthis::VennDiagram() . - upset: Use plotthis::UpsetPlot() . - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - : More arguments pased to plotthis::VennDiagram() ( https://pwwang.github.io/plotthis/reference/venndiagram1.html ) or plotthis::UpsetPlot() ( https://pwwang.github.io/plotthis/reference/upsetplot1.html ) paired_by \u2014 The column name in metadata to mark the paired samples.For example, subject. If specified, the paired test will be performed. plots (type=json) \u2014 The parameters for the plots.The keys are the names of the plots and the values are the parameters for the plots. The parameters will override the defaults in plots_defaults . If not specified, no plots will be generated. plots_defaults (ns) \u2014 The default parameters for the plots. - : Parameters passed to biopipen.utils::VizBulkDEGs() . See: https://pwwang.github.io/biopipen.utils.R/reference/VizBulkDEGs.html sigmarkers \u2014 An expression passed to dplyr::filter() to filter thesignificant markers for enrichment analysis. The default is p_val_adj < 0.05 . If tool = 'DESeq2' , the variables that can be used for filtering are: baseMean , log2FC , lfcSE , stat , p_val , p_val_adj . If tool = 'edgeR' , the variables that can be used for filtering are: logCPM , log2FC , LR , p_val , p_val_adj . subset \u2014 An expression in string to subset the cells. tool (choice) \u2014 The method to use for the differential expression analysis. - DESeq2: Use DESeq2 for the analysis. - edgeR: Use edgeR for the analysis. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna.PseudoBulkDEG"},{"location":"api/biopipen.ns.scrna/#pipenprocprocmeta_31","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna/#pipenprocprocfrom_proc_31","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_subclass_31","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna/#pipenprocprocinit_31","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna/#pipenprocprocgc_31","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna/#pipenprocproclog_31","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna/#pipenprocprocrun_31","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/","text":"module biopipen.ns . scrna_metabolic_landscape </> Metabolic landscape analysis for scRNA-seq data Classes MetabolicPathwayActivity ( Proc ) \u2014 This process calculates the pathway activities in different groups and subsets. </> MetabolicFeatures ( Proc ) \u2014 This process performs enrichment analysis for the metabolic pathwaysfor each group in each subset. </> MetabolicPathwayHeterogeneity ( Proc ) \u2014 Calculate Metabolic Pathway heterogeneity. </> ScrnaMetabolicLandscape \u2014 Metabolic landscape analysis for scRNA-seq data </> class biopipen.ns.scrna_metabolic_landscape . MetabolicPathwayActivity ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc This process calculates the pathway activities in different groups and subsets. The cells are first grouped by subsets and then the metabolic activities are examined for each groups in different subsets. For each subset, a heatmap and a violin plot will be generated. The heatmap shows the pathway activities for each group and each metabolic pathway {: width=\"80%\"} The violin plot shows the distribution of the pathway activities for each group {: width=\"45%\"} Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The Seurat object file.It should be loaded as a Seurat object Output outdir \u2014 The output directory.It will contain the pathway activity score files and plots. Envs cases (type=json) \u2014 Multiple cases for the analysis.If you only have one case, you can specify the parameters directly to envs.ntimes , envs.subset_by , envs.group_by , envs.group1 , envs.group2 , and envs.plots . The name of the case will be envs.subset_by . If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from envs.ntimes , envs.subset_by , envs.group_by , envs.group1 , envs.group2 , and envs.plots . gmtfile (pgarg) \u2014 The GMT file with the metabolic pathways.Defaults to ScrnaMetabolicLandscape.gmtfile group_by (pgarg;readonly) \u2014 Group the data by the given column in themetadata. For example, cluster . Defaults to ScrnaMetabolicLandscape.group_by ncores (type=int;pgarg) \u2014 Number of cores to use for parallelizationDefaults to ScrnaMetabolicLandscape.ncores ntimes (type=int) \u2014 Number of permutations to estimate the p-values plots (type=json) \u2014 The plots to generate.Names will be used as the prefix for the output files. Values will be a dictionary with the following keys: * plot_type is the type of plot to generate. One of heatmap , box , violin or merged_heatmap (all subsets in one plot). * devpars is a dictionary with the device parameters for the plot. * Other arguments for plotthis::Heatmap() , plotthis::BoxPlot() or plotthis::ViolinPlot() , depending on the plot_type . subset_by (pgarg;readonly) \u2014 Subset the data by the given column in themetadata. For example, Response . NA values will be removed in this column. Defaults to ScrnaMetabolicLandscape.subset_by If None, the data will not be subsetted. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna_metabolic_landscape . MetabolicFeatures ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc This process performs enrichment analysis for the metabolic pathwaysfor each group in each subset. The enrichment analysis is done with fgsea package or the GSEA_R package. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The Seurat object file in rds.It should be loaded as a Seurat object Output outdir \u2014 The output directory.It will contain the GSEA results and plots. Envs cases (type=json) \u2014 Multiple cases for the analysis.If you only have one case, you can specify the parameters directly to envs.prerank_method , envs.subset_by , envs.group_by , envs.comparisons , envs.fgsea_args and envs.plots . The name of this default case will be envs.subset_by . If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from envs.prerank_method , envs.subset_by , envs.group_by , envs.comparisons , envs.fgsea_args and envs.plots . comparisons (type=list) \u2014 The comparison groups to use for the analysis.If not provided, each group in the group_by column will be used to compare with the other groups. If a single group is provided as an element, it will be used to compare with all the other groups. For example, if we have group_by = \"cluster\" and we have 1 , 2 and 3 in the group_by column, we could have comparisons = [\"1\", \"2\"] , which will compare the group 1 with groups 2 and 3 , and the group 2 with groups 1 and 3 . We could also have comparisons = [\"1:2\", \"1:3\"] , which will compare the group 1 with group 2 and group 1 with group 3 . fgsea_args (type=json) \u2014 Other arguments for the fgsea::fgsea() function.For example, {\"minSize\": 15, \"maxSize\": 500} . See https://rdrr.io/bioc/fgsea/man/fgsea.html for more details. gmtfile (pgarg) \u2014 The GMT file with the metabolic pathways.Defaults to ScrnaMetabolicLandscape.gmtfile group_by (pgarg;readonly) \u2014 Group the data by the given column in themetadata. For example, cluster . Defaults to ScrnaMetabolicLandscape.group_by ncores (type=int;pgarg) \u2014 Number of cores to use for parallelization forthe comparisons for each subset and group. Defaults to ScrnaMetabolicLandscape.ncores . plots (type=json) \u2014 The plots to generate.Names will be used as the title for the plot. Values will be the arguments passed to biopipen.utils::VizGSEA() function. See https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html . A key level is supported to specify the level of the plot. Possible values are case , which includes all subsets and groups in the case; subset , which includes all groups in the subset; otherwise, it will plot for the groups. For case / subset level plots, current plot_type only \"dot\" is supported for now, then the values will be passed to plotthis::DotPlot() prerank_method (choice) \u2014 Method to use for gene preranking.Signal to noise: the larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \u201cclass marker.\u201d. Absolute signal to noise: the absolute value of the signal to noise. T test: Uses the difference of means scaled by the standard deviation and number of samples. Ratio of classes: Uses the ratio of class means to calculate fold change for natural scale data. Diff of classes: Uses the difference of class means to calculate fold change for nature scale data Log2 ratio of classes: Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. - signal_to_noise: Signal to noise - s2n: Alias of signal_to_noise - abs_signal_to_noise: absolute signal to noise - abs_s2n: Alias of abs_signal_to_noise - t_test: T test - ratio_of_classes: Also referred to as fold change - diff_of_classes: Difference of class means - log2_ratio_of_classes: Log2 ratio of class means subset_by (pgarg;readonly) \u2014 Subset the data by the given column in themetadata. For example, Response . NA values will be removed in this column. Defaults to ScrnaMetabolicLandscape.subset_by If None, the data will not be subsetted. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna_metabolic_landscape . MetabolicPathwayHeterogeneity ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate Metabolic Pathway heterogeneity. For each subset, the normalized enrichment score (NES) of each metabolic pathway is calculated for each group. The NES is calculated by comparing the enrichment score of the subset to the enrichment scores of the same subset in the permutations. The p-value is calculated by comparing the NES to the NESs of the same subset in the permutations. The heterogeneity can be reflected by the NES values and the p-values in different groups for the metabolic pathways. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Envs cases (type=json) \u2014 Multiple cases for the analysis.If you only have one case, you can specify the parameters directly to envs.subset_by , envs.group_by , envs.fgsea_args , envs.plots , envs.select_pcs , and envs.pathway_pval_cutoff . The name of this default case will be envs.subset_by . If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from envs.subset_by , envs.group_by , envs.fgsea_args , envs.plots , envs.select_pcs , and envs.pathway_pval_cutoff . fgsea_args (type=json) \u2014 Other arguments for the fgsea::fgsea() function.For example, {\"minSize\": 15, \"maxSize\": 500} . See https://rdrr.io/bioc/fgsea/man/fgsea.html for more details. gmtfile (pgarg) \u2014 The GMT file with the metabolic pathways.Defaults to ScrnaMetabolicLandscape.gmtfile group_by (pgarg;readonly) \u2014 Group the data by the given column in themetadata. For example, cluster . Defaults to ScrnaMetabolicLandscape.group_by ncores (type=int;pgarg) \u2014 Number of cores to use for parallelizationDefaults to ScrnaMetabolicLandscape.ncores pathway_pval_cutoff (type=float) \u2014 The p-value cutoff to selectthe enriched pathways plots (type=json) \u2014 The plots to generate.Names will be used as the title for the plot. Values will be the arguments passed to biopipen.utils::VizGSEA() function. See https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html . select_pcs (type=float) \u2014 Select the PCs to use for the analysis. subset_by (pgarg;readonly) \u2014 Subset the data by the given column in themetadata. For example, Response . NA values will be removed in this column. Defaults to ScrnaMetabolicLandscape.subset_by If None, the data will not be subsetted. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.scrna_metabolic_landscape . ScrnaMetabolicLandscape ( *args , **kwds ) </> Bases pipen_args.procgroup.ProcGroup pipen.procgroup.ProcGroup Metabolic landscape analysis for scRNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape See docs here for more details https://pwwang.github.io/biopipen/pipelines/scrna_metabolic_landscape Reference: Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Attributes parser \u2014 Pass arguments to initialize the parser The parser is a singleton and by default initalized at plugin.on_init() hook, which happens usually after the initialization of a process group. </> Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) (Pipen) \u2014 Convert the pipeline to a Pipen instance </> post_init ( ) \u2014 Load runtime processes </> class pipen.procgroup. ProcGropuMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod __init_subclass__ ( ) </> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod add_proc ( self_or_method , proc=None ) </> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method as_pipen ( name=None , desc=None , outdir=None , **kwargs ) </> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | os.pathlike | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns (Pipen) The Pipen instance method post_init ( ) </> Load runtime processes","title":"biopipen.ns.scrna_metabolic_landscape"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#biopipennsscrna_metabolic_landscape","text":"</> Metabolic landscape analysis for scRNA-seq data Classes MetabolicPathwayActivity ( Proc ) \u2014 This process calculates the pathway activities in different groups and subsets. </> MetabolicFeatures ( Proc ) \u2014 This process performs enrichment analysis for the metabolic pathwaysfor each group in each subset. </> MetabolicPathwayHeterogeneity ( Proc ) \u2014 Calculate Metabolic Pathway heterogeneity. </> ScrnaMetabolicLandscape \u2014 Metabolic landscape analysis for scRNA-seq data </> class","title":"biopipen.ns.scrna_metabolic_landscape"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#biopipennsscrna_metabolic_landscapemetabolicpathwayactivity","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc This process calculates the pathway activities in different groups and subsets. The cells are first grouped by subsets and then the metabolic activities are examined for each groups in different subsets. For each subset, a heatmap and a violin plot will be generated. The heatmap shows the pathway activities for each group and each metabolic pathway {: width=\"80%\"} The violin plot shows the distribution of the pathway activities for each group {: width=\"45%\"} Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The Seurat object file.It should be loaded as a Seurat object Output outdir \u2014 The output directory.It will contain the pathway activity score files and plots. Envs cases (type=json) \u2014 Multiple cases for the analysis.If you only have one case, you can specify the parameters directly to envs.ntimes , envs.subset_by , envs.group_by , envs.group1 , envs.group2 , and envs.plots . The name of the case will be envs.subset_by . If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from envs.ntimes , envs.subset_by , envs.group_by , envs.group1 , envs.group2 , and envs.plots . gmtfile (pgarg) \u2014 The GMT file with the metabolic pathways.Defaults to ScrnaMetabolicLandscape.gmtfile group_by (pgarg;readonly) \u2014 Group the data by the given column in themetadata. For example, cluster . Defaults to ScrnaMetabolicLandscape.group_by ncores (type=int;pgarg) \u2014 Number of cores to use for parallelizationDefaults to ScrnaMetabolicLandscape.ncores ntimes (type=int) \u2014 Number of permutations to estimate the p-values plots (type=json) \u2014 The plots to generate.Names will be used as the prefix for the output files. Values will be a dictionary with the following keys: * plot_type is the type of plot to generate. One of heatmap , box , violin or merged_heatmap (all subsets in one plot). * devpars is a dictionary with the device parameters for the plot. * Other arguments for plotthis::Heatmap() , plotthis::BoxPlot() or plotthis::ViolinPlot() , depending on the plot_type . subset_by (pgarg;readonly) \u2014 Subset the data by the given column in themetadata. For example, Response . NA values will be removed in this column. Defaults to ScrnaMetabolicLandscape.subset_by If None, the data will not be subsetted. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna_metabolic_landscape.MetabolicPathwayActivity"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#biopipennsscrna_metabolic_landscapemetabolicfeatures","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc This process performs enrichment analysis for the metabolic pathwaysfor each group in each subset. The enrichment analysis is done with fgsea package or the GSEA_R package. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input sobjfile \u2014 The Seurat object file in rds.It should be loaded as a Seurat object Output outdir \u2014 The output directory.It will contain the GSEA results and plots. Envs cases (type=json) \u2014 Multiple cases for the analysis.If you only have one case, you can specify the parameters directly to envs.prerank_method , envs.subset_by , envs.group_by , envs.comparisons , envs.fgsea_args and envs.plots . The name of this default case will be envs.subset_by . If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from envs.prerank_method , envs.subset_by , envs.group_by , envs.comparisons , envs.fgsea_args and envs.plots . comparisons (type=list) \u2014 The comparison groups to use for the analysis.If not provided, each group in the group_by column will be used to compare with the other groups. If a single group is provided as an element, it will be used to compare with all the other groups. For example, if we have group_by = \"cluster\" and we have 1 , 2 and 3 in the group_by column, we could have comparisons = [\"1\", \"2\"] , which will compare the group 1 with groups 2 and 3 , and the group 2 with groups 1 and 3 . We could also have comparisons = [\"1:2\", \"1:3\"] , which will compare the group 1 with group 2 and group 1 with group 3 . fgsea_args (type=json) \u2014 Other arguments for the fgsea::fgsea() function.For example, {\"minSize\": 15, \"maxSize\": 500} . See https://rdrr.io/bioc/fgsea/man/fgsea.html for more details. gmtfile (pgarg) \u2014 The GMT file with the metabolic pathways.Defaults to ScrnaMetabolicLandscape.gmtfile group_by (pgarg;readonly) \u2014 Group the data by the given column in themetadata. For example, cluster . Defaults to ScrnaMetabolicLandscape.group_by ncores (type=int;pgarg) \u2014 Number of cores to use for parallelization forthe comparisons for each subset and group. Defaults to ScrnaMetabolicLandscape.ncores . plots (type=json) \u2014 The plots to generate.Names will be used as the title for the plot. Values will be the arguments passed to biopipen.utils::VizGSEA() function. See https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html . A key level is supported to specify the level of the plot. Possible values are case , which includes all subsets and groups in the case; subset , which includes all groups in the subset; otherwise, it will plot for the groups. For case / subset level plots, current plot_type only \"dot\" is supported for now, then the values will be passed to plotthis::DotPlot() prerank_method (choice) \u2014 Method to use for gene preranking.Signal to noise: the larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \u201cclass marker.\u201d. Absolute signal to noise: the absolute value of the signal to noise. T test: Uses the difference of means scaled by the standard deviation and number of samples. Ratio of classes: Uses the ratio of class means to calculate fold change for natural scale data. Diff of classes: Uses the difference of class means to calculate fold change for nature scale data Log2 ratio of classes: Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. - signal_to_noise: Signal to noise - s2n: Alias of signal_to_noise - abs_signal_to_noise: absolute signal to noise - abs_s2n: Alias of abs_signal_to_noise - t_test: T test - ratio_of_classes: Also referred to as fold change - diff_of_classes: Difference of class means - log2_ratio_of_classes: Log2 ratio of class means subset_by (pgarg;readonly) \u2014 Subset the data by the given column in themetadata. For example, Response . NA values will be removed in this column. Defaults to ScrnaMetabolicLandscape.subset_by If None, the data will not be subsetted. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna_metabolic_landscape.MetabolicFeatures"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#biopipennsscrna_metabolic_landscapemetabolicpathwayheterogeneity","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate Metabolic Pathway heterogeneity. For each subset, the normalized enrichment score (NES) of each metabolic pathway is calculated for each group. The NES is calculated by comparing the enrichment score of the subset to the enrichment scores of the same subset in the permutations. The p-value is calculated by comparing the NES to the NESs of the same subset in the permutations. The heterogeneity can be reflected by the NES values and the p-values in different groups for the metabolic pathways. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Envs cases (type=json) \u2014 Multiple cases for the analysis.If you only have one case, you can specify the parameters directly to envs.subset_by , envs.group_by , envs.fgsea_args , envs.plots , envs.select_pcs , and envs.pathway_pval_cutoff . The name of this default case will be envs.subset_by . If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from envs.subset_by , envs.group_by , envs.fgsea_args , envs.plots , envs.select_pcs , and envs.pathway_pval_cutoff . fgsea_args (type=json) \u2014 Other arguments for the fgsea::fgsea() function.For example, {\"minSize\": 15, \"maxSize\": 500} . See https://rdrr.io/bioc/fgsea/man/fgsea.html for more details. gmtfile (pgarg) \u2014 The GMT file with the metabolic pathways.Defaults to ScrnaMetabolicLandscape.gmtfile group_by (pgarg;readonly) \u2014 Group the data by the given column in themetadata. For example, cluster . Defaults to ScrnaMetabolicLandscape.group_by ncores (type=int;pgarg) \u2014 Number of cores to use for parallelizationDefaults to ScrnaMetabolicLandscape.ncores pathway_pval_cutoff (type=float) \u2014 The p-value cutoff to selectthe enriched pathways plots (type=json) \u2014 The plots to generate.Names will be used as the title for the plot. Values will be the arguments passed to biopipen.utils::VizGSEA() function. See https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html . select_pcs (type=float) \u2014 Select the PCs to use for the analysis. subset_by (pgarg;readonly) \u2014 Subset the data by the given column in themetadata. For example, Response . NA values will be removed in this column. Defaults to ScrnaMetabolicLandscape.subset_by If None, the data will not be subsetted. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.scrna_metabolic_landscape.MetabolicPathwayHeterogeneity"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#biopipennsscrna_metabolic_landscapescrnametaboliclandscape","text":"</> Bases pipen_args.procgroup.ProcGroup pipen.procgroup.ProcGroup Metabolic landscape analysis for scRNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape See docs here for more details https://pwwang.github.io/biopipen/pipelines/scrna_metabolic_landscape Reference: Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Attributes parser \u2014 Pass arguments to initialize the parser The parser is a singleton and by default initalized at plugin.on_init() hook, which happens usually after the initialization of a process group. </> Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) (Pipen) \u2014 Convert the pipeline to a Pipen instance </> post_init ( ) \u2014 Load runtime processes </> class","title":"biopipen.ns.scrna_metabolic_landscape.ScrnaMetabolicLandscape"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocgroupprocgropumeta","text":"</> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod","title":"pipen.procgroup.ProcGropuMeta"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocgroupprocgroupinit_subclass","text":"</> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod","title":"pipen.procgroup.ProcGroup.init_subclass"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocgroupprocgroupadd_proc","text":"</> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method","title":"pipen.procgroup.ProcGroup.add_proc"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#pipenprocgroupprocgroupas_pipen","text":"</> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | os.pathlike | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns (Pipen) The Pipen instance method","title":"pipen.procgroup.ProcGroup.as_pipen"},{"location":"api/biopipen.ns.scrna_metabolic_landscape/#biopipennsscrna_metabolic_landscapescrnametaboliclandscapepost_init","text":"</> Load runtime processes","title":"biopipen.ns.scrna_metabolic_landscape.ScrnaMetabolicLandscape.post_init"},{"location":"api/biopipen.ns.snp/","text":"module biopipen.ns . snp </> Plink processes Classes PlinkSimulation ( Proc ) \u2014 Simulate SNPs using PLINK v2 </> MatrixEQTL ( Proc ) \u2014 Run Matrix eQTL </> PlinkFromVcf ( Proc ) \u2014 Convert VCF to PLINK format. </> Plink2GTMat ( Proc ) \u2014 Convert PLINK files to genotype matrix. </> PlinkIBD ( Proc ) \u2014 Run PLINK IBD analysis (identity by descent) </> PlinkHWE ( Proc ) \u2014 Hardy-Weinberg Equilibrium report and filtering </> PlinkHet ( Proc ) \u2014 Calculation of sample heterozygosity. </> PlinkCallRate ( Proc ) \u2014 Calculation of call rate for the samples and variants. </> PlinkFilter ( Proc ) \u2014 Filter samples and variants for PLINK files. </> PlinkFreq ( Proc ) \u2014 Calculate allele frequencies for the variants. </> PlinkUpdateName ( Proc ) \u2014 Update variant names in PLINK files. </> class biopipen.ns.snp . PlinkSimulation ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Simulate SNPs using PLINK v2 See also https://www.cog-genomics.org/plink/2.0/input#simulate and https://pwwang.github.io/biopipen/api/biopipen.ns.snp/#biopipen.ns.snp.PlinkSimulation Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 Configuration file containing the parameters for the simulation.The configuration file (in toml, yaml or json format) should contain a dictionary of parameters. The parameters are listed in envs except ncores , which is used for parallelization. You can set parameters in envs and override them in the configuration file. Output gtmat \u2014 Genotype matrix file containing the simulated data with rows representingSNPs and columns representing samples. outdir \u2014 Output directory containing the simulated data plink_sim.bed , plink_sim.bim , and plink_sim.fam will be generated. Envs args (ns) \u2014 Additional arguments to pass to PLINK. - : see https://www.cog-genomics.org/plink/2.0/input#simulate . hetodds (type=float) \u2014 Odds ratio for heterozygous genotypes. homodds (type=float) \u2014 Odds ratio for homozygous genotypes. label \u2014 Prefix label for the SNPs. maxfreq (type=float) \u2014 Maximum allele frequency. minfreq (type=float) \u2014 Minimum allele frequency. missing (type=float) \u2014 Proportion of missing genotypes. ncases (type=int) \u2014 Number of cases to simulate nctrls (type=int) \u2014 Number of controls to simulate nsnps (type=int) \u2014 Number of SNPs to simulate plink \u2014 Path to PLINK v2 prevalence (type=float) \u2014 Disease prevalence. sample_prefix \u2014 Use this prefix for the sample names. If not set, the samplenames will be per0_per0 , per1_per1 , per2_per2 , etc. If set, the sample names will be prefix0 , prefix1 , prefix2 , etc. This only affects the sample names in the genotype matrix file ( out.gtmat ). seed (type=int) \u2014 Random seed. If not set, seed will not be set. transpose_gtmat (flag) \u2014 If set, the genotype matrix ( out.gtmat ) willbe transposed. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . MatrixEQTL ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run Matrix eQTL See also https://www.bios.unc.edu/research/genomic_software/Matrix_eQTL/ Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cov \u2014 Covariate matrix file with rows representing covariates and columnsrepresenting samples. expr \u2014 Expression matrix file with rows representing genes and columnsrepresenting samples. geno \u2014 Genotype matrix file with rows representing SNPs and columnsrepresenting samples. Output alleqtls \u2014 Matrix eQTL output file cisqtls \u2014 The cis-eQTL file if snppos and genepos are provided.Otherwise it'll be empty. Envs dist (type=int) \u2014 Distance threshold for cis-eQTLs. fdr (flag) \u2014 Do FDR calculation or not (save memory if not). genepos \u2014 The path of the gene position file.It could be a BED or GFF file. match_samples (flag) \u2014 Match samples in the genotype and expression matrices.If True, an error will be raised if samples from in.geno , in.expr , and in.cov (if provided) are not the same. If False, common samples will be used to subset the matrices. model (choice) \u2014 The model to use. - linear: Linear model - modelLINEAR: Same as linear - anova: ANOVA model - modelANOVA: Same as anova pval (type=float) \u2014 P-value threshold for eQTLs snppos \u2014 The path of the SNP position file.It could be a BED, GFF, VCF or a tab-delimited file with snp , chr , pos as the first 3 columns. transp (type=float) \u2014 P-value threshold for trans-eQTLs.If cis-eQTLs are not enabled ( snppos and genepos are not set), this defaults to 1e-5. If cis-eQTLs are enabled, this defaults to None , which will disable trans-eQTL analysis. transpose_cov (flag) \u2014 If set, the covariate matrix ( in.cov )will be transposed. transpose_expr (flag) \u2014 If set, the expression matrix ( in.expr )will be transposed. transpose_geno (flag) \u2014 If set, the genotype matrix ( in.geno )will be transposed. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . PlinkFromVcf ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert VCF to PLINK format. The PLINK format consists of 3 files: .bed , .bim , and .fam . Requires PLINK v2 TODO: Handle sex when sex chromosomes are included. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input invcf \u2014 VCF file Output outdir \u2014 Output directory containing the PLINK files Envs \u2014 see https://www.cog-genomics.org/plink/2.0/ for more options.Note that _ will be replaced by - in the argument names. double_id (flag) \u2014 set both FIDs and IIDs to the VCF/BCF sample ID. max_alleles (type=int) \u2014 Maximum number of alleles per variant. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 set_missing_var_ids \u2014 update variant IDs using a template string,with a '@' where the chromosome code should go, and a '#' where the base-pair position belongs. You can also specify \\$r and \\$a for the reference and alternate alleles, respectively. See https://www.cog-genomics.org/plink/2.0/data#set_all_var_ids tabix \u2014 Path to tabix vcf_filter (auto) \u2014 skip variants which failed one or more filters trackedby the FILTER field. If True, only FILTER with PASS or . will be kept. Multiple filters can be specified by separating them with space or as a list. vcf_half_call (choice) \u2014 The current VCF standard does not specifyhow '0/.' and similar GT values should be interpreted. - error: error out and reports the line number of the anomaly - e: alias for error - haploid: treat half-calls as haploid/homozygous - h: alias for haploid - missing: treat half-calls as missing - m: alias for missing - reference: treat the missing part as reference - r: alias for reference vcf_idspace_to \u2014 convert all spaces in sample IDs to this character. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . Plink2GTMat ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert PLINK files to genotype matrix. Requires PLINK v2. The .raw/.traw file is generated by plink and then transformed to a genotype matrix file. See https://www.cog-genomics.org/plink/2.0/formats#raw and https://www.cog-genomics.org/plink/2.0/formats#traw for more information. The allelic dosage is used as the values of genotype matrix. \"--keep-allele-order\" is used to keep the allele order consistent with the reference allele first. This way, the genotype of homozygous reference alleles will be encoded as 2, heterozygous as 1, and homozygous alternate alleles as 0. This is the PLINK dosage encoding. If you want to use this encoding, you can set envs.gtcoding to plink . Otherwise, the default encoding is vcf , which will encode the genotype as 0, 1, and 2 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. Note that envs.gtcoding = \"vcf\" only works for biallelic variants for now. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outfile \u2014 Genotype matrix file with rows representing SNPs and columnsrepresenting samples if envs.transpose is False . Envs gtcoding (choice) \u2014 The genotype coding to use. - vcf: 0/1/2 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. - plink: 2/1/0 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. missing_id \u2014 what to use as the rs if missing. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2.0 samid \u2014 what to use as sample ID.Placeholders include {fid} and {iid} for family and individual IDs, respectively. trans_chr \u2014 A dictionary to translate chromosome numbers to chromosome names. transpose (flag) \u2014 If set, the genotype matrix ( out.outfile ) is transposed. varid \u2014 what to use as variant ID.Placeholders include {chr} , {pos} , {rs} , {ref} , and {alt} for chromosome, position, rsID, reference allele, and alternate allele, respectively. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . PlinkIBD ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run PLINK IBD analysis (identity by descent) See also https://www.cog-genomics.org/plink/1.9/ibd This has to run with PLINK v1.9. Plink v2 does not support IBD analysis yet. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the IBD results.Including .genome file for the original IBD report from PLINK, and .ibd.png for the heatmap of PI_HAT values. Envs anno \u2014 The annotation file for the samples, used to plot on the heatmap.Names must match the ones that are transformed by args.samid . devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot highld \u2014 High LD regions to be excluded from the analysis.If not set, no regions will be excluded. indep (type=auto) \u2014 LD pruning parameters. Either a list of numerics or a stringconcatenated by , to specify 1) consider a window of N SNPs (e.g. 50), 2) calculate LD between each pair of SNPs in the window (e.g. 5), 3) remove one of a pair of SNPs if the LD is greater than X (e.g. 0.2). ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option pihat (type=float) \u2014 PI_HAT threshold for IBD analysis.See also https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5007749/ plink \u2014 Path to PLINK v1.9 plot (flag) \u2014 If set, plot the heatmap of PI_HAT values. samid \u2014 what to use as sample ID.Placeholders include {fid} and {iid} for family and individual IDs, respectively seed (type=int) \u2014 Random seed for the analysis. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . PlinkHWE ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Hardy-Weinberg Equilibrium report and filtering See also https://www.cog-genomics.org/plink/2.0/basic_stats#hardy Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the HWE results.Including .hwe file for the original HWE report from PLINK and .hardy.fail for the variants that failed the HWE test. It also includes binary files .bed , .bim , and .fam Envs cutoff (type=float) \u2014 P-value cutoff for HWE test devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 plot (flag) \u2014 If set, plot the distribution of HWE p-values. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . PlinkHet ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculation of sample heterozygosity. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the heterozygosity results.Including .het file for the original heterozygosity report from PLINK and .het.fail for the samples that failed the heterozygosity test. It also includes binary files .bed , .bim , and .fam Envs cutoff (type=float) \u2014 Heterozygosity cutoff, samples with heterozygositybeyond mean - cutoff * sd or mean + cutoff * sd will be considered as outliers. devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2, at least v2.00a5.10 plot (flag) \u2014 If set, plot the distribution of heterozygosity values. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . PlinkCallRate ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculation of call rate for the samples and variants. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the call rate results.Including .imiss file for missing calls for samples, .lmiss for missing calls for variants, .samplecr.fail for the samples fail sample call rate cutoff ( args.samplecr ), and .varcr.fail for the SNPs fail snp call rate cutoff ( args.varcr ). It also includes binary files .bed , .bim , and .fam . Envs devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot max_iter (type=int) \u2014 Maximum number of iterations to run the call ratecalculation. Since the sample and variant call rates are affected by each other, it may be necessary to iterate the calculation to get the stable results. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 plot (flag) \u2014 If set, plot the distribution of call rates. samplecr (type=float) \u2014 Sample call rate cutoff varcr (type=float) \u2014 Variant call rate cutoff Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . PlinkFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Filter samples and variants for PLINK files. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files samples_file \u2014 File containing the sample IDs. variants_file \u2014 File containing the variant IDs or regions. Output outdir \u2014 Output directory containing the filtered PLINK files.Including .bed , .bim , and .fam files Envs autosome (flag) \u2014 Excludes all unplaced and non-autosomal variants autosome_xy (flag) \u2014 Does autosome but does not exclude the pseudo-autosomalregion of X. chr \u2014 Chromosome to keep.For example, 1-4 22 XY will keep chromosomes 1 to 4, 22, and XY. keep (flag) \u2014 Use samples / variants / samples_file / variants_file toonly keep the specified samples/variants, instead of removing them. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option not_chr \u2014 Chromosome to remove.For example, 1-4 22 XY will remove chromosomes 1 to 4, 22, and XY. plink \u2014 Path to PLINK v2 samples (auto) \u2014 Sample IDs.If both FID and IID should be provided and separatedby / . Otherwise, assuming the same FID and IID. A list of sample IDs or string concatenated by , . If either in.samples_file or envs.samples_file is set, this will be ignored. samples_file \u2014 File containing the sample IDs.If in.samples_file is set, this will be ignored. snps_only (auto) \u2014 Excludes all variants with one or more multi-characterallele codes. With 'just-acgt', variants with single-character allele codes outside of {'A', 'C', 'G', 'T', 'a', 'c', 'g', 't', } are also excluded. variants (auto) \u2014 Variant IDs.A list of variant IDs or string concatenated by , . If either in.variants_file or envs.variants_file is set, this will be ignored. variants_file \u2014 File containing the variant IDs.If in.variants_file is set, this will be ignored. vfile_type (choice) \u2014 The type of the variants file. - id: Variant IDs - bed0: 0-based BED file - bed1: 1-based BED file Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . PlinkFreq ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate allele frequencies for the variants. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the allele frequency results.By default, it includes .afreq file for the allele frequency report from PLINK. Modifiers can be added to change this behavior. See envs.modifier for more information. When envs.filter != no , it also includes binary files .bed , .bim , and .fam after filtering with envs.cutoff . Envs cutoff (auto) \u2014 Cutoffs to mark or filter the variants.If a float is given, default column will be used based on the modifier. For modifier=\"none\" , it defaults to MAF . For modifier=\"counts\" , it defaults to ALT1_CT . For modifier=\"x\" , it defaults to HOM_ALT1_CT . Or this could be a dictionary to specify the column names and cutoffs. For example, {\"MAF\": 0.05} . devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot filter (auto) \u2014 The direction of filtering variants based on cutoff .If a single value is given, it will apply to all columns provided in cutoff . If a dictionary is given, it will apply to the corresponding column. If a column cannot be found in the dictionary, it defaults to no . no: Do not filter variants (no binary files are generated in outdir). gt: Filter variants with MAF greater than cutoff . lt: Filter variants with MAF less than cutoff . ge: Filter variants with MAF greater than or equal to cutoff . le: Filter variants with MAF less than or equal to cutoff . gz (flag) \u2014 If set, compress the output files. modifier (choice) \u2014 The modifier of --freq to control the output behavior. - none: No modifier, only the .afreq file will be generated. MAF (minor allele frequency) will be added in addition to the REF_FREQ and ALT1_FREQ columns. Check .afreqx for the added columns. - counts: write allele count report to .acount . See https://www.cog-genomics.org/plink/2.0/formats#afreq . ALT1 , ALT1_CT , and REF_CT are added. Check .acountx for the added columns. - x: write genotype count report to .gcount Like --freqx in v1.9, --geno-counts will be run to generate the genotype counts. ALT1 , HET_REF_ALT1_CT , and HOM_ALT1_CT are added. Check .gcountx for the added columns. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 plot (flag) \u2014 If set, plot the distribution of allele frequencies. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.snp . PlinkUpdateName ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Update variant names in PLINK files. See also https://www.cog-genomics.org/plink/2.0/data#update_map . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files namefile \u2014 File containing the variant names to update.Either a file containing two columns, the first column is the old variant name, and the second column is the new variant name. Or a VCF file containing the variant names to update. When a VCF file is given, the chromosome, position, and reference and alternate alleles will be used to match the variants. Output outdir \u2014 Output directory containing the updated PLINK files.Including .bed , .bim , and .fam files Envs bcftools \u2014 Path to bcftools match_alt (choice) \u2014 How to match alternate alleles when in.namefile is a VCF file. - exact: Matches alternate alleles exactly. - all: Matches alternate alleles regardless of the order. chr1:100:A:T,G matches chr1:100:A:G,T or chr1:100:A:T,G . - any: Matches any alternate allele. For example, chr1:100:A:T,G matches chr1:100:A:G,C - first_included: Matches when the first allele is included. For example, chr1:100:A:T,G matches chr1:100:A:C,T . - first: Match first alternate allele For example, chr1:100:A:T,G matches chr1:100:A:T . - none: Do not match alternate alleles ncores \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.snp"},{"location":"api/biopipen.ns.snp/#biopipennssnp","text":"</> Plink processes Classes PlinkSimulation ( Proc ) \u2014 Simulate SNPs using PLINK v2 </> MatrixEQTL ( Proc ) \u2014 Run Matrix eQTL </> PlinkFromVcf ( Proc ) \u2014 Convert VCF to PLINK format. </> Plink2GTMat ( Proc ) \u2014 Convert PLINK files to genotype matrix. </> PlinkIBD ( Proc ) \u2014 Run PLINK IBD analysis (identity by descent) </> PlinkHWE ( Proc ) \u2014 Hardy-Weinberg Equilibrium report and filtering </> PlinkHet ( Proc ) \u2014 Calculation of sample heterozygosity. </> PlinkCallRate ( Proc ) \u2014 Calculation of call rate for the samples and variants. </> PlinkFilter ( Proc ) \u2014 Filter samples and variants for PLINK files. </> PlinkFreq ( Proc ) \u2014 Calculate allele frequencies for the variants. </> PlinkUpdateName ( Proc ) \u2014 Update variant names in PLINK files. </> class","title":"biopipen.ns.snp"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinksimulation","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Simulate SNPs using PLINK v2 See also https://www.cog-genomics.org/plink/2.0/input#simulate and https://pwwang.github.io/biopipen/api/biopipen.ns.snp/#biopipen.ns.snp.PlinkSimulation Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 Configuration file containing the parameters for the simulation.The configuration file (in toml, yaml or json format) should contain a dictionary of parameters. The parameters are listed in envs except ncores , which is used for parallelization. You can set parameters in envs and override them in the configuration file. Output gtmat \u2014 Genotype matrix file containing the simulated data with rows representingSNPs and columns representing samples. outdir \u2014 Output directory containing the simulated data plink_sim.bed , plink_sim.bim , and plink_sim.fam will be generated. Envs args (ns) \u2014 Additional arguments to pass to PLINK. - : see https://www.cog-genomics.org/plink/2.0/input#simulate . hetodds (type=float) \u2014 Odds ratio for heterozygous genotypes. homodds (type=float) \u2014 Odds ratio for homozygous genotypes. label \u2014 Prefix label for the SNPs. maxfreq (type=float) \u2014 Maximum allele frequency. minfreq (type=float) \u2014 Minimum allele frequency. missing (type=float) \u2014 Proportion of missing genotypes. ncases (type=int) \u2014 Number of cases to simulate nctrls (type=int) \u2014 Number of controls to simulate nsnps (type=int) \u2014 Number of SNPs to simulate plink \u2014 Path to PLINK v2 prevalence (type=float) \u2014 Disease prevalence. sample_prefix \u2014 Use this prefix for the sample names. If not set, the samplenames will be per0_per0 , per1_per1 , per2_per2 , etc. If set, the sample names will be prefix0 , prefix1 , prefix2 , etc. This only affects the sample names in the genotype matrix file ( out.gtmat ). seed (type=int) \u2014 Random seed. If not set, seed will not be set. transpose_gtmat (flag) \u2014 If set, the genotype matrix ( out.gtmat ) willbe transposed. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkSimulation"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpmatrixeqtl","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run Matrix eQTL See also https://www.bios.unc.edu/research/genomic_software/Matrix_eQTL/ Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input cov \u2014 Covariate matrix file with rows representing covariates and columnsrepresenting samples. expr \u2014 Expression matrix file with rows representing genes and columnsrepresenting samples. geno \u2014 Genotype matrix file with rows representing SNPs and columnsrepresenting samples. Output alleqtls \u2014 Matrix eQTL output file cisqtls \u2014 The cis-eQTL file if snppos and genepos are provided.Otherwise it'll be empty. Envs dist (type=int) \u2014 Distance threshold for cis-eQTLs. fdr (flag) \u2014 Do FDR calculation or not (save memory if not). genepos \u2014 The path of the gene position file.It could be a BED or GFF file. match_samples (flag) \u2014 Match samples in the genotype and expression matrices.If True, an error will be raised if samples from in.geno , in.expr , and in.cov (if provided) are not the same. If False, common samples will be used to subset the matrices. model (choice) \u2014 The model to use. - linear: Linear model - modelLINEAR: Same as linear - anova: ANOVA model - modelANOVA: Same as anova pval (type=float) \u2014 P-value threshold for eQTLs snppos \u2014 The path of the SNP position file.It could be a BED, GFF, VCF or a tab-delimited file with snp , chr , pos as the first 3 columns. transp (type=float) \u2014 P-value threshold for trans-eQTLs.If cis-eQTLs are not enabled ( snppos and genepos are not set), this defaults to 1e-5. If cis-eQTLs are enabled, this defaults to None , which will disable trans-eQTL analysis. transpose_cov (flag) \u2014 If set, the covariate matrix ( in.cov )will be transposed. transpose_expr (flag) \u2014 If set, the expression matrix ( in.expr )will be transposed. transpose_geno (flag) \u2014 If set, the genotype matrix ( in.geno )will be transposed. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.MatrixEQTL"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinkfromvcf","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert VCF to PLINK format. The PLINK format consists of 3 files: .bed , .bim , and .fam . Requires PLINK v2 TODO: Handle sex when sex chromosomes are included. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input invcf \u2014 VCF file Output outdir \u2014 Output directory containing the PLINK files Envs \u2014 see https://www.cog-genomics.org/plink/2.0/ for more options.Note that _ will be replaced by - in the argument names. double_id (flag) \u2014 set both FIDs and IIDs to the VCF/BCF sample ID. max_alleles (type=int) \u2014 Maximum number of alleles per variant. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 set_missing_var_ids \u2014 update variant IDs using a template string,with a '@' where the chromosome code should go, and a '#' where the base-pair position belongs. You can also specify \\$r and \\$a for the reference and alternate alleles, respectively. See https://www.cog-genomics.org/plink/2.0/data#set_all_var_ids tabix \u2014 Path to tabix vcf_filter (auto) \u2014 skip variants which failed one or more filters trackedby the FILTER field. If True, only FILTER with PASS or . will be kept. Multiple filters can be specified by separating them with space or as a list. vcf_half_call (choice) \u2014 The current VCF standard does not specifyhow '0/.' and similar GT values should be interpreted. - error: error out and reports the line number of the anomaly - e: alias for error - haploid: treat half-calls as haploid/homozygous - h: alias for haploid - missing: treat half-calls as missing - m: alias for missing - reference: treat the missing part as reference - r: alias for reference vcf_idspace_to \u2014 convert all spaces in sample IDs to this character. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkFromVcf"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplink2gtmat","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert PLINK files to genotype matrix. Requires PLINK v2. The .raw/.traw file is generated by plink and then transformed to a genotype matrix file. See https://www.cog-genomics.org/plink/2.0/formats#raw and https://www.cog-genomics.org/plink/2.0/formats#traw for more information. The allelic dosage is used as the values of genotype matrix. \"--keep-allele-order\" is used to keep the allele order consistent with the reference allele first. This way, the genotype of homozygous reference alleles will be encoded as 2, heterozygous as 1, and homozygous alternate alleles as 0. This is the PLINK dosage encoding. If you want to use this encoding, you can set envs.gtcoding to plink . Otherwise, the default encoding is vcf , which will encode the genotype as 0, 1, and 2 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. Note that envs.gtcoding = \"vcf\" only works for biallelic variants for now. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outfile \u2014 Genotype matrix file with rows representing SNPs and columnsrepresenting samples if envs.transpose is False . Envs gtcoding (choice) \u2014 The genotype coding to use. - vcf: 0/1/2 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. - plink: 2/1/0 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. missing_id \u2014 what to use as the rs if missing. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2.0 samid \u2014 what to use as sample ID.Placeholders include {fid} and {iid} for family and individual IDs, respectively. trans_chr \u2014 A dictionary to translate chromosome numbers to chromosome names. transpose (flag) \u2014 If set, the genotype matrix ( out.outfile ) is transposed. varid \u2014 what to use as variant ID.Placeholders include {chr} , {pos} , {rs} , {ref} , and {alt} for chromosome, position, rsID, reference allele, and alternate allele, respectively. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.Plink2GTMat"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinkibd","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run PLINK IBD analysis (identity by descent) See also https://www.cog-genomics.org/plink/1.9/ibd This has to run with PLINK v1.9. Plink v2 does not support IBD analysis yet. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the IBD results.Including .genome file for the original IBD report from PLINK, and .ibd.png for the heatmap of PI_HAT values. Envs anno \u2014 The annotation file for the samples, used to plot on the heatmap.Names must match the ones that are transformed by args.samid . devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot highld \u2014 High LD regions to be excluded from the analysis.If not set, no regions will be excluded. indep (type=auto) \u2014 LD pruning parameters. Either a list of numerics or a stringconcatenated by , to specify 1) consider a window of N SNPs (e.g. 50), 2) calculate LD between each pair of SNPs in the window (e.g. 5), 3) remove one of a pair of SNPs if the LD is greater than X (e.g. 0.2). ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option pihat (type=float) \u2014 PI_HAT threshold for IBD analysis.See also https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5007749/ plink \u2014 Path to PLINK v1.9 plot (flag) \u2014 If set, plot the heatmap of PI_HAT values. samid \u2014 what to use as sample ID.Placeholders include {fid} and {iid} for family and individual IDs, respectively seed (type=int) \u2014 Random seed for the analysis. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkIBD"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinkhwe","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Hardy-Weinberg Equilibrium report and filtering See also https://www.cog-genomics.org/plink/2.0/basic_stats#hardy Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the HWE results.Including .hwe file for the original HWE report from PLINK and .hardy.fail for the variants that failed the HWE test. It also includes binary files .bed , .bim , and .fam Envs cutoff (type=float) \u2014 P-value cutoff for HWE test devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 plot (flag) \u2014 If set, plot the distribution of HWE p-values. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkHWE"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_5","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinkhet","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculation of sample heterozygosity. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the heterozygosity results.Including .het file for the original heterozygosity report from PLINK and .het.fail for the samples that failed the heterozygosity test. It also includes binary files .bed , .bim , and .fam Envs cutoff (type=float) \u2014 Heterozygosity cutoff, samples with heterozygositybeyond mean - cutoff * sd or mean + cutoff * sd will be considered as outliers. devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2, at least v2.00a5.10 plot (flag) \u2014 If set, plot the distribution of heterozygosity values. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkHet"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_6","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_6","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_6","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_6","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_6","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_6","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_6","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinkcallrate","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculation of call rate for the samples and variants. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the call rate results.Including .imiss file for missing calls for samples, .lmiss for missing calls for variants, .samplecr.fail for the samples fail sample call rate cutoff ( args.samplecr ), and .varcr.fail for the SNPs fail snp call rate cutoff ( args.varcr ). It also includes binary files .bed , .bim , and .fam . Envs devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot max_iter (type=int) \u2014 Maximum number of iterations to run the call ratecalculation. Since the sample and variant call rates are affected by each other, it may be necessary to iterate the calculation to get the stable results. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 plot (flag) \u2014 If set, plot the distribution of call rates. samplecr (type=float) \u2014 Sample call rate cutoff varcr (type=float) \u2014 Variant call rate cutoff Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkCallRate"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_7","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_7","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_7","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_7","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_7","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_7","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_7","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinkfilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Filter samples and variants for PLINK files. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files samples_file \u2014 File containing the sample IDs. variants_file \u2014 File containing the variant IDs or regions. Output outdir \u2014 Output directory containing the filtered PLINK files.Including .bed , .bim , and .fam files Envs autosome (flag) \u2014 Excludes all unplaced and non-autosomal variants autosome_xy (flag) \u2014 Does autosome but does not exclude the pseudo-autosomalregion of X. chr \u2014 Chromosome to keep.For example, 1-4 22 XY will keep chromosomes 1 to 4, 22, and XY. keep (flag) \u2014 Use samples / variants / samples_file / variants_file toonly keep the specified samples/variants, instead of removing them. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option not_chr \u2014 Chromosome to remove.For example, 1-4 22 XY will remove chromosomes 1 to 4, 22, and XY. plink \u2014 Path to PLINK v2 samples (auto) \u2014 Sample IDs.If both FID and IID should be provided and separatedby / . Otherwise, assuming the same FID and IID. A list of sample IDs or string concatenated by , . If either in.samples_file or envs.samples_file is set, this will be ignored. samples_file \u2014 File containing the sample IDs.If in.samples_file is set, this will be ignored. snps_only (auto) \u2014 Excludes all variants with one or more multi-characterallele codes. With 'just-acgt', variants with single-character allele codes outside of {'A', 'C', 'G', 'T', 'a', 'c', 'g', 't', } are also excluded. variants (auto) \u2014 Variant IDs.A list of variant IDs or string concatenated by , . If either in.variants_file or envs.variants_file is set, this will be ignored. variants_file \u2014 File containing the variant IDs.If in.variants_file is set, this will be ignored. vfile_type (choice) \u2014 The type of the variants file. - id: Variant IDs - bed0: 0-based BED file - bed1: 1-based BED file Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkFilter"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_8","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_8","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_8","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_8","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_8","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_8","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_8","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinkfreq","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calculate allele frequencies for the variants. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files Output outdir \u2014 Output file containing the allele frequency results.By default, it includes .afreq file for the allele frequency report from PLINK. Modifiers can be added to change this behavior. See envs.modifier for more information. When envs.filter != no , it also includes binary files .bed , .bim , and .fam after filtering with envs.cutoff . Envs cutoff (auto) \u2014 Cutoffs to mark or filter the variants.If a float is given, default column will be used based on the modifier. For modifier=\"none\" , it defaults to MAF . For modifier=\"counts\" , it defaults to ALT1_CT . For modifier=\"x\" , it defaults to HOM_ALT1_CT . Or this could be a dictionary to specify the column names and cutoffs. For example, {\"MAF\": 0.05} . devpars (ns) \u2014 The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot filter (auto) \u2014 The direction of filtering variants based on cutoff .If a single value is given, it will apply to all columns provided in cutoff . If a dictionary is given, it will apply to the corresponding column. If a column cannot be found in the dictionary, it defaults to no . no: Do not filter variants (no binary files are generated in outdir). gt: Filter variants with MAF greater than cutoff . lt: Filter variants with MAF less than cutoff . ge: Filter variants with MAF greater than or equal to cutoff . le: Filter variants with MAF less than or equal to cutoff . gz (flag) \u2014 If set, compress the output files. modifier (choice) \u2014 The modifier of --freq to control the output behavior. - none: No modifier, only the .afreq file will be generated. MAF (minor allele frequency) will be added in addition to the REF_FREQ and ALT1_FREQ columns. Check .afreqx for the added columns. - counts: write allele count report to .acount . See https://www.cog-genomics.org/plink/2.0/formats#afreq . ALT1 , ALT1_CT , and REF_CT are added. Check .acountx for the added columns. - x: write genotype count report to .gcount Like --freqx in v1.9, --geno-counts will be run to generate the genotype counts. ALT1 , HET_REF_ALT1_CT , and HOM_ALT1_CT are added. Check .gcountx for the added columns. ncores (type=int) \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 plot (flag) \u2014 If set, plot the distribution of allele frequencies. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkFreq"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_9","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_9","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_9","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_9","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_9","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_9","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_9","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.snp/#biopipennssnpplinkupdatename","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Update variant names in PLINK files. See also https://www.cog-genomics.org/plink/2.0/data#update_map . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indir \u2014 Input directory containing the PLINK files.Including .bed , .bim , and .fam files namefile \u2014 File containing the variant names to update.Either a file containing two columns, the first column is the old variant name, and the second column is the new variant name. Or a VCF file containing the variant names to update. When a VCF file is given, the chromosome, position, and reference and alternate alleles will be used to match the variants. Output outdir \u2014 Output directory containing the updated PLINK files.Including .bed , .bim , and .fam files Envs bcftools \u2014 Path to bcftools match_alt (choice) \u2014 How to match alternate alleles when in.namefile is a VCF file. - exact: Matches alternate alleles exactly. - all: Matches alternate alleles regardless of the order. chr1:100:A:T,G matches chr1:100:A:G,T or chr1:100:A:T,G . - any: Matches any alternate allele. For example, chr1:100:A:T,G matches chr1:100:A:G,C - first_included: Matches when the first allele is included. For example, chr1:100:A:T,G matches chr1:100:A:C,T . - first: Match first alternate allele For example, chr1:100:A:T,G matches chr1:100:A:T . - none: Do not match alternate alleles ncores \u2014 Number of cores/threads to use, will pass to plink --threads option plink \u2014 Path to PLINK v2 Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.snp.PlinkUpdateName"},{"location":"api/biopipen.ns.snp/#pipenprocprocmeta_10","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.snp/#pipenprocprocfrom_proc_10","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_subclass_10","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.snp/#pipenprocprocinit_10","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.snp/#pipenprocprocgc_10","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.snp/#pipenprocproclog_10","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.snp/#pipenprocprocrun_10","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.stats/","text":"module biopipen.ns . stats </> Provides processes for statistics. Classes ChowTest ( Proc ) \u2014 Massive Chow tests. </> Mediation ( Proc ) \u2014 Mediation analysis. </> LiquidAssoc ( Proc ) \u2014 Liquid association tests. </> DiffCoexpr ( Proc ) \u2014 Differential co-expression analysis. </> MetaPvalue ( Proc ) \u2014 Calulation of meta p-values. </> MetaPvalue1 ( Proc ) \u2014 Calulation of meta p-values. </> class biopipen.ns.stats . ChowTest ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Massive Chow tests. See Also https://en.wikipedia.org/wiki/Chow_test Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input fmlfile \u2014 The formula file. The first column is grouping and thesecond column is the formula. It must be tab-delimited. Group Formula ... # Other columns to be added to outfile G1 Fn ~ F1 + Fx + Fy # Fx, Fy could be covariates G1 Fn ~ F2 + Fx + Fy ... Gk Fn ~ F3 + Fx + Fy groupfile \u2014 The group file. The rows are the samples and the columnsare the groupings. It must be tab-delimited. Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 infile \u2014 The input data file. The rows are samples and the columns arefeatures. It must be tab-delimited. Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 Output outfile \u2014 The output file. It is a tab-delimited file with the firstcolumn as the grouping and the second column as the p-value. Group Formula ... Pooled Groups SSR SumSSR Fstat Pval Padj G1 Fn ~ F1 0.123 2 1 0.123 0.123 0.123 0.123 G1 Fn ~ F2 0.123 2 1 0.123 0.123 0.123 0.123 ... Gk Fn ~ F3 0.123 2 1 0.123 0.123 0.123 0.123 Envs padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. transpose_group (flag) \u2014 Whether to transpose the group file. transpose_input (flag) \u2014 Whether to transpose the input file. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.stats . Mediation ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Mediation analysis. The flowchart of mediation analysis: Reference: - https://library.virginia.edu/data/articles/introduction-to-mediation-analysis - https://en.wikipedia.org/wiki/Mediation_(statistics) - https://tilburgsciencehub.com/topics/analyze/regression/linear-regression/mediation-analysis/ - https://ademos.people.uic.edu/Chapter14.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input fmlfile \u2014 The formula file. Case M Y X Cov Model_M Model_Y Case1 F1 F2 F3 F4,F5 glm lm ... Where Y is the outcome variable, X is the predictor variable, M is the mediator variable, and Case is the case name. Model_M and Model_Y are the models for M and Y, respectively. envs.cases will be ignored if this is provided. infile \u2014 The input data file. The rows are samples and the columns arefeatures. It must be tab-delimited. Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 Output outfile \u2014 The output file.Columns to help understand the results: Total Effect: a total effect of X on Y (without M) ( Y ~ X ). ADE: A Direct Effect of X on Y after taking into account a mediation effect of M ( Y ~ X + M ). ACME: The Mediation Effect, the total effect minus the direct effect, which equals to a product of a coefficient of X in the second step and a coefficient of M in the last step. The goal of mediation analysis is to obtain this indirect effect and see if it's statistically significant. Envs args (ns) \u2014 Other arguments passed to mediation::mediate function. - : More arguments passed to mediation::mediate function. See: https://rdrr.io/cran/mediation/man/mediate.html cases (type=json) \u2014 The cases for mediation analysis.Ignored if in.fmlfile is provided. A json/dict with case names as keys and values as a dict of M, Y, X, Cov, Model_M, Model_Y. For example: { \"Case1\" : { \"M\" : \"F1\" , \"Y\" : \"F2\" , \"X\" : \"F3\" , \"Cov\" : \"F4,F5\" , \"Model_M\" : \"glm\" , \"Model_Y\" : \"lm\" }, ... } ncores (type=int) \u2014 Number of cores to use for parallelization for cases. padj (choice) \u2014 The method for (ACME) p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. sims (type=int) \u2014 Number of Monte Carlo draws for nonparametric bootstrap or quasi-Bayesian approximation.Will be passed to mediation::mediate function. transpose_input (flag) \u2014 Whether to transpose the input file. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.stats . LiquidAssoc ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Liquid association tests. See Also https://github.com/gundt/fastLiquidAssociation Requires https://github.com/pwwang/fastLiquidAssociation Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input covfile \u2014 The covariate file. The rows are the samples and the columnsare the covariates. It must be tab-delimited. If provided, the data in in.infile will be adjusted by covariates by regressing out the covariates and the residuals will be used for liquid association tests. fmlfile \u2014 The formula file. The 3 columns are X3, X12 and X21. The resultswill be filtered based on the formula. It must be tab-delimited without header. groupfile \u2014 The group file. The rows are the samples and the columnsare the groupings. It must be tab-delimited. Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 This will be served as the Z column in the result of fastMLA This can be omitted. If so, envs.nvec should be specified, which is to select column from in.infile as Z. infile \u2014 The input data file. The rows are samples and the columns arefeatures. It must be tab-delimited. Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 The features (columns) will be tested pairwise, which will be the X and Y columns in the result of fastMLA Output outfile \u2014 The output file. X12 X21 X3 rhodiff MLA value estimates san.se wald Pval model C38 C46 C5 0.87 0.32 0.67 0.20 10.87 0 F C46 C38 C5 0.87 0.32 0.67 0.20 10.87 0 F C27 C39 C4 0.94 0.34 1.22 0.38 10.03 0 F Envs cut (type=int) \u2014 Value passed to the GLA function to create buckets(equal to number of buckets+1). Values placing between 15-30 samples per bucket are optimal. Must be a positive integer>1. By default, max(ceiling(nrow(data)/22), 4) is used. ncores (type=int) \u2014 Number of cores to use for parallelization. nvec \u2014 The column index (1-based) of Z in in.infile , if in.groupfile isomitted. You can specify multiple columns by comma-seperated values, or a range of columns by - . For example, 1,3,5-7,9 . It also supports column names. For example, F1,F3 . - is not supported for column names. padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. rvalue (type=float) \u2014 Tolerance value for LA approximation. Lower values ofrvalue will cause a more thorough search, but take longer. topn (type=int) \u2014 Number of results to return by fastMLA , ordered fromhighest |MLA| value descending. The default of the package is 2000, but here we set to 1e6 to return as many results as possible (also good to do pvalue adjustment). transpose_cov (flag) \u2014 Whether to transpose the covariate file. transpose_group (flag) \u2014 Whether to transpose the group file. transpose_input (flag) \u2014 Whether to transpose the input file. x \u2014 Similar as nvec , but limit X group to given features.The rest of features (other than X and Z) in in.infile will be used as Y. The features in in.infile will still be tested pairwise, but only features in X and Y will be kept. xyz_names \u2014 The names of X12, X21 and X3 in the final output file. Separatedby comma. For example, X12,X21,X3 . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.stats . DiffCoexpr ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Differential co-expression analysis. See also https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-497 and https://github.com/DavisLaboratory/dcanr/blob/8958d61788937eef3b7e2b4118651cbd7af7469d/R/inference_methods.R#L199 . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input groupfile \u2014 The group file. The rows are the samples and the columnsare the groupings. It must be tab-delimited. Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 infile \u2014 The input data file. The rows are samples and the columns arefeatures. It must be tab-delimited. Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 Output outfile \u2014 The output file. It is a tab-delimited file with the firstcolumn as the feature pair and the second column as the p-value. Group Feature1 Feature2 Pval Padj G1 F1 F2 0.123 0.123 G1 F1 F3 0.123 0.123 ... Envs beta \u2014 The beta value for the differential co-expression analysis. method (choice) \u2014 The method used to calculate the differentialco-expression. - pearson: Pearson correlation. - spearman: Spearman correlation. ncores (type=int) \u2014 The number of cores to use for parallelization padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. perm_batch (type=int) \u2014 The number of permutations to run in each batch seed (type=int) \u2014 The seed for random number generation transpose_group (flag) \u2014 Whether to transpose the group file. transpose_input (flag) \u2014 Whether to transpose the input file. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.stats . MetaPvalue ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calulation of meta p-values. If there is only one input file, only the p-value adjustment will be performed. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infiles \u2014 The input files. Each file is a tab-delimited file with multiplecolumns. There should be ID column(s) to match the rows in other files and p-value column(s) to be combined. The records will be full-joined by ID. When only one file is provided, only the pvalue adjustment will be performed when envs.padj is not none , otherwise the input file will be copied to out.outfile . Output outfile \u2014 The output file. It is a tab-delimited file with the first column asthe ID and the second column as the combined p-value. ID ID1 ... Pval Padj a x ... 0.123 0.123 b y ... 0.123 0.123 ... Envs id_cols \u2014 The column names used in all in.infiles as ID columns. Multiplecolumns can be specified by comma-seperated values. For example, ID1,ID2 , where ID1 is the ID column in the first file and ID2 is the ID column in the second file. If id_exprs is specified, this should be a single column name for the new ID column in each in.infiles and the final out.outfile . id_exprs \u2014 The R expressions for each in.infiles to get ID column(s). keep_single (flag) \u2014 Whether to keep the original p-value when there is only onep-value. method (choice) \u2014 The method used to calculate the meta-pvalue. - fisher: Fisher's method. - sumlog: Sum of logarithms (same as Fisher's method) - logitp: Logit method. - sumz: Sum of z method (Stouffer's method). - meanz: Mean of z method. - meanp: Mean of p method. - invt: Inverse t method. - sump: Sum of p method (Edgington's method). - votep: Vote counting method. - wilkinsonp: Wilkinson's method. - invchisq: Inverse chi-square method. na \u2014 The method to handle NA values. -1 to skip the record. Otherwise NAwill be replaced by the given value. padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. pval_cols \u2014 The column names used in all in.infiles as p-value columns.Different columns can be specified by comma-seperated values for each in.infiles . For example, Pval1,Pval2 . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.stats . MetaPvalue1 ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Calulation of meta p-values. Unlike MetaPvalue , this process only accepts one input file. The p-values will be grouped by the ID columns and combined by the selected method. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file.The file is a tab-delimited file with multiple columns. There should be ID column(s) to group the rows where p-value column(s) to be combined. Output outfile \u2014 The output file. It is a tab-delimited file with the first column asthe ID and the second column as the combined p-value. ID ID1 ... Pval Padj a x ... 0.123 0.123 b y ... 0.123 0.123 ... Envs id_cols \u2014 The column names used in in.infile as ID columns. Multiplecolumns can be specified by comma-seperated values. For example, ID1,ID2 . keep_single (flag) \u2014 Whether to keep the original p-value when there is only onep-value. method (choice) \u2014 The method used to calculate the meta-pvalue. - fisher: Fisher's method. - sumlog: Sum of logarithms (same as Fisher's method) - logitp: Logit method. - sumz: Sum of z method (Stouffer's method). - meanz: Mean of z method. - meanp: Mean of p method. - invt: Inverse t method. - sump: Sum of p method (Edgington's method). - votep: Vote counting method. - wilkinsonp: Wilkinson's method. - invchisq: Inverse chi-square method. na \u2014 The method to handle NA values. -1 to skip the record. Otherwise NAwill be replaced by the given value. padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. pval_col \u2014 The column name used in in.infile as p-value column. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.stats"},{"location":"api/biopipen.ns.stats/#biopipennsstats","text":"</> Provides processes for statistics. Classes ChowTest ( Proc ) \u2014 Massive Chow tests. </> Mediation ( Proc ) \u2014 Mediation analysis. </> LiquidAssoc ( Proc ) \u2014 Liquid association tests. </> DiffCoexpr ( Proc ) \u2014 Differential co-expression analysis. </> MetaPvalue ( Proc ) \u2014 Calulation of meta p-values. </> MetaPvalue1 ( Proc ) \u2014 Calulation of meta p-values. </> class","title":"biopipen.ns.stats"},{"location":"api/biopipen.ns.stats/#biopipennsstatschowtest","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Massive Chow tests. See Also https://en.wikipedia.org/wiki/Chow_test Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input fmlfile \u2014 The formula file. The first column is grouping and thesecond column is the formula. It must be tab-delimited. Group Formula ... # Other columns to be added to outfile G1 Fn ~ F1 + Fx + Fy # Fx, Fy could be covariates G1 Fn ~ F2 + Fx + Fy ... Gk Fn ~ F3 + Fx + Fy groupfile \u2014 The group file. The rows are the samples and the columnsare the groupings. It must be tab-delimited. Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 infile \u2014 The input data file. The rows are samples and the columns arefeatures. It must be tab-delimited. Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 Output outfile \u2014 The output file. It is a tab-delimited file with the firstcolumn as the grouping and the second column as the p-value. Group Formula ... Pooled Groups SSR SumSSR Fstat Pval Padj G1 Fn ~ F1 0.123 2 1 0.123 0.123 0.123 0.123 G1 Fn ~ F2 0.123 2 1 0.123 0.123 0.123 0.123 ... Gk Fn ~ F3 0.123 2 1 0.123 0.123 0.123 0.123 Envs padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. transpose_group (flag) \u2014 Whether to transpose the group file. transpose_input (flag) \u2014 Whether to transpose the input file. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.stats.ChowTest"},{"location":"api/biopipen.ns.stats/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.stats/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.stats/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.stats/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.stats/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.stats/#biopipennsstatsmediation","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Mediation analysis. The flowchart of mediation analysis: Reference: - https://library.virginia.edu/data/articles/introduction-to-mediation-analysis - https://en.wikipedia.org/wiki/Mediation_(statistics) - https://tilburgsciencehub.com/topics/analyze/regression/linear-regression/mediation-analysis/ - https://ademos.people.uic.edu/Chapter14.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input fmlfile \u2014 The formula file. Case M Y X Cov Model_M Model_Y Case1 F1 F2 F3 F4,F5 glm lm ... Where Y is the outcome variable, X is the predictor variable, M is the mediator variable, and Case is the case name. Model_M and Model_Y are the models for M and Y, respectively. envs.cases will be ignored if this is provided. infile \u2014 The input data file. The rows are samples and the columns arefeatures. It must be tab-delimited. Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 Output outfile \u2014 The output file.Columns to help understand the results: Total Effect: a total effect of X on Y (without M) ( Y ~ X ). ADE: A Direct Effect of X on Y after taking into account a mediation effect of M ( Y ~ X + M ). ACME: The Mediation Effect, the total effect minus the direct effect, which equals to a product of a coefficient of X in the second step and a coefficient of M in the last step. The goal of mediation analysis is to obtain this indirect effect and see if it's statistically significant. Envs args (ns) \u2014 Other arguments passed to mediation::mediate function. - : More arguments passed to mediation::mediate function. See: https://rdrr.io/cran/mediation/man/mediate.html cases (type=json) \u2014 The cases for mediation analysis.Ignored if in.fmlfile is provided. A json/dict with case names as keys and values as a dict of M, Y, X, Cov, Model_M, Model_Y. For example: { \"Case1\" : { \"M\" : \"F1\" , \"Y\" : \"F2\" , \"X\" : \"F3\" , \"Cov\" : \"F4,F5\" , \"Model_M\" : \"glm\" , \"Model_Y\" : \"lm\" }, ... } ncores (type=int) \u2014 Number of cores to use for parallelization for cases. padj (choice) \u2014 The method for (ACME) p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. sims (type=int) \u2014 Number of Monte Carlo draws for nonparametric bootstrap or quasi-Bayesian approximation.Will be passed to mediation::mediate function. transpose_input (flag) \u2014 Whether to transpose the input file. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.stats.Mediation"},{"location":"api/biopipen.ns.stats/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.stats/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.stats/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.stats/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.stats/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.stats/#biopipennsstatsliquidassoc","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Liquid association tests. See Also https://github.com/gundt/fastLiquidAssociation Requires https://github.com/pwwang/fastLiquidAssociation Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input covfile \u2014 The covariate file. The rows are the samples and the columnsare the covariates. It must be tab-delimited. If provided, the data in in.infile will be adjusted by covariates by regressing out the covariates and the residuals will be used for liquid association tests. fmlfile \u2014 The formula file. The 3 columns are X3, X12 and X21. The resultswill be filtered based on the formula. It must be tab-delimited without header. groupfile \u2014 The group file. The rows are the samples and the columnsare the groupings. It must be tab-delimited. Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 This will be served as the Z column in the result of fastMLA This can be omitted. If so, envs.nvec should be specified, which is to select column from in.infile as Z. infile \u2014 The input data file. The rows are samples and the columns arefeatures. It must be tab-delimited. Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 The features (columns) will be tested pairwise, which will be the X and Y columns in the result of fastMLA Output outfile \u2014 The output file. X12 X21 X3 rhodiff MLA value estimates san.se wald Pval model C38 C46 C5 0.87 0.32 0.67 0.20 10.87 0 F C46 C38 C5 0.87 0.32 0.67 0.20 10.87 0 F C27 C39 C4 0.94 0.34 1.22 0.38 10.03 0 F Envs cut (type=int) \u2014 Value passed to the GLA function to create buckets(equal to number of buckets+1). Values placing between 15-30 samples per bucket are optimal. Must be a positive integer>1. By default, max(ceiling(nrow(data)/22), 4) is used. ncores (type=int) \u2014 Number of cores to use for parallelization. nvec \u2014 The column index (1-based) of Z in in.infile , if in.groupfile isomitted. You can specify multiple columns by comma-seperated values, or a range of columns by - . For example, 1,3,5-7,9 . It also supports column names. For example, F1,F3 . - is not supported for column names. padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. rvalue (type=float) \u2014 Tolerance value for LA approximation. Lower values ofrvalue will cause a more thorough search, but take longer. topn (type=int) \u2014 Number of results to return by fastMLA , ordered fromhighest |MLA| value descending. The default of the package is 2000, but here we set to 1e6 to return as many results as possible (also good to do pvalue adjustment). transpose_cov (flag) \u2014 Whether to transpose the covariate file. transpose_group (flag) \u2014 Whether to transpose the group file. transpose_input (flag) \u2014 Whether to transpose the input file. x \u2014 Similar as nvec , but limit X group to given features.The rest of features (other than X and Z) in in.infile will be used as Y. The features in in.infile will still be tested pairwise, but only features in X and Y will be kept. xyz_names \u2014 The names of X12, X21 and X3 in the final output file. Separatedby comma. For example, X12,X21,X3 . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.stats.LiquidAssoc"},{"location":"api/biopipen.ns.stats/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.stats/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.stats/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.stats/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.stats/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.stats/#biopipennsstatsdiffcoexpr","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Differential co-expression analysis. See also https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-497 and https://github.com/DavisLaboratory/dcanr/blob/8958d61788937eef3b7e2b4118651cbd7af7469d/R/inference_methods.R#L199 . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input groupfile \u2014 The group file. The rows are the samples and the columnsare the groupings. It must be tab-delimited. Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 infile \u2014 The input data file. The rows are samples and the columns arefeatures. It must be tab-delimited. Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 Output outfile \u2014 The output file. It is a tab-delimited file with the firstcolumn as the feature pair and the second column as the p-value. Group Feature1 Feature2 Pval Padj G1 F1 F2 0.123 0.123 G1 F1 F3 0.123 0.123 ... Envs beta \u2014 The beta value for the differential co-expression analysis. method (choice) \u2014 The method used to calculate the differentialco-expression. - pearson: Pearson correlation. - spearman: Spearman correlation. ncores (type=int) \u2014 The number of cores to use for parallelization padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. perm_batch (type=int) \u2014 The number of permutations to run in each batch seed (type=int) \u2014 The seed for random number generation transpose_group (flag) \u2014 Whether to transpose the group file. transpose_input (flag) \u2014 Whether to transpose the input file. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.stats.DiffCoexpr"},{"location":"api/biopipen.ns.stats/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.stats/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.stats/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.stats/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.stats/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.stats/#biopipennsstatsmetapvalue","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calulation of meta p-values. If there is only one input file, only the p-value adjustment will be performed. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infiles \u2014 The input files. Each file is a tab-delimited file with multiplecolumns. There should be ID column(s) to match the rows in other files and p-value column(s) to be combined. The records will be full-joined by ID. When only one file is provided, only the pvalue adjustment will be performed when envs.padj is not none , otherwise the input file will be copied to out.outfile . Output outfile \u2014 The output file. It is a tab-delimited file with the first column asthe ID and the second column as the combined p-value. ID ID1 ... Pval Padj a x ... 0.123 0.123 b y ... 0.123 0.123 ... Envs id_cols \u2014 The column names used in all in.infiles as ID columns. Multiplecolumns can be specified by comma-seperated values. For example, ID1,ID2 , where ID1 is the ID column in the first file and ID2 is the ID column in the second file. If id_exprs is specified, this should be a single column name for the new ID column in each in.infiles and the final out.outfile . id_exprs \u2014 The R expressions for each in.infiles to get ID column(s). keep_single (flag) \u2014 Whether to keep the original p-value when there is only onep-value. method (choice) \u2014 The method used to calculate the meta-pvalue. - fisher: Fisher's method. - sumlog: Sum of logarithms (same as Fisher's method) - logitp: Logit method. - sumz: Sum of z method (Stouffer's method). - meanz: Mean of z method. - meanp: Mean of p method. - invt: Inverse t method. - sump: Sum of p method (Edgington's method). - votep: Vote counting method. - wilkinsonp: Wilkinson's method. - invchisq: Inverse chi-square method. na \u2014 The method to handle NA values. -1 to skip the record. Otherwise NAwill be replaced by the given value. padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. pval_cols \u2014 The column names used in all in.infiles as p-value columns.Different columns can be specified by comma-seperated values for each in.infiles . For example, Pval1,Pval2 . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.stats.MetaPvalue"},{"location":"api/biopipen.ns.stats/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.stats/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.stats/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.stats/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.stats/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.stats/#biopipennsstatsmetapvalue1","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Calulation of meta p-values. Unlike MetaPvalue , this process only accepts one input file. The p-values will be grouped by the ID columns and combined by the selected method. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file.The file is a tab-delimited file with multiple columns. There should be ID column(s) to group the rows where p-value column(s) to be combined. Output outfile \u2014 The output file. It is a tab-delimited file with the first column asthe ID and the second column as the combined p-value. ID ID1 ... Pval Padj a x ... 0.123 0.123 b y ... 0.123 0.123 ... Envs id_cols \u2014 The column names used in in.infile as ID columns. Multiplecolumns can be specified by comma-seperated values. For example, ID1,ID2 . keep_single (flag) \u2014 Whether to keep the original p-value when there is only onep-value. method (choice) \u2014 The method used to calculate the meta-pvalue. - fisher: Fisher's method. - sumlog: Sum of logarithms (same as Fisher's method) - logitp: Logit method. - sumz: Sum of z method (Stouffer's method). - meanz: Mean of z method. - meanp: Mean of p method. - invt: Inverse t method. - sump: Sum of p method (Edgington's method). - votep: Vote counting method. - wilkinsonp: Wilkinson's method. - invchisq: Inverse chi-square method. na \u2014 The method to handle NA values. -1 to skip the record. Otherwise NAwill be replaced by the given value. padj (choice) \u2014 The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. pval_col \u2014 The column name used in in.infile as p-value column. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.stats.MetaPvalue1"},{"location":"api/biopipen.ns.stats/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.stats/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.stats/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.stats/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.stats/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.stats/#pipenprocprocrun_5","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcgamaf/","text":"module biopipen.ns . tcgamaf </> Processes for TCGA MAF files. Classes Maf2Vcf ( Proc ) \u2014 Converts a MAF file to a VCF file. </> MafAddChr ( Proc ) \u2014 Adds the chr prefix to chromosome names in a MAF file if not present. </> class biopipen.ns.tcgamaf . Maf2Vcf ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Converts a MAF file to a VCF file. This is a wrapper around the maf2vcf script from the maf2vcf package. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input MAF file Output outdir \u2014 Path to output directory where VCFs will be stored,one per TN-pair outfile \u2014 Output multi-sample VCF containing all TN-pairs Envs args \u2014 Other arguments to pass to the script perl \u2014 Path to perl to run maf2vcf.pl samtools \u2014 Path to samtools to be used in maf2vcf.pl Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcgamaf . MafAddChr ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Adds the chr prefix to chromosome names in a MAF file if not present. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input MAF file Output outfile \u2014 The output MAF file Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.tcgamaf"},{"location":"api/biopipen.ns.tcgamaf/#biopipennstcgamaf","text":"</> Processes for TCGA MAF files. Classes Maf2Vcf ( Proc ) \u2014 Converts a MAF file to a VCF file. </> MafAddChr ( Proc ) \u2014 Adds the chr prefix to chromosome names in a MAF file if not present. </> class","title":"biopipen.ns.tcgamaf"},{"location":"api/biopipen.ns.tcgamaf/#biopipennstcgamafmaf2vcf","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Converts a MAF file to a VCF file. This is a wrapper around the maf2vcf script from the maf2vcf package. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input MAF file Output outdir \u2014 Path to output directory where VCFs will be stored,one per TN-pair outfile \u2014 Output multi-sample VCF containing all TN-pairs Envs args \u2014 Other arguments to pass to the script perl \u2014 Path to perl to run maf2vcf.pl samtools \u2014 Path to samtools to be used in maf2vcf.pl Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcgamaf.Maf2Vcf"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcgamaf/#biopipennstcgamafmafaddchr","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Adds the chr prefix to chromosome names in a MAF file if not present. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input MAF file Output outfile \u2014 The output MAF file Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcgamaf.MafAddChr"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcgamaf/#pipenprocprocrun_1","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/","text":"module biopipen.ns . tcr </> Tools to analyze single-cell TCR sequencing data Classes ImmunarchLoading ( Proc ) \u2014 Immuarch - Loading data </> ImmunarchFilter ( Proc ) \u2014 Immunarch - Filter data </> Immunarch ( Proc ) \u2014 Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires </> SampleDiversity ( Proc ) \u2014 Sample diversity and rarefaction analysis </> CloneResidency ( Proc ) \u2014 Identification of clone residency </> Immunarch2VDJtools ( Proc ) \u2014 Convert immuarch format into VDJtools input formats. </> ImmunarchSplitIdents ( Proc ) \u2014 Split the data into multiple immunarch datasets by Idents from Seurat </> VJUsage ( Proc ) \u2014 Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions using vdjtools. </> Attach2Seurat ( Proc ) \u2014 Attach the clonal information to a Seurat object as metadata </> TCRClustering ( Proc ) \u2014 Cluster the TCR clones by their CDR3 sequences </> TCRClusterStats ( Proc ) \u2014 Statistics of TCR clusters, generated by TCRClustering . </> CloneSizeQQPlot ( Proc ) \u2014 QQ plot of the clone sizes </> CDR3AAPhyschem ( Proc ) \u2014 CDR3 AA physicochemical feature analysis </> TESSA ( Proc ) \u2014 Tessa is a Bayesian model to integrate T cell receptor (TCR) sequenceprofiling with transcriptomes of T cells. </> TCRDock ( Proc ) \u2014 Using TCRDock to predict the structure of MHC-peptide-TCR complexes </> ScRepLoading ( Proc ) \u2014 Load the single cell TCR/BCR data into a scRepertoire compatible object </> ScRepCombiningExpression ( Proc ) \u2014 Combine the scTCR/BCR data with the expression data </> ClonalStats ( Proc ) \u2014 Visualize the clonal information. </> class biopipen.ns.tcr . ImmunarchLoading ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Immuarch - Loading data Load the raw data into immunarch object, using immunarch::repLoad() . For the data path specified at TCRData in the input file, we will first find filtered_contig_annotations.csv and filtered_config_annotations.csv.gz in the path. If neighter of them exists, we will find all_contig_annotations.csv and all_contig_annotations.csv.gz in the path and a warning will be raised (You can find it at ./.pipen/<pipeline-name>/ImmunarchLoading/0/job.stderr ). If none of the files exists, an error will be raised. This process will also generate a text file with the information for each cell. The file will be saved at ./.pipen/<pipeline-name>/ImmunarchLoading/0/output/<prefix>.tcr.txt . The file can be used by the SeuratMetadataMutater process to integrate the TCR-seq data into the Seurat object for further integrative analysis. envs.metacols can be used to specify the columns to be exported to the text file. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The meta data of the samplesA tab-delimited file Two columns are required: * Sample to specify the sample names. * TCRData to assign the path of the data to the samples, and this column will be excluded as metadata.Immunarch is able to fetch the sample names from the names of the target files. However, 10x data yields result like filtered_contig_annotations.csv , which doesn't have any name information. Output metatxt \u2014 The meta data at cell level, which can be used to attach to the Seurat object rdsfile \u2014 The RDS file with the data and metadata, which can be processed byother immunarch functions. Envs extracols (list) \u2014 The extra columns to be exported to the text file.You can refer to the immunarch documentation to get a sense for the full list of the columns. The columns may vary depending on the data source. The columns from immdata$meta and some core columns, including Barcode , CDR3.aa , Clones , Proportion , V.name , J.name , and D.name will be exported by default. You can use this option to specify the extra columns to be exported. mode \u2014 Either \"single\" for single chain data or \"paired\" forpaired chain data. For single , only TRB chain will be kept at immdata$data , information for other chains will be saved at immdata$tra and immdata$multi . prefix \u2014 The prefix to the barcodes. You can use placeholder like {Sample}_ to use the meta data from the immunarch object. The prefixed barcodes will be saved in out.metatxt . The immunarch object keeps the original barcodes, but the prefix is saved at immdata$prefix . /// Note This option is useful because the barcodes for the cells from scRNA-seq data are usually prefixed with the sample name, for example, Sample1_AAACCTGAGAAGGCTA-1 . However, the barcodes for the cells from scTCR-seq data are usually not prefixed with the sample name, for example, AAACCTGAGAAGGCTA-1 . So we need to add the prefix to the barcodes for the scTCR-seq data, and it is easier for us to integrate the data from different sources later. /// tmpdir \u2014 The temporary directory to link all data files. Immunarch scans a directory to find the data files. If the data files are not in the same directory, we can link them to a temporary directory and pass the temporary directory to Immunarch . This option is useful when the data files are in different directories. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . ImmunarchFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Immunarch - Filter data See https://immunarch.com/articles/web_only/repFilter_v3.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input filterfile \u2014 A config file in TOML.A dict of configurations with keys as the names of the group and values dicts with following keys. See envs.filters immdata \u2014 The data loaded by immunarch::repLoad() Output groupfile \u2014 Also a group file with rownames as cells and column names aseach of the keys in in.filterfile or envs.filters . The values will be subkeys of the dicts in in.filterfile or envs.filters . outfile \u2014 The filtered immdata Envs filters \u2014 The filters to filter the dataYou can have multiple cases (groups), the names will be the keys of this dict, values are also dicts with keys the methods supported by immunarch::repFilter() . There is one more method by.count supported to filter the count matrix. For by.meta , by.repertoire , by.rep , by.clonotype or by.col the values will be passed to .query of repFilter() . You can also use the helper functions provided by immunarch , including morethan , lessthan , include , exclude and interval . If these functions are not used, include(value) will be used by default. For by.count , the value of filter will be passed to dplyr::filter() to filter the count matrix. You can also specify ORDER to define the filtration order, which defaults to 0, higher ORDER gets later executed. Each subkey/subgroup must be exclusive For example: { \"name\": \"BM_Post_Clones\", \"filters\" { \"Top_20\": { \"SAVE\": True, # Save the filtered data to immdata \"by.meta\": {\"Source\": \"BM\", \"Status\": \"Post\"}, \"by.count\": { \"ORDER\": 1, \"filter\": \"TOTAL %%in%% TOTAL[1:20]\" } }, \"Rest\": { \"by.meta\": {\"Source\": \"BM\", \"Status\": \"Post\"}, \"by.count\": { \"ORDER\": 1, \"filter\": \"!TOTAL %%in%% TOTAL[1:20]\" } } } metacols \u2014 The extra columns to be exported to the group file. prefix \u2014 The prefix will be added to the cells in the output filePlaceholders like {Sample}_ can be used to from the meta data Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . Immunarch ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires See https://immunarch.com/articles/web_only/v3_basic_analysis.html After ImmunarchLoading loads the raw data into an immunarch object, this process wraps the functions from immunarch to do the following: Basic statistics, provided by immunarch::repExplore , such as number of clones or distributions of lengths and counts. The clonality of repertoires, provided by immunarch::repClonality The repertoire overlap, provided by immunarch::repOverlap The repertoire overlap, including different clustering procedures and PCA, provided by immunarch::repOverlapAnalysis The distributions of V or J genes, provided by immunarch::geneUsage The diversity of repertoires, provided by immunarch::repDiversity The dynamics of repertoires across time points/samples, provided by immunarch::trackClonotypes The spectratype of clonotypes, provided by immunarch::spectratype The distributions of kmers and sequence profiles, provided by immunarch::getKmers The V-J junction circos plots, implemented within the script of this process. Environment Variable Design: With different sets of arguments, a single function of the above can perform different tasks. For example, repExplore can be used to get the statistics of the size of the repertoire, the statistics of the length of the CDR3 region, or the statistics of the number of the clonotypes. Other than that, you can also have different ways to visualize the results, by passing different arguments to the immunarch::vis function. For example, you can pass .by to vis to visualize the results of repExplore by different groups. Before we explain each environment variable in details in the next section, we will give some examples here to show how the environment variables are organized in order for a single function to perform different tasks. ```toml # Repertoire overlapping [Immunarch.envs.overlaps] # The method to calculate the overlap, passed to `repOverlap` method = \"public\" ``` What if we want to calculate the overlap by different methods at the same time? We can use the following configuration: ```toml [Immunarch.envs.overlaps.cases] Public = { method = \"public\" } Jaccard = { method = \"jaccard\" } ``` Then, the `repOverlap` function will be called twice, once with `method = \"public\"` and once with `method = \"jaccard\"`. We can also use different arguments to visualize the results. These arguments will be passed to the `vis` function: ```toml [Immunarch.envs.overlaps.cases.Public] method = \"public\" vis_args = { \"-plot\": \"heatmap2\" } [Immunarch.envs.overlaps.cases.Jaccard] method = \"jaccard\" vis_args = { \"-plot\": \"heatmap2\" } ``` `-plot` will be translated to `.plot` and then passed to `vis`. If multiple cases share the same arguments, we can use the following configuration: ```toml [Immunarch.envs.overlaps] vis_args = { \"-plot\": \"heatmap2\" } [Immunarch.envs.overlaps.cases] Public = { method = \"public\" } Jaccard = { method = \"jaccard\" } ``` For some results, there are futher analysis that can be performed. For example, for the repertoire overlap, we can perform clustering and PCA (see also <https://immunarch.com/articles/web_only/v4_overlap.html>): ```R imm_ov1 <- repOverlap(immdata$data, .method = \"public\", .verbose = F) repOverlapAnalysis(imm_ov1, \"mds\") %>% vis() repOverlapAnalysis(imm_ov1, \"tsne\") %>% vis() ``` In such a case, we can use the following configuration: ```toml [Immunarch.envs.overlaps] method = \"public\" [Immunarch.envs.overlaps.analyses.cases] MDS = { \"-method\": \"mds\" } TSNE = { \"-method\": \"tsne\" } ``` Then, the `repOverlapAnalysis` function will be called twice on the result generated by `repOverlap(immdata$data, .method = \"public\")`, once with `.method = \"mds\"` and once with `.method = \"tsne\"`. We can also use different arguments to visualize the results. These arguments will be passed to the `vis` function: ```toml [Immunarch.envs.overlaps] method = \"public\" [Immunarch.envs.overlaps.analyses] # See: <https://immunarch.com/reference/vis.immunr_hclust.html> vis_args = { \"-plot\": \"best\" } [Immunarch.envs.overlaps.analyses.cases] MDS = { \"-method\": \"mds\" } TSNE = { \"-method\": \"tsne\" } ``` Generally, you don't need to specify `cases` if you only have one case. A default case will be created for you. For multiple cases, the arguments at the same level as `cases` will be inherited by all cases. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples [Immunarch.envs.kmers] k = 5 [Immunarch.envs.kmers] # Shared by cases k = 5 [Immunarch.envs.kmers.cases] Head5 = { head = 5 , -position = \"stack\" } Head10 = { head = 10 , -position = \"fill\" } Head30 = { head = 30 , -position = \"dodge\" } With motif profiling: [Immunarch.envs.kmers] k = 5 [Immnuarch.envs.kmers.profiles.cases] TextPlot = { method = \"self\" , vis_args = { \"-plot\" : \"text\" } } SeqPlot = { method = \"self\" , vis_args = { \"-plot\" : \"seq\" } } Input immdata \u2014 The data loaded by immunarch::repLoad() metafile \u2014 A cell-level metafile, where the first column must be the cell barcodesthat match the cell barcodes in immdata . The other columns can be any metadata that you want to use for the analysis. The loaded metadata will be left-joined to the converted cell-level data from immdata . This can also be a Seurat object RDS file. If so, the sobj@meta.data will be used as the metadata. Output outdir \u2014 The output directory Envs counts (ns) \u2014 Explore clonotype counts. - by: Groupings when visualize clonotype counts, passed to the .by argument of vis(imm_count, .by = <values>) . Multiple columns should be separated by , . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.counts will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.counts.by , envs.counts.devpars . divs (ns) \u2014 Parameters to control the diversity analysis. - method (choice): The method to calculate diversity. - chao1: a nonparameteric asymptotic estimator of species richness. (number of species in a population). - hill: Hill numbers are a mathematically unified family of diversity indices. (differing only by an exponent q). - div: true diversity, or the effective number of types. It refers to the number of equally abundant types needed for the average proportional abundance of the types to equal that observed in the dataset of interest where all types may not be equally abundant. - gini.simp: The Gini-Simpson index. It is the probability of interspecific encounter, i.e., probability that two entities represent different types. - inv.simp: Inverse Simpson index. It is the effective number of types that is obtained when the weighted arithmetic mean is used to quantify average proportional abundance of types in the dataset of interest. - gini: The Gini coefficient. It measures the inequality among values of a frequency distribution (for example levels of income). A Gini coefficient of zero expresses perfect equality, where all values are the same (for example, where everyone has the same income). A Gini coefficient of one (or 100 percents) expresses maximal inequality among values (for example where only one person has all the income). - d50: The D50 index. It is the number of types that are needed to cover 50%% of the total abundance. - raref: Species richness from the results of sampling through extrapolation. - by: The variables (column names) to group samples. Multiple columns should be separated by , . - plot_type (choice): The type of the plot, works when by is specified. Not working for raref . - box: Boxplot - bar: Barplot with error bars - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - args (type=json): Other arguments for repDiversity() . Do not include the preceding . and use - instead of . in the argument names. For example, do-norm will be compiled to .do.norm . See all arguments at https://immunarch.com/reference/repDiversity.html . - order (list): The order of the values in by on the x-axis of the plots. If not specified, the values will be used as-is. - test (ns): Perform statistical tests between each pair of groups. Does NOT work for raref . - method (choice): The method to perform the test - none: No test - t.test: Welch's t-test - wilcox.test: Wilcoxon rank sum test - padjust (choice): The method to adjust p-values. Defaults to none . - bonferroni: one-step correction - holm: step-down method using Bonferroni adjustments - hochberg: step-up method (independent) - hommel: closed method based on Simes tests (non-negative) - BH: Benjamini & Hochberg (non-negative) - BY: Benjamini & Yekutieli (negative) - fdr: Benjamini & Hochberg (non-negative) - none: no correction. - separate_by: A column name used to separate the samples into different plots. - split_by: A column name used to split the samples into different subplots. Like separate_by , but the plots will be put in the same figure. y-axis will be shared, even if align_y is False or ymin / ymax are not specified. ncol will be ignored. - split_order: The order of the values in split_by on the x-axis of the plots. It can also be used for separate_by to control the order of the plots. Values can be separated by , . - align_x (flag): Align the x-axis of multiple plots. Only works for raref . - align_y (flag): Align the y-axis of multiple plots. - ymin (type=float): The minimum value of the y-axis. The minimum value of the y-axis for plots splitting by separate_by . align_y is forced True when both ymin and ymax are specified. - ymax (type=float): The maximum value of the y-axis. The maximum value of the y-axis for plots splitting by separate_by . align_y is forced True when both ymin and ymax are specified. Works when both ymin and ymax are specified. - log (flag): Indicate whether we should plot with log-transformed x-axis using vis(.log = TRUE) . Only works for raref . - ncol (type=int): The number of columns of the plots. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If NO cases are specified, the default case will be added, with the name of envs.div.method . The values specified in envs.div will be used as the defaults for the cases here. gene_usages (ns) \u2014 Explore gene usages. - top (type=int): How many top (ranked by total usage across samples) genes to show in the plots. Use 0 to use all genes. - norm (flag): If True then use proportions of genes, else use counts of genes. - by: Groupings to show gene usages, passed to the .by argument of vis(imm_gu_top, .by = <values>) . Multiple columns should be separated by , . - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - analyses (ns;order=8): Perform gene usage analyses. - method: The method to control how the data is going to be preprocessed and analysed. One of js , cor , cosine , pca , mds and tsne . Can also be combined with following methods for the actual analyses: hclust , kmeans , dbscan , and kruskal . For example: cosine+hclust . You can also set to none to skip the analyses. See https://immunarch.com/articles/web_only/v5_gene_usage.html . - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.gene_usages.analyses will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.gene_usages.analyses.method , envs.gene_usages.analyses.vis_args and envs.gene_usages.analyses.devpars . - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.gene_usages will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.gene_usages.top , envs.gene_usages.norm , envs.gene_usages.by , envs.gene_usages.vis_args , envs.gene_usages.devpars and envs.gene_usages.analyses . hom_clones (ns) \u2014 Explore homeo clonotypes. - by: Groupings when visualize homeo clones, passed to the .by argument of vis(imm_hom, .by = <values>) . Multiple columns should be separated by , . - marks (ns): A dict with the threshold of the half-closed intervals that mark off clonal groups. Passed to the .clone.types arguments of repClonoality() . The keys could be: - Rare (type=float): the rare clonotypes - Small (type=float): the small clonotypes - Medium (type=float): the medium clonotypes - Large (type=float): the large clonotypes - Hyperexpanded (type=float): the hyperexpanded clonotypes - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.hom_clones will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.hom_clones.by , envs.hom_clones.marks and envs.hom_clones.devpars . kmers (ns) \u2014 Arguments for kmer analysis. - k (type=int): The length of kmer. - head (type=int): The number of top kmers to show. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - profiles (ns;order=8): Arguments for sequence profilings. - method (choice): The method for the position matrix. For more information see https://en.wikipedia.org/wiki/Position_weight_matrix . - freq: position frequency matrix (PFM) - a matrix with occurences of each amino acid in each position. - prob: position probability matrix (PPM) - a matrix with probabilities of each amino acid in each position. - wei: position weight matrix (PWM) - a matrix with log likelihoods of PPM elements. - self: self-information matrix (SIM) - a matrix with self-information of elements in PWM. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.kmers.profiles will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.kmers.profiles.method , envs.kmers.profiles.vis_args and envs.kmers.profiles.devpars . - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the default case will be added, with the name DEFAULT and the values of envs.kmers.k , envs.kmers.head , envs.kmers.vis_args and envs.kmers.devpars . lens (ns) \u2014 Explore clonotype CDR3 lengths. - by: Groupings when visualize clonotype lengths, passed to the .by argument of vis(imm_len, .by = <values>) . Multiple columns should be separated by , . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.lens will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.lens.by , envs.lens.devpars . mutaters (type=json;order=-9) \u2014 The mutaters passed to dplyr::mutate() on expanded cell-level datato add new columns. The keys will be the names of the columns, and the values will be the expressions. The new names can be used in volumes , lens , counts , top_clones , rare_clones , hom_clones , gene_usages , divs , etc. overlaps (ns) \u2014 Explore clonotype overlaps. - method (choice): The method to calculate overlaps. - public: number of public clonotypes between two samples. - overlap: a normalised measure of overlap similarity. It is defined as the size of the intersection divided by the smaller of the size of the two sets. - jaccard: conceptually a percentage of how many objects two sets have in common out of how many objects they have total. - tversky: an asymmetric similarity measure on sets that compares a variant to a prototype. - cosine: a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. - morisita: how many times it is more likely to randomly select two sampled points from the same quadrat (the dataset is covered by a regular grid of changing size) then it would be in the case of a random distribution generated from a Poisson process. Duplicate objects are merged with their counts are summed up. - inc+public: incremental overlaps of the N most abundant clonotypes with incrementally growing N using the public method. - inc+morisita: incremental overlaps of the N most abundant clonotypes with incrementally growing N using the morisita method. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - vis_args (type=json): Other arguments for the plotting functions vis(imm_ov, ...) . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - analyses (ns;order=8): Perform overlap analyses. - method: Plot the samples with these dimension reduction methods. The methods could be hclust , tsne , mds or combination of them, such as mds+hclust . You can also set to none to skip the analyses. They could also be combined, for example, mds+hclust . See https://immunarch.com/reference/repOverlapAnalysis.html . - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.overlaps.analyses will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.overlaps.analyses.method , envs.overlaps.analyses.vis_args and envs.overlaps.analyses.devpars . - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.overlaps will be used. If NO cases are specified, the default case will be added, with the key the default method and the values of envs.overlaps.method , envs.overlaps.vis_args , envs.overlaps.devpars and envs.overlaps.analyses . prefix \u2014 The prefix to the barcodes. You can use placeholder like {Sample}_ The prefixed barcodes will be used to match the barcodes in in.metafile . Not used if in.metafile is not specified. If None (default), immdata$prefix will be used. rare_clones (ns) \u2014 Explore rare clonotypes. - by: Groupings when visualize rare clones, passed to the .by argument of vis(imm_rare, .by = <values>) . Multiple columns should be separated by , . - marks (list;itype=int): A numerical vector with ranges of abundance for the rare clonotypes in the dataset. Passed to the .bound argument of repClonoality() . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.rare_clones will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.rare_clones.by , envs.rare_clones.marks and envs.rare_clones.devpars . spects (ns) \u2014 Spectratyping analysis. - quant: Select the column with clonal counts to evaluate. Set to id to count every clonotype once. Set to count to take into the account number of clones per clonotype. Multiple columns should be separated by , . - col: A string that specifies the column(s) to be processed. The output is one of the following strings, separated by the plus sign: \"nt\" for nucleotide sequences, \"aa\" for amino acid sequences, \"v\" for V gene segments, \"j\" for J gene segments. E.g., pass \"aa+v\" for spectratyping on CDR3 amino acid sequences paired with V gene segments, i.e., in this case a unique clonotype is a pair of CDR3 amino acid and V gene segment. Clonal counts of equal clonotypes will be summed up. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.spects will be used. By default, a By_Clonotype case will be added, with the values of quant = \"id\" and col = \"nt\" , and a By_Num_Clones case will be added, with the values of quant = \"count\" and col = \"aa+v\" . top_clones (ns) \u2014 Explore top clonotypes. - by: Groupings when visualize top clones, passed to the .by argument of vis(imm_top, .by = <values>) . Multiple columns should be separated by , . - marks (list;itype=int): A numerical vector with ranges of the top clonotypes. Passed to the .head argument of repClonoality() . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.top_clones will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.top_clones.by , envs.top_clones.marks and envs.top_clones.devpars . trackings (ns) \u2014 Parameters to control the clonotype tracking analysis. - targets: Either a set of CDR3AA seq of clonotypes to track (separated by , ), or simply an integer to track the top N clonotypes. - subject_col: The column name in meta data that contains the subjects/samples on the x-axis of the alluvial plot. If the values in this column are not unique, the values will be merged with the values in subject_col to form the x-axis. This defaults to Sample . - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - subjects (list): A list of values from subject_col to show in the alluvial plot on the x-axis. If not specified, all values in subject_col will be used. This also specifies the order of the x-axis. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments ( target , subject_col , and subjects ). If any of these arguments are not specified, the values in envs.trackings will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.trackings.target , envs.trackings.subject_col , and envs.trackings.subjects . vj_junc (ns) \u2014 Arguments for VJ junction circos plots.This analysis is not included in immunarch . It is a separate implementation using circlize . - by: Groupings to show VJ usages. Typically, this is the Sample column, so that the VJ usages are shown for each sample. But you can also use other columns, such as Subject to show the VJ usages for each subject. Multiple columns should be separated by , . - by_clones (flag): If True, the VJ usages will be calculated based on the distinct clonotypes, instead of the individual cells. - subset: Subset the data before plotting VJ usages. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data, which will affect the VJ usages at cell level (by_clones=False). - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.vj_junc will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.vj_junc.by , envs.vj_junc.by_clones envs.vj_junc.subset and envs.vj_junc.devpars . volumes (ns) \u2014 Explore clonotype volume (sizes). - by: Groupings when visualize clonotype volumes, passed to the .by argument of vis(imm_vol, .by = <values>) . Multiple columns should be separated by , . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.volumes will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.volume.by , envs.volume.devpars . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . SampleDiversity ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Sample diversity and rarefaction analysis This is part of Immunarch, in case we have multiple dataset to compare. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() Output outdir \u2014 The output directory Envs devpars \u2014 The parameters for the plotting deviceIt is a dict, and keys are the methods and values are dicts with width, height and res that will be passed to png() If not provided, 1000, 1000 and 100 will be used. div_methods \u2014 Methods to calculate diversitiesIt is a dict, keys are the method names, values are the groupings. Each one is a case, multiple columns for a case are separated by , For example: {\"div\": [\"Status\", \"Sex\", \"Status,Sex\"]} will run true diversity for samples grouped by Status , Sex , and both. The diversity for each sample without grouping will also be added anyway. Supported methods: chao1 , hill , div , gini.simp , inv.simp , gini , and raref . See also https://immunarch.com/articles/web_only/v6_diversity.html . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . CloneResidency ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Identification of clone residency This process is used to investigate the residency of clones in groups, typically two samples (e.g. tumor and normal) from the same patient. But it can be used for any two groups of clones. There are three types of output from this process Count tables of the clones in the two groups CDR3_aa Tumor Normal CASSYGLSWGSYEQYF 306 55 CASSVTGAETQYF 295 37 CASSVPSAHYNEQFF 197 9 ... ... ... Residency plots showing the residency of clones in the two groups The points in the plot are jittered to avoid overplotting. The x-axis is the residency in the first group and the y-axis is the residency in the second group. The size of the points are relative to the normalized size of the clones. You may identify different types of clones in the plot based on their residency in the two groups: Collapsed (The clones that are collapsed in the second group) Dual (The clones that are present in both groups with equal size) Expanded (The clones that are expanded in the second group) First Group Multiplet (The clones only in the First Group with size > 1) Second Group Multiplet (The clones only in the Second Group with size > 1) First Group Singlet (The clones only in the First Group with size = 1) Second Group Singlet (The clones only in the Second Group with size = 1) This idea is borrowed from this paper: Wu, Thomas D., et al. \"Peripheral T cell expansion predicts tumour infiltration and clinical response.\" Nature 579.7798 (2020): 274-278. Venn diagrams showing the overlap of the clones in the two groups {: width=\"60%\"} Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() metafile \u2014 A cell-level metafile, where the first column must be the cell barcodesthat match the cell barcodes in immdata . The other columns can be any metadata that you want to use for the analysis. The loaded metadata will be left-joined to the converted cell-level data from immdata . This can also be a Seurat object RDS file. If so, the sobj@meta.data will be used as the metadata. Output outdir \u2014 The output directory Envs cases (type=json) \u2014 If you have multiple cases, you can use this argumentto specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments. If no cases are specified, the default case will be added, with the name DEFAULT and the values of envs.subject , envs.group , envs.order and envs.section . These values are also the defaults for the other cases. group \u2014 The key of group in metadata. This usually marks the samplesthat you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, instead of venn diagram, upset plots will be used. mutaters (type=json) \u2014 The mutaters passed to dplyr::mutate() onthe cell-level data converted from in.immdata . If in.metafile is provided, the mutaters will be applied to the joined data. The keys will be the names of the new columns, and the values will be the expressions. The new names can be used in subject , group , order and section . order (list) \u2014 The order of the values in group . In scatter/residency plots, X in X,Y will be used as x-axis and Y will be used as y-axis. You can also have multiple orders. For example: [\"X,Y\", \"X,Z\"] . If you only have two groups, you can set order = [\"X\", \"Y\"] , which will be the same as order = [\"X,Y\"] . prefix \u2014 The prefix of the cell barcodes in the Seurat object. section \u2014 How the subjects aligned in the report. Multiple subjects withthe same value will be grouped together. Useful for cohort with large number of samples. subject (list) \u2014 The key of subject in metadata. The cloneresidency will be examined for this subject/patient subset \u2014 The filter passed to dplyr::filter() to filter the data for the cellsbefore calculating the clone residency. For example, Clones > 1 to filter out singletons. upset_trans \u2014 The transformation to apply to the y axis of upset bar plots.For example, log10 or sqrt . If not specified, the y axis will be plotted as is. Note that the position of the bar plots will be dodged instead of stacked when the transformation is applied. See also https://github.com/tidyverse/ggplot2/issues/3671 upset_ymax \u2014 The maximum value of the y-axis in the upset bar plots. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . Immunarch2VDJtools ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert immuarch format into VDJtools input formats. This process converts the immunarch object to the VDJtools input files, in order to perform the VJ gene usage analysis by VJUsage process. This process will generally generate a tab-delimited file for each sample, with the following columns. count : The number of reads for this clonotype frequency : The frequency of this clonotype CDR3nt : The nucleotide sequence of the CDR3 region CDR3aa : The amino acid sequence of the CDR3 region V : The V gene D : The D gene J : The J gene See also: https://vdjtools-doc.readthedocs.io/en/master/input.html#vdjtools-format . This process has no environment variables. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() Output outdir \u2014 The output directory containing the vdjtools input for eachsample Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . ImmunarchSplitIdents ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Split the data into multiple immunarch datasets by Idents from Seurat Note that only the cells in both the immdata and sobjfile will be kept. Requires immunarch >= 0.9.0 to use select_clusters() Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() sobjfile \u2014 The Seurat object file.You can set a different ident by Idents(sobj) <- \"new_ident\" to split the data by the new ident, where \"new_ident\" is the an existing column in meta data Output outdir \u2014 The output directory containing the RDS files of the splittedimmunarch datasets Envs prefix \u2014 The prefix of the cell barcodes in the Seurat object.Once could use a fixed prefix, or a placeholder with the column name in meta data. For example, \"{Sample}_\" will replace the placeholder with the value of the column Sample in meta data. sample_col \u2014 The column name in meta data that contains the sample name Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . VJUsage ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions using vdjtools. This process performs the VJ gene usage analysis using VDJtools . It wraps the PlotFancyVJUsage command in VDJtools . The output will be a V-J junction circos plot for a single sample. Arcs correspond to different V and J segments, scaled to their frequency in sample. Ribbons represent V-J pairings and their size is scaled to the pairing frequency (weighted in present case). {: width=\"80%\" } Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file, in vdjtools input format Output outfile \u2014 The V-J usage plot Envs vdjtools \u2014 The path to the VDJtools executable. vdjtools_patch (hidden) \u2014 The patch file for VDJtools . It's delivered with the pipeline ([ biopipen ][3] package). * You don't need to provide this file, unless you want to use a different patch file by yourself. * See the issue with VDJtools here . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . Attach2Seurat ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Attach the clonal information to a Seurat object as metadata Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immfile \u2014 The immunarch object in RDS sobjfile \u2014 The Seurat object file in RDS Output outfile \u2014 The Seurat object with the clonal information as metadata Envs metacols \u2014 Which meta columns to attach prefix \u2014 The prefix to the barcodes. You can use placeholder like {Sample}_ to use the meta data from the immunarch object Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . TCRClustering ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Cluster the TCR clones by their CDR3 sequences This process is used to cluster TCR clones based on their CDR3 sequences. It uses either GIANA Zhang, Hongyi, Xiaowei Zhan, and Bo Li. \"GIANA allows computationally-efficient TCR clustering and multi-disease repertoire classification by isometric transformation.\" Nature communications 12.1 (2021): 1-11. Or ClusTCR Sebastiaan Valkiers, Max Van Houcke, Kris Laukens, Pieter Meysman, ClusTCR: a Python interface for rapid clustering of large sets of CDR3 sequences with unknown antigen specificity, Bioinformatics, 2021. Both methods are based on the Faiss Clustering Library , for efficient similarity search and clustering of dense vectors, so both methods yield similar results. A text file will be generated with the cluster assignments for each cell, together with the immunarch object (in R ) with the cluster assignments at TCR_Clsuter column. This information will then be merged to a Seurat object for further downstream analysis. The cluster assignments are prefixed with S_ or M_ to indicate whether a cluster has only one unique CDR3 sequence or multiple CDR3 sequences. Note that a cluster with S_ prefix may still have multiple cells, as the same CDR3 sequence may be shared by multiple cells. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input screpfile \u2014 The TCR data object loaded by scRepertoire::CombineTCR() or scRepertoire::CombineExpression() Output outfile \u2014 The scRepertoire object in qs with TCR cluster information.Column TCR_Cluster will be added to the metadata. Envs args (type=json) \u2014 The arguments for the clustering toolFor GIANA, they will be passed to python GIAna.py See https://github.com/s175573/GIANA#usage . For ClusTCR, they will be passed to clustcr.Clustering(...) See https://svalkiers.github.io/clusTCR/docs/clustering/how-to-use.html#clustering . chain (choice) \u2014 The TCR chain to use for clustering. - alpha: TCR alpha chain (the first sequence in CTaa, separated by _ ) - beta: TCR beta chain (the second sequence in CTaa, separated by _ ) - both: Both TCR alpha and beta chains python \u2014 The path of python with GIANA 's dependencies installedor with clusTCR installed. Depending on the tool you choose. tool (choice) \u2014 The tool used to do the clustering, either GIANA or ClusTCR . For GIANA, using TRBV mutations is not supported - GIANA: by Li lab at UT Southwestern Medical Center - ClusTCR: by Sebastiaan Valkiers, etc within_sample (flag) \u2014 Whether to cluster the TCR clones within each sample.When in.screpfile is a Seurat object, the samples are marked by the Sample column in the metadata. Requires clusTCR \u2014 if: {{ proc.envs.tool == 'ClusTCR' }} check: {{ proc.envs.python }} -c \"import clustcr\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . TCRClusterStats ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Statistics of TCR clusters, generated by TCRClustering . The statistics include The number of cells in each cluster (cluster size) Sample diversity using TCR clusters instead of TCR clones Shared TCR clusters between samples Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples Cluster size [TCRClusterStats.envs.cluster_size] by = \"Sample\" {: width=\"80%\"} Shared clusters [TCRClusterStats.envs.shared_clusters] numbers_on_heatmap = true heatmap_meta = [ \"region\" ] {: width=\"80%\"} Sample diversity [TCRClusterStats.envs.sample_diversity] method = \"gini\" {: width=\"80%\"} Compared to the sample diversity using TCR clones: {: width=\"80%\"} Input immfile \u2014 The immunarch object with TCR clusters attached Output outdir \u2014 The output directory containing the stats and reports Envs cluster_size (ns) \u2014 The distribution of size of each cluster. - by: The variables (column names) used to fill the histogram. Only a single column is supported. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.cluster_size will be used. If NO cases are specified, the default case will be added, with the name DEFAULT . sample_diversity (ns) \u2014 Sample diversity using TCR clusters instead ofclones. - by: The variables (column names) to group samples. Multiple columns should be separated by , . - method (choice): The method to calculate diversity. - gini: The Gini coefficient. It measures the inequality among values of a frequency distribution (for example levels of income). - gini.simp: The Gini-Simpson index. It is the probability of interspecific encounter, i.e., probability that two entities represent different types. - inv.simp: Inverse Simpson index. It is the effective number of types that is obtained when the weighted arithmetic mean is used to quantify average proportional abundance of types in the dataset of interest. - div: true diversity, or the effective number of types. It refers to the number of equally abundant types needed for the average proportional abundance of the types to equal that observed in the dataset of interest where all types may not be equally abundant. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.sample_diversity will be used. If NO cases are specified, the default case will be added, with the name DEFAULT . shared_clusters (ns) \u2014 Stats about shared TCR clusters - numbers_on_heatmap (flag): Whether to show the numbers on the heatmap. - heatmap_meta (list): The columns of metadata to show on the heatmap. - cluster_rows (flag): Whether to cluster the rows on the heatmap. - sample_order: The order of the samples on the heatmap. Either a string separated by , or a list of sample names. This only works for columns if cluster_rows is True . - grouping: The groups to investigate the shared clusters. If specified, venn diagrams will be drawn instead of heatmaps. In such case, numbers_on_heatmap and heatmap_meta will be ignored. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.shared_clusters will be used. If NO cases are specified, the default case will be added, with the name DEFAULT . Requires r-immunarch \u2014 check: {{proc.lang}} -e \"library(immunarch)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . CloneSizeQQPlot ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc QQ plot of the clone sizes QQ plots for clones sizes of pairs of samples Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() Output outdir \u2014 The output directory Envs diag \u2014 Whether to draw the diagonal line in the QQ plot group \u2014 The key of group in metadata. This usually marks the samplesthat you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, for example, [A, B, C], the QQ plots will be generated for all the combinations of 2 groups, i.e., [A, B], [A, C], [B, C] on \u2014 The key of the metadata to use for the QQ plot. One/Both of [\"Clones\", \"Proportion\"] order \u2014 The order of the values in group . Early-ordered group willbe used as x-axis in scatter plots If there are more than 2 groups, for example, [A, B, C], the QQ plots will be drawn for pairs: B ~ A, C ~ B. subject \u2014 The key of subject in metadata, defining the pairs.The clone residency will be examined for this subject/patient Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . CDR3AAPhyschem ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc CDR3 AA physicochemical feature analysis The idea is to perform a regression between two groups of cells (e.g. Treg vs Tconv) at different length of CDR3 AA sequences. The regression will be performed for each physicochemical feature of the AA (hydrophobicity, volume and isolectric point). Reference: - Stadinski, Brian D., et al. \"Hydrophobic CDR3 residues promote the development of self-reactive T cells.\" Nature immunology 17.8 (2016): 946-955. - Lagattuta, Kaitlyn A., et al. \"Repertoire analyses reveal T cell antigen receptor sequence features that influence T cell fate.\" Nature immunology 23.3 (2022): 446-457. - Wimley, W. C. & White, S. H. Experimentally determined hydrophobicity scale for proteins at membrane - interfaces. Nat. Struct. Biol. 3, 842-848 (1996). - Handbook of chemistry & physics 72nd edition. (CRC Press, 1991). - Zamyatnin, A. A. Protein volume in solution. Prog. Biophys. Mol. Biol. 24, 107-123 (1972). Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input scrfile \u2014 The data loaded by ScRepCombiningExpression , saved in RDS or qs/qs2 format.The data is actually generated by scRepertiore::combineExpression() . The data must have both TRA and TRB chains. Output outdir \u2014 The output directory Envs comparison (type=auto) \u2014 A dict of two groups, with keys as thegroup names and values as the group labels. For example, Treg = [ \"CD4 CTL\" , \"CD4 Naive\" , \"CD4 TCM\" , \"CD4 TEM\" ] Tconv = \"Tconv\" Or simply a list of two groups, for example, [\"Treg\", \"Tconv\"] when they are both in the group column. each (auto) \u2014 A column, or a list of columns or a string of columns separated by comma.The columns will be used to split the data into multiple groups and the regression will be applied to each group separately. If not provided, all the cells will be used. group \u2014 The key of group in metadata to define the groups tocompare. For example, CellType , which has cell types annotated for each cell in the combined object (immdata + Seurat metadata) target \u2014 Which group to use as the target group. The targetgroup will be labeled as 1, and the other group will be labeled as 0 in the regression. If not specified, the first group in comparison will be used as the target group. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . TESSA ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Tessa is a Bayesian model to integrate T cell receptor (TCR) sequenceprofiling with transcriptomes of T cells. Enabled by the recently developed single cell sequencing techniques, which provide both TCR sequences and RNA sequences of each T cell concurrently, Tessa maps the functional landscape of the TCR repertoire, and generates insights into understanding human immune response to diseases. As the first part of tessa, BriseisEncoder is employed prior to the Bayesian algorithm to capture the TCR sequence features and create numerical embeddings. We showed that the reconstructed Atchley Factor matrices and CDR3 sequences, generated through the numerical embeddings, are highly similar to their original counterparts. The CDR3 peptide sequences are constructed via a RandomForest model applied on the reconstructed Atchley Factor matrices. See https://github.com/jcao89757/TESSA When finished, two columns will be added to the meta.data of the Seurat object: TESSA_Cluster : The cluster assignments from TESSA. TESSA_Cluster_Size : The number of cells in each cluster. These columns can be then used for further downstream analysis to explore the functional landscape of the TCR repertoire. Reference: - 'Mapping the Functional Landscape of TCR Repertoire.', Zhang, Z., Xiong, D., Wang, X. et al. 2021. link - 'Deep learning-based prediction of the T cell receptor-antigen binding specificity.', Lu, T., Zhang, Z., Zhu, J. et al. 2021. link Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input screpdata \u2014 The data loaded by ScRepCombiningExpression , saved in RDS orqs/qs2 format. The data is actually generated by scRepertiore::combineExpression() . The data must have both TRA and TRB chains. Output outfile \u2014 a qs fileof a Seurat object, with TESSA_Cluster and TESSA_Cluster_Size added to the meta.data Envs assay \u2014 Which assay to use to extract the expression matrix.Only works if in.srtobj is an RDS file of a Seurat object. By default, if SCTransform is performed, SCT will be used. max_iter (type=int) \u2014 The maximum number of iterations for MCMC. predefined_b (flag) \u2014 Whether use the predefined b or not.Please check the paper of tessa for more details about the b vector. If True, the tessa will not update b in the MCMC iterations. python \u2014 The path of python with TESSA 's dependencies installed save_tessa (flag) \u2014 Save tessa detailed results to seurat object?It will be saved to sobj@misc$tessa . within_sample (flag) \u2014 Whether the TCR networks are constructed onlywithin TCRs from the same sample/patient (True) or with all the TCRs in the meta data matrix (False). Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . TCRDock ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Using TCRDock to predict the structure of MHC-peptide-TCR complexes See https://github.com/phbradley/TCRdock . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 The config file for TCRDockIt's should be a toml file with the keys listed in envs , including organism , mhc_class , mhc , peptide , va , ja , vb , jb , cdr3a , and cdr3b . The values will overwrite the values in envs . Output outdir \u2014 The output directory containing the results Envs cdr3a \u2014 The CDR3 alpha sequence cdr3b \u2014 The CDR3 beta sequence data_dir \u2014 The data directory that contains the model files.The model files should be in the params subdirectory. ja \u2014 The J alpha gene jb \u2014 The J beta gene mhc \u2014 The MHC allele, e.g., A*02:01 mhc_class (type=int) \u2014 The MHC class, either 1 or 2 model_file \u2014 The model file to use.If provided as a relative path, it should be relative to the <envs.data_dir>/params/ , otherwise, it should be the full path. model_name \u2014 The model name to use organism \u2014 The organism of the TCR, peptide and MHC peptide \u2014 The peptide sequence python \u2014 The path of python with dependencies for tcrdock installed.If not provided, TCRDock.lang will be used (the same interpreter used for the wrapper script). It could also be a list to specify, for example, a python in a conda environment (e.g., [\"conda\", \"run\", \"-n\", \"myenv\", \"python\"] ). tcrdock \u2014 The path to the tcrdock source code repo.You need to clone the source code from the github repository. https://github.com/phbradley/TCRdock at revision c5a7af42eeb0c2a4492a4d4fe803f1f9aafb6193 at main branch. You also have to run download_blast.py after cloning to download the blast database in the directory. If not provided, we will clone the source code to the envs.tmpdir directory and run the download_blast.py script. tmpdir \u2014 The temporary directory used to clone the tcrdock source code if envs.tcrdock is not provided. va \u2014 The V alpha gene vb \u2014 The V beta gene Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . ScRepLoading ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Load the single cell TCR/BCR data into a scRepertoire compatible object This process loads the single cell TCR/BCR data into a scRepertoire (>= v2.0.8, < v2.3.2) compatible object. Later, scRepertoire::combineExpression can be used to combine the expression data with the TCR/BCR data. For the data path specified at TCRData / BCRData in the input file ( in.metafile ), will be used to find the TCR/BCR data files and scRepertoire::loadContigs() will be used to load the data. A directory can be specified in TCRData / BCRData , then scRepertoire::loadContigs() will be used directly to load the data from the directory. Otherwise if a file is specified, it will be symbolically linked to a directory for scRepertoire::loadContigs() to load. Note that when the file name can not be recognized by scRepertoire::loadContigs() , envs.format must be set for the correct format of the data. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The meta data of the samplesA tab-delimited file Two columns are required: * Sample to specify the sample names. * TCRData / BCRData to assign the path of the data to the samples, and this column will be excluded as metadata. Output outfile \u2014 The scRepertoire compatible object in qs/qs2 format Envs combineBCR (type=json) \u2014 The extra arguments for scRepertoire::combineBCR function. See also https://www.borch.dev/uploads/screpertoire/reference/combinebcr combineTCR (type=json) \u2014 The extra arguments for scRepertoire::combineTCR function. See also https://www.borch.dev/uploads/screpertoire/reference/combinetcr exclude (auto) \u2014 The columns to exclude from the metadata to add to the object.A list of column names to exclude or a string with column names separated by , . By default, BCRData , TCRData and RNAData will be excluded. format (choice) \u2014 The format of the TCR/BCR data files. - 10X: 10X Genomics data, which is usually in a directory with filtered_contig_annotations.csv file. - AIRR: AIRR format, which is usually in a file with airr_rearrangement.tsv file. - BD: Becton Dickinson data, which is usually in a file with Contigs_AIRR.tsv file. - Dandelion: Dandelion data, which is usually in a file with all_contig_dandelion.tsv file. - Immcantation: Immcantation data, which is usually in a file with data.tsv file. - JSON: JSON format, which is usually in a file with .json extension. - ParseBio: ParseBio data, which is usually in a file with barcode_report.tsv file. - MiXCR: MiXCR data, which is usually in a file with clones.tsv file. - Omniscope: Omniscope data, which is usually in a file with .csv extension. - TRUST4: TRUST4 data, which is usually in a file with barcode_report.tsv file. - WAT3R: WAT3R data, which is usually in a file with barcode_results.csv file. See also: https://rdrr.io/github/ncborcherding/scRepertoire/man/loadContigs.html If not provided, the format will be guessed from the file name by scRepertoire::loadContigs() . tmpdir \u2014 The temporary directory to store the symbolic links to theTCR/BCR data files. type (choice) \u2014 The type of the data to load. - TCR: T cell receptor data - BCR: B cell receptor data - auto: Automatically detect the type from the metadata. If auto is selected, the type will be determined by the presence of TCRData or BCRData columns in the metadata. If both columns are present, TCR will be selected by default. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . ScRepCombiningExpression ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Combine the scTCR/BCR data with the expression data This process combines the scTCR/BCR data with the expression data using scRepertoire::combineExpression function. The expression data should be in Seurat format. The scRepertoire object should be a combined contig object, usually generated by scRepertoire::combineTCR or scRepertoire::combineBCR . See also: https://www.borch.dev/uploads/screpertoire/reference/combineexpression . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input screpfile \u2014 The scRepertoire object in RDS/qs format srtobj \u2014 The Seurat object, saved in RDS/qs format Output outfile \u2014 The Seurat object with the TCR/BCR data combinedIn addition to the meta columns added by scRepertoire::combineExpression() , a new column VDJ_Presence will be added to the metadata. It indicates whether the cell has a TCR/BCR sequence or not. The value is TRUE if the cell has a TCR/BCR sequence, and FALSE otherwise. Envs addLabel (flag) \u2014 This will add a label to the frequency header, allowing theuser to try multiple group_by variables or recalculate frequencies after subsetting the data. chain \u2014 indicate if both or a specific chain should be usede.g. \"both\", \"TRA\", \"TRG\", \"IGH\", \"IGL\". cloneCall \u2014 How to call the clone - VDJC gene (gene), CDR3 nucleotide (nt),CDR3 amino acid (aa), VDJC gene + CDR3 nucleotide (strict) or a custom variable in the data. cloneSize (type=json) \u2014 The bins for the grouping based on proportion orfrequency. If proportion is FALSE and the cloneSizes are not set high enough based on frequency, the upper limit of cloneSizes will be automatically updated. filterNA (flag) \u2014 Method to subset Seurat/SCE object of barcodes without cloneinformation group_by \u2014 The column label in the combined clones in which clone frequency willbe calculated. NULL or \"none\" will keep the format of input.data. proportion (flag) \u2014 Whether to proportion (TRUE) or total frequency (FALSE) ofthe clone based on the group_by variable. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.tcr . ClonalStats ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Visualize the clonal information. Using scplotter to visualize the clonal information. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input screpfile \u2014 The scRepertoire object in RDS/qs format Output outdir \u2014 The output directory containing the plots Envs \u2014 The arguments for the plot functionSee the documentation of the corresponding plot function for the details cases (type=json) \u2014 The cases to generate the plots if we have multiple cases.The keys are the names of the cases, and the values are the arguments for the plot function. The arguments in envs will be used if not specified in cases , except for mutaters . Sections can be specified as the prefix of the case name, separated by :: . For example, if you have a case named Clonal Volume::Case1 , the plot will be put in the section Clonal Volume . By default, when there are multiple cases for the same 'viz_type', the name of the 'viz_type' will be used as the default section name (for example, when 'viz_type' is 'volume', the section name will be 'Clonal Volume'). When there is only a single case, the section name will default to 'DEFAULT', which will not be shown in the report. descr \u2014 The description of the plot, used to show in the report. devpars (ns) \u2014 The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device more_formats (list) \u2014 The extra formats to save the plots in, other than PNG. mutaters (type=json;order=-9) \u2014 The mutaters passed to dplyr::mutate() to add new variables.When the object loaded form in.screpfile is a list, the mutaters will be applied to each element. The keys are the names of the new variables, and the values are the expressions. When it is a Seurat object, typically an output of scRepertoire::combineExpression() , the mutaters will be applied to the meta.data . save_code (flag) \u2014 Whether to save the code used to generate the plotsNote that the data directly used to generate the plots will also be saved in an rda file. Be careful if the data is large as it may take a lot of disk space. subset \u2014 An expression to subset the data before plotting.Similar to mutaters , it will be applied to each element by dplyr::filter() if the object loaded form in.screpfile is a list; otherwise, it will be applied to subset(sobj, subset = <expr>) if the object is a Seurat object. viz_type (choice) \u2014 The type of visualization to generate. - volume: The volume of the clones using ClonalVolumePlot - abundance: The abundance of the clones using ClonalAbundancePlot - length: The length of the CDR3 sequences using ClonalLengthPlot - residency: The residency of the clones using ClonalResidencyPlot - dynamics: The dynamics of the clones using ClonalDynamicsPlot - composition: The composition of the clones using ClonalCompositionPlot - overlap: The overlap of the clones using ClonalOverlapPlot - diversity: The diversity of the clones using ClonalDiversityPlot - geneusage: The gene usage of the clones using ClonalGeneUsagePlot - positional: The positional information of the clones using ClonalPositionalPlot - kmer: The kmer information of the clones using ClonalKmerPlot - rarefaction: The rarefaction curve of the clones using ClonalRarefactionPlot Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.tcr"},{"location":"api/biopipen.ns.tcr/#biopipennstcr","text":"</> Tools to analyze single-cell TCR sequencing data Classes ImmunarchLoading ( Proc ) \u2014 Immuarch - Loading data </> ImmunarchFilter ( Proc ) \u2014 Immunarch - Filter data </> Immunarch ( Proc ) \u2014 Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires </> SampleDiversity ( Proc ) \u2014 Sample diversity and rarefaction analysis </> CloneResidency ( Proc ) \u2014 Identification of clone residency </> Immunarch2VDJtools ( Proc ) \u2014 Convert immuarch format into VDJtools input formats. </> ImmunarchSplitIdents ( Proc ) \u2014 Split the data into multiple immunarch datasets by Idents from Seurat </> VJUsage ( Proc ) \u2014 Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions using vdjtools. </> Attach2Seurat ( Proc ) \u2014 Attach the clonal information to a Seurat object as metadata </> TCRClustering ( Proc ) \u2014 Cluster the TCR clones by their CDR3 sequences </> TCRClusterStats ( Proc ) \u2014 Statistics of TCR clusters, generated by TCRClustering . </> CloneSizeQQPlot ( Proc ) \u2014 QQ plot of the clone sizes </> CDR3AAPhyschem ( Proc ) \u2014 CDR3 AA physicochemical feature analysis </> TESSA ( Proc ) \u2014 Tessa is a Bayesian model to integrate T cell receptor (TCR) sequenceprofiling with transcriptomes of T cells. </> TCRDock ( Proc ) \u2014 Using TCRDock to predict the structure of MHC-peptide-TCR complexes </> ScRepLoading ( Proc ) \u2014 Load the single cell TCR/BCR data into a scRepertoire compatible object </> ScRepCombiningExpression ( Proc ) \u2014 Combine the scTCR/BCR data with the expression data </> ClonalStats ( Proc ) \u2014 Visualize the clonal information. </> class","title":"biopipen.ns.tcr"},{"location":"api/biopipen.ns.tcr/#biopipennstcrimmunarchloading","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Immuarch - Loading data Load the raw data into immunarch object, using immunarch::repLoad() . For the data path specified at TCRData in the input file, we will first find filtered_contig_annotations.csv and filtered_config_annotations.csv.gz in the path. If neighter of them exists, we will find all_contig_annotations.csv and all_contig_annotations.csv.gz in the path and a warning will be raised (You can find it at ./.pipen/<pipeline-name>/ImmunarchLoading/0/job.stderr ). If none of the files exists, an error will be raised. This process will also generate a text file with the information for each cell. The file will be saved at ./.pipen/<pipeline-name>/ImmunarchLoading/0/output/<prefix>.tcr.txt . The file can be used by the SeuratMetadataMutater process to integrate the TCR-seq data into the Seurat object for further integrative analysis. envs.metacols can be used to specify the columns to be exported to the text file. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The meta data of the samplesA tab-delimited file Two columns are required: * Sample to specify the sample names. * TCRData to assign the path of the data to the samples, and this column will be excluded as metadata.Immunarch is able to fetch the sample names from the names of the target files. However, 10x data yields result like filtered_contig_annotations.csv , which doesn't have any name information. Output metatxt \u2014 The meta data at cell level, which can be used to attach to the Seurat object rdsfile \u2014 The RDS file with the data and metadata, which can be processed byother immunarch functions. Envs extracols (list) \u2014 The extra columns to be exported to the text file.You can refer to the immunarch documentation to get a sense for the full list of the columns. The columns may vary depending on the data source. The columns from immdata$meta and some core columns, including Barcode , CDR3.aa , Clones , Proportion , V.name , J.name , and D.name will be exported by default. You can use this option to specify the extra columns to be exported. mode \u2014 Either \"single\" for single chain data or \"paired\" forpaired chain data. For single , only TRB chain will be kept at immdata$data , information for other chains will be saved at immdata$tra and immdata$multi . prefix \u2014 The prefix to the barcodes. You can use placeholder like {Sample}_ to use the meta data from the immunarch object. The prefixed barcodes will be saved in out.metatxt . The immunarch object keeps the original barcodes, but the prefix is saved at immdata$prefix . /// Note This option is useful because the barcodes for the cells from scRNA-seq data are usually prefixed with the sample name, for example, Sample1_AAACCTGAGAAGGCTA-1 . However, the barcodes for the cells from scTCR-seq data are usually not prefixed with the sample name, for example, AAACCTGAGAAGGCTA-1 . So we need to add the prefix to the barcodes for the scTCR-seq data, and it is easier for us to integrate the data from different sources later. /// tmpdir \u2014 The temporary directory to link all data files. Immunarch scans a directory to find the data files. If the data files are not in the same directory, we can link them to a temporary directory and pass the temporary directory to Immunarch . This option is useful when the data files are in different directories. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.ImmunarchLoading"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrimmunarchfilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Immunarch - Filter data See https://immunarch.com/articles/web_only/repFilter_v3.html Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input filterfile \u2014 A config file in TOML.A dict of configurations with keys as the names of the group and values dicts with following keys. See envs.filters immdata \u2014 The data loaded by immunarch::repLoad() Output groupfile \u2014 Also a group file with rownames as cells and column names aseach of the keys in in.filterfile or envs.filters . The values will be subkeys of the dicts in in.filterfile or envs.filters . outfile \u2014 The filtered immdata Envs filters \u2014 The filters to filter the dataYou can have multiple cases (groups), the names will be the keys of this dict, values are also dicts with keys the methods supported by immunarch::repFilter() . There is one more method by.count supported to filter the count matrix. For by.meta , by.repertoire , by.rep , by.clonotype or by.col the values will be passed to .query of repFilter() . You can also use the helper functions provided by immunarch , including morethan , lessthan , include , exclude and interval . If these functions are not used, include(value) will be used by default. For by.count , the value of filter will be passed to dplyr::filter() to filter the count matrix. You can also specify ORDER to define the filtration order, which defaults to 0, higher ORDER gets later executed. Each subkey/subgroup must be exclusive For example: { \"name\": \"BM_Post_Clones\", \"filters\" { \"Top_20\": { \"SAVE\": True, # Save the filtered data to immdata \"by.meta\": {\"Source\": \"BM\", \"Status\": \"Post\"}, \"by.count\": { \"ORDER\": 1, \"filter\": \"TOTAL %%in%% TOTAL[1:20]\" } }, \"Rest\": { \"by.meta\": {\"Source\": \"BM\", \"Status\": \"Post\"}, \"by.count\": { \"ORDER\": 1, \"filter\": \"!TOTAL %%in%% TOTAL[1:20]\" } } } metacols \u2014 The extra columns to be exported to the group file. prefix \u2014 The prefix will be added to the cells in the output filePlaceholders like {Sample}_ can be used to from the meta data Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.ImmunarchFilter"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrimmunarch","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires See https://immunarch.com/articles/web_only/v3_basic_analysis.html After ImmunarchLoading loads the raw data into an immunarch object, this process wraps the functions from immunarch to do the following: Basic statistics, provided by immunarch::repExplore , such as number of clones or distributions of lengths and counts. The clonality of repertoires, provided by immunarch::repClonality The repertoire overlap, provided by immunarch::repOverlap The repertoire overlap, including different clustering procedures and PCA, provided by immunarch::repOverlapAnalysis The distributions of V or J genes, provided by immunarch::geneUsage The diversity of repertoires, provided by immunarch::repDiversity The dynamics of repertoires across time points/samples, provided by immunarch::trackClonotypes The spectratype of clonotypes, provided by immunarch::spectratype The distributions of kmers and sequence profiles, provided by immunarch::getKmers The V-J junction circos plots, implemented within the script of this process. Environment Variable Design: With different sets of arguments, a single function of the above can perform different tasks. For example, repExplore can be used to get the statistics of the size of the repertoire, the statistics of the length of the CDR3 region, or the statistics of the number of the clonotypes. Other than that, you can also have different ways to visualize the results, by passing different arguments to the immunarch::vis function. For example, you can pass .by to vis to visualize the results of repExplore by different groups. Before we explain each environment variable in details in the next section, we will give some examples here to show how the environment variables are organized in order for a single function to perform different tasks. ```toml # Repertoire overlapping [Immunarch.envs.overlaps] # The method to calculate the overlap, passed to `repOverlap` method = \"public\" ``` What if we want to calculate the overlap by different methods at the same time? We can use the following configuration: ```toml [Immunarch.envs.overlaps.cases] Public = { method = \"public\" } Jaccard = { method = \"jaccard\" } ``` Then, the `repOverlap` function will be called twice, once with `method = \"public\"` and once with `method = \"jaccard\"`. We can also use different arguments to visualize the results. These arguments will be passed to the `vis` function: ```toml [Immunarch.envs.overlaps.cases.Public] method = \"public\" vis_args = { \"-plot\": \"heatmap2\" } [Immunarch.envs.overlaps.cases.Jaccard] method = \"jaccard\" vis_args = { \"-plot\": \"heatmap2\" } ``` `-plot` will be translated to `.plot` and then passed to `vis`. If multiple cases share the same arguments, we can use the following configuration: ```toml [Immunarch.envs.overlaps] vis_args = { \"-plot\": \"heatmap2\" } [Immunarch.envs.overlaps.cases] Public = { method = \"public\" } Jaccard = { method = \"jaccard\" } ``` For some results, there are futher analysis that can be performed. For example, for the repertoire overlap, we can perform clustering and PCA (see also <https://immunarch.com/articles/web_only/v4_overlap.html>): ```R imm_ov1 <- repOverlap(immdata$data, .method = \"public\", .verbose = F) repOverlapAnalysis(imm_ov1, \"mds\") %>% vis() repOverlapAnalysis(imm_ov1, \"tsne\") %>% vis() ``` In such a case, we can use the following configuration: ```toml [Immunarch.envs.overlaps] method = \"public\" [Immunarch.envs.overlaps.analyses.cases] MDS = { \"-method\": \"mds\" } TSNE = { \"-method\": \"tsne\" } ``` Then, the `repOverlapAnalysis` function will be called twice on the result generated by `repOverlap(immdata$data, .method = \"public\")`, once with `.method = \"mds\"` and once with `.method = \"tsne\"`. We can also use different arguments to visualize the results. These arguments will be passed to the `vis` function: ```toml [Immunarch.envs.overlaps] method = \"public\" [Immunarch.envs.overlaps.analyses] # See: <https://immunarch.com/reference/vis.immunr_hclust.html> vis_args = { \"-plot\": \"best\" } [Immunarch.envs.overlaps.analyses.cases] MDS = { \"-method\": \"mds\" } TSNE = { \"-method\": \"tsne\" } ``` Generally, you don't need to specify `cases` if you only have one case. A default case will be created for you. For multiple cases, the arguments at the same level as `cases` will be inherited by all cases. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples [Immunarch.envs.kmers] k = 5 [Immunarch.envs.kmers] # Shared by cases k = 5 [Immunarch.envs.kmers.cases] Head5 = { head = 5 , -position = \"stack\" } Head10 = { head = 10 , -position = \"fill\" } Head30 = { head = 30 , -position = \"dodge\" } With motif profiling: [Immunarch.envs.kmers] k = 5 [Immnuarch.envs.kmers.profiles.cases] TextPlot = { method = \"self\" , vis_args = { \"-plot\" : \"text\" } } SeqPlot = { method = \"self\" , vis_args = { \"-plot\" : \"seq\" } } Input immdata \u2014 The data loaded by immunarch::repLoad() metafile \u2014 A cell-level metafile, where the first column must be the cell barcodesthat match the cell barcodes in immdata . The other columns can be any metadata that you want to use for the analysis. The loaded metadata will be left-joined to the converted cell-level data from immdata . This can also be a Seurat object RDS file. If so, the sobj@meta.data will be used as the metadata. Output outdir \u2014 The output directory Envs counts (ns) \u2014 Explore clonotype counts. - by: Groupings when visualize clonotype counts, passed to the .by argument of vis(imm_count, .by = <values>) . Multiple columns should be separated by , . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.counts will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.counts.by , envs.counts.devpars . divs (ns) \u2014 Parameters to control the diversity analysis. - method (choice): The method to calculate diversity. - chao1: a nonparameteric asymptotic estimator of species richness. (number of species in a population). - hill: Hill numbers are a mathematically unified family of diversity indices. (differing only by an exponent q). - div: true diversity, or the effective number of types. It refers to the number of equally abundant types needed for the average proportional abundance of the types to equal that observed in the dataset of interest where all types may not be equally abundant. - gini.simp: The Gini-Simpson index. It is the probability of interspecific encounter, i.e., probability that two entities represent different types. - inv.simp: Inverse Simpson index. It is the effective number of types that is obtained when the weighted arithmetic mean is used to quantify average proportional abundance of types in the dataset of interest. - gini: The Gini coefficient. It measures the inequality among values of a frequency distribution (for example levels of income). A Gini coefficient of zero expresses perfect equality, where all values are the same (for example, where everyone has the same income). A Gini coefficient of one (or 100 percents) expresses maximal inequality among values (for example where only one person has all the income). - d50: The D50 index. It is the number of types that are needed to cover 50%% of the total abundance. - raref: Species richness from the results of sampling through extrapolation. - by: The variables (column names) to group samples. Multiple columns should be separated by , . - plot_type (choice): The type of the plot, works when by is specified. Not working for raref . - box: Boxplot - bar: Barplot with error bars - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - args (type=json): Other arguments for repDiversity() . Do not include the preceding . and use - instead of . in the argument names. For example, do-norm will be compiled to .do.norm . See all arguments at https://immunarch.com/reference/repDiversity.html . - order (list): The order of the values in by on the x-axis of the plots. If not specified, the values will be used as-is. - test (ns): Perform statistical tests between each pair of groups. Does NOT work for raref . - method (choice): The method to perform the test - none: No test - t.test: Welch's t-test - wilcox.test: Wilcoxon rank sum test - padjust (choice): The method to adjust p-values. Defaults to none . - bonferroni: one-step correction - holm: step-down method using Bonferroni adjustments - hochberg: step-up method (independent) - hommel: closed method based on Simes tests (non-negative) - BH: Benjamini & Hochberg (non-negative) - BY: Benjamini & Yekutieli (negative) - fdr: Benjamini & Hochberg (non-negative) - none: no correction. - separate_by: A column name used to separate the samples into different plots. - split_by: A column name used to split the samples into different subplots. Like separate_by , but the plots will be put in the same figure. y-axis will be shared, even if align_y is False or ymin / ymax are not specified. ncol will be ignored. - split_order: The order of the values in split_by on the x-axis of the plots. It can also be used for separate_by to control the order of the plots. Values can be separated by , . - align_x (flag): Align the x-axis of multiple plots. Only works for raref . - align_y (flag): Align the y-axis of multiple plots. - ymin (type=float): The minimum value of the y-axis. The minimum value of the y-axis for plots splitting by separate_by . align_y is forced True when both ymin and ymax are specified. - ymax (type=float): The maximum value of the y-axis. The maximum value of the y-axis for plots splitting by separate_by . align_y is forced True when both ymin and ymax are specified. Works when both ymin and ymax are specified. - log (flag): Indicate whether we should plot with log-transformed x-axis using vis(.log = TRUE) . Only works for raref . - ncol (type=int): The number of columns of the plots. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If NO cases are specified, the default case will be added, with the name of envs.div.method . The values specified in envs.div will be used as the defaults for the cases here. gene_usages (ns) \u2014 Explore gene usages. - top (type=int): How many top (ranked by total usage across samples) genes to show in the plots. Use 0 to use all genes. - norm (flag): If True then use proportions of genes, else use counts of genes. - by: Groupings to show gene usages, passed to the .by argument of vis(imm_gu_top, .by = <values>) . Multiple columns should be separated by , . - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - analyses (ns;order=8): Perform gene usage analyses. - method: The method to control how the data is going to be preprocessed and analysed. One of js , cor , cosine , pca , mds and tsne . Can also be combined with following methods for the actual analyses: hclust , kmeans , dbscan , and kruskal . For example: cosine+hclust . You can also set to none to skip the analyses. See https://immunarch.com/articles/web_only/v5_gene_usage.html . - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.gene_usages.analyses will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.gene_usages.analyses.method , envs.gene_usages.analyses.vis_args and envs.gene_usages.analyses.devpars . - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.gene_usages will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.gene_usages.top , envs.gene_usages.norm , envs.gene_usages.by , envs.gene_usages.vis_args , envs.gene_usages.devpars and envs.gene_usages.analyses . hom_clones (ns) \u2014 Explore homeo clonotypes. - by: Groupings when visualize homeo clones, passed to the .by argument of vis(imm_hom, .by = <values>) . Multiple columns should be separated by , . - marks (ns): A dict with the threshold of the half-closed intervals that mark off clonal groups. Passed to the .clone.types arguments of repClonoality() . The keys could be: - Rare (type=float): the rare clonotypes - Small (type=float): the small clonotypes - Medium (type=float): the medium clonotypes - Large (type=float): the large clonotypes - Hyperexpanded (type=float): the hyperexpanded clonotypes - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.hom_clones will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.hom_clones.by , envs.hom_clones.marks and envs.hom_clones.devpars . kmers (ns) \u2014 Arguments for kmer analysis. - k (type=int): The length of kmer. - head (type=int): The number of top kmers to show. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - profiles (ns;order=8): Arguments for sequence profilings. - method (choice): The method for the position matrix. For more information see https://en.wikipedia.org/wiki/Position_weight_matrix . - freq: position frequency matrix (PFM) - a matrix with occurences of each amino acid in each position. - prob: position probability matrix (PPM) - a matrix with probabilities of each amino acid in each position. - wei: position weight matrix (PWM) - a matrix with log likelihoods of PPM elements. - self: self-information matrix (SIM) - a matrix with self-information of elements in PWM. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.kmers.profiles will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.kmers.profiles.method , envs.kmers.profiles.vis_args and envs.kmers.profiles.devpars . - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the default case will be added, with the name DEFAULT and the values of envs.kmers.k , envs.kmers.head , envs.kmers.vis_args and envs.kmers.devpars . lens (ns) \u2014 Explore clonotype CDR3 lengths. - by: Groupings when visualize clonotype lengths, passed to the .by argument of vis(imm_len, .by = <values>) . Multiple columns should be separated by , . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.lens will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.lens.by , envs.lens.devpars . mutaters (type=json;order=-9) \u2014 The mutaters passed to dplyr::mutate() on expanded cell-level datato add new columns. The keys will be the names of the columns, and the values will be the expressions. The new names can be used in volumes , lens , counts , top_clones , rare_clones , hom_clones , gene_usages , divs , etc. overlaps (ns) \u2014 Explore clonotype overlaps. - method (choice): The method to calculate overlaps. - public: number of public clonotypes between two samples. - overlap: a normalised measure of overlap similarity. It is defined as the size of the intersection divided by the smaller of the size of the two sets. - jaccard: conceptually a percentage of how many objects two sets have in common out of how many objects they have total. - tversky: an asymmetric similarity measure on sets that compares a variant to a prototype. - cosine: a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. - morisita: how many times it is more likely to randomly select two sampled points from the same quadrat (the dataset is covered by a regular grid of changing size) then it would be in the case of a random distribution generated from a Poisson process. Duplicate objects are merged with their counts are summed up. - inc+public: incremental overlaps of the N most abundant clonotypes with incrementally growing N using the public method. - inc+morisita: incremental overlaps of the N most abundant clonotypes with incrementally growing N using the morisita method. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - vis_args (type=json): Other arguments for the plotting functions vis(imm_ov, ...) . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - analyses (ns;order=8): Perform overlap analyses. - method: Plot the samples with these dimension reduction methods. The methods could be hclust , tsne , mds or combination of them, such as mds+hclust . You can also set to none to skip the analyses. They could also be combined, for example, mds+hclust . See https://immunarch.com/reference/repOverlapAnalysis.html . - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.overlaps.analyses will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.overlaps.analyses.method , envs.overlaps.analyses.vis_args and envs.overlaps.analyses.devpars . - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.overlaps will be used. If NO cases are specified, the default case will be added, with the key the default method and the values of envs.overlaps.method , envs.overlaps.vis_args , envs.overlaps.devpars and envs.overlaps.analyses . prefix \u2014 The prefix to the barcodes. You can use placeholder like {Sample}_ The prefixed barcodes will be used to match the barcodes in in.metafile . Not used if in.metafile is not specified. If None (default), immdata$prefix will be used. rare_clones (ns) \u2014 Explore rare clonotypes. - by: Groupings when visualize rare clones, passed to the .by argument of vis(imm_rare, .by = <values>) . Multiple columns should be separated by , . - marks (list;itype=int): A numerical vector with ranges of abundance for the rare clonotypes in the dataset. Passed to the .bound argument of repClonoality() . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.rare_clones will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.rare_clones.by , envs.rare_clones.marks and envs.rare_clones.devpars . spects (ns) \u2014 Spectratyping analysis. - quant: Select the column with clonal counts to evaluate. Set to id to count every clonotype once. Set to count to take into the account number of clones per clonotype. Multiple columns should be separated by , . - col: A string that specifies the column(s) to be processed. The output is one of the following strings, separated by the plus sign: \"nt\" for nucleotide sequences, \"aa\" for amino acid sequences, \"v\" for V gene segments, \"j\" for J gene segments. E.g., pass \"aa+v\" for spectratyping on CDR3 amino acid sequences paired with V gene segments, i.e., in this case a unique clonotype is a pair of CDR3 amino acid and V gene segment. Clonal counts of equal clonotypes will be summed up. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.spects will be used. By default, a By_Clonotype case will be added, with the values of quant = \"id\" and col = \"nt\" , and a By_Num_Clones case will be added, with the values of quant = \"count\" and col = \"aa+v\" . top_clones (ns) \u2014 Explore top clonotypes. - by: Groupings when visualize top clones, passed to the .by argument of vis(imm_top, .by = <values>) . Multiple columns should be separated by , . - marks (list;itype=int): A numerical vector with ranges of the top clonotypes. Passed to the .head argument of repClonoality() . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.top_clones will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.top_clones.by , envs.top_clones.marks and envs.top_clones.devpars . trackings (ns) \u2014 Parameters to control the clonotype tracking analysis. - targets: Either a set of CDR3AA seq of clonotypes to track (separated by , ), or simply an integer to track the top N clonotypes. - subject_col: The column name in meta data that contains the subjects/samples on the x-axis of the alluvial plot. If the values in this column are not unique, the values will be merged with the values in subject_col to form the x-axis. This defaults to Sample . - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - subjects (list): A list of values from subject_col to show in the alluvial plot on the x-axis. If not specified, all values in subject_col will be used. This also specifies the order of the x-axis. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments ( target , subject_col , and subjects ). If any of these arguments are not specified, the values in envs.trackings will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.trackings.target , envs.trackings.subject_col , and envs.trackings.subjects . vj_junc (ns) \u2014 Arguments for VJ junction circos plots.This analysis is not included in immunarch . It is a separate implementation using circlize . - by: Groupings to show VJ usages. Typically, this is the Sample column, so that the VJ usages are shown for each sample. But you can also use other columns, such as Subject to show the VJ usages for each subject. Multiple columns should be separated by , . - by_clones (flag): If True, the VJ usages will be calculated based on the distinct clonotypes, instead of the individual cells. - subset: Subset the data before plotting VJ usages. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data, which will affect the VJ usages at cell level (by_clones=False). - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.vj_junc will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.vj_junc.by , envs.vj_junc.by_clones envs.vj_junc.subset and envs.vj_junc.devpars . volumes (ns) \u2014 Explore clonotype volume (sizes). - by: Groupings when visualize clonotype volumes, passed to the .by argument of vis(imm_vol, .by = <values>) . Multiple columns should be separated by , . - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.volumes will be used. If NO cases are specified, the default case will be added, with the name DEFAULT and the values of envs.volume.by , envs.volume.devpars . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.Immunarch"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrsamplediversity","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Sample diversity and rarefaction analysis This is part of Immunarch, in case we have multiple dataset to compare. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() Output outdir \u2014 The output directory Envs devpars \u2014 The parameters for the plotting deviceIt is a dict, and keys are the methods and values are dicts with width, height and res that will be passed to png() If not provided, 1000, 1000 and 100 will be used. div_methods \u2014 Methods to calculate diversitiesIt is a dict, keys are the method names, values are the groupings. Each one is a case, multiple columns for a case are separated by , For example: {\"div\": [\"Status\", \"Sex\", \"Status,Sex\"]} will run true diversity for samples grouped by Status , Sex , and both. The diversity for each sample without grouping will also be added anyway. Supported methods: chao1 , hill , div , gini.simp , inv.simp , gini , and raref . See also https://immunarch.com/articles/web_only/v6_diversity.html . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.SampleDiversity"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrcloneresidency","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Identification of clone residency This process is used to investigate the residency of clones in groups, typically two samples (e.g. tumor and normal) from the same patient. But it can be used for any two groups of clones. There are three types of output from this process Count tables of the clones in the two groups CDR3_aa Tumor Normal CASSYGLSWGSYEQYF 306 55 CASSVTGAETQYF 295 37 CASSVPSAHYNEQFF 197 9 ... ... ... Residency plots showing the residency of clones in the two groups The points in the plot are jittered to avoid overplotting. The x-axis is the residency in the first group and the y-axis is the residency in the second group. The size of the points are relative to the normalized size of the clones. You may identify different types of clones in the plot based on their residency in the two groups: Collapsed (The clones that are collapsed in the second group) Dual (The clones that are present in both groups with equal size) Expanded (The clones that are expanded in the second group) First Group Multiplet (The clones only in the First Group with size > 1) Second Group Multiplet (The clones only in the Second Group with size > 1) First Group Singlet (The clones only in the First Group with size = 1) Second Group Singlet (The clones only in the Second Group with size = 1) This idea is borrowed from this paper: Wu, Thomas D., et al. \"Peripheral T cell expansion predicts tumour infiltration and clinical response.\" Nature 579.7798 (2020): 274-278. Venn diagrams showing the overlap of the clones in the two groups {: width=\"60%\"} Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() metafile \u2014 A cell-level metafile, where the first column must be the cell barcodesthat match the cell barcodes in immdata . The other columns can be any metadata that you want to use for the analysis. The loaded metadata will be left-joined to the converted cell-level data from immdata . This can also be a Seurat object RDS file. If so, the sobj@meta.data will be used as the metadata. Output outdir \u2014 The output directory Envs cases (type=json) \u2014 If you have multiple cases, you can use this argumentto specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments. If no cases are specified, the default case will be added, with the name DEFAULT and the values of envs.subject , envs.group , envs.order and envs.section . These values are also the defaults for the other cases. group \u2014 The key of group in metadata. This usually marks the samplesthat you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, instead of venn diagram, upset plots will be used. mutaters (type=json) \u2014 The mutaters passed to dplyr::mutate() onthe cell-level data converted from in.immdata . If in.metafile is provided, the mutaters will be applied to the joined data. The keys will be the names of the new columns, and the values will be the expressions. The new names can be used in subject , group , order and section . order (list) \u2014 The order of the values in group . In scatter/residency plots, X in X,Y will be used as x-axis and Y will be used as y-axis. You can also have multiple orders. For example: [\"X,Y\", \"X,Z\"] . If you only have two groups, you can set order = [\"X\", \"Y\"] , which will be the same as order = [\"X,Y\"] . prefix \u2014 The prefix of the cell barcodes in the Seurat object. section \u2014 How the subjects aligned in the report. Multiple subjects withthe same value will be grouped together. Useful for cohort with large number of samples. subject (list) \u2014 The key of subject in metadata. The cloneresidency will be examined for this subject/patient subset \u2014 The filter passed to dplyr::filter() to filter the data for the cellsbefore calculating the clone residency. For example, Clones > 1 to filter out singletons. upset_trans \u2014 The transformation to apply to the y axis of upset bar plots.For example, log10 or sqrt . If not specified, the y axis will be plotted as is. Note that the position of the bar plots will be dodged instead of stacked when the transformation is applied. See also https://github.com/tidyverse/ggplot2/issues/3671 upset_ymax \u2014 The maximum value of the y-axis in the upset bar plots. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.CloneResidency"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrimmunarch2vdjtools","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert immuarch format into VDJtools input formats. This process converts the immunarch object to the VDJtools input files, in order to perform the VJ gene usage analysis by VJUsage process. This process will generally generate a tab-delimited file for each sample, with the following columns. count : The number of reads for this clonotype frequency : The frequency of this clonotype CDR3nt : The nucleotide sequence of the CDR3 region CDR3aa : The amino acid sequence of the CDR3 region V : The V gene D : The D gene J : The J gene See also: https://vdjtools-doc.readthedocs.io/en/master/input.html#vdjtools-format . This process has no environment variables. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() Output outdir \u2014 The output directory containing the vdjtools input for eachsample Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.Immunarch2VDJtools"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_5","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrimmunarchsplitidents","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Split the data into multiple immunarch datasets by Idents from Seurat Note that only the cells in both the immdata and sobjfile will be kept. Requires immunarch >= 0.9.0 to use select_clusters() Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() sobjfile \u2014 The Seurat object file.You can set a different ident by Idents(sobj) <- \"new_ident\" to split the data by the new ident, where \"new_ident\" is the an existing column in meta data Output outdir \u2014 The output directory containing the RDS files of the splittedimmunarch datasets Envs prefix \u2014 The prefix of the cell barcodes in the Seurat object.Once could use a fixed prefix, or a placeholder with the column name in meta data. For example, \"{Sample}_\" will replace the placeholder with the value of the column Sample in meta data. sample_col \u2014 The column name in meta data that contains the sample name Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.ImmunarchSplitIdents"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_6","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_6","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_6","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_6","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_6","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_6","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_6","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrvjusage","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Circos-style V-J usage plot displaying the frequency ofvarious V-J junctions using vdjtools. This process performs the VJ gene usage analysis using VDJtools . It wraps the PlotFancyVJUsage command in VDJtools . The output will be a V-J junction circos plot for a single sample. Arcs correspond to different V and J segments, scaled to their frequency in sample. Ribbons represent V-J pairings and their size is scaled to the pairing frequency (weighted in present case). {: width=\"80%\" } Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input file, in vdjtools input format Output outfile \u2014 The V-J usage plot Envs vdjtools \u2014 The path to the VDJtools executable. vdjtools_patch (hidden) \u2014 The patch file for VDJtools . It's delivered with the pipeline ([ biopipen ][3] package). * You don't need to provide this file, unless you want to use a different patch file by yourself. * See the issue with VDJtools here . Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.VJUsage"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_7","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_7","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_7","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_7","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_7","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_7","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_7","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrattach2seurat","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Attach the clonal information to a Seurat object as metadata Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immfile \u2014 The immunarch object in RDS sobjfile \u2014 The Seurat object file in RDS Output outfile \u2014 The Seurat object with the clonal information as metadata Envs metacols \u2014 Which meta columns to attach prefix \u2014 The prefix to the barcodes. You can use placeholder like {Sample}_ to use the meta data from the immunarch object Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.Attach2Seurat"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_8","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_8","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_8","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_8","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_8","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_8","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_8","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrtcrclustering","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Cluster the TCR clones by their CDR3 sequences This process is used to cluster TCR clones based on their CDR3 sequences. It uses either GIANA Zhang, Hongyi, Xiaowei Zhan, and Bo Li. \"GIANA allows computationally-efficient TCR clustering and multi-disease repertoire classification by isometric transformation.\" Nature communications 12.1 (2021): 1-11. Or ClusTCR Sebastiaan Valkiers, Max Van Houcke, Kris Laukens, Pieter Meysman, ClusTCR: a Python interface for rapid clustering of large sets of CDR3 sequences with unknown antigen specificity, Bioinformatics, 2021. Both methods are based on the Faiss Clustering Library , for efficient similarity search and clustering of dense vectors, so both methods yield similar results. A text file will be generated with the cluster assignments for each cell, together with the immunarch object (in R ) with the cluster assignments at TCR_Clsuter column. This information will then be merged to a Seurat object for further downstream analysis. The cluster assignments are prefixed with S_ or M_ to indicate whether a cluster has only one unique CDR3 sequence or multiple CDR3 sequences. Note that a cluster with S_ prefix may still have multiple cells, as the same CDR3 sequence may be shared by multiple cells. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input screpfile \u2014 The TCR data object loaded by scRepertoire::CombineTCR() or scRepertoire::CombineExpression() Output outfile \u2014 The scRepertoire object in qs with TCR cluster information.Column TCR_Cluster will be added to the metadata. Envs args (type=json) \u2014 The arguments for the clustering toolFor GIANA, they will be passed to python GIAna.py See https://github.com/s175573/GIANA#usage . For ClusTCR, they will be passed to clustcr.Clustering(...) See https://svalkiers.github.io/clusTCR/docs/clustering/how-to-use.html#clustering . chain (choice) \u2014 The TCR chain to use for clustering. - alpha: TCR alpha chain (the first sequence in CTaa, separated by _ ) - beta: TCR beta chain (the second sequence in CTaa, separated by _ ) - both: Both TCR alpha and beta chains python \u2014 The path of python with GIANA 's dependencies installedor with clusTCR installed. Depending on the tool you choose. tool (choice) \u2014 The tool used to do the clustering, either GIANA or ClusTCR . For GIANA, using TRBV mutations is not supported - GIANA: by Li lab at UT Southwestern Medical Center - ClusTCR: by Sebastiaan Valkiers, etc within_sample (flag) \u2014 Whether to cluster the TCR clones within each sample.When in.screpfile is a Seurat object, the samples are marked by the Sample column in the metadata. Requires clusTCR \u2014 if: {{ proc.envs.tool == 'ClusTCR' }} check: {{ proc.envs.python }} -c \"import clustcr\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.TCRClustering"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_9","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_9","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_9","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_9","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_9","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_9","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_9","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrtcrclusterstats","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Statistics of TCR clusters, generated by TCRClustering . The statistics include The number of cells in each cluster (cluster size) Sample diversity using TCR clusters instead of TCR clones Shared TCR clusters between samples Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Examples","title":"biopipen.ns.tcr.TCRClusterStats"},{"location":"api/biopipen.ns.tcr/#cluster-size","text":"[TCRClusterStats.envs.cluster_size] by = \"Sample\" {: width=\"80%\"}","title":"Cluster size"},{"location":"api/biopipen.ns.tcr/#shared-clusters","text":"[TCRClusterStats.envs.shared_clusters] numbers_on_heatmap = true heatmap_meta = [ \"region\" ] {: width=\"80%\"}","title":"Shared clusters"},{"location":"api/biopipen.ns.tcr/#sample-diversity","text":"[TCRClusterStats.envs.sample_diversity] method = \"gini\" {: width=\"80%\"} Compared to the sample diversity using TCR clones: {: width=\"80%\"} Input immfile \u2014 The immunarch object with TCR clusters attached Output outdir \u2014 The output directory containing the stats and reports Envs cluster_size (ns) \u2014 The distribution of size of each cluster. - by: The variables (column names) used to fill the histogram. Only a single column is supported. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.cluster_size will be used. If NO cases are specified, the default case will be added, with the name DEFAULT . sample_diversity (ns) \u2014 Sample diversity using TCR clusters instead ofclones. - by: The variables (column names) to group samples. Multiple columns should be separated by , . - method (choice): The method to calculate diversity. - gini: The Gini coefficient. It measures the inequality among values of a frequency distribution (for example levels of income). - gini.simp: The Gini-Simpson index. It is the probability of interspecific encounter, i.e., probability that two entities represent different types. - inv.simp: Inverse Simpson index. It is the effective number of types that is obtained when the weighted arithmetic mean is used to quantify average proportional abundance of types in the dataset of interest. - div: true diversity, or the effective number of types. It refers to the number of equally abundant types needed for the average proportional abundance of the types to equal that observed in the dataset of interest where all types may not be equally abundant. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.sample_diversity will be used. If NO cases are specified, the default case will be added, with the name DEFAULT . shared_clusters (ns) \u2014 Stats about shared TCR clusters - numbers_on_heatmap (flag): Whether to show the numbers on the heatmap. - heatmap_meta (list): The columns of metadata to show on the heatmap. - cluster_rows (flag): Whether to cluster the rows on the heatmap. - sample_order: The order of the samples on the heatmap. Either a string separated by , or a list of sample names. This only works for columns if cluster_rows is True . - grouping: The groups to investigate the shared clusters. If specified, venn diagrams will be drawn instead of heatmaps. In such case, numbers_on_heatmap and heatmap_meta will be ignored. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in envs.shared_clusters will be used. If NO cases are specified, the default case will be added, with the name DEFAULT . Requires r-immunarch \u2014 check: {{proc.lang}} -e \"library(immunarch)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"Sample diversity"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_10","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_10","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_10","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_10","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_10","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_10","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_10","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrclonesizeqqplot","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc QQ plot of the clone sizes QQ plots for clones sizes of pairs of samples Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input immdata \u2014 The data loaded by immunarch::repLoad() Output outdir \u2014 The output directory Envs diag \u2014 Whether to draw the diagonal line in the QQ plot group \u2014 The key of group in metadata. This usually marks the samplesthat you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, for example, [A, B, C], the QQ plots will be generated for all the combinations of 2 groups, i.e., [A, B], [A, C], [B, C] on \u2014 The key of the metadata to use for the QQ plot. One/Both of [\"Clones\", \"Proportion\"] order \u2014 The order of the values in group . Early-ordered group willbe used as x-axis in scatter plots If there are more than 2 groups, for example, [A, B, C], the QQ plots will be drawn for pairs: B ~ A, C ~ B. subject \u2014 The key of subject in metadata, defining the pairs.The clone residency will be examined for this subject/patient Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.CloneSizeQQPlot"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_11","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_11","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_11","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_11","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_11","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_11","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_11","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrcdr3aaphyschem","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc CDR3 AA physicochemical feature analysis The idea is to perform a regression between two groups of cells (e.g. Treg vs Tconv) at different length of CDR3 AA sequences. The regression will be performed for each physicochemical feature of the AA (hydrophobicity, volume and isolectric point). Reference: - Stadinski, Brian D., et al. \"Hydrophobic CDR3 residues promote the development of self-reactive T cells.\" Nature immunology 17.8 (2016): 946-955. - Lagattuta, Kaitlyn A., et al. \"Repertoire analyses reveal T cell antigen receptor sequence features that influence T cell fate.\" Nature immunology 23.3 (2022): 446-457. - Wimley, W. C. & White, S. H. Experimentally determined hydrophobicity scale for proteins at membrane - interfaces. Nat. Struct. Biol. 3, 842-848 (1996). - Handbook of chemistry & physics 72nd edition. (CRC Press, 1991). - Zamyatnin, A. A. Protein volume in solution. Prog. Biophys. Mol. Biol. 24, 107-123 (1972). Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input scrfile \u2014 The data loaded by ScRepCombiningExpression , saved in RDS or qs/qs2 format.The data is actually generated by scRepertiore::combineExpression() . The data must have both TRA and TRB chains. Output outdir \u2014 The output directory Envs comparison (type=auto) \u2014 A dict of two groups, with keys as thegroup names and values as the group labels. For example, Treg = [ \"CD4 CTL\" , \"CD4 Naive\" , \"CD4 TCM\" , \"CD4 TEM\" ] Tconv = \"Tconv\" Or simply a list of two groups, for example, [\"Treg\", \"Tconv\"] when they are both in the group column. each (auto) \u2014 A column, or a list of columns or a string of columns separated by comma.The columns will be used to split the data into multiple groups and the regression will be applied to each group separately. If not provided, all the cells will be used. group \u2014 The key of group in metadata to define the groups tocompare. For example, CellType , which has cell types annotated for each cell in the combined object (immdata + Seurat metadata) target \u2014 Which group to use as the target group. The targetgroup will be labeled as 1, and the other group will be labeled as 0 in the regression. If not specified, the first group in comparison will be used as the target group. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.CDR3AAPhyschem"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_12","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_12","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_12","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_12","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_12","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_12","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_12","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrtessa","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Tessa is a Bayesian model to integrate T cell receptor (TCR) sequenceprofiling with transcriptomes of T cells. Enabled by the recently developed single cell sequencing techniques, which provide both TCR sequences and RNA sequences of each T cell concurrently, Tessa maps the functional landscape of the TCR repertoire, and generates insights into understanding human immune response to diseases. As the first part of tessa, BriseisEncoder is employed prior to the Bayesian algorithm to capture the TCR sequence features and create numerical embeddings. We showed that the reconstructed Atchley Factor matrices and CDR3 sequences, generated through the numerical embeddings, are highly similar to their original counterparts. The CDR3 peptide sequences are constructed via a RandomForest model applied on the reconstructed Atchley Factor matrices. See https://github.com/jcao89757/TESSA When finished, two columns will be added to the meta.data of the Seurat object: TESSA_Cluster : The cluster assignments from TESSA. TESSA_Cluster_Size : The number of cells in each cluster. These columns can be then used for further downstream analysis to explore the functional landscape of the TCR repertoire. Reference: - 'Mapping the Functional Landscape of TCR Repertoire.', Zhang, Z., Xiong, D., Wang, X. et al. 2021. link - 'Deep learning-based prediction of the T cell receptor-antigen binding specificity.', Lu, T., Zhang, Z., Zhu, J. et al. 2021. link Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input screpdata \u2014 The data loaded by ScRepCombiningExpression , saved in RDS orqs/qs2 format. The data is actually generated by scRepertiore::combineExpression() . The data must have both TRA and TRB chains. Output outfile \u2014 a qs fileof a Seurat object, with TESSA_Cluster and TESSA_Cluster_Size added to the meta.data Envs assay \u2014 Which assay to use to extract the expression matrix.Only works if in.srtobj is an RDS file of a Seurat object. By default, if SCTransform is performed, SCT will be used. max_iter (type=int) \u2014 The maximum number of iterations for MCMC. predefined_b (flag) \u2014 Whether use the predefined b or not.Please check the paper of tessa for more details about the b vector. If True, the tessa will not update b in the MCMC iterations. python \u2014 The path of python with TESSA 's dependencies installed save_tessa (flag) \u2014 Save tessa detailed results to seurat object?It will be saved to sobj@misc$tessa . within_sample (flag) \u2014 Whether the TCR networks are constructed onlywithin TCRs from the same sample/patient (True) or with all the TCRs in the meta data matrix (False). Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.TESSA"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_13","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_13","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_13","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_13","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_13","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_13","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_13","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrtcrdock","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Using TCRDock to predict the structure of MHC-peptide-TCR complexes See https://github.com/phbradley/TCRdock . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input configfile \u2014 The config file for TCRDockIt's should be a toml file with the keys listed in envs , including organism , mhc_class , mhc , peptide , va , ja , vb , jb , cdr3a , and cdr3b . The values will overwrite the values in envs . Output outdir \u2014 The output directory containing the results Envs cdr3a \u2014 The CDR3 alpha sequence cdr3b \u2014 The CDR3 beta sequence data_dir \u2014 The data directory that contains the model files.The model files should be in the params subdirectory. ja \u2014 The J alpha gene jb \u2014 The J beta gene mhc \u2014 The MHC allele, e.g., A*02:01 mhc_class (type=int) \u2014 The MHC class, either 1 or 2 model_file \u2014 The model file to use.If provided as a relative path, it should be relative to the <envs.data_dir>/params/ , otherwise, it should be the full path. model_name \u2014 The model name to use organism \u2014 The organism of the TCR, peptide and MHC peptide \u2014 The peptide sequence python \u2014 The path of python with dependencies for tcrdock installed.If not provided, TCRDock.lang will be used (the same interpreter used for the wrapper script). It could also be a list to specify, for example, a python in a conda environment (e.g., [\"conda\", \"run\", \"-n\", \"myenv\", \"python\"] ). tcrdock \u2014 The path to the tcrdock source code repo.You need to clone the source code from the github repository. https://github.com/phbradley/TCRdock at revision c5a7af42eeb0c2a4492a4d4fe803f1f9aafb6193 at main branch. You also have to run download_blast.py after cloning to download the blast database in the directory. If not provided, we will clone the source code to the envs.tmpdir directory and run the download_blast.py script. tmpdir \u2014 The temporary directory used to clone the tcrdock source code if envs.tcrdock is not provided. va \u2014 The V alpha gene vb \u2014 The V beta gene Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.TCRDock"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_14","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_14","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_14","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_14","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_14","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_14","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_14","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrscreploading","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Load the single cell TCR/BCR data into a scRepertoire compatible object This process loads the single cell TCR/BCR data into a scRepertoire (>= v2.0.8, < v2.3.2) compatible object. Later, scRepertoire::combineExpression can be used to combine the expression data with the TCR/BCR data. For the data path specified at TCRData / BCRData in the input file ( in.metafile ), will be used to find the TCR/BCR data files and scRepertoire::loadContigs() will be used to load the data. A directory can be specified in TCRData / BCRData , then scRepertoire::loadContigs() will be used directly to load the data from the directory. Otherwise if a file is specified, it will be symbolically linked to a directory for scRepertoire::loadContigs() to load. Note that when the file name can not be recognized by scRepertoire::loadContigs() , envs.format must be set for the correct format of the data. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input metafile \u2014 The meta data of the samplesA tab-delimited file Two columns are required: * Sample to specify the sample names. * TCRData / BCRData to assign the path of the data to the samples, and this column will be excluded as metadata. Output outfile \u2014 The scRepertoire compatible object in qs/qs2 format Envs combineBCR (type=json) \u2014 The extra arguments for scRepertoire::combineBCR function. See also https://www.borch.dev/uploads/screpertoire/reference/combinebcr combineTCR (type=json) \u2014 The extra arguments for scRepertoire::combineTCR function. See also https://www.borch.dev/uploads/screpertoire/reference/combinetcr exclude (auto) \u2014 The columns to exclude from the metadata to add to the object.A list of column names to exclude or a string with column names separated by , . By default, BCRData , TCRData and RNAData will be excluded. format (choice) \u2014 The format of the TCR/BCR data files. - 10X: 10X Genomics data, which is usually in a directory with filtered_contig_annotations.csv file. - AIRR: AIRR format, which is usually in a file with airr_rearrangement.tsv file. - BD: Becton Dickinson data, which is usually in a file with Contigs_AIRR.tsv file. - Dandelion: Dandelion data, which is usually in a file with all_contig_dandelion.tsv file. - Immcantation: Immcantation data, which is usually in a file with data.tsv file. - JSON: JSON format, which is usually in a file with .json extension. - ParseBio: ParseBio data, which is usually in a file with barcode_report.tsv file. - MiXCR: MiXCR data, which is usually in a file with clones.tsv file. - Omniscope: Omniscope data, which is usually in a file with .csv extension. - TRUST4: TRUST4 data, which is usually in a file with barcode_report.tsv file. - WAT3R: WAT3R data, which is usually in a file with barcode_results.csv file. See also: https://rdrr.io/github/ncborcherding/scRepertoire/man/loadContigs.html If not provided, the format will be guessed from the file name by scRepertoire::loadContigs() . tmpdir \u2014 The temporary directory to store the symbolic links to theTCR/BCR data files. type (choice) \u2014 The type of the data to load. - TCR: T cell receptor data - BCR: B cell receptor data - auto: Automatically detect the type from the metadata. If auto is selected, the type will be determined by the presence of TCRData or BCRData columns in the metadata. If both columns are present, TCR will be selected by default. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.ScRepLoading"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_15","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_15","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_15","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_15","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_15","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_15","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_15","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrscrepcombiningexpression","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Combine the scTCR/BCR data with the expression data This process combines the scTCR/BCR data with the expression data using scRepertoire::combineExpression function. The expression data should be in Seurat format. The scRepertoire object should be a combined contig object, usually generated by scRepertoire::combineTCR or scRepertoire::combineBCR . See also: https://www.borch.dev/uploads/screpertoire/reference/combineexpression . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input screpfile \u2014 The scRepertoire object in RDS/qs format srtobj \u2014 The Seurat object, saved in RDS/qs format Output outfile \u2014 The Seurat object with the TCR/BCR data combinedIn addition to the meta columns added by scRepertoire::combineExpression() , a new column VDJ_Presence will be added to the metadata. It indicates whether the cell has a TCR/BCR sequence or not. The value is TRUE if the cell has a TCR/BCR sequence, and FALSE otherwise. Envs addLabel (flag) \u2014 This will add a label to the frequency header, allowing theuser to try multiple group_by variables or recalculate frequencies after subsetting the data. chain \u2014 indicate if both or a specific chain should be usede.g. \"both\", \"TRA\", \"TRG\", \"IGH\", \"IGL\". cloneCall \u2014 How to call the clone - VDJC gene (gene), CDR3 nucleotide (nt),CDR3 amino acid (aa), VDJC gene + CDR3 nucleotide (strict) or a custom variable in the data. cloneSize (type=json) \u2014 The bins for the grouping based on proportion orfrequency. If proportion is FALSE and the cloneSizes are not set high enough based on frequency, the upper limit of cloneSizes will be automatically updated. filterNA (flag) \u2014 Method to subset Seurat/SCE object of barcodes without cloneinformation group_by \u2014 The column label in the combined clones in which clone frequency willbe calculated. NULL or \"none\" will keep the format of input.data. proportion (flag) \u2014 Whether to proportion (TRUE) or total frequency (FALSE) ofthe clone based on the group_by variable. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.ScRepCombiningExpression"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_16","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_16","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_16","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_16","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_16","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_16","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_16","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.tcr/#biopipennstcrclonalstats","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Visualize the clonal information. Using scplotter to visualize the clonal information. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input screpfile \u2014 The scRepertoire object in RDS/qs format Output outdir \u2014 The output directory containing the plots Envs \u2014 The arguments for the plot functionSee the documentation of the corresponding plot function for the details cases (type=json) \u2014 The cases to generate the plots if we have multiple cases.The keys are the names of the cases, and the values are the arguments for the plot function. The arguments in envs will be used if not specified in cases , except for mutaters . Sections can be specified as the prefix of the case name, separated by :: . For example, if you have a case named Clonal Volume::Case1 , the plot will be put in the section Clonal Volume . By default, when there are multiple cases for the same 'viz_type', the name of the 'viz_type' will be used as the default section name (for example, when 'viz_type' is 'volume', the section name will be 'Clonal Volume'). When there is only a single case, the section name will default to 'DEFAULT', which will not be shown in the report. descr \u2014 The description of the plot, used to show in the report. devpars (ns) \u2014 The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device more_formats (list) \u2014 The extra formats to save the plots in, other than PNG. mutaters (type=json;order=-9) \u2014 The mutaters passed to dplyr::mutate() to add new variables.When the object loaded form in.screpfile is a list, the mutaters will be applied to each element. The keys are the names of the new variables, and the values are the expressions. When it is a Seurat object, typically an output of scRepertoire::combineExpression() , the mutaters will be applied to the meta.data . save_code (flag) \u2014 Whether to save the code used to generate the plotsNote that the data directly used to generate the plots will also be saved in an rda file. Be careful if the data is large as it may take a lot of disk space. subset \u2014 An expression to subset the data before plotting.Similar to mutaters , it will be applied to each element by dplyr::filter() if the object loaded form in.screpfile is a list; otherwise, it will be applied to subset(sobj, subset = <expr>) if the object is a Seurat object. viz_type (choice) \u2014 The type of visualization to generate. - volume: The volume of the clones using ClonalVolumePlot - abundance: The abundance of the clones using ClonalAbundancePlot - length: The length of the CDR3 sequences using ClonalLengthPlot - residency: The residency of the clones using ClonalResidencyPlot - dynamics: The dynamics of the clones using ClonalDynamicsPlot - composition: The composition of the clones using ClonalCompositionPlot - overlap: The overlap of the clones using ClonalOverlapPlot - diversity: The diversity of the clones using ClonalDiversityPlot - geneusage: The gene usage of the clones using ClonalGeneUsagePlot - positional: The positional information of the clones using ClonalPositionalPlot - kmer: The kmer information of the clones using ClonalKmerPlot - rarefaction: The rarefaction curve of the clones using ClonalRarefactionPlot Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.tcr.ClonalStats"},{"location":"api/biopipen.ns.tcr/#pipenprocprocmeta_17","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.tcr/#pipenprocprocfrom_proc_17","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_subclass_17","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.tcr/#pipenprocprocinit_17","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.tcr/#pipenprocprocgc_17","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.tcr/#pipenprocproclog_17","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.tcr/#pipenprocprocrun_17","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/","text":"module biopipen.ns . vcf </> Tools to handle VCF files Classes VcfLiftOver ( Proc ) \u2014 Liftover a VCF file using GATK </> VcfFilter ( Proc ) \u2014 Filter records in vcf file </> VcfIndex ( Proc ) \u2014 Index VCF files. If they are already index, use the index files </> Vcf2Bed ( Proc ) \u2014 Convert Vcf file to Bed file </> VcfDownSample ( Proc ) \u2014 Down-sample VCF files to keep only a subset of variants in there </> VcfSplitSamples ( Proc ) \u2014 Split a VCF file into multiple VCF files, one for each sample </> VcfIntersect ( Proc ) \u2014 Find variants in both VCF files </> VcfFix ( Proc ) \u2014 Fix some issues with VCF files </> VcfAnno ( Proc ) \u2014 Annotate a VCF file using vcfanno </> TruvariBench ( Proc ) \u2014 Run truvari bench to compare a VCF with CNV calls andbase CNV standards </> TruvariBenchSummary ( Proc ) \u2014 Summarise the statistics from TruvariBench for multiple jobs (VCFs) </> TruvariConsistency ( Proc ) \u2014 Run truvari consistency to check consistency of CNV calls </> BcftoolsAnnotate ( Proc ) \u2014 Add or remove annotations from VCF files </> BcftoolsFilter ( Proc ) \u2014 Apply fixed threshold filters to VCF files </> BcftoolsSort ( Proc ) \u2014 Sort VCF files using bcftools sort . </> BcftoolsMerge ( Proc ) \u2014 Merge multiple VCF files using bcftools merge . </> BcftoolsView ( Proc ) \u2014 View, subset and filter VCF files by position and filtering expression. </> class biopipen.ns.vcf . VcfLiftOver ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Liftover a VCF file using GATK Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input invcf \u2014 The input VCF file Output outvcf \u2014 The output VCF file Envs args \u2014 Other CLI arguments for gatk LiftoverVcf chain \u2014 The map chain file for liftover gatk \u2014 The path to gatk4, which should be installed via conda tmpdir \u2014 Directory for temporary storage of working files Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . VcfFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Filter records in vcf file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input invcf \u2014 The input vcf file, could be bgzipped. Output outfile \u2014 The filtered vcf file. If in.invcf is bgzipped, thenthis will be bgzipped. Envs filter_descs \u2014 Descriptions for the filters. Will be saved to the headerof the output vcf file filters \u2014 A dict of filters with keys the filter names. Typically lambda variant: Things to notice 1. Filters should return False to get variant filtered out 2. See https://brentp.github.io/cyvcf2/docstrings.html#cyvcf2.cyvcf2.Variant For what you can do with the variant 3. The filter python functions should be in string representation 4. Builtin filters can have parameters {\"QUAL\": 30} 5. List of builtin filters. Specify them like: {\"FILTER\": params} SNPONLY : keeps only SNPs ( {\"SNPONLY\": False} to filter SNPs out) QUAL : keeps variants with QUAL>=param ( {\"QUAL\": (30, False)} ) to keep only variants with QUAL<30 helper \u2014 Some helper code for the filters keep \u2014 Keep the variants not passing the filters? Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . VcfIndex ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Index VCF files. If they are already index, use the index files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The output VCF file (bgzipped) outidx \u2014 The index file of the output VCF file Envs tabix \u2014 Path to tabix Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . Vcf2Bed ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert Vcf file to Bed file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The vcf file Output outfile \u2014 The converted bed file Envs inbase \u2014 The coordinate base of the vcf file outbase \u2014 The coordinate base of the base file Requires cyvcf2 \u2014 check: {{proc.lang}} -c \"import cyvcf2\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . VcfDownSample ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Down-sample VCF files to keep only a subset of variants in there Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The output VCF file with subet variantsGzipped if in.infile is gzipped Envs n \u2014 Fraction/Number of variants to keepIf n > 1 , it is the number. If n <= 1 , it is the fraction. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . VcfSplitSamples ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Split a VCF file into multiple VCF files, one for each sample Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outdir \u2014 The output directory containing the split VCF files Envs bcftools \u2014 Path to bcftools gz \u2014 Gzip the output VCF files? Has to be True if envs.index is True index \u2014 Index the output VCF files? ncores \u2014 Number of cores, used to extract samples, but not to index private \u2014 Keep sites where only the sample carries an non-ref allele.That means, sites with genotypes like 0/0 will be removed. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . VcfIntersect ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Find variants in both VCF files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile1 \u2014 The first VCF file infile2 \u2014 The second VCF file Output outfile \u2014 The output VCF file with subet variants in both files Envs bcftools \u2014 Path to bcftools collapse \u2014 How to match the variants in the two files? Will be passed to bcftools isec -c option. See also https://samtools.github.io/bcftools/bcftools.html#common_options - none: only records with identical REF and ALT alleles are compatible - some: only records where some subset of ALT alleles match are compatible - all: all records are compatible, regardless of whether the ALT alleles match or not. - snps: any SNP records are compatible, regardless of whether the ALT alleles match or not. - indels: any indel records are compatible, regardless of whether the ALT alleles match or not. - both: abbreviates snps and indels - id: only records with identical ID are compatible gz \u2014 Gzip the output VCF files? Has to be True if envs.index is True index \u2014 Index the output VCF files? keep_as \u2014 Keep the variants as presented in the first (0) orthe second (1) file? Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . VcfFix ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Fix some issues with VCF files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The output VCF file Envs fixes \u2014 A list of fixes to apply.Each one is a dict with keys kind , id , regex and fix kind : The kind of fix. Including filter the FILTERs in the header, info the info INFOs in the header, contig the contig lines in the header format the FORMATs in the header, colnames the column names in the header header general header item variant the variants None matches everything id : The ID the match. If kind is filter , info , contig or format , then it matches the ID of the item. If kind is variant , then it matches the ID of the variant. If a list is given, then it matches any of the IDs in the list. regex : The regular expression to match. When id is given, this is ignored. append : Whether to append a record instead of to replace an existing one. When it is True, kind has to not be None fix : The fix to apply in the format of a lambda function (in string), with a single argument. The function should either return a string (raw representation) for the record, the record itself, None , False . If None is returned, the original record is used. if False , the record is removed. If append is True , then the function should either return a string or an object. And the argument is None The argument is a different object based on different kind s. When kind is None , the argument is the plain line of the record with line ending. When kind is info or format , the record is a dict with keys ID , Description , Type and Number . When kind is filter , the record is a dict with keys ID and Description . When kind is contig , the record is a dict with keys ID and length . When kind is header , the record is a dict with key the name of the header and value the value of the header. When kind is colnames , the record is a list of column names. When kind is variant , the record is a dict with keys CHROM , POS , REF , ALT , QUAL , FILTER , INFO , FORMAT and SAMPLES . INFO is a dict with key-value pairs and SAMPLES are a list of values for each sample. Each value is also a list of values for each FORMAT. If a record matches multiple fixes, the first one is applied. helpers \u2014 raw code the provide some helpers for the fixesThe code will automatically dedented if given as a string. A list of strings is also supported and will be joined with newlines. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . VcfAnno ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Annotate a VCF file using vcfanno https://github.com/brentp/vcfanno Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input conffile \u2014 The configuration file for vcfanno or configuration dictitself infile \u2014 The input VCF file Output outfile \u2014 The output VCF file Envs args \u2014 Additional arguments to pass to vcfanno conffile \u2014 configuration file for vcfanno or configuration dict itselfThis is ignored when conffile is given as input ncores \u2014 Number of cores to use vcfanno \u2014 Path to vcfanno Requires - name \u2014 vcfannocheck: | {{proc.envs.vcfanno}} --help Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . TruvariBench ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run truvari bench to compare a VCF with CNV calls andbase CNV standards Requires truvari v4+ See https://github.com/ACEnglish/truvari/wiki/bench Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input basevcf \u2014 The VCF file with standard CNVs compvcf \u2014 The VCF file with CNV calls to compare Output outdir \u2014 The output directory Envs ` ` \u2014 Ohter truvari bench arguments truvari \u2014 Path to truvari Requires truvari \u2014 check: {{proc.envs.truvari}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . TruvariBenchSummary ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Summarise the statistics from TruvariBench for multiple jobs (VCFs) Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indirs \u2014 The input directories, which should be the output directoriesof TruvariBench Output outdir \u2014 The output directory, including the summary table and plots Envs devpars \u2014 The parameters to use for the plots. plots \u2014 The stats to plot with barplots.Candidates are TP-base , TP-call , FP , FN , precision , recall , f1 , base cnt , call cnt , TP-call_TP-gt , TP-call_FP-gt , TP-base_TP-gt , TP-base_FP-gt , and gt_concordance See https://github.com/ACEnglish/truvari/wiki/bench Requires r-dplyr \u2014 check: {{proc.lang}} -e \"library(dplyr)\" r-ggplot2 \u2014 check: {{proc.lang}} -e \"library(ggplot2)\" r-ggprism \u2014 check: {{proc.lang}} -e \"library(ggprism)\" r-rjson \u2014 check: {{proc.lang}} -e \"library(rjson)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . TruvariConsistency ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Run truvari consistency to check consistency of CNV calls See https://github.com/ACEnglish/truvari/wiki/consistency Requires truvari v4+ Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input vcfs \u2014 The vcf files with CNV calls Output outfile \u2014 The output file with the report Envs heatmap \u2014 Whether to generate a heatmap of the consistencySet to False to disable annofile: The annotation file for the heatmap, multiple columns but the first column must be the sample name. Note that the stem of the vcf file name from consistency file will be used. These annotations will be added as row annotations. Other options see also biopipen.ns.plot.Heatmap . truvari \u2014 Path to truvari Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . BcftoolsAnnotate ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Add or remove annotations from VCF files See also: https://samtools.github.io/bcftools/bcftools.html#annotate Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input annfile \u2014 The annotation file.Currently only VCF files are supported. infile \u2014 The input VCF file Output outfile \u2014 The VCF file with annotations added or removed. Envs \u2014 Other arguments for bcftools annotate See also https://samtools.github.io/bcftools/bcftools.html#annotate Note that the underscore _ will be replaced with dash - in the argument name. annfile \u2014 The annotation file. If in.annfile is provided,this is ignored bcftools \u2014 Path to bcftools columns (auto) \u2014 Comma-separated or list of columns or tags to carry over fromthe annotation file. Overrides -c, --columns gz (flag) \u2014 Whether to gzip the output file header (list) \u2014 Headers to be added index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) ncores (type=int) \u2014 Number of cores ( --threads ) to use remove (auto) \u2014 Remove the specified columns from the input file tabix \u2014 Path to tabix, used to index infile and annfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . BcftoolsFilter ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Apply fixed threshold filters to VCF files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The filtered VCF file. If the in.infile is gzipped, this isgzipped as well. Envs \u2014 Other arguments for bcftools filter See also https://samtools.github.io/bcftools/bcftools.html#filter bcftools \u2014 Path to bcftools excludes \u2014 include/exclude only sites for which EXPRESSION is true.See: https://samtools.github.io/bcftools/bcftools.html#expressions If provided, envs.include/exclude will be ignored. If str / list used, The filter names will be Filter_<type>_<index> . A dict is used where keys are filter names and values are expressions gz (flag) \u2014 Whether to gzip the output file includes \u2014 and index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) keep \u2014 Whether we should keep the filtered variants or not.If True, the filtered variants will be kept in the output file, but with a new FILTER. ncores (type=int) \u2014 Number of cores ( --threads ) to use tabix \u2014 Path to tabix, used to index infile/outfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . BcftoolsSort ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Sort VCF files using bcftools sort . bcftools sort is used to sort VCF files by chromosome and position based on the order of contigs in the header. Here we provide a chrsize file to first sort the contigs in the header and then sort the VCF file using bcftools sort . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The sorted VCF file. Envs \u2014 Other arguments for bcftools sort . For example max_mem .See also https://samtools.github.io/bcftools/bcftools.html#sort bcftools \u2014 Path to bcftools chrsize \u2014 The chromosome size file, from which the chromosome order is usedto sort the contig in the header first. If not provided, bcftools sort will be used directly. gz (flag) \u2014 Whether to gzip the output file index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) ncores (type=int) \u2014 Number of cores ( --threads ) to use notfound (choice) \u2014 What if the contig in the VCF file is not found in the chrsize file. - error: Report error - remove: Remove the contig from the header. Note that if there are records with the removed contig, an error will be raised by bcftools sort - start: Move the contig to the start of the contigs from chrsize - end: Move the contig to the end of the contigs from chrsize tabix \u2014 Path to tabix, used to index infile/outfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . BcftoolsMerge ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Merge multiple VCF files using bcftools merge . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infiles \u2014 The input VCF files Output outfile \u2014 The merged VCF file. Envs \u2014 Other arguments for bcftools merge .See also https://samtools.github.io/bcftools/bcftools.html#merge bcftools \u2014 Path to bcftools gz (flag) \u2014 Whether to gzip the output file index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) ncores (type=int) \u2014 Number of cores ( --threads ) to use tabix \u2014 Path to tabix, used to index infile/outfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.vcf . BcftoolsView ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc View, subset and filter VCF files by position and filtering expression. Also convert between VCF and BCF. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file regions_file \u2014 The region file used to subset the input VCF file. samples_file \u2014 The samples file used to subset the input VCF file. Output outfile \u2014 The output VCF file. Envs \u2014 Other arguments for bcftools view .See also https://samtools.github.io/bcftools/bcftools.html#view Note that the underscore _ will be replaced with dash - in the argument name. bcftools \u2014 Path to bcftools gz (flag) \u2014 Whether to gzip the output file index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) ncores (type=int) \u2014 Number of cores ( --threads ) to use regions_file \u2014 The region file used to subset the input VCF file.If in.regions_file is provided, this is ignored. samples_file \u2014 The samples file used to subset the input VCF file.If in.samples_file is provided, this is ignored. tabix \u2014 Path to tabix, used to index infile/outfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.vcf"},{"location":"api/biopipen.ns.vcf/#biopipennsvcf","text":"</> Tools to handle VCF files Classes VcfLiftOver ( Proc ) \u2014 Liftover a VCF file using GATK </> VcfFilter ( Proc ) \u2014 Filter records in vcf file </> VcfIndex ( Proc ) \u2014 Index VCF files. If they are already index, use the index files </> Vcf2Bed ( Proc ) \u2014 Convert Vcf file to Bed file </> VcfDownSample ( Proc ) \u2014 Down-sample VCF files to keep only a subset of variants in there </> VcfSplitSamples ( Proc ) \u2014 Split a VCF file into multiple VCF files, one for each sample </> VcfIntersect ( Proc ) \u2014 Find variants in both VCF files </> VcfFix ( Proc ) \u2014 Fix some issues with VCF files </> VcfAnno ( Proc ) \u2014 Annotate a VCF file using vcfanno </> TruvariBench ( Proc ) \u2014 Run truvari bench to compare a VCF with CNV calls andbase CNV standards </> TruvariBenchSummary ( Proc ) \u2014 Summarise the statistics from TruvariBench for multiple jobs (VCFs) </> TruvariConsistency ( Proc ) \u2014 Run truvari consistency to check consistency of CNV calls </> BcftoolsAnnotate ( Proc ) \u2014 Add or remove annotations from VCF files </> BcftoolsFilter ( Proc ) \u2014 Apply fixed threshold filters to VCF files </> BcftoolsSort ( Proc ) \u2014 Sort VCF files using bcftools sort . </> BcftoolsMerge ( Proc ) \u2014 Merge multiple VCF files using bcftools merge . </> BcftoolsView ( Proc ) \u2014 View, subset and filter VCF files by position and filtering expression. </> class","title":"biopipen.ns.vcf"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcfliftover","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Liftover a VCF file using GATK Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input invcf \u2014 The input VCF file Output outvcf \u2014 The output VCF file Envs args \u2014 Other CLI arguments for gatk LiftoverVcf chain \u2014 The map chain file for liftover gatk \u2014 The path to gatk4, which should be installed via conda tmpdir \u2014 Directory for temporary storage of working files Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.VcfLiftOver"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcffilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Filter records in vcf file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input invcf \u2014 The input vcf file, could be bgzipped. Output outfile \u2014 The filtered vcf file. If in.invcf is bgzipped, thenthis will be bgzipped. Envs filter_descs \u2014 Descriptions for the filters. Will be saved to the headerof the output vcf file filters \u2014 A dict of filters with keys the filter names.","title":"biopipen.ns.vcf.VcfFilter"},{"location":"api/biopipen.ns.vcf/#typically","text":"lambda variant: Things to notice 1. Filters should return False to get variant filtered out 2. See https://brentp.github.io/cyvcf2/docstrings.html#cyvcf2.cyvcf2.Variant For what you can do with the variant 3. The filter python functions should be in string representation 4. Builtin filters can have parameters {\"QUAL\": 30} 5. List of builtin filters. Specify them like: {\"FILTER\": params} SNPONLY : keeps only SNPs ( {\"SNPONLY\": False} to filter SNPs out) QUAL : keeps variants with QUAL>=param ( {\"QUAL\": (30, False)} ) to keep only variants with QUAL<30 helper \u2014 Some helper code for the filters keep \u2014 Keep the variants not passing the filters? Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"Typically"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcfindex","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Index VCF files. If they are already index, use the index files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The output VCF file (bgzipped) outidx \u2014 The index file of the output VCF file Envs tabix \u2014 Path to tabix Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.VcfIndex"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcf2bed","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Convert Vcf file to Bed file Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The vcf file Output outfile \u2014 The converted bed file Envs inbase \u2014 The coordinate base of the vcf file outbase \u2014 The coordinate base of the base file Requires cyvcf2 \u2014 check: {{proc.lang}} -c \"import cyvcf2\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.Vcf2Bed"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_3","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcfdownsample","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Down-sample VCF files to keep only a subset of variants in there Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The output VCF file with subet variantsGzipped if in.infile is gzipped Envs n \u2014 Fraction/Number of variants to keepIf n > 1 , it is the number. If n <= 1 , it is the fraction. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.VcfDownSample"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_4","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_4","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_4","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_4","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_4","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_4","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_4","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcfsplitsamples","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Split a VCF file into multiple VCF files, one for each sample Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outdir \u2014 The output directory containing the split VCF files Envs bcftools \u2014 Path to bcftools gz \u2014 Gzip the output VCF files? Has to be True if envs.index is True index \u2014 Index the output VCF files? ncores \u2014 Number of cores, used to extract samples, but not to index private \u2014 Keep sites where only the sample carries an non-ref allele.That means, sites with genotypes like 0/0 will be removed. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.VcfSplitSamples"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_5","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_5","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_5","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_5","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_5","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_5","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_5","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcfintersect","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Find variants in both VCF files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile1 \u2014 The first VCF file infile2 \u2014 The second VCF file Output outfile \u2014 The output VCF file with subet variants in both files Envs bcftools \u2014 Path to bcftools collapse \u2014 How to match the variants in the two files? Will be passed to bcftools isec -c option. See also https://samtools.github.io/bcftools/bcftools.html#common_options - none: only records with identical REF and ALT alleles are compatible - some: only records where some subset of ALT alleles match are compatible - all: all records are compatible, regardless of whether the ALT alleles match or not. - snps: any SNP records are compatible, regardless of whether the ALT alleles match or not. - indels: any indel records are compatible, regardless of whether the ALT alleles match or not. - both: abbreviates snps and indels - id: only records with identical ID are compatible gz \u2014 Gzip the output VCF files? Has to be True if envs.index is True index \u2014 Index the output VCF files? keep_as \u2014 Keep the variants as presented in the first (0) orthe second (1) file? Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.VcfIntersect"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_6","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_6","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_6","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_6","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_6","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_6","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_6","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcffix","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Fix some issues with VCF files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The output VCF file Envs fixes \u2014 A list of fixes to apply.Each one is a dict with keys kind , id , regex and fix kind : The kind of fix. Including filter the FILTERs in the header, info the info INFOs in the header, contig the contig lines in the header format the FORMATs in the header, colnames the column names in the header header general header item variant the variants None matches everything id : The ID the match. If kind is filter , info , contig or format , then it matches the ID of the item. If kind is variant , then it matches the ID of the variant. If a list is given, then it matches any of the IDs in the list. regex : The regular expression to match. When id is given, this is ignored. append : Whether to append a record instead of to replace an existing one. When it is True, kind has to not be None fix : The fix to apply in the format of a lambda function (in string), with a single argument. The function should either return a string (raw representation) for the record, the record itself, None , False . If None is returned, the original record is used. if False , the record is removed. If append is True , then the function should either return a string or an object. And the argument is None The argument is a different object based on different kind s. When kind is None , the argument is the plain line of the record with line ending. When kind is info or format , the record is a dict with keys ID , Description , Type and Number . When kind is filter , the record is a dict with keys ID and Description . When kind is contig , the record is a dict with keys ID and length . When kind is header , the record is a dict with key the name of the header and value the value of the header. When kind is colnames , the record is a list of column names. When kind is variant , the record is a dict with keys CHROM , POS , REF , ALT , QUAL , FILTER , INFO , FORMAT and SAMPLES . INFO is a dict with key-value pairs and SAMPLES are a list of values for each sample. Each value is also a list of values for each FORMAT. If a record matches multiple fixes, the first one is applied. helpers \u2014 raw code the provide some helpers for the fixesThe code will automatically dedented if given as a string. A list of strings is also supported and will be joined with newlines. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.VcfFix"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_7","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_7","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_7","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_7","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_7","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_7","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_7","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfvcfanno","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Annotate a VCF file using vcfanno https://github.com/brentp/vcfanno Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input conffile \u2014 The configuration file for vcfanno or configuration dictitself infile \u2014 The input VCF file Output outfile \u2014 The output VCF file Envs args \u2014 Additional arguments to pass to vcfanno conffile \u2014 configuration file for vcfanno or configuration dict itselfThis is ignored when conffile is given as input ncores \u2014 Number of cores to use vcfanno \u2014 Path to vcfanno Requires - name \u2014 vcfannocheck: | {{proc.envs.vcfanno}} --help Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.VcfAnno"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_8","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_8","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_8","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_8","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_8","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_8","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_8","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcftruvaribench","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run truvari bench to compare a VCF with CNV calls andbase CNV standards Requires truvari v4+ See https://github.com/ACEnglish/truvari/wiki/bench Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input basevcf \u2014 The VCF file with standard CNVs compvcf \u2014 The VCF file with CNV calls to compare Output outdir \u2014 The output directory Envs ` ` \u2014 Ohter truvari bench arguments truvari \u2014 Path to truvari Requires truvari \u2014 check: {{proc.envs.truvari}} version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.TruvariBench"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_9","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_9","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_9","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_9","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_9","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_9","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_9","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcftruvaribenchsummary","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Summarise the statistics from TruvariBench for multiple jobs (VCFs) Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input indirs \u2014 The input directories, which should be the output directoriesof TruvariBench Output outdir \u2014 The output directory, including the summary table and plots Envs devpars \u2014 The parameters to use for the plots. plots \u2014 The stats to plot with barplots.Candidates are TP-base , TP-call , FP , FN , precision , recall , f1 , base cnt , call cnt , TP-call_TP-gt , TP-call_FP-gt , TP-base_TP-gt , TP-base_FP-gt , and gt_concordance See https://github.com/ACEnglish/truvari/wiki/bench Requires r-dplyr \u2014 check: {{proc.lang}} -e \"library(dplyr)\" r-ggplot2 \u2014 check: {{proc.lang}} -e \"library(ggplot2)\" r-ggprism \u2014 check: {{proc.lang}} -e \"library(ggprism)\" r-rjson \u2014 check: {{proc.lang}} -e \"library(rjson)\" Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.TruvariBenchSummary"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_10","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_10","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_10","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_10","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_10","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_10","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_10","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcftruvariconsistency","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Run truvari consistency to check consistency of CNV calls See https://github.com/ACEnglish/truvari/wiki/consistency Requires truvari v4+ Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input vcfs \u2014 The vcf files with CNV calls Output outfile \u2014 The output file with the report Envs heatmap \u2014 Whether to generate a heatmap of the consistencySet to False to disable annofile: The annotation file for the heatmap, multiple columns but the first column must be the sample name. Note that the stem of the vcf file name from consistency file will be used. These annotations will be added as row annotations. Other options see also biopipen.ns.plot.Heatmap . truvari \u2014 Path to truvari Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.TruvariConsistency"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_11","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_11","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_11","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_11","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_11","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_11","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_11","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfbcftoolsannotate","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Add or remove annotations from VCF files See also: https://samtools.github.io/bcftools/bcftools.html#annotate Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input annfile \u2014 The annotation file.Currently only VCF files are supported. infile \u2014 The input VCF file Output outfile \u2014 The VCF file with annotations added or removed. Envs \u2014 Other arguments for bcftools annotate See also https://samtools.github.io/bcftools/bcftools.html#annotate Note that the underscore _ will be replaced with dash - in the argument name. annfile \u2014 The annotation file. If in.annfile is provided,this is ignored bcftools \u2014 Path to bcftools columns (auto) \u2014 Comma-separated or list of columns or tags to carry over fromthe annotation file. Overrides -c, --columns gz (flag) \u2014 Whether to gzip the output file header (list) \u2014 Headers to be added index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) ncores (type=int) \u2014 Number of cores ( --threads ) to use remove (auto) \u2014 Remove the specified columns from the input file tabix \u2014 Path to tabix, used to index infile and annfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.BcftoolsAnnotate"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_12","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_12","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_12","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_12","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_12","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_12","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_12","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfbcftoolsfilter","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Apply fixed threshold filters to VCF files Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The filtered VCF file. If the in.infile is gzipped, this isgzipped as well. Envs \u2014 Other arguments for bcftools filter See also https://samtools.github.io/bcftools/bcftools.html#filter bcftools \u2014 Path to bcftools excludes \u2014 include/exclude only sites for which EXPRESSION is true.See: https://samtools.github.io/bcftools/bcftools.html#expressions If provided, envs.include/exclude will be ignored. If str / list used, The filter names will be Filter_<type>_<index> . A dict is used where keys are filter names and values are expressions gz (flag) \u2014 Whether to gzip the output file includes \u2014 and index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) keep \u2014 Whether we should keep the filtered variants or not.If True, the filtered variants will be kept in the output file, but with a new FILTER. ncores (type=int) \u2014 Number of cores ( --threads ) to use tabix \u2014 Path to tabix, used to index infile/outfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.BcftoolsFilter"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_13","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_13","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_13","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_13","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_13","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_13","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_13","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfbcftoolssort","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Sort VCF files using bcftools sort . bcftools sort is used to sort VCF files by chromosome and position based on the order of contigs in the header. Here we provide a chrsize file to first sort the contigs in the header and then sort the VCF file using bcftools sort . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file Output outfile \u2014 The sorted VCF file. Envs \u2014 Other arguments for bcftools sort . For example max_mem .See also https://samtools.github.io/bcftools/bcftools.html#sort bcftools \u2014 Path to bcftools chrsize \u2014 The chromosome size file, from which the chromosome order is usedto sort the contig in the header first. If not provided, bcftools sort will be used directly. gz (flag) \u2014 Whether to gzip the output file index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) ncores (type=int) \u2014 Number of cores ( --threads ) to use notfound (choice) \u2014 What if the contig in the VCF file is not found in the chrsize file. - error: Report error - remove: Remove the contig from the header. Note that if there are records with the removed contig, an error will be raised by bcftools sort - start: Move the contig to the start of the contigs from chrsize - end: Move the contig to the end of the contigs from chrsize tabix \u2014 Path to tabix, used to index infile/outfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.BcftoolsSort"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_14","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_14","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_14","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_14","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_14","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_14","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_14","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfbcftoolsmerge","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Merge multiple VCF files using bcftools merge . Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infiles \u2014 The input VCF files Output outfile \u2014 The merged VCF file. Envs \u2014 Other arguments for bcftools merge .See also https://samtools.github.io/bcftools/bcftools.html#merge bcftools \u2014 Path to bcftools gz (flag) \u2014 Whether to gzip the output file index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) ncores (type=int) \u2014 Number of cores ( --threads ) to use tabix \u2014 Path to tabix, used to index infile/outfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.BcftoolsMerge"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_15","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_15","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_15","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_15","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_15","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_15","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_15","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.vcf/#biopipennsvcfbcftoolsview","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc View, subset and filter VCF files by position and filtering expression. Also convert between VCF and BCF. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input infile \u2014 The input VCF file regions_file \u2014 The region file used to subset the input VCF file. samples_file \u2014 The samples file used to subset the input VCF file. Output outfile \u2014 The output VCF file. Envs \u2014 Other arguments for bcftools view .See also https://samtools.github.io/bcftools/bcftools.html#view Note that the underscore _ will be replaced with dash - in the argument name. bcftools \u2014 Path to bcftools gz (flag) \u2014 Whether to gzip the output file index (flag) \u2014 Whether to index the output file (tbi) ( envs.gz forced to True) ncores (type=int) \u2014 Number of cores ( --threads ) to use regions_file \u2014 The region file used to subset the input VCF file.If in.regions_file is provided, this is ignored. samples_file \u2014 The samples file used to subset the input VCF file.If in.samples_file is provided, this is ignored. tabix \u2014 Path to tabix, used to index infile/outfile Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.vcf.BcftoolsView"},{"location":"api/biopipen.ns.vcf/#pipenprocprocmeta_16","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.vcf/#pipenprocprocfrom_proc_16","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_subclass_16","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.vcf/#pipenprocprocinit_16","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.vcf/#pipenprocprocgc_16","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.vcf/#pipenprocproclog_16","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.vcf/#pipenprocprocrun_16","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.web/","text":"module biopipen.ns . web </> Get data from the web Classes Download ( Proc ) \u2014 Download data from URLs </> DownloadList ( Proc ) \u2014 Download data from URLs in a file. </> GCloudStorageDownloadFile ( Proc ) \u2014 Download file from Google Cloud Storage </> GCloudStorageDownloadBucket ( Proc ) \u2014 Download all files from a Google Cloud Storage bucket </> class biopipen.ns.web . Download ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Download data from URLs Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input url \u2014 The URL to download data from Output outfile \u2014 The file downloaded Envs args \u2014 The arguments to pass to the tool aria2c \u2014 Path to aria2c ncores \u2014 The number of cores to use tool (choice) \u2014 Which tool to use to download the data - wget: Use wget - aria2c: Use aria2c - urllib: Use python's urllib - aria: Alias for aria2c wget \u2014 Path to wget Requires aria2c \u2014 Only required when envs.tool == \"aria2c\" - check: {{proc.envs.aria2c}} --version wget \u2014 Only required when envs.tool == \"wget\" - check: {{proc.envs.wget}} --version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.web . DownloadList ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Download data from URLs in a file. This does not work by iterating over the URLs in the file. The whole file is passed to wget or aria2c at once. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input urlfile \u2014 The file containing the URLs to download data from Output outdir \u2014 The directory containing the downloaded files Envs args \u2014 The arguments to pass to the tool aria2c \u2014 Path to aria2c ncores \u2014 The number of cores to use tool (choice) \u2014 Which tool to use to download the data - wget: Use wget - aria2c: Use aria2c - urllib: Use python's urllib - aria: Alias for aria2c wget \u2014 Path to wget Requires aria2c \u2014 Only required when envs.tool == \"aria2c\" - check: {{proc.envs.aria2c}} --version wget \u2014 Only required when envs.tool == \"wget\" - check: {{proc.envs.wget}} --version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.web . GCloudStorageDownloadFile ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Download file from Google Cloud Storage Before using this, make sure you have the gcloud tool installed and logged in with the appropriate credentials using gcloud auth login . Also make sure you have google-crc32c installed to verify the integrity of the downloaded files. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input url \u2014 The URL to download data from.It should be in the format gs://bucket/path/to/file Output outfile \u2014 The file downloaded Envs args (ns) \u2014 Other arguments to pass to the gcloud storage cp command - do_not_decompress (flag): Do not decompress the file. - : More arguments to pass to the gcloud storage cp command See gcloud storage cp --help for more information gcloud \u2014 Path to gcloud Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process class biopipen.ns.web . GCloudStorageDownloadBucket ( *args , **kwds ) \u2192 Proc </> Bases biopipen.core.proc.Proc pipen.proc.Proc Download all files from a Google Cloud Storage bucket Before using this, make sure you have the gcloud tool installed and logged in with the appropriate credentials using gcloud auth login . Note that this will not use the --recursive flag of gcloud storage cp . The files will be listed and downloaded one by one so that they can be parallelized. Also make sure you have google-crc32c installed to verify the integrity of the downloaded files. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input url \u2014 The URL to download data from.It should be in the format gs://bucket Output outdir \u2014 The directory containing the downloaded files Envs args (ns) \u2014 Other arguments to pass to the gcloud storage cp command - do_not_decompress (flag): Do not decompress the file. - : More arguments to pass to the gcloud storage cp command See gcloud storage cp --help for more information gcloud \u2014 Path to gcloud keep_structure (flag) \u2014 Keep the directory structure of the bucket ncores (type=int) \u2014 The number of cores to use to download the files in parallel Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method init ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method run ( ) </> Run the process","title":"biopipen.ns.web"},{"location":"api/biopipen.ns.web/#biopipennsweb","text":"</> Get data from the web Classes Download ( Proc ) \u2014 Download data from URLs </> DownloadList ( Proc ) \u2014 Download data from URLs in a file. </> GCloudStorageDownloadFile ( Proc ) \u2014 Download file from Google Cloud Storage </> GCloudStorageDownloadBucket ( Proc ) \u2014 Download all files from a Google Cloud Storage bucket </> class","title":"biopipen.ns.web"},{"location":"api/biopipen.ns.web/#biopipennswebdownload","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Download data from URLs Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input url \u2014 The URL to download data from Output outfile \u2014 The file downloaded Envs args \u2014 The arguments to pass to the tool aria2c \u2014 Path to aria2c ncores \u2014 The number of cores to use tool (choice) \u2014 Which tool to use to download the data - wget: Use wget - aria2c: Use aria2c - urllib: Use python's urllib - aria: Alias for aria2c wget \u2014 Path to wget Requires aria2c \u2014 Only required when envs.tool == \"aria2c\" - check: {{proc.envs.aria2c}} --version wget \u2014 Only required when envs.tool == \"wget\" - check: {{proc.envs.wget}} --version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.web.Download"},{"location":"api/biopipen.ns.web/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.web/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.web/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.web/#pipenprocprocinit","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.web/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.web/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.web/#pipenprocprocrun","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.web/#biopipennswebdownloadlist","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Download data from URLs in a file. This does not work by iterating over the URLs in the file. The whole file is passed to wget or aria2c at once. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input urlfile \u2014 The file containing the URLs to download data from Output outdir \u2014 The directory containing the downloaded files Envs args \u2014 The arguments to pass to the tool aria2c \u2014 Path to aria2c ncores \u2014 The number of cores to use tool (choice) \u2014 Which tool to use to download the data - wget: Use wget - aria2c: Use aria2c - urllib: Use python's urllib - aria: Alias for aria2c wget \u2014 Path to wget Requires aria2c \u2014 Only required when envs.tool == \"aria2c\" - check: {{proc.envs.aria2c}} --version wget \u2014 Only required when envs.tool == \"wget\" - check: {{proc.envs.wget}} --version Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.web.DownloadList"},{"location":"api/biopipen.ns.web/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.web/#pipenprocprocfrom_proc_1","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.web/#pipenprocprocinit_subclass_1","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.web/#pipenprocprocinit_1","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.web/#pipenprocprocgc_1","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.web/#pipenprocproclog_1","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.web/#pipenprocprocrun_1","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.web/#biopipennswebgcloudstoragedownloadfile","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Download file from Google Cloud Storage Before using this, make sure you have the gcloud tool installed and logged in with the appropriate credentials using gcloud auth login . Also make sure you have google-crc32c installed to verify the integrity of the downloaded files. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input url \u2014 The URL to download data from.It should be in the format gs://bucket/path/to/file Output outfile \u2014 The file downloaded Envs args (ns) \u2014 Other arguments to pass to the gcloud storage cp command - do_not_decompress (flag): Do not decompress the file. - : More arguments to pass to the gcloud storage cp command See gcloud storage cp --help for more information gcloud \u2014 Path to gcloud Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.web.GCloudStorageDownloadFile"},{"location":"api/biopipen.ns.web/#pipenprocprocmeta_2","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.web/#pipenprocprocfrom_proc_2","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.web/#pipenprocprocinit_subclass_2","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.web/#pipenprocprocinit_2","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.web/#pipenprocprocgc_2","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.web/#pipenprocproclog_2","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.web/#pipenprocprocrun_2","text":"</> Run the process class","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.ns.web/#biopipennswebgcloudstoragedownloadbucket","text":"</> Bases biopipen.core.proc.Proc pipen.proc.Proc Download all files from a Google Cloud Storage bucket Before using this, make sure you have the gcloud tool installed and logged in with the appropriate credentials using gcloud auth login . Note that this will not use the --recursive flag of gcloud storage cp . The files will be listed and downloaded one by one so that they can be parallelized. Also make sure you have google-crc32c installed to verify the integrity of the downloaded files. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Input url \u2014 The URL to download data from.It should be in the format gs://bucket Output outdir \u2014 The directory containing the downloaded files Envs args (ns) \u2014 Other arguments to pass to the gcloud storage cp command - do_not_decompress (flag): Do not decompress the file. - : More arguments to pass to the gcloud storage cp command See gcloud storage cp --help for more information gcloud \u2014 Path to gcloud keep_structure (flag) \u2014 Keep the directory structure of the bucket ncores (type=int) \u2014 The number of cores to use to download the files in parallel Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> init ( ) \u2014 Init all other properties and jobs </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Run the process </> class","title":"biopipen.ns.web.GCloudStorageDownloadBucket"},{"location":"api/biopipen.ns.web/#pipenprocprocmeta_3","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) (Proc) \u2014 Make sure Proc subclasses are singletons </> __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns (Proc) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/biopipen.ns.web/#pipenprocprocfrom_proc_3","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/biopipen.ns.web/#pipenprocprocinit_subclass_3","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/biopipen.ns.web/#pipenprocprocinit_3","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.init"},{"location":"api/biopipen.ns.web/#pipenprocprocgc_3","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/biopipen.ns.web/#pipenprocproclog_3","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/biopipen.ns.web/#pipenprocprocrun_3","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/biopipen.utils.common_docstrs/","text":"module biopipen.utils . common_docstrs </> Common docstrings for biopipen procs. Functions format_placeholder ( **kwargs ) (Callable) \u2014 A decorator to format a docstring placeholder. </> indent_docstr ( docstr , indent ) (str) \u2014 Indent the docstring. </> function biopipen.utils.common_docstrs . indent_docstr ( docstr , indent ) </> Indent the docstring. Parameters docstr (str) \u2014 The docstring. indent (str) \u2014 The indent. Returns (str) The indented docstring. function biopipen.utils.common_docstrs . format_placeholder ( **kwargs ) </> A decorator to format a docstring placeholder. Parameters **kwargs \u2014 The docstring placeholder. Returns (Callable) The decorated function.","title":"biopipen.utils.common_docstrs"},{"location":"api/biopipen.utils.common_docstrs/#biopipenutilscommon_docstrs","text":"</> Common docstrings for biopipen procs. Functions format_placeholder ( **kwargs ) (Callable) \u2014 A decorator to format a docstring placeholder. </> indent_docstr ( docstr , indent ) (str) \u2014 Indent the docstring. </> function","title":"biopipen.utils.common_docstrs"},{"location":"api/biopipen.utils.common_docstrs/#biopipenutilscommon_docstrsindent_docstr","text":"</> Indent the docstring. Parameters docstr (str) \u2014 The docstring. indent (str) \u2014 The indent. Returns (str) The indented docstring. function","title":"biopipen.utils.common_docstrs.indent_docstr"},{"location":"api/biopipen.utils.common_docstrs/#biopipenutilscommon_docstrsformat_placeholder","text":"</> A decorator to format a docstring placeholder. Parameters **kwargs \u2014 The docstring placeholder. Returns (Callable) The decorated function.","title":"biopipen.utils.common_docstrs.format_placeholder"},{"location":"api/biopipen.utils.gene/","text":"module biopipen.utils . gene </> Do gene name conversion Classes QueryGenesNotFound \u2014 When genes cannot be found </> Functions gene_name_conversion ( genes , infmt , outfmt , dup , species , notfound , suppress_messages ) \u2014 Convert gene names using MyGeneInfo </> class biopipen.utils.gene . QueryGenesNotFound ( ) </> Bases ValueError Exception BaseException When genes cannot be found function biopipen.utils.gene . gene_name_conversion ( genes , infmt , outfmt , dup='first' , species='human' , notfound='na' , suppress_messages=False ) </> Convert gene names using MyGeneInfo Parameters genes (list) \u2014 A character/integer vector of gene names/ids infmt (str | list[str]) \u2014 A character vector of input gene name formatsSee the available scopes at https://docs.mygene.info/en/latest/doc/data.html#available-fields You can use ensg as a shortcut for ensembl.gene outfmt (str) \u2014 A character vector of output gene name formats dup (str, optional) \u2014 How to deal with duplicate gene names found.first: keep the first one (default), sorted by score descendingly last: keep the last one, sorted by score descendingly all: keep all of them, each will be a separate row : combine them into a single string, separated by X species (str, optional) \u2014 A character vector of species names notfound (str, optional) \u2014 How to deal with gene names that are not founderror: stop with an error message use-query: use the query gene name as the converted gene name skip: skip the gene names that are not found ignore: Same as \"skip\" na: use NA as the converted gene name (default) suppress_messages (bool, optional) \u2014 Suppress the messages while querying Returns A dataframe with the query gene names and the converted gene namesWhen a gene name is not found, the converted name will be \"NA\" When duplicate gene names are found, the one with the highest score will be kept","title":"biopipen.utils.gene"},{"location":"api/biopipen.utils.gene/#biopipenutilsgene","text":"</> Do gene name conversion Classes QueryGenesNotFound \u2014 When genes cannot be found </> Functions gene_name_conversion ( genes , infmt , outfmt , dup , species , notfound , suppress_messages ) \u2014 Convert gene names using MyGeneInfo </> class","title":"biopipen.utils.gene"},{"location":"api/biopipen.utils.gene/#biopipenutilsgenequerygenesnotfound","text":"</> Bases ValueError Exception BaseException When genes cannot be found function","title":"biopipen.utils.gene.QueryGenesNotFound"},{"location":"api/biopipen.utils.gene/#biopipenutilsgenegene_name_conversion","text":"</> Convert gene names using MyGeneInfo Parameters genes (list) \u2014 A character/integer vector of gene names/ids infmt (str | list[str]) \u2014 A character vector of input gene name formatsSee the available scopes at https://docs.mygene.info/en/latest/doc/data.html#available-fields You can use ensg as a shortcut for ensembl.gene outfmt (str) \u2014 A character vector of output gene name formats dup (str, optional) \u2014 How to deal with duplicate gene names found.first: keep the first one (default), sorted by score descendingly last: keep the last one, sorted by score descendingly all: keep all of them, each will be a separate row : combine them into a single string, separated by X species (str, optional) \u2014 A character vector of species names notfound (str, optional) \u2014 How to deal with gene names that are not founderror: stop with an error message use-query: use the query gene name as the converted gene name skip: skip the gene names that are not found ignore: Same as \"skip\" na: use NA as the converted gene name (default) suppress_messages (bool, optional) \u2014 Suppress the messages while querying Returns A dataframe with the query gene names and the converted gene namesWhen a gene name is not found, the converted name will be \"NA\" When duplicate gene names are found, the one with the highest score will be kept","title":"biopipen.utils.gene.gene_name_conversion"},{"location":"api/biopipen.utils/","text":"package biopipen. utils </> module biopipen.utils . reference </> Utilities for indexing reference files Functions bam_index ( bam , bamdir , tool , samtools , sambamba , ncores , ext , force ) (Path) \u2014 Index a bam file </> tabix_index ( infile , preset , tmpdir , tabix ) (str | os.pathlike) \u2014 Index input file using tabix </> module biopipen.utils . common_docstrs </> Common docstrings for biopipen procs. Functions format_placeholder ( **kwargs ) (Callable) \u2014 A decorator to format a docstring placeholder. </> indent_docstr ( docstr , indent ) (str) \u2014 Indent the docstring. </> module biopipen.utils . gene </> Do gene name conversion Classes QueryGenesNotFound \u2014 When genes cannot be found </> Functions gene_name_conversion ( genes , infmt , outfmt , dup , species , notfound , suppress_messages ) \u2014 Convert gene names using MyGeneInfo </>","title":"biopipen.utils"},{"location":"api/biopipen.utils/#biopipenutils","text":"</> module","title":"biopipen.utils"},{"location":"api/biopipen.utils/#biopipenutilsreference","text":"</> Utilities for indexing reference files Functions bam_index ( bam , bamdir , tool , samtools , sambamba , ncores , ext , force ) (Path) \u2014 Index a bam file </> tabix_index ( infile , preset , tmpdir , tabix ) (str | os.pathlike) \u2014 Index input file using tabix </> module","title":"biopipen.utils.reference"},{"location":"api/biopipen.utils/#biopipenutilscommon_docstrs","text":"</> Common docstrings for biopipen procs. Functions format_placeholder ( **kwargs ) (Callable) \u2014 A decorator to format a docstring placeholder. </> indent_docstr ( docstr , indent ) (str) \u2014 Indent the docstring. </> module","title":"biopipen.utils.common_docstrs"},{"location":"api/biopipen.utils/#biopipenutilsgene","text":"</> Do gene name conversion Classes QueryGenesNotFound \u2014 When genes cannot be found </> Functions gene_name_conversion ( genes , infmt , outfmt , dup , species , notfound , suppress_messages ) \u2014 Convert gene names using MyGeneInfo </>","title":"biopipen.utils.gene"},{"location":"api/biopipen.utils.misc/","text":"module biopipen.utils . misc </> Functions run_command ( cmd , fg , wait , print_command , print_command_handler , **kwargs ) (subprocess.popen | str) \u2014 Run a command. </> function biopipen.utils.misc . run_command ( cmd , fg=False , wait=True , print_command=True , print_command_handler=<built-in function print> , **kwargs ) </> Run a command. Parameters cmd (Union) \u2014 A string or list of strings representing the command to run. fg (bool, optional) \u2014 Whether to run the command in the foreground.Redirects stdout and stderr to the current process. wait (bool, optional) \u2014 Whether to wait for the command to finish.The command will be waited for if fg is True . print_command (bool, optional) \u2014 Whether to print the command before running it. print_command_handler (Callable, optional) \u2014 The function to use to print the command. kwargs \u2014 Keyword arguments to pass to subprocess.Popen . Returns (subprocess.popen | str) The Popen object, or str when stdout is RETURN or return .","title":"biopipen.utils.misc"},{"location":"api/biopipen.utils.misc/#biopipenutilsmisc","text":"</> Functions run_command ( cmd , fg , wait , print_command , print_command_handler , **kwargs ) (subprocess.popen | str) \u2014 Run a command. </> function","title":"biopipen.utils.misc"},{"location":"api/biopipen.utils.misc/#biopipenutilsmiscrun_command","text":"</> Run a command. Parameters cmd (Union) \u2014 A string or list of strings representing the command to run. fg (bool, optional) \u2014 Whether to run the command in the foreground.Redirects stdout and stderr to the current process. wait (bool, optional) \u2014 Whether to wait for the command to finish.The command will be waited for if fg is True . print_command (bool, optional) \u2014 Whether to print the command before running it. print_command_handler (Callable, optional) \u2014 The function to use to print the command. kwargs \u2014 Keyword arguments to pass to subprocess.Popen . Returns (subprocess.popen | str) The Popen object, or str when stdout is RETURN or return .","title":"biopipen.utils.misc.run_command"},{"location":"api/biopipen.utils.reference/","text":"module biopipen.utils . reference </> Utilities for indexing reference files Functions bam_index ( bam , bamdir , tool , samtools , sambamba , ncores , ext , force ) (Path) \u2014 Index a bam file </> tabix_index ( infile , preset , tmpdir , tabix ) (str | os.pathlike) \u2014 Index input file using tabix </> function biopipen.utils.reference . tabix_index ( infile , preset , tmpdir=None , tabix='tabix' ) </> Index input file using tabix Try to check if there is an index file in the same directory where infile is. If so, return the infile Otherwise, check if infile is bgzipped, if not bgzip it and save it in tmpdir Index the bgzipped file and return the bgzipped file Parameters infile (str | os.pathlike) \u2014 The input file to be indexed preset (Literal) \u2014 The preset used to index the file tmpdir (Union, optional) \u2014 The directory to link the infile there and index itIf False, try to index the infile directly. The directory where the infile is should be writable. tabix (str, optional) \u2014 The path to tabix Returns (str | os.pathlike) The infile itself or re-bgzipped infile. This file comes with theindex file in the same directory function biopipen.utils.reference . bam_index ( bam , bamdir='/tmp' , tool='samtools' , samtools='samtools' , sambamba='sambamba' , ncores=1 , ext='.bam.bai' , force=False ) </> Index a bam file First look for the index file in the same directory as the bam file, if found, return the bam file. Otherwise, generate a symbolic link of the bam file in bamdir, and generate a index there, return the path to the symbolic link Parameters bam (str | pathlib.path) \u2014 The path to the bam file bamdir (pathlib.path | str, optional) \u2014 If index file can't be found in the directory as the bam file,create a symbolic link to the bam file, and generate the index here tool (str, optional) \u2014 The tool used to generate the index file, either samtools or sambamba samtools (str, optional) \u2014 The path to samtools sambamba (str, optional) \u2014 The path to sambamba ncores (int, optional) \u2014 Number of cores (threads) used to generate the index file ext (str, optional) \u2014 The ext of the index file, default .bam.bai , in case, .bai isalso treated as index file force (bool, optional) \u2014 Force to generate the index file, with given bamfile.Don't check if the index file exists. Returns (Path) The bam file if index exists in the directory as the bam file.Otherwise symbolic link to the bam file in bamdir.","title":"biopipen.utils.reference"},{"location":"api/biopipen.utils.reference/#biopipenutilsreference","text":"</> Utilities for indexing reference files Functions bam_index ( bam , bamdir , tool , samtools , sambamba , ncores , ext , force ) (Path) \u2014 Index a bam file </> tabix_index ( infile , preset , tmpdir , tabix ) (str | os.pathlike) \u2014 Index input file using tabix </> function","title":"biopipen.utils.reference"},{"location":"api/biopipen.utils.reference/#biopipenutilsreferencetabix_index","text":"</> Index input file using tabix Try to check if there is an index file in the same directory where infile is. If so, return the infile Otherwise, check if infile is bgzipped, if not bgzip it and save it in tmpdir Index the bgzipped file and return the bgzipped file Parameters infile (str | os.pathlike) \u2014 The input file to be indexed preset (Literal) \u2014 The preset used to index the file tmpdir (Union, optional) \u2014 The directory to link the infile there and index itIf False, try to index the infile directly. The directory where the infile is should be writable. tabix (str, optional) \u2014 The path to tabix Returns (str | os.pathlike) The infile itself or re-bgzipped infile. This file comes with theindex file in the same directory function","title":"biopipen.utils.reference.tabix_index"},{"location":"api/biopipen.utils.reference/#biopipenutilsreferencebam_index","text":"</> Index a bam file First look for the index file in the same directory as the bam file, if found, return the bam file. Otherwise, generate a symbolic link of the bam file in bamdir, and generate a index there, return the path to the symbolic link Parameters bam (str | pathlib.path) \u2014 The path to the bam file bamdir (pathlib.path | str, optional) \u2014 If index file can't be found in the directory as the bam file,create a symbolic link to the bam file, and generate the index here tool (str, optional) \u2014 The tool used to generate the index file, either samtools or sambamba samtools (str, optional) \u2014 The path to samtools sambamba (str, optional) \u2014 The path to sambamba ncores (int, optional) \u2014 Number of cores (threads) used to generate the index file ext (str, optional) \u2014 The ext of the index file, default .bam.bai , in case, .bai isalso treated as index file force (bool, optional) \u2014 Force to generate the index file, with given bamfile.Don't check if the index file exists. Returns (Path) The bam file if index exists in the directory as the bam file.Otherwise symbolic link to the bam file in bamdir.","title":"biopipen.utils.reference.bam_index"},{"location":"api/biopipen.utils.vcf/","text":"module biopipen.utils . vcf </> Classes HeaderItem \u2014 The base class of header items </> HeaderInfo \u2014 The INFO items in the header </> HeaderFormat \u2014 The FORMAT items in the header </> HeaderFilter \u2014 The FILTER items in the header </> HeaderContig \u2014 The contig items in the header </> HeaderGeneral \u2014 The general items in the header </> Fields \u2014 The fields/column names </> Info \u2014 The INFO of the variant </> Format \u2014 The FORMAT of the variant </> Alt \u2014 The ALT of the variant </> Filter \u2014 The FILTER of the variant </> Sample \u2014 One sample of the variant </> Samples \u2014 The samples of the variant </> class biopipen.utils.vcf . HeaderItem ( *args , **kwargs ) </> Bases dict The base class of header items class biopipen.utils.vcf . HeaderInfo ( *args , **kwargs ) </> Bases biopipen.utils.vcf.HeaderItem dict The INFO items in the header class biopipen.utils.vcf . HeaderFormat ( *args , **kwargs ) </> Bases biopipen.utils.vcf.HeaderItem dict The FORMAT items in the header class biopipen.utils.vcf . HeaderFilter ( *args , **kwargs ) </> Bases biopipen.utils.vcf.HeaderItem dict The FILTER items in the header class biopipen.utils.vcf . HeaderContig ( *args , **kwargs ) </> Bases biopipen.utils.vcf.HeaderItem dict The contig items in the header class biopipen.utils.vcf . HeaderGeneral ( *args , **kwargs ) </> Bases biopipen.utils.vcf.HeaderItem dict The general items in the header class biopipen.utils.vcf . Fields ( *args , **kwargs ) </> Bases list The fields/column names class biopipen.utils.vcf . Info ( ) </> Bases dict The INFO of the variant class biopipen.utils.vcf . Format ( iterable=() ) </> Bases list The FORMAT of the variant class biopipen.utils.vcf . Alt ( iterable=() ) </> Bases list The ALT of the variant class biopipen.utils.vcf . Filter ( iterable=() ) </> Bases list The FILTER of the variant class biopipen.utils.vcf . Sample ( values , format ) </> Bases dict One sample of the variant class biopipen.utils.vcf . Samples ( samples , format ) </> Bases list The samples of the variant","title":"biopipen.utils.vcf"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcf","text":"</> Classes HeaderItem \u2014 The base class of header items </> HeaderInfo \u2014 The INFO items in the header </> HeaderFormat \u2014 The FORMAT items in the header </> HeaderFilter \u2014 The FILTER items in the header </> HeaderContig \u2014 The contig items in the header </> HeaderGeneral \u2014 The general items in the header </> Fields \u2014 The fields/column names </> Info \u2014 The INFO of the variant </> Format \u2014 The FORMAT of the variant </> Alt \u2014 The ALT of the variant </> Filter \u2014 The FILTER of the variant </> Sample \u2014 One sample of the variant </> Samples \u2014 The samples of the variant </> class","title":"biopipen.utils.vcf"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfheaderitem","text":"</> Bases dict The base class of header items class","title":"biopipen.utils.vcf.HeaderItem"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfheaderinfo","text":"</> Bases biopipen.utils.vcf.HeaderItem dict The INFO items in the header class","title":"biopipen.utils.vcf.HeaderInfo"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfheaderformat","text":"</> Bases biopipen.utils.vcf.HeaderItem dict The FORMAT items in the header class","title":"biopipen.utils.vcf.HeaderFormat"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfheaderfilter","text":"</> Bases biopipen.utils.vcf.HeaderItem dict The FILTER items in the header class","title":"biopipen.utils.vcf.HeaderFilter"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfheadercontig","text":"</> Bases biopipen.utils.vcf.HeaderItem dict The contig items in the header class","title":"biopipen.utils.vcf.HeaderContig"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfheadergeneral","text":"</> Bases biopipen.utils.vcf.HeaderItem dict The general items in the header class","title":"biopipen.utils.vcf.HeaderGeneral"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcffields","text":"</> Bases list The fields/column names class","title":"biopipen.utils.vcf.Fields"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfinfo","text":"</> Bases dict The INFO of the variant class","title":"biopipen.utils.vcf.Info"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfformat","text":"</> Bases list The FORMAT of the variant class","title":"biopipen.utils.vcf.Format"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfalt","text":"</> Bases list The ALT of the variant class","title":"biopipen.utils.vcf.Alt"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcffilter","text":"</> Bases list The FILTER of the variant class","title":"biopipen.utils.vcf.Filter"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfsample","text":"</> Bases dict One sample of the variant class","title":"biopipen.utils.vcf.Sample"},{"location":"api/biopipen.utils.vcf/#biopipenutilsvcfsamples","text":"</> Bases list The samples of the variant","title":"biopipen.utils.vcf.Samples"},{"location":"api/source/biopipen.core.config/","text":"SOURCE CODE biopipen.core. config DOCS \"\"\"Provides the envs from configuration files\"\"\" from typing import Any from pathlib import Path from tempfile import gettempdir from diot import Diot from simpleconf import Config from .defaults import BIOPIPEN_DIR DEFAULT_CONFIG_FILE = BIOPIPEN_DIR / \"core\" / \"config.toml\" USER_CONFIG_FILE = Path ( \"~\" ) . expanduser () / \".biopipen.toml\" PROJ_CONFIG_FILE = Path ( \".\" ) / \".biopipen.toml\" class ConfigItems ( Diot ): DOCS \"\"\"Provides the envs from configuration files and defaults the non-existing values to None.\"\"\" def __getattr__ ( self , name : str ) -> Any : try : return super () . __getattr__ ( name ) except ( KeyError , AttributeError ): return None def __getitem__ ( self , name : str ) -> Any : DOCS try : return super () . __getitem__ ( name ) except ( KeyError , AttributeError ): return None config_profiles = [ { \"path\" : { \"tmpdir\" : gettempdir ()}}, DEFAULT_CONFIG_FILE , USER_CONFIG_FILE , PROJ_CONFIG_FILE , ] config = ConfigItems ( Config . load ( * config_profiles , ignore_nonexist = True ))","title":"biopipen.core.config"},{"location":"api/source/biopipen.core.defaults/","text":"SOURCE CODE biopipen.core. defaults DOCS \"\"\"Provide default variables - BIOPIPEN_DIR: the root directory of the biopipen source - REPORT_DIR: the root directory of the report - SCRIPTS_DIR: the root directory of the scripts \"\"\" from pathlib import Path BIOPIPEN_DIR = Path ( __file__ ) . parent . parent . resolve () REPORT_DIR = BIOPIPEN_DIR / \"reports\" SCRIPT_DIR = BIOPIPEN_DIR / \"scripts\"","title":"biopipen.core.defaults"},{"location":"api/source/biopipen.core.filters/","text":"SOURCE CODE biopipen.core. filters DOCS \"\"\"Additional filters for pipen\"\"\" from __future__ import annotations import re import shlex from pathlib import Path from typing import Any , List , Mapping from argx import Namespace # pyright: ignore[reportPrivateImportUsage] from liquid.filters.manager import FilterManager from yunpath import CloudPath from pipen_report.filters import register_component , _tag # from .defaults import BIOPIPEN_DIR filtermanager = FilterManager () @filtermanager . register DOCS def dict_to_cli_args ( dic : Mapping [ str , Any ], exclude : List [ str ] | None = None , prefix : str | None = None , sep : str | None = \" \" , dup_key : bool = True , join : bool = False , start_key : str = \"\" , end_key : str = \"_\" , dashify : bool = False , ) -> str | List [ str ]: \"\"\"Convert a python dict to a string of CLI arguments Args: dic: The dict to convert exclude: The keys to exclude prefix: The prefix of the keys after conversion Defaults to `None`, mean `-` for short keys and `--` for long keys sep: The separator between key and value If `None`, using `\" \"` for short keys and `\"=\"` for long keys dup_key: Whether to duplicate the key in cli arguments for list values When `True`, `{\"a\": [1, 2]}` will be converted to `\"-a 1 -a 2\"` When `False`, `{\"a\": [1, 2]}` will be converted to `\"-a 1 2\"` If `sep` is `None` or `=`, this must be True, otherwise an error will be raised join: Whether to join the arguments into a single string start_key: The key to start the arguments This is useful when you want to put some arguments at the beginning of the command line end_key: The key to end the arguments This is useful when you want to put some arguments at the end of the command line dashify: Whether to replace `_` with `-` in the keys Returns: The converted string or list of strings \"\"\" if sep in [ None , \"=\" ] and not dup_key : raise ValueError ( \"`dup_key` must be True when sep is `None` or `=`\" ) if exclude : dic = { k : v for k , v in dic . items () if k not in exclude } starts = [] ends = [] out = [] for k , v in dic . items (): if k == start_key : container = starts elif k == end_key : container = ends else : container = out k = str ( k ) dashified_k = k . replace ( \"_\" , \"-\" ) if dashify else k if v is None or v is False : continue if prefix is None : pref = \"--\" if len ( k ) > 1 else \"-\" else : pref = prefix if sep is None : s = \"=\" if len ( k ) > 1 else \" \" else : s = sep if v is True : # You can use {'-': True} to introduce a separator # like `--` if k in [ start_key , end_key ]: raise ValueError ( f \"Cannot use ` { start_key } ` or ` { end_key } ` as key for True\" ) container . append ( f \" { pref }{ dashified_k } \" ) elif isinstance ( v , ( list , tuple )): for i , val in enumerate ( v ): if s == \" \" : if ( i == 0 or dup_key ) and k not in [ start_key , end_key ]: container . append ( f \" { pref }{ dashified_k } \" ) container . append ( str ( val )) else : if ( i == 0 or dup_key ) and k not in [ start_key , end_key ]: container . append ( f \" { pref }{ dashified_k }{ s }{ val } \" ) else : container . append ( str ( val )) elif k in [ start_key , end_key ]: container . append ( str ( v )) elif s == \" \" : container . append ( f \" { pref }{ dashified_k } \" ) container . append ( str ( v )) else : container . append ( f \" { pref }{ dashified_k }{ s }{ v } \" ) out = starts + out + ends return shlex . join ( out ) if join else out @filtermanager . register DOCS def r ( obj : Any , ignoreintkey : bool = True , todot : str | None = None , sortkeys : bool = False , skip : int = 0 , _i : int = 0 , ) -> str : \"\"\"Convert a python object into R repr Examples: >>> True -> \"TRUE\" >>> None -> \"NULL\" >>> [1, 2] -> c(1, 2) >>> {\"a\": 1, \"b\": 2} -> list(a = 1, b = 2) Args: obj: The object to convert ignoreintkey: When keys of a dict are integers, whether we should ignore them. For example, when `True`, `{1: 1, 2: 2}` will be translated into `\"list(1, 2)\"`, but `\"list(`1` = 1, `2` = 2)\"` when `False` todot: If not None, the string will be converted to a dot For example, `todot=\"-\"` will convert `\"a-b\"` to `\"a.b\"` Only applies to the keys of obj when it is a dict sortkeys: Whether to sort the keys of a dict. True by default, in case the order of keys matters, for example, it could affect whether a job is cached. But sometimes, you want to keep orginal order, for example, arguments passed the `dplyr::mutate` function. Because the later arguments can refer to the earlier ones. skip: Levels to skip for `todot`. For example, `skip=1` will skip the first level of the keys. When `todot` is `\"-\"`, `skip=1` will convert `{\"a-b\": {\"c-d\": 1}}` to ``list(`a-b` = list(`c.d` = 1))`` _i: Current level of the keys. Used internally Returns: Then converted string representation of the object \"\"\" if obj is True : return \"TRUE\" if obj is False : return \"FALSE\" if obj is None : return \"NULL\" if isinstance ( obj , str ): if obj . upper () in [ \"+INF\" , \"INF\" ]: return \"Inf\" if obj . upper () == \"-INF\" : return \"-Inf\" if obj . upper () == \"TRUE\" : return \"TRUE\" if obj . upper () == \"FALSE\" : return \"FALSE\" if obj . upper () == \"NA\" or obj . upper () == \"NULL\" or obj == \"None\" : return obj . upper () if re . match ( r \"^\\d+:\\d+$\" , obj ): return obj if obj . startswith ( \"r:\" ) or obj . startswith ( \"R:\" ): return str ( obj )[ 2 :] return repr ( str ( obj )) if isinstance ( obj , ( Path , CloudPath )): return repr ( str ( obj )) if isinstance ( obj , ( list , tuple , set )): if any ( isinstance ( i , dict ) for i in obj ): # c(list(a=1), list(b=2)) will be combined as list(a=1, b=2) # but we want list(list(a=1), list(b=2)) wrapper = \"list\" else : wrapper = \"c\" return \" {} ( {} )\" . format ( wrapper , \", \" . join ( [ r ( i , ignoreintkey , todot , sortkeys , skip , _i + 1 ) for i in obj ] ), ) if isinstance ( obj , dict ): # list allow repeated names items = [] keys = obj . keys () if sortkeys : keys = sorted ( keys ) for k in keys : v = obj [ k ] if isinstance ( k , int ) and not ignoreintkey : item = ( f \"` { k } `= { r ( v , ignoreintkey , todot , sortkeys , skip , _i + 1 ) } \" ) elif isinstance ( k , int ) and ignoreintkey : item = r ( v , ignoreintkey , todot , sortkeys , skip , _i + 1 ) else : key = str ( k ) if todot and _i >= skip : key = key . replace ( todot , \".\" ) item = ( f \"` { key } `=\" f \" { r ( v , ignoreintkey , todot , sortkeys , skip , _i + 1 ) } \" ) items . append ( item ) return f \"list( { ', ' . join ( items ) } )\" if isinstance ( obj , Namespace ): return r ( vars ( obj ), ignoreintkey , todot , sortkeys , skip , _i ) return repr ( obj ) @filtermanager . register DOCS def source_r ( path : str | Path , chdir : bool = False ) -> str : \"\"\"Source an R script. In addition to generating `source(path)`, we also include the mtime for the script to trigger the job not cached when the script is updated. If your process is used in a cloud environment, it is recommended to use the `read` filter to load the script content instead of sourcing it using the `source` function in R to void the path issue (path could be different in different environments). Args: path: The path to the R script Returns: The R code to source the script \"\"\" path = Path ( path ) mtime = int ( path . stat () . st_mtime ) return ( f \"# Last modified: { mtime } \\n \" # f\"biopipen_dir = {r(BIOPIPEN_DIR)}\\n\" f \"source(' { path } ', chdir = { r ( chdir ) } )\" ) @register_component ( \"pdf\" ) def _render_pdf ( cont : Mapping [ str , Any ], job : Mapping [ str , Any ], level : int , ) -> str : \"\"\"Render pdf report\"\"\" # cont[\"src\"] is required height = cont . get ( \"height\" , \"600\" ) return _tag ( \"embed\" , src = str ( cont [ \"src\" ]), type = \"application/pdf\" , width = \"100%\" , height = height , ) @register_component ( \"gsea\" ) def _render_gsea ( cont : Mapping [ str , Any ], job : Mapping [ str , Any ], level : int , ) -> str : \"\"\"Render gsea report\"\"\" # cont[\"dir\"] is required raise NotImplementedError ()","title":"biopipen.core.filters"},{"location":"api/source/biopipen.core/","text":"SOURCE CODE biopipen. core DOCS","title":"biopipen.core"},{"location":"api/source/biopipen.core.proc/","text":"SOURCE CODE biopipen.core. proc DOCS \"\"\"Provides a base class for the processes to subclass\"\"\" from __future__ import annotations from diot import Diot # type: ignore from liquid.defaults import SEARCH_PATHS from pipen import Proc as PipenProc # type: ignore from pipen_filters.filters import FILTERS from .filters import filtermanager from .defaults import BIOPIPEN_DIR , REPORT_DIR def _repr ( x ): if isinstance ( x , Diot ): return repr ( x . to_dict ()) return repr ( x ) filtermanager . register ( \"repr\" )( _repr ) class Proc ( PipenProc ): DOCS \"\"\"Base class for all processes in biopipen to subclass\"\"\" template_opts = { \"globals\" : { ** FILTERS , \"biopipen_dir\" : str ( BIOPIPEN_DIR )}, \"filters\" : { ** FILTERS , ** filtermanager . filters }, \"search_paths\" : SEARCH_PATHS + [ str ( REPORT_DIR )], # type: ignore } plugin_opts = { \"poplog_pattern\" : ( r \"^(?P<level>INFO|WARN|WARNING|CRITICAL|ERROR|DEBUG?)\\s*\" r \"\\[\\d+-\\d+-\\d+ \\d+:\\d+:\\d+\\] (?P<message>.*)$\" ) }","title":"biopipen.core.proc"},{"location":"api/source/biopipen.core.testing/","text":"SOURCE CODE biopipen.core. testing DOCS \"\"\"Provide utilities for testing.\"\"\" import tempfile from functools import wraps from pathlib import Path from pipen import Pipen TESTING_INDEX_INIT = 1 TESTING_PARENT_DIR = Path ( tempfile . gettempdir ()) TESTING_DIR = str ( TESTING_PARENT_DIR . joinpath ( \"biopipen-tests- %(index)s \" )) RSCRIPT_DIR = TESTING_PARENT_DIR . joinpath ( \"biopipen-tests-rscripts\" ) RSCRIPT_DIR . mkdir ( exist_ok = True ) def _find_testing_index ( new ): \"\"\"Find the next available testing index\"\"\" index = TESTING_INDEX_INIT while True : dir = TESTING_DIR % { \"index\" : index } if not Path ( dir ) . exists (): if new : break else : return max ( index - 1 , TESTING_INDEX_INIT ) index += 1 return index def _get_test_dirs ( testfile , new ): \"\"\"Get the workdir and outdir for a test pipeline\"\"\" index = _find_testing_index ( new ) workdir = TESTING_DIR % { \"index\" : index } procname = Path ( testfile ) . parent . stem nsname = Path ( testfile ) . parent . parent . stem name = f \" { nsname } . { procname } \" outdir = f \" { workdir } / { nsname } /output\" workdir = f \" { workdir } / { procname } /pipen\" Path ( workdir ) . mkdir ( parents = True , exist_ok = True ) Path ( outdir ) . mkdir ( parents = True , exist_ok = True ) return name , workdir , outdir def get_pipeline ( testfile , loglevel = \"debug\" , enable_report = False , ** kwargs ): DOCS \"\"\"Get a pipeline for a test file\"\"\" name , workdir , outdir = _get_test_dirs ( testfile , False ) report_plugin_prefix = \"+\" if enable_report else \"-\" plugins = kwargs . pop ( \"plugins\" , []) if any ( \"report\" in p for p in plugins if isinstance ( p , str )): raise ValueError ( \"Do not pass `report` plugin to `get_pipeline(plugins=[...])`, \" \"use `enable_report` instead.\" ) plugins . append ( f \" { report_plugin_prefix } report\" ) kws = { \"name\" : name , \"workdir\" : workdir , \"outdir\" : outdir , \"loglevel\" : loglevel , \"plugins\" : plugins , } kws . update ( kwargs ) return Pipen ( ** kws ) def _run_rcode ( rcode : str ) -> str : \"\"\"Run R code and return the output\"\"\" import hashlib import textwrap import subprocess as sp # Use sha256 of rcode to name the file rcode_hash = hashlib . sha256 ( rcode . encode ()) . hexdigest () script_file = RSCRIPT_DIR . joinpath ( f \"rcode- { rcode_hash } .R\" ) script_file . write_text ( rcode ) p = sp . Popen ([ \"Rscript\" , str ( script_file )], stdout = sp . PIPE , stderr = sp . PIPE ) out , err = p . communicate () if p . returncode != 0 : out = ( f \"R codefile: \\n { script_file } \\n \" f \"Error: \\n { textwrap . indent ( err . decode (), ' ' ) } \" ) return out return out . decode () . strip () def r_test ( mem : callable ) -> callable : DOCS \"\"\"A decorator to test R code\"\"\" @wraps ( mem ) def decorator ( self , * args , ** kwargs ): rcode = mem ( self , * args , ** kwargs ) source = getattr ( self , \"SOURCE_FILE\" , None ) expect = ( \"expect <- function(expr, ...) { \\n \" \" if (!expr) { \\n \" \" msg <- lapply( \\n \" \" list(...), \\n \" \" function(x) { ifelse(is.null(x), 'NULL', x) } \\n \" \" ) \\n \" \" stop(paste0(unlist(msg), collapse = ' ')) \\n \" \" } \\n \" \"} \\n \" ) rcode = f \" { expect } \\n\\n { rcode } \\n\\n cat('PASSED') \\n \" if source is not None : if not isinstance ( source , ( list , tuple )): source = [ source ] libs = \" \\n \" . join ([ f \"suppressWarnings(source(' { s } '))\" for s in source ]) rcode = f ' { libs } \\n\\n { rcode } ' out = _run_rcode ( rcode ) self . assertEqual ( out , \"PASSED\" , \" \\n ----------------------------- \\n \" f \" { out } \" \" \\n ----------------------------- \\n \" ) return decorator","title":"biopipen.core.testing"},{"location":"api/source/biopipen.ns.bam/","text":"SOURCE CODE biopipen.ns. bam DOCS \"\"\"Tools to process sam/bam/cram files\"\"\" from ..core.proc import Proc from ..core.config import config # +-------------------------------------------------------------------+ # | CNV callers | # +-------------------------------------------------------------------+ class CNVpytor ( Proc ): DOCS \"\"\"Detect CNV using CNVpytor Input: bamfile: The bam file Will try to index it if it's not indexed. snpfile: The snp file Output: outdir: The output directory Envs: cnvpytor: Path to cnvpytor samtools: Path to samtools, used to index bam file in case it's not ncores: Number of cores to use (`-j` for cnvpytor) refdir: The directory containing the fasta file for each chromosome genome: The genome assembly to put in the VCF file chrsize: The geome size file to fix missing contigs in VCF header chrom: The chromosomes to run on binsizes: The binsizes snp: How to read snp data filters: The filters to filter the result See - https://github.com/abyzovlab/CNVpytor/blob/master/GettingStarted.md#predicting-cnv-regions mask_snps: Whether mask 1000 Genome snps baf_nomask: Do not use P mask in BAF histograms Requires: cnvpytor: - check: {{proc.envs.cnvpytor}} --version \"\"\" # noqa: E501 input = \"bamfile:file, snpfile:file\" output = \"outdir:dir:{{in.bamfile | stem}}.cnvpytor\" lang = config . lang . python envs = { \"cnvpytor\" : config . exe . cnvpytor , \"samtools\" : config . exe . samtools , \"ncores\" : config . misc . ncores , \"refdir\" : config . ref . refdir , \"genome\" : config . ref . genome , \"chrsize\" : config . ref . chrsize , \"chrom\" : [], \"binsizes\" : [ 10000 , 100000 ], # set False to disable snp data importing \"snp\" : { \"sample\" : \"\" , \"name1\" : [], \"ad\" : \"AD\" , \"gt\" : \"GT\" , \"noAD\" : False , }, \"filters\" : { # https://github.com/abyzovlab/CNVpytor/blob/master/ # GettingStarted.md#predicting-cnv-regions \"CNVsize\" : [ 50000 , \"inf\" ], # CNV size \"eval1\" : [ 0 , 0.0001 ], # filter on e-val1 \"eval2\" : [], # filter on e-val2 \"eval3\" : [], # filter on e-val3 \"eval4\" : [], # filter on e-val4 \"q0\" : [ - 1 , 0.5 ], # filter on Q0 \"pN\" : [ 0 , 0.5 ], # filter on pN \"dG\" : [ 100000 , \"inf\" ], # filter on dG }, \"mask_snps\" : True , \"baf_nomask\" : False , # other arguments for -rd } script = \"file://../scripts/bam/CNVpytor.py\" plugin_opts = { \"report\" : \"file://../reports/bam/CNVpytor.svelte\" } class ControlFREEC ( Proc ): DOCS \"\"\"Detect CNVs using Control-FREEC Input: bamfile: The bam file snpfile: The snp file Output: outdir: The output directory Envs: freec: Path to Control-FREEC executable ncores: Number of cores to use arggs: Other arguments for Control-FREEC \"\"\" input = \"bamfile:file, snpfile:file\" output = \"outdir:dir:{{in.bamfile | stem}}.freec\" lang = config . lang . python envs = { \"freec\" : config . exe . freec , \"ncores\" : config . misc . ncores , \"tabix\" : config . exe . tabix , \"bedtools\" : config . exe . bedtools , \"sambamba\" : config . exe . sambamba , \"samtools\" : config . exe . samtools , # The \"<ref>.fai\" file will be used as chrLenFile \"ref\" : config . ref . reffa , \"refdir\" : config . ref . refdir , \"rscript\" : config . lang . rscript , \"binsize\" : 50_000 , # shortcut for args.general.window \"args\" : { \"general\" : { \"ploidy\" : 2 , \"breakPointThreshold\" : 0.8 , }, \"sample\" : { \"mateOrientation\" : \"FR\" }, \"control\" : {}, \"BAF\" : {}, \"target\" : {}, }, } script = \"file://../scripts/bam/ControlFREEC.py\" plugin_opts = { \"report\" : \"file://../reports/bam/ControlFREEC.svelte\" } class CNAClinic ( Proc ): DOCS \"\"\"Detect CNVs using CNAClinic Input: metafile: The meta file, header included, tab-delimited, including following columns: - Bam: The path to bam file - Sample: Optional. The sample names, if you don't want filename of bam file to be used - Group: Optional. The group names, either \"Case\" or \"Control\" - Patient: Optional. The patient names. Since CNAClinic only supports paired samples, you need to provide the patient names for each sample. Required if \"Group\" is provided. - Binsizer: Optional. Samples used to estimate the bin size \"Y\", \"Yes\", \"T\", \"True\", will be treated as True If not provided, will use `envs.binsizer` to get the samples to use. Either this column or `envs.binsizer` should be provided. Output: outdir: The output directory Envs: ncores: Number of cores to use seed: The seed for random number generator for choosing samples for estimating bin size binsizer: The samples used to estimate the bin size, it could be: A list of sample names A float number (0 < x <= 1), the fraction of samples to use A integer number (x > 1), the number of samples to use binsize: Directly use this binsize for CNAClinic, in bp. genome: The genome assembly run_args: The arguments for CNAClinic::runSegmentation plot_args: The arguments for CNAClinic::plotSampleData plot_multi_args: The arguments for CNAClinic::plotMultiSampleData \"\"\" input = \"metafile:file\" output = \"outdir:dir:{{in.metafile | stem}}.cnaclinic\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"binsizer\" : None , \"binsize\" : None , \"seed\" : 123 , \"genome\" : config . ref . genome , \"run_args\" : { # HMM is errored \"segmentType\" : [ \"CBS\" , \"LACBS\" , \"PLS\" ], \"segmentsToSummarise\" : [ \"CBS\" , \"LACBS\" , \"PLS\" ], \"summaryMethod\" : \"mean\" , }, \"plot_args\" : {}, \"plot_multi_args\" : False , } script = \"file://../scripts/bam/CNAClinic.R\" plugin_opts = { \"report\" : \"file://../reports/bam/CNAClinic.svelte\" , \"report_paging\" : 20 , } # +-------------------------------------------------------------------+ # | Bam processing tools | # +-------------------------------------------------------------------+ class BamSplitChroms ( Proc ): DOCS \"\"\"Split bam file by chromosomes Input: bamfile: The bam file Output: outdir: The output directory with bam files for each chromosome Envs: ncores: Number of cores to use samtools: Path to samtools executable sambamba: Path to sambamba executable tool: The tool to use, either \"samtools\" or \"sambamba\" keep_other_sq: Keep other chromosomes in \"@SQ\" field in header chroms: The chromosomes to keep, if not provided, will use all index: Whether to index the output bam files. Requires the input bam file to be sorted. \"\"\" input = \"bamfile:file\" output = \"outdir:dir:{{in.bamfile | stem}}.split\" lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"samtools\" : config . exe . samtools , \"sambamba\" : config . exe . sambamba , \"tool\" : \"samtools\" , \"keep_other_sq\" : False , \"chroms\" : [], \"index\" : True , } script = \"file://../scripts/bam/BamSplitChroms.py\" class BamMerge ( Proc ): DOCS \"\"\"Merge bam files Input: bamfiles: The bam files Output: outfile: The output bam file Envs: ncores: Number of cores to use tool: The tool to use, either \"samtools\" or \"sambamba\" samtools: Path to samtools executable sambamba: Path to sambamba executable sort: Whether to sort the output bam file index: Whether to index the output bam file Requires envs.sort to be True merge_args: The arguments for merging bam files `samtools merge` or `sambamba merge`, depending on `tool` For `samtools`, these keys are not allowed: `-o`, `-O`, `--output-fmt`, `-@`, and `--threads`, as they are managed by the script For `sambamba`, these keys are not allowed: `-t`, and `--nthreads`, as they are managed by the script sort_args: The arguments for sorting bam files `samtools sort` or `sambamba sort`, depending on `tool` For `samtools`, these keys are not allowed: `-o`, `-@`, and `--threads`, as they are managed by the script For `sambamba`, these keys are not allowed: `-t`, `--nthreads`, `-o` and `--out`, as they are managed by the script \"\"\" input = \"bamfiles:files\" output = \"outfile:file:{{in.bamfiles | first | stem}}.merged.bam\" lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"samtools\" : config . exe . samtools , \"sambamba\" : config . exe . sambamba , \"tool\" : \"samtools\" , \"sort\" : True , \"index\" : True , \"merge_args\" : [], \"sort_args\" : [], } script = \"file://../scripts/bam/BamMerge.py\" class BamSampling ( Proc ): DOCS \"\"\"Keeping only a fraction of read pairs from a bam file Input: bamfile: The bam file Output: outfile: The output bam file Envs: ncores: Number of cores to use samtools: Path to samtools executable tool: The tool to use, currently only \"samtools\" is supported fraction (type=float): The fraction of reads to keep. If `0 < fraction <= 1`, it's the fraction of reads to keep. If `fraction > 1`, it's the number of reads to keep. Note that when fraction > 1, you may not get the exact number of reads specified but a close number. seed: The seed for random number generator index: Whether to index the output bam file sort: Whether to sort the output bam file sort_args: The arguments for sorting bam file using `samtools sort`. These keys are not allowed: `-o`, `-@`, and `--threads`, as they are managed by the script. \"\"\" input = \"bamfile:file\" output = \"outfile:file:{{in.bamfile | stem}}.sampled{{envs.fraction}}.bam\" lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"samtools\" : config . exe . samtools , \"tool\" : \"samtools\" , \"fraction\" : None , \"seed\" : 8525 , \"index\" : True , \"sort\" : True , \"sort_args\" : [], } script = \"file://../scripts/bam/BamSampling.py\" class BamSubsetByBed ( Proc ): DOCS \"\"\"Subset bam file by the regions in a bed file Input: bamfile: The bam file bedfile: The bed file Output: outfile: The output bam file Envs: ncores: Number of cores to use samtools: Path to samtools executable tool: The tool to use, currently only \"samtools\" is supported index: Whether to index the output bam file \"\"\" input = \"bamfile:file, bedfile:file\" output = \"outfile:file:{{in.bamfile | stem}}-subset.bam\" lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"samtools\" : config . exe . samtools , \"tool\" : \"samtools\" , \"index\" : True , } script = \"file://../scripts/bam/BamSubsetByBed.py\" class BamSort ( Proc ): DOCS \"\"\"Sort bam file Input: bamfile: The bam file Output: outfile: The output bam file Envs: tool (choice): The tool to use. - samtools: Use `samtools` - sambamba: Use `sambamba` ncores (type=int): Number of cores to use samtools: Path to samtools executable sambamba: Path to sambamba executable tmpdir: The temporary directory to use byname (flag): Whether to sort by read name index (flag): Whether to index the output bam file The index file will be created in the same directory as the output bam file <more>: Other arguments passed to the sorting tool See `samtools sort` or `sambamba sort` \"\"\" input = \"bamfile:file\" output = \"outfile:file:{{in.bamfile | stem}}.sorted.bam\" lang = config . lang . python envs = { \"tool\" : \"samtools\" , \"ncores\" : config . misc . ncores , \"samtools\" : config . exe . samtools , \"sambamba\" : config . exe . sambamba , \"tmpdir\" : config . path . tmpdir , \"byname\" : False , \"index\" : True , } script = \"file://../scripts/bam/BamSort.py\" class SamtoolsView ( Proc ): DOCS \"\"\"View bam file using samtools, mostly used for filtering This is a wrapper for `samtools view` command. It will create a new bam file with the same name as the input bam file. Input: bamfile: The bam file Output: outfile: The output bam file Envs: ncores: Number of cores to use samtools: Path to samtools executable index: Whether to index the output bam file Requires the input bam file to be sorted. <more>: Other arguments passed to the view tool See `samtools view` or `sambamba view`. \"\"\" input = \"bamfile:file\" output = \"outfile:file:{{in.bamfile | stem}}.bam\" lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"samtools\" : config . exe . samtools , \"index\" : True , } script = \"file://../scripts/bam/SamtoolsView.py\"","title":"biopipen.ns.bam"},{"location":"api/source/biopipen.ns.bed/","text":"SOURCE CODE biopipen.ns. bed DOCS \"\"\"Tools to handle BED files\"\"\" from ..core.proc import Proc from ..core.config import config class BedLiftOver ( Proc ): DOCS \"\"\"Liftover a BED file using liftOver Input: inbed: The input BED file Output: outbed: The output BED file Envs: liftover: The path to liftOver chain: The map chain file for liftover Requires: liftOver: - check: {{proc.envs.liftover}} 2>&1 | grep \"usage\" \"\"\" input = \"inbed:file\" output = \"outbed:file:{{in.inbed | basename}}\" envs = { \"liftover\" : config . exe . liftover , \"chain\" : config . path . liftover_chain , } lang = config . lang . bash script = \"file://../scripts/bed/BedLiftOver.sh\" class Bed2Vcf ( Proc ): DOCS \"\"\"Convert a BED file to a valid VCF file with minimal information Input: inbed: The input BED file Output: outvcf: The output VCF file Envs: sample: The sample name to be used in the VCF file You can use a lambda function (in string) to generate the sample name from the stem of input file ref: The reference fasta file, used to grab the reference allele. To add contigs in header, the `fai` file is also required at `<ref>.fai` genome: The genome assembly, added as `source` in header base: 0 or 1, whether the coordinates in BED file are 0- or 1-based headers: The header lines to be added to the VCF file infos: The INFO dicts to be added to the VCF file formats: The FORMAT dicts to be added to the VCF file The keys 'ID', 'Description', 'Type', and 'Number' are required. converters: A dict of converters to be used for each INFO or FORMAT The key is the ID of an INFO or FORMAT, and the value is Any converts return `None` will skip the record nonexisting_contigs: Whether to `keep` or `drop` the non-existing contigs in `ref`. helpers: Raw code to be executed to provide some helper functions since only lambda functions are supported in converters index: Sort and index output file Requires: cyvcf2: - check: {{proc.lang}} -c \"import cyvcf2\" pysam: - check: {{proc.lang}} -c \"import pysam\" bcftools: - if: {{proc.envs.index}} - check: {{proc.envs.bcftools}} --version \"\"\" input = \"inbed:file\" output = ( \"outvcf:file:{{in.inbed | stem}}.vcf{{'.gz' if envs.index else ''}}\" ) lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"sample\" : \"lambda stem: stem\" , \"ref\" : config . ref . reffa , \"genome\" : config . ref . genome , \"nonexisting_contigs\" : \"drop\" , \"base\" : 0 , \"index\" : True , \"headers\" : [], \"infos\" : [], \"formats\" : [], \"converters\" : {}, \"helpers\" : \"\" , } script = \"file://../scripts/bed/Bed2Vcf.py\" class BedConsensus ( Proc ): DOCS \"\"\"Find consensus regions from multiple BED files. Unlike `bedtools merge/cluster`, it does not find the union regions nor intersect regions. Instead, it finds the consensus regions using the distributions of the scores of the bins ``` bedtools cluster Bedfile A |----------| 1 Bedfile B |--------| 1 Bedfile C |------| 1 BedConsensus |--------| with cutoff >= 2 bedtools intesect |----| bedtools merge |------------| Distribution |1|2|3333|2|1| (later normalized into 0~1) ``` Input: bedfiles: Input BED files Output: outbed: The output BED file Envs: bedtools: The path to bedtools cutoff: The cutoff to determine the ends of consensus regions If `cutoff` < 1, it applies to the normalized scores (0~1), which is the percentage of the number of files that cover the region. If `cutoff` >= 1, it applies to the number of files that cover the region directly. chrsize: The chromosome sizes file distance: When the distance between two bins is smaller than this value, they are merged into one bin using `bedtools merge -d`. `0` means no merging. \"\"\" input = \"bedfiles:files\" output = ( \"outbed:file:{{in.bedfiles | first | stem | append: '_consensus'}}.bed\" ) lang = config . lang . python envs = { \"bedtools\" : config . exe . bedtools , \"cutoff\" : 0.5 , \"distance\" : 1 , \"chrsize\" : config . ref . chrsize , } script = \"file://../scripts/bed/BedConsensus.py\" class BedtoolsMerge ( Proc ): DOCS \"\"\"Merge overlapping intervals in a BED file, using `bedtools merge` Input: inbed: The input BED file Output: outbed: The output BED file Envs: bedtools: The path to bedtools <more>: Other options to be passed to `bedtools merge` See https://bedtools.readthedocs.io/en/latest/content/tools/merge.html \"\"\" # noqa: E501 input = \"inbed:file\" output = \"outbed:file:{{in.inbed | stem}}_merged.bed\" lang = config . lang . python envs = { \"bedtools\" : config . exe . bedtools , } script = \"file://../scripts/bed/BedtoolsMerge.py\" class BedtoolsIntersect ( Proc ): DOCS \"\"\"Find the intersection of two BED files, using `bedtools intersect` See <https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html> Input: afile: The first BED file bfile: The second BED file Output: outfile: The output BED file Envs: bedtools: The path to bedtools sort: Sort `afile` and `bfile` before intersecting. By default, `-sorted` is used, assuming the input files are sorted. If error occurs, try to set `sort` to `True`. chrsize: Alias for `g` in `bedtools intersect`. postcmd: The command to be executed for the output file after intersecting. You can use `$infile`, `$outfile`, and `$outdir` to refer to the input, output, and output directory, respectively. <more>: Other options to be passed to `bedtools intersect` \"\"\" # noqa: E501 input = \"afile:file\" , \"bfile:file\" output = \"outfile:file:{{in.afile | stem0}}_{{in.bfile | stem0}}-intersect.bt\" lang = config . lang . python envs = { \"bedtools\" : config . exe . bedtools , \"sort\" : False , \"chrsize\" : config . ref . chrsize , \"postcmd\" : None , } script = \"file://../scripts/bed/BedtoolsIntersect.py\" class BedtoolsMakeWindows ( Proc ): DOCS \"\"\"Make windows from a BED file or genome size file, using `bedtools makewindows`. Input: infile: The input BED file or a genome size file Type will be detected by the number of columns in the file. If it has 3+ columns, it is treated as a BED file, otherwise a genome size file. Output: outfile: The output BED file Envs: bedtools: The path to bedtools window (type=int): The size of the windows step (type=int): The step size of the windows nwin (type=int): The number of windows to be generated Exclusive with `window` and `step`. Either `nwin` or `window` and `step` should be provided. reverse (flag): Reverse numbering of windows in the output name (choice): How to name the generated windows/regions - none: Do not add any name - src: Use the source interval's name - winnum: Use the window number - srcwinnum: Use the source interval's name and window number \"\"\" # noqa: E501 input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}_windows.bed\" lang = config . lang . python envs = { \"bedtools\" : config . exe . bedtools , \"window\" : None , \"step\" : None , \"nwin\" : None , \"reverse\" : False , \"name\" : \"none\" , } script = \"file://../scripts/bed/BedtoolsMakeWindows.py\"","title":"biopipen.ns.bed"},{"location":"api/source/biopipen.ns.cellranger/","text":"SOURCE CODE biopipen.ns. cellranger DOCS \"\"\"Cellranger pipeline module for BioPipen\"\"\" from ..core.proc import Proc from ..core.config import config class CellRangerCount ( Proc ): DOCS \"\"\"Run cellranger count to count gene expression and/or feature barcode reads requires cellranger v7+. Input: fastqs: The input fastq files Either a list of fastq files or a directory containing fastq files If a directory is provided, it should be passed as a list with one element. id: The id defining output directory. If not provided, it is inferred from the fastq files. Note that, unlike the `--id` argument of cellranger, this will not select the samples from `in.fastqs`. In stead, it will symlink the fastq files to a temporary directory with this `id` as prefix and pass that to cellranger. Output: outdir: The output directory Envs: ncores: Number of cores to use cellranger: Path to cellranger ref: Path of folder containing 10x-compatible transcriptome reference tmpdir: Path to temporary directory, used to save the soft-lined fastq files to pass to cellranger include_introns (flag): Set to false to exclude intronic reads in count. create_bam (flag): Enable or disable BAM file generation. This is required by cellrange v8+. When using cellrange v8-, it will be transformed to `--no-bam`. <more>: Other environment variables required by `cellranger count` See `cellranger count --help` for more details or https://www.10xgenomics.com/support/software/cell-ranger/advanced/cr-command-line-arguments#count \"\"\" # noqa: E501 input = \"fastqs:files, id\" output = \"\"\"outdir:dir: { %- s et fastqs = in.fastqs -%} { %- i f len(fastqs) == 1 and isdir(fastqs[0]) -%} { %- s et fastqs = fastqs[0] | glob: \"*.fastq.gz\" -%} { %- e ndif -%} { %- i f in.id -%} {{in.id}} { %- e lse -%} { %- s et id = commonprefix(*fastqs) | regex_replace: \"_L \\\\ d+(:?_.*)?$\", \"\" | regex_replace: \"_S \\\\ d+$\", \"\" -%} {{- id -}} { %- e ndif -%} \"\"\" lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"cellranger\" : config . exe . cellranger , \"ref\" : config . ref . ref_cellranger_gex , \"tmpdir\" : config . path . tmpdir , \"include_introns\" : True , \"create_bam\" : False , } script = \"file://../scripts/cellranger/CellRangerCount.py\" plugin_opts = { \"report\" : \"file://../reports/cellranger/CellRangerCount.svelte\" , \"report_paging\" : 5 , } class CellRangerVdj ( Proc ): DOCS \"\"\"Run cellranger vdj to perform sequence assembly and paired clonotype calling. requires cellranger v7+. Input: fastqs: The input fastq files Either a list of fastq files or a directory containing fastq files If a directory is provided, it should be passed as a list with one element. id: The id determining the output directory. If not provided, it is inferred from the fastq files. Output: outdir: The output directory Envs: ncores: Number of cores to use cellranger: Path to cellranger ref: Path of folder containing 10x-compatible transcriptome reference tmpdir: Path to temporary directory, used to save the soft-lined fastq files to pass to cellranger <more>: Other environment variables required by `cellranger vdj` See `cellranger vdj --help` for more details or https://www.10xgenomics.com/support/software/cell-ranger/advanced/cr-command-line-arguments#vdj \"\"\" # noqa: E501 input = \"fastqs:files, id\" output = \"\"\"outdir:dir: { %- s et fastqs = in.fastqs -%} { %- i f len(fastqs) == 1 and isdir(fastqs[0]) -%} { %- s et fastqs = fastqs[0] | glob: \"*.fastq.gz\" -%} { %- e ndif -%} { %- i f in.id -%} {{in.id}} { %- e lse -%} { %- s et id = commonprefix(*fastqs) | regex_replace: \"_L \\\\ d+(:?_.*)?$\", \"\" | regex_replace: \"_S \\\\ d+$\", \"\" -%} {{- id -}} { %- e ndif -%} \"\"\" lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"cellranger\" : config . exe . cellranger , \"ref\" : config . ref . ref_cellranger_vdj , \"tmpdir\" : config . path . tmpdir , } script = \"file://../scripts/cellranger/CellRangerVdj.py\" plugin_opts = { \"report\" : \"file://../reports/cellranger/CellRangerVdj.svelte\" , \"report_paging\" : 5 , } class CellRangerSummary ( Proc ): DOCS \"\"\"Summarize cellranger metrics Input: indirs: The directories containing cellranger results from `CellRangerCount`/`CellRangerVdj`. Output: outdir: The output directory Envs: group (type=auto): The group of the samples for boxplots. If `None`, don't do boxplots. It can be a dict of group names and sample names, e.g. `{\"group1\": [\"sample1\", \"sample2\"], \"group2\": [\"sample3\"]}` or a file containing the group information, with the first column being the sample names and the second column being the group names. The file should be tab-delimited with no header. \"\"\" input = \"indirs:dirs\" input_data = lambda ch : [ list ( ch . iloc [:, 0 ])] output = \"outdir:dir:{{in.indirs | first | stem | append: '-etc.summary'}}\" lang = config . lang . rscript script = \"file://../scripts/cellranger/CellRangerSummary.R\" envs = { \"group\" : None } plugin_opts = { \"report\" : \"file://../reports/cellranger/CellRangerSummary.svelte\" , \"report_paging\" : 8 , }","title":"biopipen.ns.cellranger"},{"location":"api/source/biopipen.ns.cellranger_pipeline/","text":"SOURCE CODE biopipen.ns. cellranger_pipeline DOCS \"\"\"The cellranger pipelines Primarily cellranger process plus summary for summarizing the metrics for multiple samples. \"\"\" from __future__ import annotations from typing import TYPE_CHECKING from diot import Diot from pipen.utils import is_loading_pipeline from pipen_args.procgroup import ProcGroup if TYPE_CHECKING : from pipen import Proc class CellRangerCountPipeline ( ProcGroup ): DOCS \"\"\"The cellranger count pipeline Run cellranger count for multiple samples and summarize the metrics. Args: input (list): The list of lists of fastq files. or the list of comma-separated string of fastq files. ids (list): The list of ids for the samples. \"\"\" DEFAULTS = Diot ( input = None , ids = None ) def post_init ( self ): DOCS \"\"\"Check if the input is a list of fastq files\"\"\" if not is_loading_pipeline ( \"-h\" , \"-h+\" , \"--help\" , \"--help+\" ) and ( not isinstance ( self . opts . input , ( list , tuple )) or len ( self . opts . input ) == 0 ): raise TypeError ( \"The input of `CellRangerCountPipeline` should be a list of lists of \" \"fastq files.\" ) if isinstance ( self . opts . input , ( list , tuple )): self . opts . input = [ [ y . strip () for y in x . split ( \",\" )] if isinstance ( x , str ) else x for x in self . opts . input ] @ProcGroup . add_proc def p_cellranger_count ( self ) -> Proc : \"\"\"Build CellRangerCount process\"\"\" from .cellranger import CellRangerCount as _CellRangerCount class CellRangerCount ( _CellRangerCount ): if self . opts . ids : input_data = list ( zip ( self . opts . input , self . opts . ids )) else : input_data = self . opts . input return CellRangerCount @ProcGroup . add_proc def p_cellranger_count_summary ( self ) -> Proc : \"\"\"Build CellRangerCountSummary process\"\"\" from .cellranger import CellRangerSummary class CellRangerCountSummary ( CellRangerSummary ): requires = self . p_cellranger_count input_data = lambda ch : [ list ( ch . iloc [:, 0 ])] return CellRangerCountSummary class CellRangerVdjPipeline ( ProcGroup ): DOCS \"\"\"The cellranger vdj pipeline Run cellranger vdj for multiple samples and summarize the metrics. Args: input (list): The list of lists of fastq files. or the list of comma-separated string of fastq files. ids (list): The list of ids for the samples. \"\"\" DEFAULTS = Diot ( input = None , ids = None ) def post_init ( self ): DOCS \"\"\"Check if the input is a list of fastq files\"\"\" if not is_loading_pipeline ( \"-h\" , \"-h+\" , \"--help\" , \"--help+\" ) and ( not isinstance ( self . opts . input , ( list , tuple )) or len ( self . opts . input ) == 0 ): raise TypeError ( \"The input of `CellRangerVdjPipeline` should be a list of lists of \" \"fastq files.\" ) if isinstance ( self . opts . input , ( list , tuple )): self . opts . input = [ [ y . strip () for y in x . split ( \",\" )] if isinstance ( x , str ) else x for x in self . opts . input ] @ProcGroup . add_proc def p_cellranger_vdj ( self ) -> Proc : \"\"\"Build CellRangerVdj process\"\"\" from .cellranger import CellRangerVdj as _CellRangerVdj class CellRangerVdj ( _CellRangerVdj ): if self . opts . ids : input_data = list ( zip ( self . opts . input , self . opts . ids )) else : input_data = self . opts . input return CellRangerVdj @ProcGroup . add_proc def p_cellranger_vdj_summary ( self ) -> Proc : \"\"\"Build CellRangerVdjSummary process\"\"\" from .cellranger import CellRangerSummary class CellRangerVdjSummary ( CellRangerSummary ): requires = self . p_cellranger_vdj input_data = lambda ch : [ list ( ch . iloc [:, 0 ])] return CellRangerVdjSummary","title":"biopipen.ns.cellranger_pipeline"},{"location":"api/source/biopipen.ns.cnv/","text":"SOURCE CODE biopipen.ns. cnv DOCS \"\"\"CNV/CNA-related processes, mostly tertiary analysis\"\"\" from ..core.proc import Proc from ..core.config import config class AneuploidyScore ( Proc ): DOCS \"\"\"Chromosomal arm SCNA/aneuploidy The CAAs in this process are calculated using Cohen-Sharir method See https://github.com/quevedor2/aneuploidy_score Input: segfile: The seg file, generally including chrom, start, end and seg.mean (the log2 ratio). It is typically a tab-delimited file or a BED file. If so, envs.chrom_col, envs.start_col, envs.end_col and envs.seg_col are the 1st, 2nd, 3rd and 5th columns, respectively. It can also be a VCF file. If so, envs.chrom_col and envs.start_col are not required. `end_col` and `envs.seg_col` will be a field in the INFO column. [`VariantAnnotation`](https://rdrr.io/bioc/VariantAnnotation/) is required to extract the INFO field. Output: outdir: The output directory containing the CAAs, AS and a histogram plot to show the CAAs for each chromosome arm Envs: chrom_col: The column name for chromosome start_col: The column name for start position end_col: The column name for end position seg_col: The column name for seg.mean cn_col: The column name for copy number segmean_transform (text): A R function to transform `seg.mean` The transformed value will be used to calculate the CAAs cn_transform (type=auto): A R function to transform `seg.mean` into copy number, or a list of cutoffs to determine the copy number. See https://cnvkit.readthedocs.io/en/stable/pipeline.html#calling-methods. If this is give, `cn_col` will be ignored. genome: The genome version, hg19 or hg38 threshold (type=float): The threshold to determine whether a chromosome arm is gained or lost. wgd_gf (type=float): The fraction of the genome that is affected by WGD excl_chroms (list): The chromosomes to be excluded Works with/without `chr` prefix. Requires: AneuploidyScore: - check: {{proc.lang}} <(echo \"library(AneuploidyScore)\") ucsc.hg19.cytoband: - if: {{ proc.envs.genome == 'hg19' }} - check: {{proc.lang}} <(echo \"library(ucsc.hg19.cytoband)\") ucsc.hg38.cytoband: - if: {{ proc.envs.genome == 'hg38' }} - check: {{proc.lang}} <(echo \"library(ucsc.hg38.cytoband)\") \"\"\" # noqa: E501 input = \"segfile:file\" output = \"outdir:dir:{{in.segfile | stem}}.aneuploidy_score\" lang = config . lang . rscript envs = { \"chrom_col\" : \"chrom\" , \"start_col\" : \"loc.start\" , \"end_col\" : \"loc.end\" , \"seg_col\" : \"seg.mean\" , \"cn_col\" : None , \"segmean_transform\" : None , \"cn_transform\" : None , \"genome\" : config . ref . genome , \"threshold\" : 0.5 , \"wgd_gf\" : 0.5 , \"excl_chroms\" : [ 'chrX' , 'chrY' ], } script = \"file://../scripts/cnv/AneuploidyScore.R\" plugin_opts = { \"report\" : \"file://../reports/cnv/AneuploidyScore.svelte\" , \"report_paging\" : 10 , } class AneuploidyScoreSummary ( Proc ): DOCS \"\"\"Summary table and plots from AneuploidyScore Input: asdirs: The output directories from AneuploidyScore metafile: The metafile containing the sample information Output: outdir: The output directory containing the summary table and plots Envs: group_cols (type=auto): The column name in the metafile to group the samples. We also support multiple columns, e.g. `[\"group1\", \"group2\"]` You can also use `group1,group2` to add a secondary grouping based on `group2` within each `group1` (only works for 2 groups) heatmap_cases (type=json): The cases to be included in the heatmap By default, all arms are included. If specified, keys are the names of the cases and values are the arms, which will be included in the heatmap. The list of arms should be a subset of `chr<N>_p` and `chr<N>_q`, where `<N>` is the chromosome number from 1 to 22, X, Y. You can also use `ALL` to include all arms. sample_name (text): An R function to extract the sample name from the file stem (not including `.aneuploidy_score` part) \"\"\" input = \"asdirs:dirs, metafile:file\" output = ( \"outdir:dir:{{in.asdirs | first | stem}}_etc.aneuploidy_score_summary\" ) lang = config . lang . rscript script = \"file://../scripts/cnv/AneuploidyScoreSummary.R\" envs = { \"group_cols\" : None , \"heatmap_cases\" : { \"All-Arms\" : \"ALL\" }, \"sample_name\" : None , } plugin_opts = { \"report\" : \"file://../reports/cnv/AneuploidyScoreSummary.svelte\" , } class TMADScore ( Proc ): DOCS \"\"\"Trimmed Median Absolute Deviation (TMAD) score for CNV Reference: Mouliere, Chandrananda, Piskorz and Moore et al. Enhanced detection of circulating tumor DNA by fragment size analysis Science Translational Medicine (2018). Input: segfile: The seg file, two columns are required: * chrom: The chromosome name, used for filtering * seg.mean: The log2 ratio. It is typically a tab-delimited file or a BED file. If so, envs.chrom_col and envs.seg_col are the 1st and 5th columns, respectively. It can also be a VCF file. If so, envs.chrom_col and envs.start_col are not required. `end_col` and `envs.seg_col` will be a field in the INFO column. [`VariantAnnotation`](https://rdrr.io/bioc/VariantAnnotation/) is required to extract the INFO field. Output: outfile: The output file containing the TMAD score Envs: chrom_col: The column name for chromosome seg_col: The column name for seg.mean segmean_transform: The transformation function for seg.mean excl_chroms (list): The chromosomes to be excluded \"\"\" input = \"segfile:file\" output = \"outfile:file:{{in.segfile | stem}}.tmad.txt\" lang = config . lang . rscript envs = { \"chrom_col\" : \"chrom\" , \"seg_col\" : \"seg.mean\" , \"segmean_transform\" : None , \"excl_chroms\" : [ \"chrX\" , \"chrY\" ], } script = \"file://../scripts/cnv/TMADScore.R\" class TMADScoreSummary ( Proc ): DOCS \"\"\"Summary table and plots for TMADScore Input: tmadfiles: The output files from TMADScore metafile: The metafile containing the sample information The first column must be the sample ID Output: outdir: The output directory containing the summary table and plots Envs: group_cols (type=auto): The column name in the metafile to group the samples Could also be a list of column names If not specified, samples will be plotted individually as a barplot We also support multiple columns, e.g. `[\"group1\", \"group2\"]` You can also use `group1,group2` to add a secondary grouping based on `group2` within each `group1` (only works for 2 groups) sample_name (text): An R function to extract the sample name from the file stem (not including `.tmad.txt` part) \"\"\" input = \"tmadfiles:files, metafile:file\" output = \"outdir:dir:{{in.tmadfiles | first | stem0}}_etc.tmad_summary\" lang = config . lang . rscript script = \"file://../scripts/cnv/TMADScoreSummary.R\" envs = { \"group_cols\" : None , \"sample_name\" : None } plugin_opts = { \"report\" : \"file://../reports/cnv/TMADScoreSummary.svelte\" , }","title":"biopipen.ns.cnv"},{"location":"api/source/biopipen.ns.cnvkit/","text":"SOURCE CODE biopipen.ns. cnvkit DOCS \"\"\"CNVkit commnads\"\"\" from ..core.proc import Proc from ..core.config import config class CNVkitAccess ( Proc ): DOCS \"\"\"Calculate the sequence-accessible coordinates in chromosomes from the given reference genome using `cnvkit.py access` Input: excfiles: Additional regions to exclude, in BED format Output: outfile: The output file Envs: cnvkit: Path to `cnvkit.py` min_gap_size (type=int): Minimum gap size between accessible sequence regions ref: The reference genome fasta file Requires: cnvkit: - check: {{proc.envs.cnvkit}} version \"\"\" input = \"excfiles:files\" output = ( \"outfile:file:{{envs.ref | stem0}}.access.{{envs.min_gap_size}}.bed\" ) lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"min_gap_size\" : 5000 , \"ref\" : config . ref . reffa , } script = \"file://../scripts/cnvkit/CNVkitAccess.py\" class CNVkitAutobin ( Proc ): DOCS \"\"\"Quickly estimate read counts or depths in a BAM file to estimate reasonable on- and (if relevant) off-target bin sizes. Using `cnvkit.py autobin`. If multiple BAMs are given, use the BAM with median file size. Input: bamfiles: The bamfiles accfile: The access file baitfile: Potentially targeted genomic regions. E.g. all possible exons for the reference genome. Format - BED, interval list, etc. Output: target_file: The target BED output antitarget_file: The antitarget BED output Envs: cnvkit: Path to `cnvkit.py` method (choice): Sequencing protocol. Determines whether and how to use antitarget bins. - hybrid: Hybridization capture - amplicon: Targeted amplicon sequencing - wgs: Whole genome sequencing bp_per_bin (type=int): Desired average number of sequencing read bases mapped to each bin. target_max_size (type=int): Maximum size of target bins. target_min_size (type=int): Minimum size of target bins. antitarget_max_size (type=int): Maximum size of antitarget bins. antitarget_min_size (type=int): Minimum size of antitarget bins. annotate: Use gene models from this file to assign names to the target regions. Format: UCSC refFlat.txt or ensFlat.txt file (preferred), or BED, interval list, GFF, or similar. short_names (flag): Reduce multi-accession bait labels to be short and consistent. ref: The reference genome fasta file Requires: cnvkit: - check: {{proc.envs.cnvkit}} version \"\"\" input = \"bamfiles:files, accfile:file, baitfile:file\" output = [ \"target_file:file:{{in.bamfiles | first | stem0}}-etc.target.bed\" , \"antitarget_file:file:{{in.bamfiles | first | stem0}}\" \"-etc.antitarget.bed\" , ] lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"method\" : \"hybrid\" , \"bp_per_bin\" : 100000 , \"target_max_size\" : 20000 , \"target_min_size\" : 20 , \"antitarget_max_size\" : 500000 , \"antitarget_min_size\" : 500 , \"annotate\" : None , \"short_names\" : False , \"ref\" : config . ref . reffa , } script = \"file://../scripts/cnvkit/CNVkitAutobin.py\" class CNVkitCoverage ( Proc ): DOCS \"\"\"Run cnvkit coverage Input: bamfile: The bamfile target_file: The target file or anti-target file Output: outfile: The output coverage file Envs: cnvkit: Path to cnvkit.py count (flag): Get read depths by counting read midpoints within each bin. (An alternative algorithm). min_mapq (type=int): Minimum mapping quality to include a read. ncores (type=int): Number of subprocesses to calculate coverage in parallel ref: The reference genome fasta file Requires: cnvkit: - check: {{proc.envs.cnvkit}} version \"\"\" input = \"bamfile:file, target_file:file\" output = \"\"\"outfile:file: { %- i f \"antitarget\" in basename(in.target_file) -%} {{in.bamfile | stem0}}.antitargetcoverage.cnn { %- e lse -%} {{in.bamfile | stem0}}.targetcoverage.cnn { %- e ndif -%} \"\"\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"count\" : False , \"min_mapq\" : 0 , \"ncores\" : config . misc . ncores , \"ref\" : config . ref . reffa , } script = \"file://../scripts/cnvkit/CNVkitCoverage.py\" class CNVkitReference ( Proc ): DOCS \"\"\"Run cnvkit reference To genearte a reference file from normal samples, provide the cnn coverage files from the normal samples. To generate a flat reference file, provide the target/antitarget file. Input: covfiles: The coverage files from normal samples target_file: Target intervals (.bed or .list) antitarget_file: Antitarget intervals (.bed or .list) sample_sex: Specify the chromosomal sex of all given samples as male or female. Guess each sample from coverage of X and Y chromosomes if not given. Output: outfile: The reference cnn file Envs: cnvkit: Path to cnvkit.py cluster (flag): Calculate and store summary stats for clustered subsets of the normal samples with similar coverage profiles. min_cluster_size (type=int): Minimum cluster size to keep in reference profiles. male_reference (flag): Create a male reference: shift female samples chrX log-coverage by -1, so the reference chrX average is -1. Otherwise, shift male samples chrX by +1, so the reference chrX average is 0. no_gc (flag): Skip GC correction. no_edge (flag): Skip edge-effect correction. no_rmask (flag): Skip RepeatMasker correction. ref: The reference genome fasta file Requires: cnvkit: - check: {{proc.envs.cnvkit}} version \"\"\" input = [ \"covfiles:files\" , \"target_file:file\" , \"antitarget_file:file\" , \"sample_sex:var\" , ] output = \"\"\"outfile:file: { %- i f not in.covfiles -%} flat.reference.cnn { %- e lse -%} {{in.covfiles | first | stem0 }}-etc.reference.cnn { %- e ndif -%} \"\"\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"cluster\" : False , \"min_cluster_size\" : 4 , \"male_reference\" : False , \"no_gc\" : False , \"no_edge\" : False , \"no_rmask\" : False , \"ref\" : config . ref . reffa , } script = \"file://../scripts/cnvkit/CNVkitReference.py\" class CNVkitFix ( Proc ): DOCS \"\"\"Run cnvkit.py fix Input: target_file: The target file antitarget_file: The antitarget file reference: The refence cnn file sample_id: Sample ID for target/antitarget files. Otherwise inferred from file names. Output: outfile: The fixed coverage files (.cnr) Envs: cnvkit: Path to cnvkit.py cluster (flag): Compare and use cluster-specific values present in the reference profile. (requires `envs.cluster=True` for `CNVkitReference`). no_gc (flag): Skip GC correction. no_edge (flag): Skip edge-effect correction. no_rmask (flag): Skip RepeatMasker correction. Requires: cnvkit: - check: {{proc.envs.cnvkit}} version \"\"\" input = ( \"target_file:file, antitarget_file:file, reference:file, sample_id:var\" ) output = ( \"outfile:file:{{in.sample_id | default: stem0(in.target_file)}}.cnr\" ) lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"cluster\" : False , \"no_gc\" : False , \"no_edge\" : False , \"no_rmask\" : False , } script = \"file://../scripts/cnvkit/CNVkitFix.py\" class CNVkitSegment ( Proc ): DOCS \"\"\"Run cnvkit.py segment For segmentation methods, see https://cnvkit.readthedocs.io/en/stable/pipeline.html#segmentation-methods Input: cnrfile: The fixed coverage files (.cnr) vcf: VCF file name containing variants for segmentation by allele frequencies (optional). sample_id: Specify the name of the sample in the VCF to use for b-allele frequency extraction and as the default plot title. normal_id: Corresponding normal sample ID in the input VCF. This sample is used to select only germline SNVs to plot b-allele frequencies. Output: outfile: The segmentation file (.cns) Envs: cnvkit: Path to cnvkit.py method: Method to use for segmentation. Candidates - cbs, flasso, haar, none, hmm, hmm-tumor, hmm-germline threshold: Significance threshold (p-value or FDR, depending on method) to accept breakpoints during segmentation. For HMM methods, this is the smoothing window size. drop_low_coverage (flag): Drop very-low-coverage bins before segmentation to avoid false-positive deletions in poor-quality tumor samples. drop_outliers (type=int): Drop outlier bins more than this many multiples of the 95th quantile away from the average within a rolling window. Set to 0 for no outlier filtering. rscript: Path to Rscript ncores (type=int): Number of subprocesses to segment in parallel. 0 or negative for all available cores smooth_cbs (flag): Perform an additional smoothing before CBS segmentation, which in some cases may increase the sensitivity. Used only for CBS method. min_variant_depth (type=int): Minimum read depth for a SNV to be displayed in the b-allele frequency plot. zygosity_freq (type=float): Ignore VCF's genotypes (GT field) and instead infer zygosity from allele frequencies. Requires: cnvkit: - check: {{proc.envs.cnvkit}} version r-DNAcopy: - check: {{proc.envs.rscript}} <(echo \"library(DNAcopy)\") \"\"\" input = \"cnrfile:file, vcf:file, sample_id:var, normal_id:var\" output = \"outfile:file:{{in.cnrfile | stem0}}.cns\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"method\" : \"cbs\" , \"threshold\" : None , \"drop_low_coverage\" : False , \"drop_outliers\" : 10 , \"rscript\" : config . lang . rscript , \"ncores\" : config . misc . ncores , \"smooth_cbs\" : False , \"min_variant_depth\" : 20 , \"zygosity_freq\" : 0.25 , } script = \"file://../scripts/cnvkit/CNVkitSegment.py\" class CNVkitScatter ( Proc ): DOCS \"\"\"Run cnvkit.py scatter Input: cnrfile: The fixed cnr file (.cnr) cnsfile: The segmentation file (.cns) vcf: VCF file name containing variants for segmentation by allele frequencies (optional). sample_id: Specify the name of the sample in the VCF to use for b-allele frequency extraction and as the default plot title. normal_id: Corresponding normal sample ID in the input VCF. This sample is used to select only germline SNVs to plot b-allele frequencies. Output: outdir: Output directory with plots for multiple cases Envs: cnvkit: Path to cnvkit.py convert: Path to `convert` to convert pdf to png file convert_args (ns): The arguments for `convert` - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - <more>: See `convert -help` and also: https://linux.die.net/man/1/convert chromosome: Chromosome or chromosomal range, e.g. 'chr1' or 'chr1:2333000-2444000', to display. If a range is given, all targeted genes in this range will be shown, unless -g/--gene is also given. gene: Name of gene or genes (comma-separated) to display. width (type=int): Width of margin to show around the selected gene(s) (-g/--gene) or small chromosomal region (-c/--chromosome). antitarget_marker (flag): Plot antitargets using this symbol when plotting in a selected chromosomal region (-g/--gene or -c/--chromosome). by_bin (flag): Plot data x-coordinates by bin indices instead of genomic coordinates. All bins will be shown with equal width, no blank regions will be shown, and x-axis values indicate bin number (within chromosome) instead of genomic position. segment_color: Plot segment lines in this color. Value can be any string accepted by matplotlib, e.g. 'red' or '#CC0000'. trend (flag): Draw a smoothed local trendline on the scatter plot. y_max (type=int): y-axis upper limit. y_min (tyoe=int): y-axis lower limit. min_variant_depth (type=int): Minimum read depth for a SNV to be displayed in the b-allele frequency plot. zygosity_freq (typ=float): Ignore VCF's genotypes (GT field) and instead infer zygosity from allele frequencies. title: Plot title. Sample ID if not provided. cases (type=json): The cases for different plots with keys as case names and values to overwrite the default args given by `envs.<args>`, including `convert_args`, `by_bin`, `chromosome`, `gene`, `width` `antitarget_marker`, `segment_color`, `trend`, `y_max`, `y_min`, `min_variant_depth`, `zygosity_freq` and `title. By default, an `all` case will be created with default arguments if no case specified Requires: cnvkit: - check: {{proc.envs.cnvkit}} version convert: - check: {{proc.envs.convert}} -version \"\"\" input = ( \"cnrfile:file, cnsfile:file, config:var, \" \"vcf:file, sample_id:var, normal_id:var\" ) output = \"outdir:dir:{{in.cnrfile | stem0}}.scatter\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"convert\" : config . exe . convert , \"convert_args\" : { \"density\" : 150 , \"quality\" : 90 , \"background\" : \"white\" , \"alpha\" : \"remove\" , }, \"chromosome\" : None , \"gene\" : None , \"width\" : 1000000 , \"antitarget_marker\" : False , \"by_bin\" : False , \"segment_color\" : None , \"trend\" : False , \"y_max\" : None , \"y_min\" : None , \"min_variant_depth\" : 20 , \"zygosity_freq\" : 0.25 , \"title\" : None , \"cases\" : {}, } script = \"file://../scripts/cnvkit/CNVkitScatter.py\" plugin_opts = { \"report\" : \"file://../reports/cnvkit/CNVkitScatter.svelte\" , \"report_paging\" : 10 , } class CNVkitDiagram ( Proc ): DOCS \"\"\"Run cnvkit.py diagram Input: cnrfile: The fixed cnr file (.cnr) cnsfile: The segmentation file (.cns) sample_sex: Specify the sample's chromosomal sex as male or female. (Otherwise guessed from X and Y coverage). Output: outdir: Output directory with the scatter plots Envs: cnvkit: Path to cnvkit.py convert: Path to `convert` to convert pdf to png file convert_args (ns): The arguments for `convert` - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - <more>: See `convert -help` and also: https://linux.die.net/man/1/convert threshold (type=float): Copy number change threshold to label genes. min_probes (type=int): Minimum number of covered probes to label a gene. male_reference (flag): Assume inputs were normalized to a male reference (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). no_shift_xy (flag): Don't adjust the X and Y chromosomes according to sample sex. title: Plot title. Sample ID if not provided. cases (type=json): The cases with keys as names and values as different configs, including `threshold`, `min_probes`, `male_reference`, `no_shift_xy` and `title` Requires: cnvkit: - check: {{proc.envs.cnvkit}} version convert: - check: {{proc.envs.convert}} -version \"\"\" input = \"cnrfile:file, cnsfile:file, sample_sex:var\" output = \"outdir:dir:{{in.cnrfile | stem0}}.diagram\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"convert\" : config . exe . convert , \"convert_args\" : { \"density\" : 150 , \"quality\" : 90 , \"background\" : \"white\" , \"alpha\" : \"remove\" , }, \"threshold\" : 0.5 , \"min_probes\" : 3 , \"male_reference\" : False , \"no_shift_xy\" : False , \"title\" : None , \"cases\" : {}, } script = \"file://../scripts/cnvkit/CNVkitDiagram.py\" plugin_opts = { \"report\" : \"file://../reports/cnvkit/CNVkitDiagram.svelte\" , \"report_paging\" : 10 , } class CNVkitHeatmap ( Proc ): DOCS \"\"\"Run cnvkit.py heatmap for multiple cases Input: segfiles: Sample coverages as raw probes (.cnr) or segments (.cns). sample_sex: Specify the chromosomal sex of all given samples as male or female. Separated by comma. (Default: guess each sample from coverage of X and Y chromosomes). Output: outdir: Output directory with heatmaps of multiple cases Envs: cnvkit: Path to cnvkit.py convert: Path to `convert` to convert pdf to png file convert_args (ns): The arguments for `convert` - density (type=int): Horizontal and vertical density of the image - quality (type=int): JPEG/MIFF/PNG compression level - background: Background color - alpha: Activate, deactivate, reset, or set the alpha channel - <more>: See `convert -help` and also: https://linux.die.net/man/1/convert by_bin (flag): Plot data x-coordinates by bin indices instead of genomic coordinates. All bins will be shown with equal width, no blank regions will be shown, and x-axis values indicate bin number (within chromosome) instead of genomic position. chromosome: Chromosome (e.g. 'chr1') or chromosomal range (e.g. 'chr1:2333000-2444000') to display. desaturate (flag): Tweak color saturation to focus on significant changes. male_reference (flag): Assume inputs were normalized to a male reference. (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). no_shift_xy (flag): Don't adjust the X and Y chromosomes according to sample sex. order: A file with sample names in the desired order. cases (type=json): The cases for different plots with keys as case names and values to overwrite the default args given by `envs.<args>`, including `convert_args`, `by_bin`, `chromosome`, `desaturate`, `male_reference`, and, `no_shift_xy`. By default, an `all` case will be created with default arguments if no case specified Requires: cnvkit: - check: {{proc.envs.cnvkit}} version convert: - check: {{proc.envs.convert}} -version \"\"\" input = \"segfiles:files, sample_sex: var\" output = \"outdir:dir:{{in.segfiles | first | stem0}}-etc.heatmap\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"convert\" : config . exe . config , \"convert_args\" : { \"density\" : 150 , \"quality\" : 90 , \"background\" : \"white\" , \"alpha\" : \"remove\" , }, \"by_bin\" : False , \"chromosome\" : False , \"desaturate\" : False , \"male_reference\" : False , \"no_shift_xy\" : False , \"order\" : None , \"cases\" : {}, } script = \"file://../scripts/cnvkit/CNVkitHeatmap.py\" plugin_opts = { \"report\" : \"file://../reports/cnvkit/CNVkitHeatmap.svelte\" } class CNVkitCall ( Proc ): DOCS \"\"\"Run cnvkit.py call Input: cnrfile: The fixed cnr file (.cnr), used to generate VCF file cnsfile: The segmentation file (.cns) vcf: VCF file name containing variants for segmentation by allele frequencies (optional). sample_id: Specify the name of the sample in the VCF to use for b-allele frequency extraction and as the default plot title. normal_id: Corresponding normal sample ID in the input VCF. This sample is used to select only germline SNVs to plot b-allele frequencies. sample_sex: Specify the sample's chromosomal sex as male or female. (Otherwise guessed from X and Y coverage). purity: Estimated tumor cell fraction, a.k.a. purity or cellularity. Output: outdir: The output directory including the call file (.call.cns) bed file, and the vcf file Envs: cnvkit: Path to cnvkit.py center: Re-center the log2 ratio values using this estimator of the center or average value. center_at (type=float): Subtract a constant number from all log2 ratios. For \"manual\" re-centering, in case the --center option gives unsatisfactory results.) filter: Merge segments flagged by the specified filter(s) with the adjacent segment(s). method (choice): Calling method (threshold, clonal or none). - threshold: Using hard thresholds for calling each integer copy number. Use `thresholds` to set a list of threshold log2 values for each copy number state - clonal: Rescaling and rounding. For a given known tumor cell fraction and normal ploidy, then simple rounding to the nearest integer copy number - none: Do not add a \u201ccn\u201d column or allele copy numbers. But still performs rescaling, re-centering, and extracting b-allele frequencies from a VCF (if requested). thresholds: Hard thresholds for calling each integer copy number, separated by commas. ploidy (type=float): Ploidy of the sample cells. drop_low_coverage (flag): Drop very-low-coverage bins before segmentation to avoid false-positive deletions in poor-quality tumor samples. male_reference (flag): Assume inputs were normalized to a male reference. (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). min_variant_depth (type=int): Minimum read depth for a SNV to be displayed in the b-allele frequency plot. zygosity_freq (type=float): Ignore VCF's genotypes (GT field) and instead infer zygosity from allele frequencies. Requires: cnvkit: - check: {{proc.envs.cnvkit}} version \"\"\" input = [ \"cnrfile:file\" , \"cnsfile:file\" , \"vcf:file\" , \"sample_id:var\" , \"normal_id:var\" , \"sample_sex:var\" , \"purity:var\" , ] output = \"outdir:dir:{{in.cnsfile | stem0}}.cnvkit\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"center\" : \"median\" , \"center_at\" : None , \"filter\" : None , \"method\" : \"threshold\" , \"thresholds\" : \"-1.1,-0.25,0.2,0.7\" , \"ploidy\" : 2 , \"drop_low_coverage\" : False , \"male_reference\" : False , \"min_variant_depth\" : 20 , \"zygosity_freq\" : 0.25 , } script = \"file://../scripts/cnvkit/CNVkitCall.py\" class CNVkitBatch ( Proc ): DOCS \"\"\"Run cnvkit batch If you need in-depth control of the parameters, for example, multiple scatter plots in different regions, or you need to specify sample-sex for different samples, take a look at `biopipen.ns.cnvkit_pipeline` Input: metafile: The meta data file containing the sample information Two columns BamFile and `envs.type_col` are required. The tumor samples should be labeled as `envs.type_tumor` and the normal samples should be labeled as `envs.type_normal` in the `envs.type_col` column. If normal samples are not found, a flat reference will be used. The could be other columns in the meta file, but they could be used in `biopipen.ns.cnvkit_pipeline`. Output: outdir: The output directory Envs: cnvkit: Path to cnvkit.py method: Sequencing assay type: hybridization capture ('hybrid'), targeted amplicon sequencing ('amplicon'), or whole genome sequencing ('wgs'). Determines whether and how to use antitarget bins. segment_method: cbs,flasso,haar,none,hmm,hmm-tumor,hmm-germline Method used in the 'segment' step. male_reference: Use or assume a male reference (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). count_reads: Get read depths by counting read midpoints within each bin. (An alternative algorithm). drop_low_coverage: Drop very-low-coverage bins before segmentation to avoid false-positive deletions in poor-quality tumor samples. ncores: Number of subprocesses used to running each of the BAM files in parallel rscript: Path to the Rscript excecutable to use for running R code. Use this option to specify a non-default R installation. ref: Path to a FASTA file containing the reference genome. targets: Target intervals (.bed or .list) (optional for wgs) antitargets: Anti-target intervals (.bed or .list) (optional for wgs) annotate: Use gene models from this file to assign names to the target regions. Format: UCSC refFlat.txt or ensFlat.txt file (preferred), or BED, interval list, GFF, or similar. short_names: Reduce multi-accession bait labels to be short and consistent. target_avg_size: Average size of split target bins (results are approximate). access: Regions of accessible sequence on chromosomes (.bed), as output by the 'access' command. access_min_gap_size: Minimum gap size between accessible sequence regions if `envs.access` is not specified. access_excludes: Exclude these regions from the accessible genome Used when `envs.access` is not specified. antitarget_avg_size: Average size of antitarget bins (results are approximate). antitarget_min_size: Minimum size of antitarget bins (smaller regions are dropped). cluster: Calculate and use cluster-specific summary stats in the reference pool to normalize samples. reference: Copy number reference file (.cnn) to reuse scatter: Create a whole-genome copy ratio profile as a PDF scatter plot. diagram: Create an ideogram of copy ratios on chromosomes as a PDF. type_col: type_col: The column name in the metafile that indicates the sample type. type_tumor: The type of tumor samples in `envs.type_col` column of `in.metafile` type_normal: The type of normal samples in `envs.type_col` column of `in.metafile` Requires: cnvkit: - check: {{proc.envs.cnvkit}} version r-DNAcopy: - check: {{proc.envs.rscript}} <(echo \"library(DNAcopy)\") \"\"\" input = \"metafile:file\" output = \"outdir:dir:{{in.metafile | stem0}}.cnvkit\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"method\" : \"hybrid\" , \"segment_method\" : \"cbs\" , \"male_reference\" : False , \"count_reads\" : False , \"drop_low_coverage\" : False , \"ncores\" : config . misc . ncores , \"rscript\" : config . lang . rscript , \"ref\" : config . ref . reffa , \"targets\" : False , \"antitargets\" : False , \"annotate\" : False , \"short_names\" : False , \"target_avg_size\" : False , \"access\" : False , \"access_min_gap_size\" : 5000 , \"access_excludes\" : False , \"antitarget_avg_size\" : False , \"antitarget_min_size\" : False , \"cluster\" : False , \"reference\" : False , \"scatter\" : True , \"diagram\" : True , \"type_tumor\" : \"Tumor\" , \"type_normal\" : \"Normal\" , \"type_col\" : \"SampleType\" , } script = \"file://../scripts/cnvkit/CNVkitBatch.py\" class CNVkitGuessBaits ( Proc ): DOCS \"\"\"Guess the bait intervals from the bam files It runs scripts/guess_baits.py from the cnvkit repo. Input: bamfiles: The bam files atfile: The potential target file or access file e.g. all known exons in the reference genome or from `cnvkit.py access` Output: targetfile: The target file Envs: cnvkit: Path to cnvkit.py guided (flag): `in.atfile` is a potential target file when `True`, otherwise it is an access file. samtools: Path to samtools executable ncores (type=int): Number of subprocesses to segment in parallel `0` to use the maximum number of available CPUs. ref: Path to a FASTA file containing the reference genome. min_depth (type=int): Minimum sequencing read depth to accept as captured. For guided only. min_gap (type=int): Merge regions separated by gaps smaller than this. min_length (type=int): Minimum region length to accept as captured. `min_gap` and `min_length` are for unguided only. \"\"\" input = \"bamfiles:files, atfile:file\" output = \"targetfile:file:{{in.bamfiles | first | stem}}_etc.baits.bed\" lang = config . lang . python envs = { \"cnvkit\" : config . exe . cnvkit , \"samtools\" : config . exe . samtools , \"ncores\" : config . misc . ncores , \"ref\" : config . ref . reffa , \"guided\" : None , \"min_depth\" : 5 , \"min_gap\" : 25 , \"min_length\" : 50 , } script = \"file://../scripts/cnvkit/CNVkitGuessBaits.py\"","title":"biopipen.ns.cnvkit"},{"location":"api/source/biopipen.ns.cnvkit_pipeline/","text":"SOURCE CODE biopipen.ns. cnvkit_pipeline DOCS \"\"\"The CNVkit pipeline.\"\"\" from __future__ import annotations from typing import TYPE_CHECKING , Any from functools import lru_cache import pandas from diot import Diot from datar.tibble import tibble from pipen.utils import mark , is_loading_pipeline from biopipen.core.proc import Proc from pipen_annotate import annotate from pipen_args.procgroup import ProcGroup from ..core.config import config from functools import cached_property if TYPE_CHECKING : from pandas import DataFrame @lru_cache () def _metadf ( metafile : str ) -> DataFrame : return pandas . read_csv ( metafile , sep = \" \\t \" , header = 0 ) def _1st ( df : DataFrame ) -> Any : return df . iloc [ 0 , 0 ] class _MetaCol : \"\"\"Get the column name from the metafile\"\"\" def __init__ ( self , cols , default_cols ): self . cols = cols or {} self . default_cols = default_cols def __getattr__ ( self , name ): return self . cols . get ( name , self . default_cols [ name ]) class CNVkitPipeline ( ProcGroup ): DOCS \"\"\"The CNVkit pipeline Unlike `cnvkit.py batch`, this decouples the steps of the `batch` command so that we can control the details of each step. Options for different processes can be specified by `[CNVkitXXX.envs.xxx]` See `biopipen.ns.cnvkit.CNVkitXXX` for more details. To run this pipeline from command line, with the `pipen-run` plugin: >>> # In this case, `pipeline.cnvkit_pipeline.metafile` must be provided >>> pipen run cnvkit_pipeline CNVkitPipeline <other pipeline args> To use this as a dependency for other pipelines - >>> from biopipen.ns.cnvkit_pipeline import CNVkitPipeline >>> pipeline = CNVkitPipeline(<options>) >>> # pipeline.starts: Start processes of the pipeline >>> # pipeline.ends: End processes of the pipeline >>> # pipeline.procs.<proc>: The process with name <proc> See also the docs for details <https://pwwang.github.io/biopipen/pipelines/cnvkit_pipeline/> Args: metafile (order=-99): A tab-separated file. * Sample: Unique IDs of the samples. Required. * `<bam>`: The path to the bam file, better using absolute path. * `<group>`: The type of the sample, defining the tumor/normal samples. * `<sex>`: Guess each sample from coverage of X and Y chromosomes if not given. * `<purity>`: Estimated tumor cell fraction, a.k.a. purity or cellularity. * `<snpvcf>`: file name containing variants for segmentation by allele frequencies. * `<vcf_sample_id>`: Sample ID in the VCF file. * `<vcf_normal_id>`: Normal sample ID in the VCF file. * `<guess_baits>`: Whether use this bam file to guess the baits metacols (ns;order=-98): The column names for each type of information in metafile. - group (default=Group): The column name in the metafile that indicates the sample group - purity: The column name in the metafile that indicates the sample purity - snpvcf: The column name in the metafile that indicates the path to the SNP VCFflag - bam: The column name in the metafile that indicates the path to the BAM file - vcf_sample_id: column name in the metafile that indicates the sample ID in the VCF file - vcf_normal_id: olumn name in the metafile that indicates the normal sample ID in the VCF file - sex:flagin the metafile that indicates the sample sex - guess_baits: The column name in the metafile that indicates whether to guess the bait file from the bam files baitfile: Potentially targeted genomic regions. E.g. all possible exons for the reference genome. This is optional when `method` is `wgs`. accfile: The accessible genomic regions. If not given, use `cnvkit.py access` to generate one. access_excludes (list): File(s) with regions to be excluded for `cnvkit.py access`. guessbaits_guided (flag): Whether to use guided mode for guessing baits using `baitfile`, otherwise unguided, using the `accfile`. guessbaits (flag): Guess the bait file from the bam files, either guided or unguided. If False, `baitfile` is used. Otherwise, if `baitfile` is given, use it (guided), otherwise use `accfile` (unguided). The bam files with `metacols.guess_baits` column set to `True`, `TRUE`, `true`, `1`, `Yes`, `YES`, or `yes` will be used to guess the bait file. heatmap_cnr (flag): Whether to generate a heatmap of the `.cnr` files (bin-level signals). This is allowed to set to `False`, it will take longer to run. case: The group name of samples in `metacols.group` to call CNVs for. If not specified, use all samples. In such a case, `control` must not be specified, as we are using a flat reference. control: The group name of samples in `metacols.group` to use as reference if not specified, use a flat reference. cnvkit: the path to the cnvkit.py executable, defaults to `config.exe.cnvkit` from `./.biopipen.toml` or `~/.biopipen.toml`. rscript: Path to the Rscript excecutable to use for running R code. Requires `DNAcopy` to be installed in R, defaults to `config.lang.rscript` samtools: Path to samtools, used for guessing bait file. convert: Linux `convert` command to convert pdf to png So that they can be embedded in the HTML report. ncores: Default number of cores to use for all processes with `envs.ncores`, defaults to `config.misc.ncores` reffa: the reference genome (e.g. hg19.fa). Used by `CNVkitAccess`, `CNVkitAutobin` and `CNVkitReference` annotate: Use gene models from this file to assign names to the target regions. Format: UCSC `refFlat.txt` or `ensFlat.txt` file (preferred), or BED, interval list, GFF, or similar. short_names (flag): Reduce multi-accession bait labels to be short and consistent. method (choice): Sequencing protocol, determines whether and how to use antitarget bins. - hybrid: hybridization capture - amplicon: targeted amplicon sequencing - wgs: whole genome sequencing male_reference (flag): Use or assume a male reference (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). Used by `CNVkitReference`, `CNVkitCall`, `CNVkitHeatmapCns` and `CNVkitHeatmapCnr`. drop_low_coverage (flag): Drop very-low-coverage bins before segmentation to avoid false-positive deletions in poor-quality tumor samples. Used by `CNVkitSegment` and `CNVkitCall` no_gc (flag): Skip GC correction for `cnvkit.py reference/fix`. no_edge (flag): Skip edge-effect correction for `cnvkit.py reference/fix`. no_rmask (flag): Skip RepeatMasker correction for `cnvkit.py reference/fix`. no_* options are used by `CNVkitReference` and `CNVkitFix` min_variant_depth (type=int): Minimum read depth for a SNV to be displayed in the b-allele frequency plot. Used by `CNVkitSegment` and `CNVkitCall` zygosity_freq (type=float): Ignore VCF's genotypes (GT field) and instead infer zygosity from allele frequencies. Used by `CNVkitSegment` and `CNVkitCall` \"\"\" DEFAULTS = Diot ( metafile = None , baitfile = None , accfile = None , cnvkit = config . exe . cnvkit , convert = config . exe . convert , rscript = config . lang . rscript , samtools = config . exe . samtools , ncores = config . misc . ncores , reffa = config . ref . reffa , annotate = config . ref . refflat , short_names = True , method = \"hybrid\" , guessbaits = False , heatmap_cnr = False , case = None , control = None , access_excludes = [], guessbaits_guided = False , male_reference = False , drop_low_coverage = False , min_variant_depth = 20 , no_gc = False , no_edge = False , no_rmask = False , zygosity_freq = 0.25 , metacols = Diot ( group = \"Group\" , purity = \"Purity\" , snpvcf = \"SnpVcf\" , bam = \"Bam\" , vcf_sample_id = \"VcfSampleId\" , vcf_normal_id = \"VcfNormalId\" , sex = \"Sex\" , guess_baits = \"GuessBaits\" , ), ) @cached_property def col ( self ): \"\"\"Get the column names by self.col.<colname>\"\"\" return _MetaCol ( self . opts . get ( \"metacols\" ), self . __class__ . DEFAULTS . metacols , ) @ProcGroup . add_proc def p_metafile ( self ): \"\"\"Build MetaFile process\"\"\" from .misc import File2Proc @mark ( board_config_hidden = True ) class MetaFile ( File2Proc ): \"\"\"Pass by the metafile to the next process. When the group argument `metafile` is provided, it will be used as the input data, otherwise, this process group should be a part of a pipeline, and the metafile will be passed by its required processes. \"\"\" # Do not require metafile, as we could use the pipeline as part of # another pipeline, which can generate a metafile # Remember to set the dependency in the pipeline: # >>> pipeline.procs.MetaFile.requires = [other_pipeline.procs] # where other_pipeline.procs generate the metafile if self . opts . metafile : input_data = [ self . opts . metafile ] return MetaFile @ProcGroup . add_proc def p_cnvkit_access ( self ): \"\"\"Build CNVkitAccess process\"\"\" if self . opts . get ( \"accfile\" ): from .misc import File2Proc @mark ( board_config_hidden = True ) class CNVkitAccess ( File2Proc ): \"\"\"Pass by the access file to the next process.\"\"\" input_data = [ self . opts . accfile ] else : from .cnvkit import CNVkitAccess excludes = self . opts . get ( \"excludes\" , []) if not isinstance ( excludes , ( list , tuple )): excludes = [ excludes ] @annotate . format_doc ( indent = 4 ) class CNVkitAccess ( CNVkitAccess ): \"\"\"{{Summary}} **When group argument `accfile` is provided, the arguments won't work. The `accfile` will just be passed by to the next process.** Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 24}}. Defaults to group argument `cnvkit`. ref (pgarg=reffa): {{Envs.ref.help | indent: 24}}. Defaults group argument `reffa`. \"\"\" input_data = [ excludes ] envs = { \"cnvkit\" : self . opts . cnvkit , \"ref\" : self . opts . reffa , } return CNVkitAccess @ProcGroup . add_proc def p_cnvkit_guessbaits ( self ): \"\"\"Build CNVkitGuessBaits process\"\"\" from .cnvkit import CNVkitGuessBaits if ( not self . opts . guessbaits and not is_loading_pipeline ( \"-h\" , \"-h+\" , \"--help\" , \"--help+\" ) ): return None def _guess_baits_bams ( ch ): df = _metadf ( _1st ( ch )) if self . col . guess_baits not in df : # Use all bams return df . loc [:, self . col . bam ] . tolist () # Use only specified guess_baits = df [ self . col . guess_baits ] return df . loc [ ( guess_baits == True ) # noqa | ( guess_baits == \"True\" ) | ( guess_baits == \"TRUE\" ) | ( guess_baits == \"true\" ) | ( guess_baits == \"1\" ) | ( guess_baits == 1 ) | ( guess_baits == \"yes\" ) | ( guess_baits == \"YES\" ) | ( guess_baits == \"Yes\" ), self . col . bam , ] . tolist () if self . opts . guessbaits_guided : if not self . opts . baitfile : raise ValueError ( \"`baitfile` must be specified for guided mode \" \"to guess baits. See: \" \"https://cnvkit.readthedocs.io/en/stable/scripts.html\" ) @annotate . format_doc ( indent = 4 ) class CNVkitGuessBaits ( CNVkitGuessBaits ): \"\"\"{{Summary}} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 24}}. Defaults to group argument `cnvkit`. samtools (pgarg): {{Envs.samtools.help | indent: 24}}. Defaults to group argument `samtools`. ncores (pgarg): {{Envs.ncores.help | indent: 24}}. Defaults to group argument `ncores`. ref (pgarg=reffa): {{Envs.ref.help | indent: 24}}. Defaults to group argument `reffa`. guided (pgarg): {{Envs.guided.help | indent: 24}}. Defaults to group argument `guessbaits_guided`. \"\"\" requires = self . p_metafile input_data = lambda metafile_ch : tibble ( bamfiles = [ _guess_baits_bams ( metafile_ch )], atfile = self . opts . baitfile , ) envs = { \"cnvkit\" : self . opts . cnvkit , \"samtools\" : self . opts . samtools , \"ncores\" : self . opts . ncores , \"ref\" : self . opts . reffa , \"guided\" : True , } else : # unguided @annotate . format_doc ( indent = 4 ) class CNVkitGuessBaits ( CNVkitGuessBaits ): \"\"\"{{Summary}} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 24}}. Defaults to group argument `cnvkit`. samtools (pgarg): {{Envs.samtools.help | indent: 24}}. Defaults to group argument `samtools`. ncores (pgarg): {{Envs.ncores.help | indent: 24}}. Defaults to group argument `ncores`. ref (pgarg=reffa): {{Envs.ref.help | indent: 24}}. Defaults to group argument `reffa`. guided (pgarg): {{Envs.guided.help | indent: 24}}. Defaults to group argument `guessbaits_guided`. \"\"\" requires = self . p_metafile , self . p_cnvkit_access input_data = lambda metafile_ch , access_ch : tibble ( bamfiles = [ _guess_baits_bams ( metafile_ch )], accessfile = _1st ( access_ch ), ) envs = { \"cnvkit\" : self . opts . cnvkit , \"samtools\" : self . opts . samtools , \"ncores\" : self . opts . ncores , \"ref\" : self . opts . reffa , \"guided\" : False , } return CNVkitGuessBaits @ProcGroup . add_proc def p_cnvkit_autobin ( self ): \"\"\"Build CNVkitAutobin process\"\"\" from .cnvkit import CNVkitAutobin @annotate . format_doc ( indent = 3 ) class CNVkitAutobin ( CNVkitAutobin ): \"\"\"{{Summary}} Envs: method (pgarg): {{Envs.method.help | indent: 20}}. cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. ref (pgarg=reffa): {{Envs.ref.help | indent: 20}}. Defaults to group argument `reffa`. annotate (pgarg): {{Envs.annotate.help | indent: 20}}. Defaults to group argument `annotate`. short_names (pgarg): {{Envs.short_names.help | indent:20}}. Defaults to group argument `short_names`. \"\"\" if self . p_cnvkit_guessbaits : requires = ( self . p_metafile , self . p_cnvkit_access , self . p_cnvkit_guessbaits , ) input_data = lambda ch1 , ch2 , ch3 : tibble ( bamfiles = [ _metadf ( _1st ( ch1 ))[ self . col . bam ] . tolist ()], accfile = _1st ( ch2 ), baitfile = ( _1st ( ch3 ) if self . opts . guessbaits else self . opts . baitfile ), ) else : requires = self . p_metafile , self . p_cnvkit_access input_data = lambda ch1 , ch2 : tibble ( bamfiles = [ _metadf ( _1st ( ch1 ))[ self . col . bam ] . tolist ()], accfile = _1st ( ch2 ), baitfile = self . opts . baitfile , ) envs = { \"cnvkit\" : self . opts . cnvkit , \"method\" : self . opts . method , \"annotate\" : self . opts . annotate , \"short_names\" : self . opts . short_names , \"ref\" : self . opts . reffa , } return CNVkitAutobin def _p_cnvkit_coverage ( self , anti : bool ): \"\"\"Build CNVkitTargetCoverage and CNVkitAntiTargetCoverage processes\"\"\" from .cnvkit import CNVkitCoverage p = Proc . from_proc ( CNVkitCoverage , name = \"CNVkitCoverageAnittarget\" if anti else \"CNVkitCoverageTarget\" , requires = [ self . p_metafile , self . p_cnvkit_autobin ], input_data = lambda ch1 , ch2 : tibble ( _metadf ( _1st ( ch1 ))[ self . col . bam ] . tolist (), target_file = ch2 [ \"antitarget_file\" if anti else \"target_file\" ] . tolist ()[ 0 ], ), envs = { \"cnvkit\" : self . opts . cnvkit , \"ncores\" : self . opts . ncores , \"ref\" : self . opts . reffa , } ) if anti : p . __doc__ = \"\"\"Build the coverage for the anti-target regions\"\"\" else : p . __doc__ = \"\"\"Build the coverage for the target regions\"\"\" p . __doc__ += \"\"\" {{* Summary.long }} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 16}}. Defaults to group argument `cnvkit`. ncores (pgarg): {{Envs.ncores.help | indent: 16}}. Defaults to group argument `ncores`. ref (pgarg=reffa): {{Envs.ref.help | indent: 16}}. Defaults to group argument `reffa`. \"\"\" return annotate . format_doc ( indent = 2 )( p ) @ProcGroup . add_proc def p_cnvkit_coverage_target ( self ): \"\"\"Build CNVkitCoverageTarget process\"\"\" return self . _p_cnvkit_coverage ( anti = False ) @ProcGroup . add_proc def p_cnvkit_coverage_antitarget ( self ): \"\"\"Build CNVkitCoverageAntiTarget process\"\"\" return self . _p_cnvkit_coverage ( anti = True ) @ProcGroup . add_proc def p_cnvkit_reference ( self ): \"\"\"Build CNVkitReference process\"\"\" from .cnvkit import CNVkitReference def _input_data ( ch1 , ch2 , ch3 , ch4 ): metadf = _metadf ( _1st ( ch1 )) if self . opts . control : # Use control samples to build reference control_masks = metadf [ self . col . group ] == self . opts . control covfiles = [ ch2 . outfile [ control_masks ] . tolist () + ch3 . outfile [ control_masks ] . tolist () ] target_file = None antitarget_file = None if self . col . sex in metadf : all_sex = metadf [ self . col . sex ][ control_masks ] . unique () sample_sex = [ None ] if len ( all_sex ) > 1 else all_sex [ 0 ] else : sample_sex = [ None ] else : # Build a flat reference covfiles = [ None ] target_file = ch4 . target_file antitarget_file = ch4 . antitarget_file sample_sex = [ None ] return tibble ( covfiles = covfiles , target_file = target_file , antitarget_file = antitarget_file , sample_sex = sample_sex , ) @annotate . format_doc ( indent = 3 ) class CNVkitReference ( CNVkitReference ): \"\"\"{{Summary}} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. no_gc (pgarg): {{Envs.no_gc.help | indent: 20}}. Defaults to group argument `no_gc`. no_edge (pgarg): {{Envs.no_edge.help | indent: 20}}. Defaults to group argument `no_edge`. no_rmask (pgarg): {{Envs.no_rmask.help | indent: 20}}. Defaults to group argument `no_rmask`. ref (pgarg=reffa): {{Envs.ref.help | indent: 20}}. Defaults to group argument `reffa`. male_reference (pgarg): {{ Envs.male_reference.help | indent: 20 }}. Defaults to group argument `male_reference`. \"\"\" requires = [ self . p_metafile , self . p_cnvkit_coverage_target , self . p_cnvkit_coverage_antitarget , self . p_cnvkit_autobin , ] input_data = _input_data envs = { \"cnvkit\" : self . opts . cnvkit , \"no_gc\" : self . opts . no_gc , \"no_edge\" : self . opts . no_edge , \"no_rmask\" : self . opts . no_rmask , \"ref\" : self . opts . reffa , \"male_reference\" : self . opts . male_reference , } return CNVkitReference @ProcGroup . add_proc def p_cnvkit_fix ( self ): \"\"\"Build CNVkitFix process\"\"\" from .cnvkit import CNVkitFix if not self . opts . case and self . opts . control : raise ValueError ( \"`case` is not specified, meaning using all samples as cases, \" \"but `control` is specified (we can only use a flat reference \" \"in this case).\" ) def _input_data ( ch1 , ch2 , ch3 , ch4 ): metadf = _metadf ( _1st ( ch1 )) if not self . opts . case : tumor_masks = [ True ] * len ( metadf ) else : tumor_masks = metadf [ self . col . group ] == self . opts . case return tibble ( target_file = ch2 . outfile [ tumor_masks ], antitarget_file = ch3 . outfile [ tumor_masks ], reference = ch4 . outfile , sample_id = metadf [ \"Sample\" ][ tumor_masks ], ) @annotate . format_doc ( indent = 3 ) class CNVkitFix ( CNVkitFix ): \"\"\"{{Summary}} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. no_gc (pgarg): {{Envs.no_gc.help | indent: 20}}. Defaults to group argument `no_gc`. no_edge (pgarg): {{Envs.no_edge.help | indent: 20}}. Defaults to group argument `no_edge`. no_rmask (pgarg): {{Envs.no_rmask.help | indent: 20}}. Defaults to group argument `no_rmask`. \"\"\" requires = [ self . p_metafile , self . p_cnvkit_coverage_target , self . p_cnvkit_coverage_antitarget , self . p_cnvkit_reference , ] input_data = _input_data envs = { \"cnvkit\" : self . opts . cnvkit , \"no_gc\" : self . opts . no_gc , \"no_edge\" : self . opts . no_edge , \"no_rmask\" : self . opts . no_rmask , } return CNVkitFix @ProcGroup . add_proc def p_cnvkit_segment ( self ): \"\"\"Build CNVkitSegment process\"\"\" from .cnvkit import CNVkitSegment def _input_data ( ch1 , ch2 ): metadf = _metadf ( _1st ( ch1 )) if not self . opts . case : tumor_masks = [ True ] * len ( metadf ) else : tumor_masks = metadf [ self . col . group ] == self . opts . case return tibble ( chrfile = ch2 . outfile , vcf = ( metadf [ self . col . snpvcf ][ tumor_masks ] if self . col . snpvcf in metadf else [ None ] ), sample_id = ( metadf [ self . col . vcf_sample_id ][ tumor_masks ] if self . col . vcf_sample_id in metadf else [ None ] ), normal_id = ( metadf [ self . col . vcf_normal_id ][ tumor_masks ] if self . col . vcf_normal_id in metadf . columns else [ None ] ), ) @annotate . format_doc ( indent = 3 ) class CNVkitSegment ( CNVkitSegment ): \"\"\"{{Summary}} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. rscript (pgarg): {{Envs.rscript.help | indent: 20}}. Defaults to group argument `rscript`. ncores (pgarg): {{Envs.ncores.help | indent: 20}}. Defaults to group argument `ncores`. drop_low_coverage (pgarg): {{ Envs.drop_low_coverage.help | indent: 20}}. Defaults to group argument `drop_low_coverage`. min_variant_depth (pgarg): {{ Envs.min_variant_depth.help | indent: 20}}. Defaults to group argument `min_variant_depth`. zygosity_freq (pgarg): {{ Envs.zygosity_freq.help | indent: 20}}. Defaults to group argument `zygosity_freq`. \"\"\" requires = self . p_metafile , self . p_cnvkit_fix input_data = _input_data envs = { \"cnvkit\" : self . opts . cnvkit , \"rscript\" : self . opts . rscript , \"ncores\" : self . opts . ncores , \"drop_low_coverage\" : self . opts . drop_low_coverage , \"min_variant_depth\" : self . opts . min_variant_depth , \"zygosity_freq\" : self . opts . zygosity_freq , } return CNVkitSegment @ProcGroup . add_proc def p_cnvkit_scatter ( self ): \"\"\"Build CNVkitScatter process\"\"\" from .cnvkit import CNVkitScatter def _input_data ( ch1 , ch2 , ch3 ): metadf = _metadf ( _1st ( ch1 )) if not self . opts . case : tumor_masks = [ True ] * len ( metadf ) else : tumor_masks = metadf [ self . col . group ] == self . opts . case return tibble ( chrfile = ch2 . outfile , cnsfile = ch3 . outfile , vcf = ( metadf [ self . col . snpvcf ][ tumor_masks ] if self . col . snpvcf in metadf else [ None ] ), sample_id = ( metadf [ self . col . vcf_sample_id ][ tumor_masks ] if self . col . vcf_sample_id in metadf else [ None ] ), normal_id = ( metadf [ self . col . vcf_normal_id ][ tumor_masks ] if self . col . vcf_normal_id in metadf else [ None ] ), ) @annotate . format_doc ( indent = 3 ) class CNVkitScatter ( CNVkitScatter ): \"\"\"{{Summary}} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. convert (pgarg): {{Envs.convert.help | indent: 20}}. Defaults to group argument `convert`. min_variant_depth (pgarg): {{ Envs.min_variant_depth.help | indent: 20}}. Defaults to group argument `min_variant_depth`. \"\"\" requires = self . p_metafile , self . p_cnvkit_fix , self . p_cnvkit_segment input_data = _input_data envs = { \"cnvkit\" : self . opts . cnvkit , \"convert\" : self . opts . convert , \"min_variant_depth\" : self . opts . min_variant_depth , } return CNVkitScatter @ProcGroup . add_proc def p_cnvkit_diagram ( self ): \"\"\"Build CNVkitDiagram process\"\"\" from .cnvkit import CNVkitDiagram def _input_data ( ch1 , ch2 , ch3 ): metadf = _metadf ( _1st ( ch1 )) if not self . opts . case : tumor_masks = [ True ] * len ( metadf ) else : tumor_masks = metadf [ self . col . group ] == self . opts . case return tibble ( chrfile = ch2 . outfile , cnsfile = ch3 . outfile , sample_sex = ( metadf [ self . col . sex ][ tumor_masks ] if self . col . sex in metadf else [ None ] ), ) @annotate . format_doc ( indent = 3 ) class CNVkitDiagram ( CNVkitDiagram ): \"\"\"{{Summary}} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. convert (pgarg): {{Envs.convert.help | indent: 20}}. Defaults to group argument `convert`. male_reference (pgarg): {{ Envs.male_reference.help | indent: 20}}. Defaults to group argument `male_reference`. \"\"\" requires = self . p_metafile , self . p_cnvkit_fix , self . p_cnvkit_segment input_data = _input_data envs = { \"cnvkit\" : self . opts . cnvkit , \"convert\" : self . opts . convert , \"male_reference\" : self . opts . male_reference , } return CNVkitDiagram @ProcGroup . add_proc def p_cnvkit_heatmap_cns ( self ): \"\"\"Build CNVkitHeatmapCns process\"\"\" from .cnvkit import CNVkitHeatmap def _input_data ( ch1 , ch2 ): metadf = _metadf ( _1st ( ch1 )) if not self . opts . case : tumor_masks = [ True ] * len ( metadf ) else : tumor_masks = metadf [ self . col . group ] == self . opts . case if self . col . sex in metadf : all_sex = metadf [ self . col . sex ][ tumor_masks ] . unique () sample_sex = [ None ] if len ( all_sex ) > 1 else all_sex [ 0 ] else : sample_sex = [ None ] return tibble ( segfiles = [ ch2 . outfile . tolist ()], sample_sex = sample_sex , ) @annotate . format_doc ( indent = 3 ) class CNVkitHeatmapCns ( CNVkitHeatmap ): \"\"\"Generate heatmaps of segment-level signals of multiple samples {{* Summary.long }} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. convert (pgarg): {{Envs.convert.help | indent: 20}}. Defaults to group argument `convert`. male_reference (pgarg): {{ Envs.male_reference.help | indent: 20}}. Defaults to group argument `male_reference`. \"\"\" requires = self . p_metafile , self . p_cnvkit_segment input_data = _input_data envs = { \"cnvkit\" : self . opts . cnvkit , \"convert\" : self . opts . convert , \"male_reference\" : self . opts . male_reference , } return CNVkitHeatmapCns @ProcGroup . add_proc def p_cnvkit_heatmap_cnr ( self ): \"\"\"Build CNVkitHeatmapCnr process\"\"\" from .cnvkit import CNVkitHeatmap if not self . opts . heatmap_cnr : return None def _input_data ( ch1 , ch2 ): metadf = _metadf ( _1st ( ch1 )) if not self . opts . case : tumor_masks = [ True ] * len ( metadf ) else : tumor_masks = metadf [ self . col . group ] == self . opts . case if self . col . sex in metadf : all_sex = metadf [ self . col . sex ][ tumor_masks ] . unique () sample_sex = [ None ] if len ( all_sex ) > 1 else all_sex [ 0 ] else : sample_sex = [ None ] return tibble ( segfiles = [ ch2 . outfile . tolist ()], sample_sex = sample_sex , ) @annotate . format_doc ( indent = 3 ) class CNVkitHeatmapCnr ( CNVkitHeatmap ): \"\"\"Heatmap of bin-level signals of multiple samples Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. convert (pgarg): {{Envs.convert.help | indent: 20}}. Defaults to group argument `convert`. male_reference (pgarg): {{ Envs.male_reference.help | indent: 20}}. Defaults to group argument `male_reference`. \"\"\" requires = self . p_metafile , self . p_cnvkit_fix input_data = _input_data envs = { \"cnvkit\" : self . opts . cnvkit , \"convert\" : self . opts . convert , \"male_reference\" : self . opts . male_reference , } return CNVkitHeatmapCnr @ProcGroup . add_proc def p_cnvkit_call ( self ): \"\"\"Build CNVkitCall process\"\"\" from .cnvkit import CNVkitCall def _input_data ( ch1 , ch2 , ch3 ): metadf = _metadf ( _1st ( ch1 )) if not self . opts . case : tumor_masks = [ True ] * len ( metadf ) else : tumor_masks = metadf [ self . col . group ] == self . opts . case return tibble ( cnrfile = ch2 . outfile , cnsfile = ch3 . outfile , vcf = ( metadf [ self . col . snpvcf ][ tumor_masks ] if self . col . snpvcf in metadf else [ None ] ), sample_id = ( metadf [ self . col . vcf_sample_id ][ tumor_masks ] if self . col . vcf_sample_id in metadf else [ None ] ), normal_id = ( metadf [ self . col . vcf_normal_id ][ tumor_masks ] if self . col . vcf_normal_id in metadf else [ None ] ), sample_sex = ( metadf [ self . col . sex ][ tumor_masks ] if self . col . sex in metadf else [ None ] ), purity = ( metadf [ self . col . purity ][ tumor_masks ] if self . col . purity in metadf else [ None ] ), ) @annotate . format_doc ( indent = 3 ) class CNVkitCall ( CNVkitCall ): \"\"\"{{Summary}} Envs: cnvkit (pgarg): {{Envs.cnvkit.help | indent: 20}}. Defaults to group argument `cnvkit`. drop_low_coverage (pgarg): {{ Envs.drop_low_coverage.help | indent: 20}}. Defaults to group argument `drop_low_coverage`. male_reference (pgarg): {{ Envs.male_reference.help | indent: 20}}. Defaults to group argument `male_reference`. min_variant_depth (pgarg): {{ Envs.min_variant_depth.help | indent: 20}}. Defaults to group argument `min_variant_depth`. zygosity_freq (pgarg): {{ Envs.zygosity_freq.help | indent: 20}}. Defaults to group argument `zygosity_freq`. \"\"\" requires = self . p_metafile , self . p_cnvkit_fix , self . p_cnvkit_segment input_data = _input_data envs = { \"cnvkit\" : self . opts . cnvkit , \"drop_low_coverage\" : self . opts . drop_low_coverage , \"male_reference\" : self . opts . male_reference , \"min_variant_depth\" : self . opts . min_variant_depth , \"zygosity_freq\" : self . opts . zygosity_freq , } return CNVkitCall if __name__ == \"__main__\" : CNVkitPipeline () . as_pipen ( # If we run this procgroup as a whole, we don't want to collapse # the processes in the index page of report. plugin_opts = { \"report_no_collapse_pgs\" : True } ) . run ()","title":"biopipen.ns.cnvkit_pipeline"},{"location":"api/source/biopipen.ns.delim/","text":"SOURCE CODE biopipen.ns. delim DOCS \"\"\"Tools to deal with csv/tsv files\"\"\" from ..core.config import config from ..core.proc import Proc class RowsBinder ( Proc ): DOCS \"\"\"Bind rows of input files Input: infiles: The input files to bind. The input files should have the same number of columns, and same delimiter. Output: outfile: The output file with rows bound Envs: sep: The separator of the input files header (flag): Whether the input files have header filenames: Whether to add filename as the last column. Either a string of an R function that starts with `function` or a list of names (or string separated by comma) to add for each input file. The R function takes the path of the input file as the only argument and should return a string. The string will be added as the last column of the output file. filenames_col: The column name for the `filenames` columns \"\"\" input = \"infiles:files\" output = ( \"outfile:file:\" \"{{in.infiles | first | stem}}_rbound{{in.infiles | first | ext}}\" ) envs = { \"sep\" : \" \\t \" , \"header\" : True , \"filenames\" : None , \"filenames_col\" : \"Filename\" , } lang = config . lang . rscript script = \"file://../scripts/delim/RowsBinder.R\" class SampleInfo ( Proc ): DOCS \"\"\"List sample information and perform statistics Input: infile: The input file to list sample information The input file should be a csv/tsv file with header Output: outfile: The output file with sample information, with mutated columns if `envs.save_mutated` is True. The basename of the output file will be the same as the input file. The file name of each plot will be slugified from the case name. Each plot has 3 formats: pdf, png and code.zip, which contains the data and R code to reproduce the plot. Envs: sep: The separator of the input file. mutaters (type=json): A dict of mutaters to mutate the data frame. The key is the column name and the value is the R expression to mutate the column. The dict will be transformed to a list in R and passed to `dplyr::mutate`. You may also use `paired()` to identify paired samples. The function takes following arguments: * `df`: The data frame. Use `.` if the function is called in a dplyr pipe. * `id_col`: The column name in `df` for the ids to be returned in the final output. * `compare_col`: The column name in `df` to compare the values for each id in `id_col`. * `idents`: The values in `compare_col` to compare. It could be either an an integer or a vector. If it is an integer, the number of values in `compare_col` must be the same as the integer for the `id` to be regarded as paired. If it is a vector, the values in `compare_col` must be the same as the values in `idents` for the `id` to be regarded as paired. * `uniq`: Whether to return unique ids or not. Default is `TRUE`. If `FALSE`, you can mutate the meta data frame with the returned ids. Non-paired ids will be `NA`. save_mutated (flag): Whether to save the mutated columns. exclude_cols (auto): The columns to exclude in the table in the report. Could be a list or a string separated by comma. defaults (ns): The default parameters for `envs.stats`. - plot_type: The type of the plot. See the supported plot types here: <https://pwwang.github.io/plotthis/reference/index.html> The plot_type should be lower case and the plot function used in `plotthis` should be used. The mapping from plot_type to the plot function is like `bar -> BarPlot`, `box -> BoxPlot`, etc. - more_formats (list): The additional formats to save the plot. By default, the plot will be saved in png, which is also used to display in the report. You can add more formats to save the plot. For example, `more_formats = [\"pdf\", \"svg\"]`. - save_code (flag): Whether to save the R code to reproduce the plot. The data used to plot will also be saved. - subset: An expression to subset the data frame before plotting. The expression should be a string of R expression that will be passed to `dplyr::filter`. For example, `subset = \"Sample == 'A'\"`. - section: The section name in the report. In case you want to group the plots in the report. - devpars (ns): The device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - descr: The description of the plot, shown in the report. - <more>: You can add more parameters to the defaults. These parameters will be expanded to the `envs.stats` for each case, and passed to individual plot functions. stats (type=json): The statistics to perform. The keys are the case names and the values are the parameters inheirted from `envs.defaults`. \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" envs = { \"sep\" : \" \\t \" , \"mutaters\" : {}, \"save_mutated\" : False , \"exclude_cols\" : None , \"defaults\" : { \"plot_type\" : \"bar\" , \"more_formats\" : [], \"save_code\" : False , \"subset\" : None , \"section\" : None , \"descr\" : None , \"devpars\" : { \"width\" : None , \"height\" : None , \"res\" : 100 }, }, \"stats\" : {}, } lang = config . lang . rscript script = \"file://../scripts/delim/SampleInfo.R\" plugin_opts = { \"report\" : \"file://../reports/common.svelte\" }","title":"biopipen.ns.delim"},{"location":"api/source/biopipen.ns.gene/","text":"SOURCE CODE biopipen.ns. gene DOCS \"\"\"Gene related processes\"\"\" from ..core.proc import Proc from ..core.config import config class GeneNameConversion ( Proc ): DOCS \"\"\"Convert gene names back and forth using MyGeneInfo Input: infile: The input file with original gene names It should be a tab-separated file with header Output: outfile: The output file with converted gene names Envs: notfound (choice): What to do if a conversion cannot be done. - use-query: Ignore the conversion and use the original name - skip: Ignore the conversion and skip the entire row in input file - ignore: Same as skip - error: Report error - na: Use NA dup (choice): What to do if a conversion results in multiple names. - first: Use the first name, sorted by matching score descendingly (default) - last: Use the last name, sorted by matching score descendingly - combine: Combine all names using `;` as separator genecol: The index (1-based) or name of the column where genes are present output (choice): How to output. - append: Add the converted names as new columns at the end using `envs.outfmt` as the column name. - replace: Drop the original name column, and insert the converted names at the original position. - converted: Only keep the converted names. - with-query: Output 2 columns with original and converted names. infmt: What's the original gene name format Available fields https://docs.mygene.info/en/latest/doc/query_service.html#available-fields outfmt: What's the target gene name format. Currently only a single format is supported. species: Limit gene query to certain species. Supported: human, mouse, rat, fruitfly, nematode, zebrafish, thale-cress, frog and pig \"\"\" # noqa: E501 input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" lang = config . lang . rscript envs = { \"notfound\" : \"error\" , \"genecol\" : 1 , \"dup\" : \"first\" , \"output\" : \"append\" , \"infmt\" : [ \"symbol\" , \"alias\" ], \"outfmt\" : \"symbol\" , \"species\" : \"human\" , } script = \"file://../scripts/gene/GeneNameConversion.R\" class GenePromoters ( Proc ): DOCS \"\"\"Get gene promoter regions by specifying the flanking regions of TSS Input: infile: The input file with gene ids/names Output: outfile: The output file with promoter regions in BED format Envs: up (type=int): The upstream distance from TSS down (type=int): The downstream distance from TSS If not specified, the default is `envs.up` notfound (choice): What to do if a gene is not found. - skip: Skip the gene - error: Report error refgene: The reference gene annotation file in GTF format header (flag): Whether the input file has a header genecol (type=int): The index (1-based) of the gene column match_id (flag): Should we match the genes in `in.infile` by `gene_id` instead of `gene_name` in `envs.refgene` sort (flag): Sort the output by chromosome and start position chrsize: The chromosome size file, from which the chromosome order is used to sort the output \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}-promoters.bed\" lang = config . lang . rscript envs = { \"up\" : 2000 , \"down\" : None , \"notfound\" : \"error\" , \"refgene\" : config . ref . refgene , \"header\" : True , \"genecol\" : 1 , \"match_id\" : False , \"sort\" : False , \"chrsize\" : config . ref . chrsize , } script = \"file://../scripts/gene/GenePromoters.R\"","title":"biopipen.ns.gene"},{"location":"api/source/biopipen.ns.gsea/","text":"SOURCE CODE biopipen.ns. gsea DOCS \"\"\"Gene set enrichment analysis\"\"\" from pipen.utils import mark from ..core.proc import Proc from ..core.config import config @mark ( deprecated = '[ {proc.name} ] is deprecated, use `FGSEA` instead.' ) DOCS class GSEA ( Proc ): \"\"\"Gene set enrichment analysis Need `devtools::install_github(\"GSEA-MSigDB/GSEA_R\")` Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.infmt) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample `[Group]`: The groups/classes of the samples gmtfile: The GMT file of reference gene sets configfile: The configuration file in TOML format to specify some envs. `clscol`: If not provided, will use `envs.clscol` `doc.string`: Documentation string used as a prefix to name result files. If not provided, will use `envs['doc.string']` Output: outdir: The output directory Envs: inopts: The options for `read.table()` to read the input file If `rds` will use `readRDS()` metaopts: The options for `read.table()` to read the meta file clscol: The column of the metafile determining the classes doc_string: Documentation string used as a prefix to name result files Other configs passed to `GSEA()` directly Requires: GSEA-MSigDB/GSEA_R: - check: {{proc.lang}} <(echo \"library(GSEA)\") \"\"\" input = \"infile:file, metafile:file, gmtfile:file, configfile:file\" output = \"outdir:dir:{{in.infile | stem}}.gsea\" lang = config . lang . rscript envs = { \"inopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"metaopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"clscol\" : None , \"doc_string\" : \"gsea_result\" , } script = \"file://../scripts/gsea/GSEA.R\" plugin_opts = { \"report\" : \"file://../reports/gsea/GSEA.svelte\" } @mark ( deprecated = '[ {proc.name} ] is deprecated, use `FGSEA` directly.' ) DOCS class PreRank ( Proc ): \"\"\"PreRank the genes for GSEA analysis Input: infile: The expression file. Either a tab-delimited matrix or an RDS file (on envs.inopts) metafile: The meta data file, determining the class of the samples Two columns are required Sample: The unique sample id for each sample `[Group]`: The groups/classes of the samples configfile: The configuration file in TOML format to specify some envs. `clscol`: If not provided, will use `envs.clscol` `classes`: Defines pos and neg labels. If not provided, use will `envs.classes`. Output: outfile: The rank file with 1st column the genes, and the rest the ranks for different class pairs provided by `envs.classes` or `in.configfile` Envs: inopts: Options for `read.table()` to read `in.infile` metaopts: Options for `read.table()` to read `in.metafile` method: The method to do the preranking. Supported: `s2n(signal_to_noise)`, `abs_s2n(abs_signal_to_noise)`, `t_test`, `ratio_of_classes`, `diff_of_classes` and `log2_ratio_of_classes`. clscol: The column of metafile specifying the classes of the samples classes: The classes to specify the pos and neg labels. It could be a pair of labels (e.g. `[\"CASE\", \"CNTRL\"]`), where the first one is pos and second is neg. Or you can have multiple pairs of labels (e.g. `[[\"CASE1\", \"CNTRL\"], [\"CASE2\", \"CNTRL\"]]`) \"\"\" input = \"infile:file, metafile:file, configfile:file\" output = \"outfile:file:{{in.infile | stem}}.rank\" lang = config . lang . rscript envs = { \"inopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"metaopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"method\" : \"s2n\" , \"clscol\" : None , \"classes\" : None , } script = \"file://../scripts/gsea/PreRank.R\" class FGSEA ( Proc ): DOCS \"\"\"Gene set enrichment analysis using `fgsea` Input: infile: The expression file (genes x samples). Either a tab-delimited file. metafile: The meta data file, determining the class of the samples Two columns are required. If column `Sample` is found, it will be used as the samples; otherwise the first column should be the samples. The other column should be the group/class of the samples, whose name is specified by `envs.clscol`. Output: outdir: The output directory containing the results, including the table and plots. Envs: ncores (type=int): Number of cores for parallelization Passed to `nproc` of `fgseaMultilevel()`. case: The case label for the positive class. control: The control label for the negative class. When there are only two classes in `in.metafile` at column `envs.clscol`, either `case` or `control` can be specified and the other will be automatically set to the other class. gmtfile: The pathways in GMT format, with the gene names/ids in the same format as the seurat object. One could also use a URL to a GMT file. For example, from <https://download.baderlab.org/EM_Genesets/current_release/Human/symbol/Pathways/>. method (choice): The method to do the preranking. - signal_to_noise: Signal to noise. The larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \"class marker\". - s2n: Alias of signal_to_noise. - abs_signal_to_noise: The absolute value of signal_to_noise. - abs_s2n: Alias of abs_signal_to_noise. - t_test: T test. Uses the difference of means scaled by the standard deviation and number of samples. - ratio_of_classes: Also referred to as fold change. Uses the ratio of class means to calculate fold change for natural scale data. - diff_of_classes: Difference of class means. Uses the difference of class means to calculate fold change for nature scale data - log2_ratio_of_classes: Log2 ratio of class means. Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. clscol: The column of metafile specifying the classes of the samples When `in.metafile` is not specified, it can also be specified as a list of classes, in the same order as the samples in `in.infile`. top (type=auto): Do gsea table and enrich plot for top N pathways. If it is < 1, will apply it to `padj`, selecting pathways with `padj` < `top`. eps (type=float): This parameter sets the boundary for calculating the p value. See <https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html> minsize (type=int): Minimal size of a gene set to test. All pathways below the threshold are excluded. maxsize (type=int): Maximal size of a gene set to test. All pathways above the threshold are excluded. rest (type=json;order=98): Rest arguments for [`fgsea()`](https://rdrr.io/bioc/fgsea/man/fgsea.html) See also <https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html> cases (type=json;order=99): If you have multiple cases, you can specify them here. The keys are the names of the cases and the values are the above options except `mutaters`. If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the name `GSEA`. Requires: bioconductor-fgsea: - check: {{proc.lang}} -e \"library(fgsea)\" \"\"\" # noqa: E501 input = \"infile:file, metafile:file\" output = \"outdir:dir:{{in.infile | stem}}.fgsea\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"case\" : None , \"control\" : None , \"gmtfile\" : None , \"method\" : \"signal_to_noise\" , \"clscol\" : None , \"top\" : 10 , \"eps\" : 0 , \"minsize\" : 10 , \"maxsize\" : 100 , \"rest\" : {}, \"cases\" : {}, } script = \"file://../scripts/gsea/FGSEA.R\" plugin_opts = { \"report\" : \"file://../reports/gsea/FGSEA.svelte\" } class Enrichr ( Proc ): DOCS \"\"\"Gene set enrichment analysis using Enrichr Need `devtools::install_github(\"wjawaid/enrichR\")` Input: infile: The gene list file. You can specify whether this file has header and the index (0-based) of the columns where the genes are present Output: outdir: The output directory Envs: inopts: Options for `read.table()` to read `in.infile` genecol: Which column has the genes (0-based index or column name) dbs: The databases to enrich against. See https://maayanlab.cloud/Enrichr/#libraries for all available databases/libaries \"\"\" input = \"infile:file\" output = \"outdir:dir:{{in.infile | stem}}.enrichr\" lang = config . lang . rscript envs = { \"inopts\" : {}, \"genecol\" : 0 , \"genename\" : \"symbol\" , \"dbs\" : [ \"KEGG_2021_Human\" ], } script = \"file://../scripts/gsea/Enrichr.R\" plugin_opts = { \"report\" : \"file://../reports/gsea/Enrichr.svelte\" }","title":"biopipen.ns.gsea"},{"location":"api/source/biopipen.ns/","text":"SOURCE CODE biopipen. ns DOCS","title":"biopipen.ns"},{"location":"api/source/biopipen.ns.misc/","text":"SOURCE CODE biopipen.ns. misc DOCS \"\"\"Misc processes\"\"\" from ..core.proc import Proc from ..core.config import config class File2Proc ( Proc ): DOCS \"\"\"Accept a file and pass it down with a symbolic link Input: infile: The input file Output: outfile: The output symbolic link to the input file \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" lang = config . lang . bash script = \"\"\" # in case of deadlink rm -f {{out.outfile | quote}} if [[ ! -e {{in.infile | quote}} ]]; then echo \"File {{in.infile | quote}} does not exist.\" 1>&2 exit 1 fi ln -s {{in.infile | quote}} {{out.outfile | quote}} \"\"\" class Glob2Dir ( Proc ): DOCS \"\"\"Create symbolic links in output directory for the files given by the glob pattern\"\"\" input = \"pattern:var\" output = \"outdir:dir:from_glob\" lang = config . lang . bash script = \"\"\" for infile in {{in.pattern}}; do if [[ -e $infile ]]; then ln -s $(realpath $infile) \"{{out.outdir}}/$(basename $infile)\"; fi done \"\"\" class Config2File ( Proc ): DOCS \"\"\"Write a configurationn in string to a configuration file Requires python package `rtoml` Input: config: A string representation of configuration name: The name for output file. Will be `config` if not given Output: outfile: The output file with the configuration Envs: infmt: The input format. `json` or `toml`. outfmt: The output format. `json` or `toml`. \"\"\" input = \"config:var, name:var\" output = \"outfile:file:{{(in.name or 'config') | slugify}}.{{envs.outfmt}}\" envs = { \"infmt\" : \"toml\" , \"outfmt\" : \"toml\" } lang = config . lang . python script = \"file://../scripts/misc/Config2File.py\" class Str2File ( Proc ): DOCS \"\"\"Write the given string to a file Input: str: The string to write to file name: The name of the file If not given, use `envs.name` Output: outfile: The output file Envs: name: The name of the output file \"\"\" input = \"str, name\" output = \"outfile:file:{{in.name | default: 'unnamed.txt'}}\" lang = config . lang . python envs = { \"name\" : None } script = \"file://../scripts/misc/Str2File.py\" class Shell ( Proc ): DOCS \"\"\"Run a shell command Input: infile: The input file Output: outfile: The output file Envs: cmd: The shell command to run Use `$infile` and `$outfile` to refer to input and output files outdir: Whether the `out.outfile` should be a directory. If so a directory will be created before running the command. \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" envs = { \"cmd\" : \"\" , \"outdir\" : False } lang = config . lang . bash script = \"file://../scripts/misc/Shell.sh\" class Plot ( Proc ): DOCS \"\"\"Plot given data using plotthis package in R Input: datafile: The input data file in RDS or qs/qs2 format. If it is not in RDS nor qs/qs2 format, read.table will be used to read the data file with the options provided by `envs.read_opts`. Output: plotfile: The output plot file in PNG format envs: fn: The plot function to use. Required. devpars (ns): The device parameters for the plot. - width: The width of the plot in pixels. - height: The height of the plot in pixels. - res: The resolution of the plot in DPI. more_formats: The additional formats to save the plot in other than PNG. The file will be saved in the same directory as the plotfile. save_code: Whether to save the R code used for plotting. read_opts: Options to read the data file. If the data file is not in RDS nor qs/qs2 format, these options will be passed to `read.table`. <more>: Additional parameters to the plot function. \"\"\" input = \"datafile:file\" output = \"plotfile:file:{{in.datafile | stem}}.png\" envs = { \"fn\" : None , \"devpars\" : { \"res\" : 100 }, \"more_formats\" : [], \"save_code\" : False , \"read_opts\" : {}, } lang = config . lang . rscript script = \"file://../scripts/misc/Plot.R\"","title":"biopipen.ns.misc"},{"location":"api/source/biopipen.ns.plot/","text":"SOURCE CODE biopipen.ns. plot DOCS \"\"\"Plotting data\"\"\" import warnings from ..core.proc import Proc from ..core.config import config warnings . warn ( \"The `biopipen.ns.plot` module is deprecated and will be removed in the future. \" \"Please use `biopipen.ns.misc.Plot` process instead.\" , DeprecationWarning , ) class VennDiagram ( Proc ): DOCS \"\"\"Plot Venn diagram Needs `ggVennDiagram` Input: infile: The input file for data If `envs.intype` is raw, it should be a data frame with row names as categories and only column as elements separated by comma (`,`) If it is `computed`, it should be a data frame with row names the elements and columns the categories. The data should be binary indicator (`0, 1`) indicating whether the elements are present in the categories. Output: outfile: The output figure file Envs: inopts: The options for `read.table()` to read `in.infile` intype: `raw` or `computed`. See `in.infile` devpars: The parameters for `png()` args: Additional arguments for `ggVennDiagram()` ggs: Additional ggplot expression to adjust the plot \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.venn.png\" lang = config . lang . rscript envs = { \"inopts\" : { \"row.names\" : - 1 , \"header\" : False }, \"intype\" : \"raw\" , \"devpars\" : { \"res\" : 100 , \"width\" : 800 , \"height\" : 600 }, \"args\" : {}, \"ggs\" : None , } script = \"file://../scripts/plot/VennDiagram.R\" class Heatmap ( Proc ): DOCS \"\"\"Plot heatmaps using `ComplexHeatmap` Examples: >>> pipen run plot Heatmap \\ >>> --in.infile data.txt \\ >>> --in.annofiles anno.txt \\ >>> --envs.args.row_names_gp 'r:fontsize5' \\ >>> --envs.args.column_names_gp 'r:fontsize5' \\ >>> --envs.args.clustering_distance_rows pearson \\ >>> --envs.args.clustering_distance_columns pearson \\ >>> --envs.args.show_row_names false \\ >>> --envs.args.row_split 3 \\ >>> --args.devpars.width 5000 \\ >>> --args.devpars.height 5000 \\ >>> --args.draw.merge_legends \\ >>> --envs.args.heatmap_legend_param.title AUC \\ >>> --envs.args.row_dend_reorder \\ >>> --envs.args.column_dend_reorder \\ >>> --envs.args.top_annotation \\ >>> 'r:HeatmapAnnotation( \\ >>> Mutation = as.matrix(annos[,(length(groups)+1):ncol(annos)]) \\ >>> )' \\ >>> --envs.args.right_annotation \\ >>> 'r:rowAnnotation( \\ >>> AUC = anno_boxplot(as.matrix(data), outline = F) \\ >>> )' \\ >>> --args.globals \\ >>> 'fontsize8 = gpar(fontsize = 12); \\ >>> fontsize5 = gpar(fontsize = 8); \\ >>> groups = c (\"Group1\", \"Group2\", \"Group3\")' \\ >>> --args.seed 8525 Input: infile: The data matrix file annofiles: The files for annotation data Output: outfile: The heatmap plot outdir: Other data of the heatmap Including RDS file of the heatmap, row clusters and col clusters. Envs: inopts: Options for `read.table()` to read `in.infile` anopts: Options for `read.table()` to read `in.annofiles` draw: Options for `ComplexHeatmap::draw()` args: Arguments for `ComplexHeatmap::Heatmap()` devpars: The parameters for device. seed: The seed globals: Some globals for the expression in `args` to be evaluated Requires: bioconductor-complexheatmap: - check: {{proc.lang}} <(echo \"library(ComplexHeatmap)\") \"\"\" input = \"infile:file, annofiles:files\" output = [ 'outfile:file:{{in.infile | stem0 | append: \".heatmap\"}}/' '{{in.infile | stem0 | append: \".heatmap\"}}.png' , 'outdir:dir:{{in.infile | stem0 | append: \".heatmap\"}}' , ] lang = config . lang . rscript envs = { \"inopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"anopts\" : { \"header\" : True , \"row.names\" : - 1 }, \"draw\" : {}, \"devpars\" : {}, \"args\" : { \"heatmap_legend_param\" : {}}, \"seed\" : None , \"globals\" : \"\" , } script = \"file://../scripts/plot/Heatmap.R\" class ROC ( Proc ): DOCS \"\"\"Plot ROC curve using [`plotROC`](https://cran.r-project.org/web/packages/plotROC/vignettes/examples.html). Input: infile: The input file for data, tab-separated. The first column should be ids of the records (this is optional if `envs.noids` is True). The second column should be the labels of the records (1 for positive, 0 for negative). If they are not binary, you can specify the positive label by `envs.pos_label`. From the third column, it should be the scores of the different models. Output: outfile: The output figure file Envs: noids: Whether the input file has ids (first column) or not. pos_label: The positive label. ci: Whether to use `geom_rocci()` instead of `geom_roc()`. devpars: The parameters for `png()` args: Additional arguments for `geom_roc()` or `geom_rocci()` if `envs.ci` is True. style_roc: Arguments for `style_roc()` \"\"\" # noqa: E501 input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.roc.png\" lang = config . lang . rscript envs = { \"noids\" : False , \"pos_label\" : 1 , \"ci\" : False , \"devpars\" : { \"res\" : 100 , \"width\" : 750 , \"height\" : 600 }, \"args\" : { \"labels\" : False }, \"style_roc\" : {}, \"show_auc\" : True , } script = \"file://../scripts/plot/ROC.R\" class Manhattan ( Proc ): DOCS \"\"\"Plot Manhattan plot. Using the [`ggmanh`](https://bioconductor.org/packages/devel/bioc/vignettes/ggmanh/inst/doc/ggmanh.html) package. Requires `ggmanh` v1.9.6 or later. Input: infile: The input file for data It should contain at least three columns, the chromosome, the position and the p-value of the SNPs. Header is required. Output: outfile: The output figure file Envs: chrom_col: The column for chromosome An integer (1-based) or a string indicating the column name. pos_col: The column for position An integer (1-based) or a string indicating the column name. pval_col: The column for p-value An integer (1-based) or a string indicating the column name. label_col: The column for label. Once specified, the significant SNPs will be labeled on the plot. devpars (ns): The parameters for `png()` - res (type=int): The resolution - width (type=int): The width - height (type=int): The height title: The title of the plot ylabel: The y-axis label rescale (flag): Whether to rescale the p-values rescale_ratio_threshold (type=float): Threshold of that triggers the rescale signif (auto): A single value or a list of values to indicate the significance levels Multiple values should be also separated by comma (`,`). The minimum value will be used as the cutoff to determine if the SNPs are significant. hicolors (auto): The colors for significant and non-significant SNPs If a single color is given, the non-significant SNPs will be in grey. Set it to None to disable the highlighting. thin_n (type=int): Number of max points per horizontal partitions of the plot. `0` or `None` to disable thinning. thin_bins (type=int): Number of bins to partition the data. zoom (auto): Chromosomes to zoom in Each chromosome should be separated by comma (`,`) or in a list. Single chromosome is also accepted. Ranges are also accepted, see `envs.chroms`. Each chromosome will be saved in a separate file. zoom_devpars (ns): The parameters for the zoomed plot - width (type=int): The width - height (type=int): The height, inherited from `devpars` by default - res (type=int): The resolution, inherited from `devpars` by default chroms (auto): The chromosomes and order to plot A hyphen (`-`) can be used to indicate a range. For example `chr1-22,chrX,chrY,chrM` will plot all autosomes, X, Y and M. if `auto`, only the chromosomes in the data will be plotted in the order they appear in the data. args (ns): Additional arguments for `manhattan_plot()`. See <https://rdrr.io/github/leejs-abv/ggmanh/man/manhattan_plot.html>. Note that `-` will be replaced by `.` in the argument names. - <more>: Additional arguments for `manhattan_plot()` \"\"\" # noqa: E501 input = \"infile:file\" output = \"outfile:file:{{in.infile | stem0}}.manhattan.png\" lang = config . lang . rscript envs = { \"chrom_col\" : 1 , \"pos_col\" : 2 , \"pval_col\" : 3 , \"label_col\" : None , \"devpars\" : { \"res\" : 100 , \"width\" : 1000 , \"height\" : 500 }, \"zoom_devpars\" : { \"width\" : 500 , \"height\" : None , \"res\" : None }, \"title\" : None , \"ylabel\" : \"-log10(p-value)\" , \"rescale\" : True , \"rescale_ratio_threshold\" : 5 , \"signif\" : [ 5e-8 , 1e-5 ], \"hicolors\" : None , \"thin_n\" : None , \"thin_bins\" : 200 , \"zoom\" : None , \"chroms\" : \"auto\" , \"args\" : {}, } script = \"file://../scripts/plot/Manhattan.R\" class QQPlot ( Proc ): DOCS \"\"\"Generate QQ-plot or PP-plot using qqplotr. See <https://cran.r-project.org/web/packages/qqplotr/vignettes/introduction.html>. Input: infile: The input file for data It should contain at least one column of p-values or the values to be plotted. Header is required. theorfile: The file for theoretical values (optional) This file should contain at least one column of theoretical values. The values will be passed to `envs.theor_qfunc` to calculate the theoretical quantiles. Header is required. Output: outfile: The output figure file Envs: val_col: The column for values to be plotted An integer (1-based) or a string indicating the column name. devpars (ns): The parameters for `png()` - res (type=int): The resolution - width (type=int): The width - height (type=int): The height xlabel: The x-axis label ylabel: The y-axis label title: The title of the plot trans: The transformation of the values You can use `-log10` to transform the values to `-log10(values)`. Otherwise you can a direct R function or a custom R function. For example `function(x) -log10(x)`. kind (choice): The kind of the plot, `qq` or `pp` - qq: QQ-plot - pp: PP-plot theor_col: The column for theoretical values in `in.theorfile` if provided, otherwise in `in.infile`. An integer (1-based) or a string indicating the column name. If `distribution` of `band`, `line`, or `point` is `custom`, this column must be provided. theor_trans: The transformation of the theoretical values. The `theor_funs` have default functions to take the theoretical values. This transformation will be applied to the theoretical values before passing to the `theor_funs`. theor_funs (ns): The R functions to generate density, quantile and deviates of the theoretical distribution base on the theoretical values if `distribution` of `band`, `line`, or `point` is `custom`. - dcustom: The density function, used by band - qcustom: The quantile function, used by point - rcustom: The deviates function, used by line args (ns): The common arguments for `envs.band`, `envs.line` and `envs.point`. - distribution: The distribution of the theoretical quantiles When `custom` is used, the `envs.theor_col` should be provided and `values` will be added to `dparams` automatically. - dparams (type=json): The parameters for the distribution - <more>: Other shared arguments between `stat_*_band`, `stat_*_line` and `stat_*_point`. band (ns): The arguments for `stat_qq_band()` or `stat_pp_band()`. See <https://rdrr.io/cran/qqplotr/man/stat_qq_band.html> and <https://rdrr.io/cran/qqplotr/man/stat_pp_band.html>. Set to `None` or `band.disabled` to True to disable the band. - disabled (flag): Disable the band - distribution: The distribution of the theoretical quantiles When `custom` is used, the `envs.theor_col` should be provided and `values` will be added to `dparams` automatically. - dparams (type=json): The parameters for the distribution - <more>: Additional arguments for `stat_qq_band()` or `stat_pp_band()` line (ns): The arguments for `stat_qq_line()` or `stat_pp_line()`. See <https://rdrr.io/cran/qqplot/man/stat_qq_line.html> and <https://rdrr.io/cran/qqplot/man/stat_pp_line.html>. Set to `None` or `line.disabled` to True to disable the line. - disabled (flag): Disable the line - distribution: The distribution of the theoretical quantiles When `custom` is used, the `envs.theor_col` should be provided and `values` will be added to `dparams` automatically. - dparams (type=json): The parameters for the distribution - <more>: Additional arguments for `stat_qq_line()` or `stat_pp_line()` point (ns): The arguments for `geom_qq_point()` or `geom_pp_point()`. See <https://rdrr.io/cran/qqplot/man/stat_qq_point.html> and <https://rdrr.io/cran/qqplot/man/stat_pp_point.html>. Set to `None` or `point.disabled` to True to disable the point. - disabled (flag): Disable the point - distribution: The distribution of the theoretical quantiles When `custom` is used, the `envs.theor_col` should be provided and `values` will be added to `dparams` automatically. - dparams (type=json): The parameters for the distribution - <more>: Additional arguments for `geom_qq_point()` or `geom_pp_point()` ggs (list): Additional ggplot expression to adjust the plot. \"\"\" input = \"infile:file, theorfile:file\" output = \"outfile:file:{{in.infile | stem}}.{{envs.kind}}.png\" lang = config . lang . rscript envs = { \"val_col\" : 1 , \"theor_col\" : None , \"theor_trans\" : None , \"theor_funs\" : { \"dcustom\" : \"\"\" function(x, values, ...) { density(values, from = min(values), to = max(values), n = length(x))$y } \"\"\" , \"qcustom\" : \"function(p, values, ...) {quantile(values, probs = p)}\" , \"rcustom\" : \"function(n, values, ...) { sample(values, n, replace = TRUE) }\" , }, \"args\" : { \"distribution\" : \"norm\" , \"dparams\" : {}}, \"devpars\" : { \"res\" : 100 , \"width\" : 1000 , \"height\" : 1000 }, \"xlabel\" : \"Theoretical Quantiles\" , \"ylabel\" : \"Observed Quantiles\" , \"title\" : \"QQ-plot\" , \"trans\" : None , \"kind\" : \"qq\" , \"band\" : { \"disabled\" : False , \"distribution\" : None , \"dparams\" : None }, \"line\" : { \"disabled\" : False , \"distribution\" : None , \"dparams\" : None }, \"point\" : { \"disabled\" : False , \"distribution\" : None , \"dparams\" : None }, \"ggs\" : None , } script = \"file://../scripts/plot/QQPlot.R\" class Scatter ( Proc ): DOCS \"\"\"Generate scatter plot using ggplot2. [`ggpmisc`](https://cran.r-project.org/web/packages/ggpmisc/index.html) is used for the stats and labels. See also https://cran.r-project.org/web/packages/ggpmisc/vignettes/model-based-annotations.html Input: infile: The input file for data It should contain at least two columns for x and y values. Header is required. Output: outfile: The output figure file Envs: x_col: The column for x values An integer (1-based) or a string indicating the column name. y_col: The column for y values An integer (1-based) or a string indicating the column name. devpars (ns): The parameters for `png()` - res (type=int): The resolution - width (type=int): The width - height (type=int): The height args (ns): Additional arguments for `geom_point()` See <https://ggplot2.tidyverse.org/reference/geom_point.html>. - <more>: Additional arguments for `geom_point()` mapping: Extra mapping for all geoms, including `stats`. Should be `aes(color = group)` but all these are valid: `color = group` or `(color = group)`. ggs (list): Additional ggplot expression to adjust the plot. formula: The formula for the model stats (type=json): The stats to add to the plot. A dict with keys available stats in `ggpmisc` (without `stat_`). See <https://cran.r-project.org/web/packages/ggpmisc/vignettes/model-based-annotations.html#statistics>. The values should be the arguments for the stats. If you want a stat to be added multiple times, add a suffix `#x` to the key. For example, `poly_line#1` and `poly_line#2` will add two polynomial lines. \"\"\" # noqa: E501 input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.scatter.png\" lang = config . lang . rscript envs = { \"x_col\" : 1 , \"y_col\" : 2 , \"devpars\" : { \"res\" : 100 , \"width\" : 1000 , \"height\" : 800 }, \"args\" : {}, \"mapping\" : None , \"ggs\" : [], \"formula\" : \"y ~ x\" , \"stats\" : {}, } script = \"file://../scripts/plot/Scatter.R\"","title":"biopipen.ns.plot"},{"location":"api/source/biopipen.ns.protein/","text":"SOURCE CODE biopipen.ns. protein DOCS \"\"\"Protein-related processes.\"\"\" from ..core.proc import Proc from ..core.config import config class Prodigy ( Proc ): DOCS \"\"\"Prediction of binding affinity of protein-protein complexes based on intermolecular contacts using Prodigy. See <https://rascar.science.uu.nl/prodigy/> and <https://github.com/haddocking/prodigy>. `prodigy-prot` must be installed under the given python of `proc.lang`. Input: infile: The structure file in PDB or mmCIF format. Output: outfile: The output file generated by Prodigy. outdir: The output directory containing all output files. Envs: distance_cutoff (type=float): The distance cutoff to calculate intermolecular contacts. acc_threshold (type=float): The accessibility threshold for BSA analysis. temperature (type=float): The temperature (C) for Kd prediction. contact_list (flag): Whether to generate contact list. pymol_selection (flag): Whether output a script to highlight the interface residues in PyMOL. selection (list): The selection of the chains to analyze. `['A', 'B']` will analyze chains A and B. `['A,B', 'C']` will analyze chain A and C; and B and C. `['A', 'B', 'C']` will analyze all combinations of A, B, and C. outtype (choice): Set the format of the output file (`out.outfile`). All three files will be generated. This option only determines which is assigned to `out.outfile`. - raw: The raw output file from prodigy. - json: The output file in JSON format. - tsv: The output file in CSV format. \"\"\" input = \"infile:file\" output = [ \"outfile:file:{{in.infile | stem}}_prodigy/\" \"{{in.infile | stem}}.{{envs.outtype if envs.outtype != 'raw' else 'out'}}\" , \"outdir:dir:{{in.infile | stem}}_prodigy\" , ] lang = config . lang . python envs = { \"distance_cutoff\" : 5.5 , \"acc_threshold\" : 0.05 , \"temperature\" : 25.0 , \"contact_list\" : True , \"pymol_selection\" : True , \"selection\" : None , \"outtype\" : \"json\" , } script = \"file://../scripts/protein/Prodigy.py\" class ProdigySummary ( Proc ): DOCS \"\"\"Summary of the output from `Prodigy`. Input: infiles: The output json file generated by `Prodigy`. Output: outdir: The directory of summary files generated by `ProdigySummary`. Envs: group (type=auto): The group of the samples for boxplots. If `None`, don't do boxplots. It can be a dict of group names and sample names, e.g. `{\"group1\": [\"sample1\", \"sample2\"], \"group2\": [\"sample3\"]}` or a file containing the group information, with the first column being the sample names and the second column being the group names. The file should be tab-delimited with no header. \"\"\" input = \"infiles:files\" input_data = lambda ch : [[ f \" { odir } /_prodigy.tsv\" for odir in ch . outdir ]] output = \"outdir:dir:prodigy_summary\" lang = config . lang . rscript envs = { \"group\" : None } script = \"file://../scripts/protein/ProdigySummary.R\" plugin_opts = { \"report\" : \"file://../reports/protein/ProdigySummary.svelte\" } class MMCIF2PDB ( Proc ): DOCS \"\"\"Convert mmCIF or PDBx file to PDB file. Using [BeEM](https://github.com/kad-ecoli/BeEM) Input: infile: The input mmCIF or PDBx file. Output: outfile: The output PDB file. The \"outfmt\" set to 3 to always output a single PDB file. Envs: tool (choice): The tool to use for conversion. - maxit: Use MAXIT. - beem: Use BeEM. maxit: The path to the MAXIT executable. beem: The path to the BeEM executable. <more>: Other options for MAXIT/BeEM. For BeEM, \"outfmt\" will not be used as it is set to 3. \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.pdb\" lang = config . lang . python envs = { \"tool\" : \"maxit\" , \"maxit\" : config . exe . maxit , \"beem\" : config . exe . beem , } script = \"file://../scripts/protein/MMCIF2PDB.py\" class RMSD ( Proc ): DOCS \"\"\"Calculate the RMSD between two structures. See also https://github.com/charnley/rmsd. If the input is in mmCIF format, convert it to PDB first. Input: infile1: The first structure file. infile2: The second structure file. Output: outfile: The output file containing the RMSD value. Envs: beem: The path to the BeEM executable. calculate_rmsd: The path to the calculate_rmsd executable. conv_tool (choice): The tool to use for conversion. - maxit: Use MAXIT. - beem: Use BeEM. ca_only (flag): Whether to calculate RMSD using only C-alpha atoms. duel (choice): How to handle the duel atoms. Default is \"keep\". - keep: Keep both atoms. - keep_first: Keep the first atom. - keep_last: Keep the last atom. - average: Average the coordinates. reorder (flag): Whether to reorder the atoms in the structures. <more>: Other options for calculate_rmsd. \"\"\" input = \"infile1:file, infile2:file\" output = \"outfile:file:{{in.infile1 | stem}}-{{in.infile2 | stem}}.rmsd.txt\" lang = config . lang . python envs = { \"maxit\" : config . exe . maxit , \"beem\" : config . exe . beem , \"calculate_rmsd\" : config . exe . calculate_rmsd , \"conv_tool\" : \"maxit\" , \"ca_only\" : False , \"duel\" : \"keep\" , \"reorder\" : True , } script = \"file://../scripts/protein/RMSD.py\" class PDB2Fasta ( Proc ): DOCS \"\"\"Convert PDB file to FASTA file. Input: infile: The input PDB file. Output: outfile: The output FASTA file. Envs: chains (auto): The chains to extract. A list of chain IDs or separated by commas. If None, extract all chains. wrap (type=int): The number of residues per line in the output FASTA file. Set to 0 to disable wrapping. \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.fasta\" lang = config . lang . python envs = { \"chains\" : None , \"wrap\" : 80 } script = \"file://../scripts/protein/PDB2Fasta.py\"","title":"biopipen.ns.protein"},{"location":"api/source/biopipen.ns.regulatory/","text":"SOURCE CODE biopipen.ns. regulatory DOCS \"\"\"Provides processes for the regulatory related\"\"\" from ..core.proc import Proc from ..core.config import config class MotifScan ( Proc ): DOCS \"\"\"Scan the input sequences for binding sites using motifs. Currently only [fimo](https://meme-suite.org/meme/tools/fimo) from MEME suite is supported, based on the research/comparisons done by the following reference. Reference: - [Evaluating tools for transcription factor binding site prediction](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6889335/) Input: motiffile: File containing motif names. The file contains the motif and regulator names. The motif names should match the names in the motif database. This file must have a header. If multiple columns are present, it should be delimited by tab. seqfile: File containing sequences in FASTA format. Output: outdir: Directory containing the results. Especially `fimo_output.txt` extending from `fimo.tsv`, which contains: 1. the results with the regulator information if `envs.regulator_col` is provided, otherwise, the `regulator` columns will be filled with the motif names. 2. the original sequence from the fasta file (in.seqfile) 3. corrected genomic coordinates if the genomic coordinates are included in the sequence names. See also the `Output` section of <https://meme-suite.org/meme/doc/fimo.html>. Note that `--no-pgc` is passed to fimo to not parse the genomic coordinates from the sequence names by fimo. When fimo parses the genomic coordinates, `DDX11L1` in `>DDX11L1::chr1:11869-14412` will be lost. The purpose of this is to keep the sequence names as they are in the output. If the sequence names are in the format of `>NAME::chr1:START-END`, we will correct the coordinates in the output. Also note that it requires meme/fimo v5.5.5+ to do this (where the --no-pgc option is available). Envs: tool (choice): The tool to use for scanning. Currently only fimo is supported. - fimo: Use fimo from MEME suite. fimo: The path to fimo binary. motif_col: The column name in the motif file containing the motif names. regulator_col: The column name in the motif file containing the regulator names. Both `motif_col` and `regulator_col` should be the direct column names or the index (1-based) of the columns. If no `regulator_col` is provided, no regulator information is written in the output. notfound (choice): What to do if a motif is not found in the database. - error: Report error and stop the process. - ignore: Ignore the motif and continue. motifdb: The path to the motif database. This is required. It should be in the format of MEME motif database. Databases can be downloaded here: <https://meme-suite.org/meme/doc/download.html>. See also introduction to the databases: <https://meme-suite.org/meme/db/motifs>. cutoff (type=float): The cutoff for p-value to write the results. When `envs.q_cutoff` is set, this is applied to the q-value. This is passed to `--thresh` in fimo. q (flag): Calculate q-value. When `False`, `--no-qvalue` is passed to fimo. The q-value calculation is that of Benjamini and Hochberg (BH) (1995). q_cutoff (flag): Apply `envs.cutoff` to q-value. args (ns): Additional arguments to pass to the tool. - <more>: Additional arguments for fimo. See: <https://meme-suite.org/meme/doc/fimo.html> \"\"\" # noqa: E501 input = \"motiffile:file, seqfile:file\" output = \"outdir:dir:{{in.motiffile | stem}}.fimo\" lang = config . lang . python envs = { \"tool\" : \"fimo\" , \"fimo\" : config . exe . fimo , \"motif_col\" : 1 , \"regulator_col\" : None , \"notfound\" : \"error\" , \"motifdb\" : config . tf_motifdb , \"cutoff\" : 1e-4 , \"q\" : False , \"q_cutoff\" : False , \"args\" : {}, } script = \"file://../scripts/regulatory/MotifScan.py\" class MotifAffinityTest ( Proc ): DOCS \"\"\"Test the affinity of motifs to the sequences and the affinity change due the mutations. See also <https://simon-coetzee.github.io/motifBreakR> and <https://www.bioconductor.org/packages/release/bioc/vignettes/atSNP/inst/doc/atsnp-vignette.html> When using atSNP, motifBreakR is also required to plot the variants and motifs. Input: motiffile: File containing motif names. The file contains the motif and regulator names. The motif names should match the names in the motif database. This file must have a header. If multiple columns are present, it should be delimited by tab. varfile: File containing the variants. It could be a VCF file or a BED-like file. If it is a VCF file, it does not need to be indexed. Only records with `PASS` in the `FILTER` column are used. If it is a BED-like file, it should contain the following columns, `chrom`, `start`, `end`, `name`, `score`, `strand`, `ref`, `alt`. Output: outdir: Directory containing the results. For motifBreakR, `motifbreakr.txt` will be created. Records with effect `strong`/`weak` are written (`neutral` is not). For atSNP, `atsnp.txt` will be created. Records with p-value (`envs.atsnp_args.p`) < `envs.cutoff` are written. Envs: ncores (type=int): The number of cores to use. tool (choice): The tool to use for the test. - motifbreakr: Use motifBreakR. - motifBreakR: Use motifBreakR. - atsnp: Use atSNP. - atSNP: Use atSNP. bcftools: The path to bcftools binary. Used to convert the VCF file to the BED file when the input is a VCF file. motif_col: The column name in the motif file containing the motif names. If this is not provided, `envs.regulator_col` and `envs.regmotifs` are required, which are used to infer the motif names from the regulator names. regulator_col: The column name in the motif file containing the regulator names. Both `motif_col` and `regulator_col` should be the direct column names or the index (1-based) of the columns. If no `regulator_col` is provided, no regulator information is written in the output. Otherwise, the regulator information is written in the output in the `Regulator` column. var_col: The column names in the `in.motiffile` containing the variant information. It has to be matching the names in the `in.varfile`. This is helpful when we only need to test the pairs of variants and motifs in the `in.motiffile`. notfound (choice): What to do if a motif is not found in the database, or a regulator is not found in the regulator-motif mapping (envs.regmotifs) file. - error: Report error and stop the process. - ignore: Ignore the motif and continue. motifdb: The path to the motif database. This is required. It should be in the format of MEME motif database. Databases can be downloaded here: <https://meme-suite.org/meme/doc/download.html>. See also introduction to the databases: <https://meme-suite.org/meme/db/motifs>. [universalmotif](https://github.com/bjmt/universalmotif) is required to read the motif database. genome: The genome assembly. Used to fetch the sequences around the variants by package, for example, `BSgenome.Hsapiens.UCSC.hg19` is required if `hg19`. If it is an organism other than human, please specify the full name of the package, for example, `BSgenome.Mmusculus.UCSC.mm10`. cutoff (type=float): The cutoff for p-value to write the results. devpars (ns): The default device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. plot_nvars (type=int): Number of variants to plot. Plot top `<plot_nvars>` variants with the largest `abs(alleleDiff)` (motifBreakR) or smallest p-values (atSNP). plots (type=json): Specify the details for the plots. When specified, `plot_nvars` is ignored. The keys are the variant names and the values are the details for the plots, including: devpars: The device parameters for the plot to override the default (envs.devpars). which: An expression passed to `subset(results, subset = ...)` to get the motifs for the variant to plot. Or an integer to get the top `which` motifs. For example, `effect == \"strong\"` to get the motifs with strong effect in motifBreakR result. regmotifs: The path to the regulator-motif mapping file. It must have header and the columns `Motif` or `Model` for motif names and `TF`, `Regulator` or `Transcription factor` for regulator names. motifbreakr_args (ns): Additional arguments to pass to motifBreakR. - method (choice): The method to use. See details of <https://rdrr.io/bioc/motifbreakR/man/motifbreakR.html> and <https://simon-coetzee.github.io/motifBreakR/#methods>. - default: Use the default method. - log: Use the standard summation of log probabilities - ic: Use information content - notrans: Use the default method without transformation atsnp_args (ns): Additional arguments to pass to atSNP. - padj_cutoff (flag): The `envs.cutoff` will be applied to the adjusted p-value. Only works for `atSNP`. - padj (choice): The method to adjust the p-values. Only works for `atSNP` - holm: Holm's method - hochberg: Hochberg's method - hommel: Hommel's method - bonferroni: Bonferroni method - BH: Benjamini & Hochberg's method - BY: Benjamini & Yekutieli's method - fdr: False discovery rate - none: No adjustment - p (choice): Which p-value to use for adjustment and cutoff. - pval_ref: p-value for the reference allele affinity score. - pval_snp: p-value for the SNP allele affinity score. - pval_cond_ref: and - pval_cond_snp: conditional p-values for the affinity scores of the reference and SNP alleles. - pval_diff: p-value for the affinity score change between the two alleles. - pval_rank: p-value for the rank test between the two alleles. \"\"\" # noqa: E501 input = \"motiffile:file, varfile:file\" output = \"outdir:dir:{{in.motiffile | stem}}.{{envs.tool | lower}}\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"tool\" : \"atsnp\" , \"bcftools\" : config . exe . bcftools , \"motif_col\" : None , \"regulator_col\" : None , \"var_col\" : None , \"notfound\" : \"error\" , \"motifdb\" : config . ref . tf_motifdb , \"regmotifs\" : config . ref . tf_motifs , \"genome\" : config . ref . genome , \"cutoff\" : 0.05 , \"devpars\" : { \"width\" : None , \"height\" : None , \"res\" : 100 }, \"plot_nvars\" : 10 , \"plots\" : {}, \"motifbreakr_args\" : { \"method\" : \"default\" }, \"atsnp_args\" : { \"padj_cutoff\" : True , \"padj\" : \"BH\" , \"p\" : \"pval_diff\" }, } script = \"file://../scripts/regulatory/MotifAffinityTest.R\" class VariantMotifPlot ( Proc ): DOCS \"\"\"A plot with a genomic region surrounding a genomic variant, and potentially disrupted motifs. Currently only SNVs are supported. Input: infile: File containing the variants and motifs. It is a TAB-delimited file with the following columns: - chrom: The chromosome of the SNV. Alias: chr, seqnames. - start: The start position of the SNV, no matter 0- or 1-based. - end: The end position of the SNV, which will be used as the position of the SNV. - strand: Indicating the direction of the surrounding sequence matching the motif. - SNP_id: The name of the SNV. - REF: The reference allele of the SNV. - ALT: The alternative allele of the SNV. - providerId: The motif id. It can be specified by `envs.motif_col`. - providerName: The name of the motif provider. Optional. - Regulator: The regulator name. Optional, can be specified by `envs.regulator_col`. - motifPos: The position of the motif, relative to the position of the SNV. For example, '-8, 4' means the motif is 8 bp upstream and 4 bp downstream of the SNV. Envs: genome: The genome assembly. Used to fetch the sequences around the variants by package, for example, `BSgenome.Hsapiens.UCSC.hg19` is required if `hg19`. If it is an organism other than human, please specify the full name of the package, for example, `BSgenome.Mmusculus.UCSC.mm10`. motifdb: The path to the motif database. This is required. It should be in the format of MEME motif database. Databases can be downloaded here: <https://meme-suite.org/meme/doc/download.html>. See also introduction to the databases: <https://meme-suite.org/meme/db/motifs>. [universalmotif](https://github.com/bjmt/universalmotif) is required to read the motif database. motif_col: The column name in the motif file containing the motif names. If this is not provided, `envs.regulator_col` and `envs.regmotifs` are required, which are used to infer the motif names from the regulator names. regulator_col: The column name in the motif file containing the regulator names. Both `motif_col` and `regulator_col` should be the direct column names or the index (1-based) of the columns. If no `regulator_col` is provided, no regulator information is written in the output. Otherwise, the regulator information is written in the output in the `Regulator` column. regmotifs: The path to the regulator-motif mapping file. It must have header and the columns `Motif` or `Model` for motif names and `TF`, `Regulator` or `Transcription factor` for regulator names. notfound (choice): What to do if a motif is not found in the database, or a regulator is not found in the regulator-motif mapping (envs.regmotifs) file. - error: Report error and stop the process. - ignore: Ignore the motif and continue. devpars (ns): The default device parameters for the plot. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. plot_vars (type=auto): The variants (SNP_id) to plot. A list of variant names to plot or a string with the variant names separated by comma. When not specified, all variants are plotted. \"\"\" # noqa: E501 input = \"infile:file\" output = \"outdir:dir:{{in.infile | stem}}.vmplots\" lang = config . lang . rscript envs = { \"genome\" : config . ref . genome , \"motifdb\" : config . ref . tf_motifdb , \"motif_col\" : \"providerId\" , \"regulator_col\" : None , \"regmotifs\" : config . ref . tf_motifs , \"notfound\" : \"error\" , \"devpars\" : { \"width\" : 800 , \"height\" : None , \"res\" : 100 }, \"plot_vars\" : None , } script = \"file://../scripts/regulatory/VariantMotifPlot.R\"","title":"biopipen.ns.regulatory"},{"location":"api/source/biopipen.ns.rnaseq/","text":"SOURCE CODE biopipen.ns. rnaseq DOCS \"\"\"RNA-seq data analysis\"\"\" from ..core.proc import Proc from ..core.config import config class UnitConversion ( Proc ): DOCS \"\"\"Convert expression value units back and forth See <https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/> and <https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/#fpkm>. Following converstions are supported - * `count -> cpm, fpkm/rpkm, fpkmuq/rpkmrq, tpm, tmm` * `fpkm/rpkm -> count, tpm, cpm` * `tpm -> count, fpkm/rpkm, cpm` * `cpm -> count, fpkm/rpkm, tpm` NOTE that during some conversions, `sum(counts/effLen)` is approximated to `sum(counts)/sum(effLen) * length(effLen))` You can also use this process to just transform the expression values, e.g., take log2 of the expression values. In this case, you can set `inunit` and `outunit` to `count` and `log2(count + 1)` respectively. Input: infile: Input file containing expression values The file should be a matrix with rows representing genes and columns representing samples. It could be an RDS file containing a data frame or a matrix, or a text file containing a matrix with tab as the delimiter. The text file can be gzipped. Output: outfile: Output file containing the converted expression values The file will be a matrix with rows representing genes and columns representing samples. Envs: inunit: The input unit of the expression values. You can also use an expression to indicate the input unit, e.g., `log2(counts + 1)`. The expression should be like `A * fn(B*X + C) + D`, where `A`, `B`, `C` and `D` are constants, `fn` is a function, and X is the input unit. Currently only `expr`, `sqrt`, `log2`, `log10` and `log` are supported as functions. Supported input units are: * counts/count/rawcounts/rawcount: raw counts. * cpm: counts per million. * fpkm/rpkm: fragments per kilobase of transcript per million. * fpkmuq/rpkmuq: upper quartile normalized FPKM/RPKM. * tpm: transcripts per million. * tmm: trimmed mean of M-values. outunit: The output unit of the expression values. An expression can also be used for transformation (e.g. `log2(tpm + 1)`). If `inunit` is `count`, then this means we are converting raw counts to tpm, and transforming it to `log2(tpm + 1)` as the output. Any expression supported by `R` can be used. Same units as `inunit` are supported. refexon: Path to the reference exon gff file. meanfl (type=auto): A file containing the mean fragment length for each sample by rows (samples as rowname), without header. Or a fixed universal estimated number (1 used by TCGA). nreads (type=auto): The estimatied total number of reads for each sample. or you can pass a file with the number for each sample by rows (samples as rowname), without header. When converting `fpkm/rpkm -> count`, it should be total reads of that sample. When converting `cpm -> count`: it should be total reads of that sample. When converting `tpm -> count`: it should be total reads of that sample. When converting `tpm -> cpm`: it should be total reads of that sample. When converting `tpm -> fpkm/rpkm`: it should be `sum(fpkm)` of that sample. It is not used when converting `count -> cpm, fpkm/rpkm, tpm`. \"\"\" # noqa: E501 input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" lang = config . lang . rscript envs = { \"inunit\" : None , \"outunit\" : None , \"refexon\" : config . ref . refexon , \"meanfl\" : 1 , \"nreads\" : 1_000_000 , } script = \"file://../scripts/rnaseq/UnitConversion.R\" class Simulation ( Proc ): DOCS \"\"\"Simulate RNA-seq data using ESCO/RUVcorr package Input: ngenes: Number of genes to simulate nsamples: Number of samples to simulate If you want to force the process to re-simulate for the same `ngenes` and `nsamples`, you can set a different value for `envs.seed`. Note that the samples will be shown as cells in the output (since the simulation is designed for single-cell RNA-seq data). Output: outfile: Output file containing the simulated data with rows representing genes and columns representing samples. outdir: Output directory containing the simulated data `sim.rds` and `True.rds` will be generated. For `ESCO`, `sim.rds` contains the simulated data in a `SingleCellExperiment` object, and `True.rds` contains the matrix of true counts. For `RUVcorr`, `sim.rds` contains the simulated data in list with `Truth`, A matrix containing the values of X\u03b2; `Y` A matrix containing the values in `Y`; `Noise` A matrix containing the values in `W\u03b1`; `Sigma` A matrix containing the true gene-gene correlations, as defined by X\u03b2; and `Info` A matrix containing some of the general information about the simulation. For all matrices, rows represent genes and columns represent samples. Envs: tool (choice): Which tool to use for simulation. - ESCO: uses the [ESCO](https://github.com/JINJINT/ESCO) package. - RUVcorr: uses the [RUVcorr](https://rdrr.io/bioc/RUVcorr/) package. ncores (type=int): Number of cores to use. seed (type=int): Random seed. If not set, seed will not be set. esco_args (ns): Additional arguments to pass to the simulation function. - save (choice): Which type of data to save to `out.outfile`. - `simulated-truth`: saves the simulated true counts. - `zero-inflated`: saves the zero-inflated counts. - `down-sampled`: saves the down-sampled counts. - type (choice): Which type of heterogenounity to use. - single: produces a single population. - group: produces distinct groups. - tree: produces distinct groups but admits a tree structure. - traj: produces distinct groups but admits a smooth trajectory structure. - <more>: See <https://rdrr.io/github/JINJINT/ESCO/man/escoParams.html>. ruvcorr_args (ns): Additional arguments to pass to the simulation function. - <more>: See <https://rdrr.io/bioc/RUVcorr/man/simulateGEdata.html>. transpose_output (flag): If set, the output will be transposed. index_start (type=int): The index to start from when naming the samples. Affects the sample names in `out.outfile` only. \"\"\" input = \"ngenes:var, nsamples:var\" output = [ \"outfile:file:{{in.ngenes}}x{{in.nsamples}}.sim/simulated.txt\" , \"outdir:dir:{{in.ngenes}}x{{in.nsamples}}.sim\" , ] lang = config . lang . rscript envs = { \"tool\" : \"RUVcorr\" , \"ncores\" : config . misc . ncores , \"type\" : \"single\" , \"esco_args\" : { \"dropout-type\" : \"none\" , \"save\" : \"simulated-truth\" , \"type\" : \"single\" , }, \"ruvcorr_args\" : {}, \"seed\" : None , \"transpose_output\" : False , \"index_start\" : 1 , } script = \"file://../scripts/rnaseq/Simulation.R\"","title":"biopipen.ns.rnaseq"},{"location":"api/source/biopipen.ns.scrna/","text":"SOURCE CODE biopipen.ns. scrna DOCS \"\"\"Tools to analyze single-cell RNA\"\"\" from pipen.utils import mark from ..core.proc import Proc from ..core.config import config # from ..utils.common_docstrs import ( # indent_docstr, # format_placeholder, # MUTATE_HELPERS_CLONESIZE, # ENVS_SECTION_EACH, # ) # MUTATE_HELPERS_CLONESIZE_INDENTED = indent_docstr(MUTATE_HELPERS_CLONESIZE, \" \" * 3) # ENVS_SECTION_EACH_INDENTED = indent_docstr(ENVS_SECTION_EACH, \" \" * 3) class SeuratLoading ( Proc ): DOCS \"\"\"Seurat - Loading data Deprecated, should be superseded by SeuratPreparing Input: metafile: The metadata of the samples A tab-delimited file Two columns are required: - `Sample` to specify the sample names. - `RNAData` to assign the path of the data to the samples The path will be read by `Read10X()` from `Seurat` Output: rdsfile: The RDS file with a list of Seurat object Envs: qc: The QC filter for each sample. This will be passed to `subset(obj, subset=<qc>)`. For example `nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5` \"\"\" input = \"metafile:file\" output = \"rdsfile:file:{{in.metafile | stem}}.seurat.RDS\" envs = { \"qc\" : \"\" } lang = config . lang . rscript script = \"file://../scripts/scrna/SeuratLoading.R\" class SeuratPreparing ( Proc ): DOCS \"\"\"Load, prepare and apply QC to data, using `Seurat` This process will - - Prepare the seurat object - Apply QC to the data - Integrate the data from different samples See also - <https://satijalab.org/seurat/articles/pbmc3k_tutorial.html#standard-pre-processing-workflow-1)> - <https://satijalab.org/seurat/articles/integration_introduction> This process will read the scRNA-seq data, based on the information provided by `SampleInfo`, specifically, the paths specified by the `RNAData` column. Those paths should be either paths to directoies containing `matrix.mtx`, `barcodes.tsv` and `features.tsv` files that can be loaded by [`Seurat::Read10X()`](https://satijalab.org/seurat/reference/read10x), or paths of loom files that can be loaded by `SeuratDisk::LoadLoom()`, or paths to `h5` files that can be loaded by [`Seurat::Read10X_h5()`](https://satijalab.org/seurat/reference/read10x_h5). Each sample will be loaded individually and then merged into one `Seurat` object, and then perform QC. In order to perform QC, some additional columns are added to the meta data of the `Seurat` object. They are: - `precent.mt`: The percentage of mitochondrial genes. - `percent.ribo`: The percentage of ribosomal genes. - `precent.hb`: The percentage of hemoglobin genes. - `percent.plat`: The percentage of platelet genes. For integration, two routes are available: - [Performing integration on datasets normalized with `SCTransform`](https://satijalab.org/seurat/articles/seurat5_integration#perform-streamlined-one-line-integrative-analysis) - [Using `NormalizeData` and `FindIntegrationAnchors`](https://satijalab.org/seurat/articles/seurat5_integration#layers-in-the-seurat-v5-object) /// Note When using `SCTransform`, the default Assay will be set to `SCT` in output, rather than `RNA`. If you are using `cca` or `rpca` interation, the default assay will be `integrated`. /// /// Note From `biopipen` v0.23.0, this requires `Seurat` v5.0.0 or higher. /// Input: metafile: The metadata of the samples A tab-delimited file Two columns are required: `Sample` to specify the sample names. `RNAData` to assign the path of the data to the samples The path will be read by `Read10X()` from `Seurat`, or the path to the h5 file that can be read by `Read10X_h5()` from `Seurat`. It can also be an RDS or qs2 file containing a `Seurat` object. Note that it must has a column named `Sample` in the meta.data to specify the sample names. Output: outfile: The qs2 file with the Seurat object with all samples integrated. Note that the cell ids are prefixied with sample names. Envs: ncores (type=int): Number of cores to use. Used in `future::plan(strategy = \"multicore\", workers = <ncores>)` to parallelize some Seurat procedures. mutaters (type=json): The mutaters to mutate the metadata to the cells. These new columns will be added to the metadata of the Seurat object and will be saved in the output file. min_cells (type=int): The minimum number of cells that a gene must be expressed in to be kept. This is used in `Seurat::CreateSeuratObject()`. Futher QC (`envs.cell_qc`, `envs.gene_qc`) will be performed after this. It doesn't work when data is loaded from loom files or RDS/qs2 files. min_features (type=int): The minimum number of features that a cell must express to be kept. This is used in `Seurat::CreateSeuratObject()`. Futher QC (`envs.cell_qc`, `envs.gene_qc`) will be performed after this. It doesn't work when data is loaded from loom files or RDS/qs2 files. cell_qc: Filter expression to filter cells, using `tidyrseurat::filter()`. It can also be a dictionary of expressions, where the names of the list are sample names. You can have a default expression in the list with the name \"DEFAULT\" for the samples that are not listed. Available QC keys include `nFeature_RNA`, `nCount_RNA`, `percent.mt`, `percent.ribo`, `percent.hb`, and `percent.plat`. /// Tip | Example Including the columns added above, all available QC keys include `nFeature_RNA`, `nCount_RNA`, `percent.mt`, `percent.ribo`, `percent.hb`, and `percent.plat`. For example: ```toml [SeuratPreparing.envs] cell_qc = \"nFeature_RNA > 200 & percent.mt < 5\" ``` will keep cells with more than 200 genes and less than 5%% mitochondrial genes. /// gene_qc (ns): Filter genes. `gene_qc` is applied after `cell_qc`. - min_cells: The minimum number of cells that a gene must be expressed in to be kept. - excludes: The genes to exclude. Multiple genes can be specified by comma separated values, or as a list. /// Tip | Example ```toml [SeuratPreparing.envs] gene_qc = { min_cells = 3 } ``` will keep genes that are expressed in at least 3 cells. /// qc_plots (type=json): The plots for QC metrics. It should be a json (or python dict) with the keys as the names of the plots and the values also as dicts with the following keys: * kind: The kind of QC. Either `gene` or `cell` (default). * devpars: The device parameters for the plot. A dict with `res`, `height`, and `width`. * more_formats: The formats to save the plots other than `png`. * save_code: Whether to save the code to reproduce the plot. * other arguments passed to [`biopipen.utils::VizSeuratCellQC`](https://pwwang.github.io/biopipen.utils.R/reference/VizSeuratCellQC.html) when `kind` is `cell` or [`biopipen.utils::VizSeuratGeneQC`](https://pwwang.github.io/biopipen.utils.R/reference/VizSeuratGeneQC.html) when `kind` is `gene`. use_sct (flag): Whether use SCTransform routine to integrate samples or not. Before the following procedures, the `RNA` layer will be split by samples. If `False`, following procedures will be performed in the order: * [`NormalizeData`](https://satijalab.org/seurat/reference/normalizedata). * [`FindVariableFeatures`](https://satijalab.org/seurat/reference/findvariablefeatures). * [`ScaleData`](https://satijalab.org/seurat/reference/scaledata). See <https://satijalab.org/seurat/articles/seurat5_integration#layers-in-the-seurat-v5-object> and <https://satijalab.org/seurat/articles/pbmc3k_tutorial.html> If `True`, following procedures will be performed in the order: * [`SCTransform`](https://satijalab.org/seurat/reference/sctransform). See <https://satijalab.org/seurat/articles/seurat5_integration#perform-streamlined-one-line-integrative-analysis> no_integration (flag): Whether to skip integration or not. NormalizeData (ns): Arguments for [`NormalizeData()`](https://satijalab.org/seurat/reference/normalizedata). `object` is specified internally, and `-` in the key will be replaced with `.`. - <more>: See <https://satijalab.org/seurat/reference/normalizedata> FindVariableFeatures (ns): Arguments for [`FindVariableFeatures()`](https://satijalab.org/seurat/reference/findvariablefeatures). `object` is specified internally, and `-` in the key will be replaced with `.`. - <more>: See <https://satijalab.org/seurat/reference/findvariablefeatures> ScaleData (ns): Arguments for [`ScaleData()`](https://satijalab.org/seurat/reference/scaledata). `object` and `features` is specified internally, and `-` in the key will be replaced with `.`. - <more>: See <https://satijalab.org/seurat/reference/scaledata> RunPCA (ns): Arguments for [`RunPCA()`](https://satijalab.org/seurat/reference/runpca). `object` and `features` is specified internally, and `-` in the key will be replaced with `.`. - npcs (type=int): The number of PCs to compute. For each sample, `npcs` will be no larger than the number of columns - 1. - <more>: See <https://satijalab.org/seurat/reference/runpca> SCTransform (ns): Arguments for [`SCTransform()`](https://satijalab.org/seurat/reference/sctransform). `object` is specified internally, and `-` in the key will be replaced with `.`. - return-only-var-genes: Whether to return only variable genes. - min_cells: The minimum number of cells that a gene must be expressed in to be kept. A hidden argument of `SCTransform` to filter genes. If you try to keep all genes in the `RNA` assay, you can set `min_cells` to `0` and `return-only-var-genes` to `False`. See <https://github.com/satijalab/seurat/issues/3598#issuecomment-715505537> - <more>: See <https://satijalab.org/seurat/reference/sctransform> IntegrateLayers (ns): Arguments for [`IntegrateLayers()`](https://satijalab.org/seurat/reference/integratelayers). `object` is specified internally, and `-` in the key will be replaced with `.`. When `use_sct` is `True`, `normalization-method` defaults to `SCT`. - method (choice): The method to use for integration. - CCAIntegration: Use `Seurat::CCAIntegration`. - CCA: Same as `CCAIntegration`. - cca: Same as `CCAIntegration`. - RPCAIntegration: Use `Seurat::RPCAIntegration`. - RPCA: Same as `RPCAIntegration`. - rpca: Same as `RPCAIntegration`. - HarmonyIntegration: Use `Seurat::HarmonyIntegration`. - Harmony: Same as `HarmonyIntegration`. - harmony: Same as `HarmonyIntegration`. - FastMNNIntegration: Use `Seurat::FastMNNIntegration`. - FastMNN: Same as `FastMNNIntegration`. - fastmnn: Same as `FastMNNIntegration`. - scVIIntegration: Use `Seurat::scVIIntegration`. - scVI: Same as `scVIIntegration`. - scvi: Same as `scVIIntegration`. - <more>: See <https://satijalab.org/seurat/reference/integratelayers> doublet_detector (choice): The doublet detector to use. - none: Do not use any doublet detector. - DoubletFinder: Use `DoubletFinder` to detect doublets. - doubletfinder: Same as `DoubletFinder`. - scDblFinder: Use `scDblFinder` to detect doublets. - scdblfinder: Same as `scDblFinder`. DoubletFinder (ns): Arguments to run [`DoubletFinder`](https://github.com/chris-mcginnis-ucsf/DoubletFinder). See also <https://demultiplexing-doublet-detecting-docs.readthedocs.io/en/latest/DoubletFinder.html>. - PCs (type=int): Number of PCs to use for 'doubletFinder' function. - doublets (type=float): Number of expected doublets as a proportion of the pool size. - pN (type=float): Number of doublets to simulate as a proportion of the pool size. - ncores (type=int): Number of cores to use for `DoubletFinder::paramSweep`. Set to `None` to use `envs.ncores`. Since parallelization of the function usually exhausts memory, if big `envs.ncores` does not work for `DoubletFinder`, set this to a smaller number. scDblFinder (ns): Arguments to run [`scDblFinder`](https://rdrr.io/bioc/scDblFinder/man/scDblFinder.html). - dbr (type=float): The expected doublet rate. - ncores (type=int): Number of cores to use for `scDblFinder`. Set to `None` to use `envs.ncores`. - <more>: See <https://rdrr.io/bioc/scDblFinder/man/scDblFinder.html>. cache (type=auto): Whether to cache the information at different steps. If `True`, the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. The cached seurat object will be saved as `<signature>.<kind>.RDS` file, where `<signature>` is the signature determined by the input and envs of the process. See <https://github.com/satijalab/seurat/issues/7849>, <https://github.com/satijalab/seurat/issues/5358> and <https://github.com/satijalab/seurat/issues/6748> for more details also about reproducibility issues. To not use the cached seurat object, you can either set `cache` to `False` or delete the cached file at `<signature>.RDS` in the cache directory. Requires: r-seurat: - check: {{proc.lang}} <(echo \"library(Seurat)\") r-future: - check: {{proc.lang}} <(echo \"library(future)\") r-bracer: - check: {{proc.lang}} <(echo \"library(bracer)\") \"\"\" # noqa: E501 input = \"metafile:file\" output = \"outfile:file:{{in.metafile | stem}}.seurat.qs\" lang = config . lang . rscript envs_depth = 4 envs = { \"ncores\" : config . misc . ncores , \"mutaters\" : {}, \"min_cells\" : 0 , \"min_features\" : 0 , \"cell_qc\" : None , # \"nFeature_RNA > 200 & percent.mt < 5\", \"gene_qc\" : { \"min_cells\" : 0 , \"excludes\" : []}, \"qc_plots\" : { \"Violin Plots\" : { \"kind\" : \"cell\" , \"plot_type\" : \"violin\" , \"devpars\" : { \"res\" : 100 , \"height\" : 600 , \"width\" : 1200 }, }, \"Scatter Plots\" : { \"kind\" : \"cell\" , \"plot_type\" : \"scatter\" , \"devpars\" : { \"res\" : 100 , \"height\" : 800 , \"width\" : 1200 }, }, \"Ridge Plots\" : { \"kind\" : \"cell\" , \"plot_type\" : \"ridge\" , \"devpars\" : { \"res\" : 100 , \"height\" : 800 , \"width\" : 1200 }, }, \"Distribution of number of cells a gene is expressed in\" : { \"kind\" : \"gene\" , \"plot_type\" : \"histogram\" , \"devpars\" : { \"res\" : 100 , \"height\" : 1200 , \"width\" : 1200 }, }, }, \"use_sct\" : False , \"no_integration\" : False , \"NormalizeData\" : {}, \"FindVariableFeatures\" : {}, \"ScaleData\" : {}, \"RunPCA\" : {}, \"SCTransform\" : { \"return-only-var-genes\" : True , \"min_cells\" : 5 , \"verbose\" : True , }, \"IntegrateLayers\" : { \"method\" : \"harmony\" }, \"doublet_detector\" : \"none\" , \"DoubletFinder\" : { \"PCs\" : 10 , \"pN\" : 0.25 , \"doublets\" : 0.075 , \"ncores\" : 1 }, \"scDblFinder\" : { \"dbr\" : 0.075 , \"ncores\" : 1 }, \"cache\" : config . path . tmpdir , } script = \"file://../scripts/scrna/SeuratPreparing.R\" plugin_opts = { \"report\" : \"file://../reports/common.svelte\" , } class SeuratClustering ( Proc ): DOCS \"\"\"Determine the clusters of cells without reference using Seurat FindClusters procedure. Input: srtobj: The seurat object loaded by SeuratPreparing Output: outfile: The seurat object with cluster information at `seurat_clusters`. Envs: ncores (type=int;order=-100): Number of cores to use. Used in `future::plan(strategy = \"multicore\", workers = <ncores>)` to parallelize some Seurat procedures. See also: <https://satijalab.org/seurat/articles/future_vignette.html> RunUMAP (ns): Arguments for [`RunUMAP()`](https://satijalab.org/seurat/reference/runumap). `object` is specified internally, and `-` in the key will be replaced with `.`. `dims=N` will be expanded to `dims=1:N`; The maximal value of `N` will be the minimum of `N` and the number of columns - 1 for each sample. - dims (type=int): The number of PCs to use - reduction: The reduction to use for UMAP. If not provided, `sobj@misc$integrated_new_reduction` will be used. - <more>: See <https://satijalab.org/seurat/reference/runumap> RunPCA (ns): Arguments for [`RunPCA()`](https://satijalab.org/seurat/reference/runpca). FindNeighbors (ns): Arguments for [`FindNeighbors()`](https://satijalab.org/seurat/reference/findneighbors). `object` is specified internally, and `-` in the key will be replaced with `.`. - reduction: The reduction to use. If not provided, `sobj@misc$integrated_new_reduction` will be used. - <more>: See <https://satijalab.org/seurat/reference/findneighbors> FindClusters (ns): Arguments for [`FindClusters()`](https://satijalab.org/seurat/reference/findclusters). `object` is specified internally, and `-` in the key will be replaced with `.`. The cluster labels will be saved in `seurat_clusters` and prefixed with \"c\". The first cluster will be \"c1\", instead of \"c0\". - resolution (type=auto): The resolution of the clustering. You can have multiple resolutions as a list or as a string separated by comma. Ranges are also supported, for example: `0.1:0.5:0.1` will generate `0.1, 0.2, 0.3, 0.4, 0.5`. The step can be omitted, defaulting to 0.1. The results will be saved in `seurat_clusters_<resolution>`. The final resolution will be used to define the clusters at `seurat_clusters`. - <more>: See <https://satijalab.org/seurat/reference/findclusters> cache (type=auto): Where to cache the information at different steps. If `True`, the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. Set to `False` to not cache the results. Requires: r-seurat: - check: {{proc.lang}} <(echo \"library(Seurat)\") r-tidyr: - check: {{proc.lang}} <(echo \"library(tidyr)\") r-dplyr: - check: {{proc.lang}} <(echo \"library(dplyr)\") \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outfile:file:{{in.srtobj | stem}}.qs\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"RunPCA\" : {}, \"RunUMAP\" : {}, \"FindNeighbors\" : {}, \"FindClusters\" : { \"resolution\" : 0.8 }, \"cache\" : config . path . tmpdir , } script = \"file://../scripts/scrna/SeuratClustering.R\" class SeuratSubClustering ( Proc ): DOCS \"\"\"Find clusters of a subset of cells. It's unlike [`Seurat::FindSubCluster`], which only finds subclusters of a single cluster. Instead, it will perform the whole clustering procedure on the subset of cells. One can use metadata to specify the subset of cells to perform clustering on. For the subset of cells, the reductions will be re-performed on the subset of cells, and then the clustering will be performed on the subset of cells. The reduction will be saved in `object@reduction$<casename>.<reduction>` of the original object and the clustering will be saved in the metadata of the original object using the casename as the column name. Input: srtobj: The seurat object in RDS or qs/qs2 format. Output: outfile: The seurat object with the subclustering information in qs/qs2 format. Envs: ncores (type=int;order=-100): Number of cores to use. Used in `future::plan(strategy = \"multicore\", workers = <ncores>)` to parallelize some Seurat procedures. mutaters (type=json): The mutaters to mutate the metadata to subset the cells. The mutaters will be applied in the order specified. subset: An expression to subset the cells, will be passed to [`tidyseurat::filter()`](https://stemangiola.github.io/tidyseurat/reference/filter.html). RunPCA (ns): Arguments for [`RunPCA()`](https://satijalab.org/seurat/reference/runpca). `object` is specified internally as the subset object, and `-` in the key will be replaced with `.`. - <more>: See <https://satijalab.org/seurat/reference/runpca> RunUMAP (ns): Arguments for [`RunUMAP()`](https://satijalab.org/seurat/reference/runumap). `object` is specified internally as the subset object, and `-` in the key will be replaced with `.`. `dims=N` will be expanded to `dims=1:N`; The maximal value of `N` will be the minimum of `N` and the number of columns - 1 for each sample. - dims (type=int): The number of PCs to use - reduction: The reduction to use for UMAP. If not provided, `sobj@misc$integrated_new_reduction` will be used. - <more>: See <https://satijalab.org/seurat/reference/runumap> FindNeighbors (ns): Arguments for [`FindNeighbors()`](https://satijalab.org/seurat/reference/findneighbors). `object` is specified internally, and `-` in the key will be replaced with `.`. - reduction: The reduction to use. If not provided, `object@misc$integrated_new_reduction` will be used. - <more>: See <https://satijalab.org/seurat/reference/findneighbors> FindClusters (ns): Arguments for [`FindClusters()`](https://satijalab.org/seurat/reference/findclusters). `object` is specified internally, and `-` in the key will be replaced with `.`. The cluster labels will be prefixed with \"s\". The first cluster will be \"s1\", instead of \"s0\". - resolution (type=auto): The resolution of the clustering. You can have multiple resolutions as a list or as a string separated by comma. Ranges are also supported, for example: `0.1:0.5:0.1` will generate `0.1, 0.2, 0.3, 0.4, 0.5`. The step can be omitted, defaulting to 0.1. The results will be saved in `<casename>_<resolution>`. The final resolution will be used to define the clusters at `<casename>`. - <more>: See <https://satijalab.org/seurat/reference/findclusters> cache (type=auto): Whether to cache the results. If `True`, the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. Set to `False` to not cache the results. cases (type=json): The cases to perform subclustering. Keys are the names of the cases and values are the dicts inherited from `envs` except `mutaters` and `cache`. If empty, a case with name `subcluster` will be created with default parameters. The case name will be passed to `biopipen.utils::SeuratSubCluster()` as `name`. It will be used as the prefix for the reduction name, keys and cluster names. For reduction keys, it will be `toupper(<name>)` + \"PC_\" and `toupper(<name>)` + \"UMAP_\". For cluster names, it will be `<name>` + \".\" + resolution. And the final cluster name will be `<name>`. Note that the `name` should be alphanumeric and anything other than alphanumeric will be removed. \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outfile:file:{{in.srtobj | stem}}.qs\" lang = config . lang . rscript envs_depth = 1 envs = { \"ncores\" : config . misc . ncores , \"mutaters\" : {}, \"subset\" : None , \"RunPCA\" : {}, \"RunUMAP\" : {}, \"FindNeighbors\" : {}, \"FindClusters\" : { \"resolution\" : 0.8 }, \"cache\" : config . path . tmpdir , \"cases\" : {}, } script = \"file://../scripts/scrna/SeuratSubClustering.R\" class SeuratClusterStats ( Proc ): DOCS \"\"\"Statistics of the clustering. Including the number/fraction of cells in each cluster, the gene expression values and dimension reduction plots. It's also possible to perform stats on TCR clones/clusters or other metadata for each T-cell cluster. Examples: ### Number of cells in each cluster ```toml [SeuratClusterStats.envs.stats] # suppose you have nothing set in `envs.stats_defaults` # otherwise, the settings will be inherited here nCells_All = { } ``` ![nCells_All](https://pwwang.github.io/immunopipe/latest/processes/images/SeuratClusterStats_nCells_All.png){: width=\"80%\" } ### Number of cells in each cluster by groups ```toml [SeuratClusterStats.envs.stats] nCells_Sample = { group_by = \"Sample\" } ``` ![nCells_Sample](https://pwwang.github.io/immunopipe/latest/processes/images/SeuratClusterStats_nCells_Sample.png){: width=\"80%\" } ### Violin plots for the gene expressions ```toml [SeuratClusterStats.envs.features] features = \"CD4,CD8A\" # Remove the dots in the violin plots vlnplots = { pt-size = 0, kind = \"vln\" } # Don't use the default genes vlnplots_1 = { features = [\"FOXP3\", \"IL2RA\"], pt-size = 0, kind = \"vln\" } ``` ![vlnplots](https://pwwang.github.io/immunopipe/latest/processes/images/SeuratClusterStats_vlnplots.png){: width=\"80%\" } ![vlnplots_1](https://pwwang.github.io/immunopipe/latest/processes/images/SeuratClusterStats_vlnplots_1.png){: width=\"80%\" } ### Dimension reduction plot with labels ```toml [SeuratClusterStats.envs.dimplots.Idents] label = true ``` ![dimplots](https://pwwang.github.io/immunopipe/latest/processes/images/SeuratClusterStats_dimplots.png){: width=\"80%\" } Input: srtobj: The seurat object loaded by `SeuratClustering` Output: outdir: The output directory. Different types of plots will be saved in different subdirectories. For example, `clustree` plots will be saved in `clustrees` subdirectory. For each case in `envs.clustrees`, both the png and pdf files will be saved. Envs: mutaters (type=json): The mutaters to mutate the metadata to subset the cells. The mutaters will be applied in the order specified. You can also use the clone selectors to select the TCR clones/clusters. See <https://pwwang.github.io/scplotter/reference/clone_selectors.html>. cache (type=auto): Whether to cache the plots. Currently only plots for features are supported, since creating the those plots can be time consuming. If `True`, the plots will be cached in the job output directory, which will be not cleaned up when job is rerunning. clustrees_defaults (ns): The parameters for the clustree plots. - devpars (ns): The device parameters for the clustree plot. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - more_formats (type=list): The formats to save the plots other than `png`. - save_code (flag): Whether to save the code to reproduce the plot. - prefix (type=auto): string indicating columns containing clustering information. The trailing dot is not necessary and will be added automatically. When `TRUE`, clustrees will be plotted when there is `FindClusters` or `FindClusters.*` in the `obj@commands`. The latter is generated by `SeuratSubClustering`. This will be ignored when `envs.clustrees` is specified (the prefix of each case must be specified separately). - <more>: Other arguments passed to `scplotter::ClustreePlot`. See <https://pwwang.github.io/scplotter/reference/ClustreePlot.html> clustrees (type=json): The cases for clustree plots. Keys are the names of the plots and values are the dicts inherited from `env.clustrees_defaults` except `prefix`. There is no default case for `clustrees`. stats_defaults (ns): The default parameters for `stats`. This is to do some basic statistics on the clusters/cells. For more comprehensive analysis, see <https://pwwang.github.io/scplotter/reference/CellStatPlot.html>. The parameters from the cases can overwrite the default parameters. - subset: An expression to subset the cells, will be passed to `tidyrseurat::filter()`. - devpars (ns): The device parameters for the clustree plot. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - descr: The description of the plot, showing in the report. - more_formats (type=list): The formats to save the plots other than `png`. - save_code (flag): Whether to save the code to reproduce the plot. - save_data (flag): Whether to save the data used to generate the plot. - <more>: Other arguments passed to `scplotter::CellStatPlot`. See <https://pwwang.github.io/scplotter/reference/CellStatPlot.html>. stats (type=json): The number/fraction of cells to plot. Keys are the names of the plots and values are the dicts inherited from `env.stats_defaults`. Here are some examples - >>> { >>> \"nCells_All\": {}, >>> \"nCells_Sample\": {\"group_by\": \"Sample\"}, >>> \"fracCells_Sample\": {\"scale_y\": True, \"group_by\": \"Sample\", plot_type = \"pie\"}, >>> } ngenes_defaults (ns): The default parameters for `ngenes`. The default parameters to plot the number of genes expressed in each cell. - more_formats (type=list): The formats to save the plots other than `png`. - subset: An expression to subset the cells, will be passed to `tidyrseurat::filter()`. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. ngenes (type=json): The number of genes expressed in each cell. Keys are the names of the plots and values are the dicts inherited from `env.ngenes_defaults`. features_defaults (ns): The default parameters for `features`. - features (type=auto): The features to plot. It can be either a string with comma separated features, a list of features, a file path with `file://` prefix with features (one per line), or an integer to use the top N features from `VariantFeatures(srtobj)`. It can also be a dict with the keys as the feature group names and the values as the features, which is used for heatmap to group the features. - order_by (type=auto): The order of the clusters to show on the plot. An expression passed to `dplyr::arrange()` on the grouped meta data frame (by `ident`). For example, you can order the clusters by the activation score of the cluster: `desc(mean(ActivationScore, na.rm = TRUE))`, suppose you have a column `ActivationScore` in the metadata. You may also specify the literal order of the clusters by a list of strings (at least two). - subset: An expression to subset the cells, will be passed to `tidyrseurat::filter()`. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - descr: The description of the plot, showing in the report. - more_formats (type=list): The formats to save the plots other than `png`. - save_code (flag): Whether to save the code to reproduce the plot. - save_data (flag): Whether to save the data used to generate the plot. - <more>: Other arguments passed to `scplotter::FeatureStatPlot`. See <https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html> features (type=json): The plots for features, include gene expressions, and columns from metadata. Keys are the titles of the cases and values are the dicts inherited from `env.features_defaults`. dimplots_defaults (ns): The default parameters for `dimplots`. - group_by: The identity to use. If it is from subclustering (reduction `sub_umap_<ident>` exists), this reduction will be used if `reduction` is set to `dim` or `auto`. - split_by: The column name in metadata to split the cells into different plots. - subset: An expression to subset the cells, will be passed to `tidyrseurat::filter()`. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - reduction (choice): Which dimensionality reduction to use. - dim: Use `Seurat::DimPlot`. First searches for `umap`, then `tsne`, then `pca`. If `ident` is from subclustering, `sub_umap_<ident>` will be used. - auto: Same as `dim` - umap: Use `Seurat::UMAPPlot`. - tsne: Use `Seurat::TSNEPlot`. - pca: Use `Seurat::PCAPlot`. - <more>: See <https://pwwang.github.io/scplotter/reference/CellDimPlot.html> dimplots (type=json): The dimensional reduction plots. Keys are the titles of the plots and values are the dicts inherited from `env.dimplots_defaults`. It can also have other parameters from [`scplotter::CellDimPlot`](https://pwwang.github.io/scplotter/reference/CellDimPlot.html). Requires: r-seurat: - check: {{proc.lang}} -e \"library(Seurat)\" \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outdir:dir:{{in.srtobj | stem}}.cluster_stats\" lang = config . lang . rscript envs = { \"mutaters\" : {}, \"cache\" : config . path . tmpdir , \"clustrees_defaults\" : { \"devpars\" : { \"res\" : 100 }, \"more_formats\" : [], \"save_code\" : False , \"prefix\" : True , }, \"clustrees\" : {}, \"stats_defaults\" : { \"subset\" : None , \"descr\" : None , \"devpars\" : { \"res\" : 100 }, \"more_formats\" : [], \"save_code\" : False , \"save_data\" : False , }, \"stats\" : { \"Number of cells in each cluster (Bar Chart)\" : { \"plot_type\" : \"bar\" , \"x_text_angle\" : 90 , }, \"Number of cells in each cluster by Sample (Bar Chart)\" : { \"plot_type\" : \"bar\" , \"group_by\" : \"Sample\" , \"x_text_angle\" : 90 , }, }, \"ngenes_defaults\" : { \"subset\" : None , \"more_formats\" : [], \"devpars\" : { \"res\" : 100 , \"height\" : 800 , \"width\" : 1000 }, }, \"ngenes\" : { \"Number of genes expressed in each cluster\" : {}, }, \"features_defaults\" : { \"features\" : None , \"order_by\" : None , \"subset\" : None , \"devpars\" : { \"res\" : 100 }, \"descr\" : None , \"more_formats\" : [], \"save_code\" : False , \"save_data\" : False , }, \"features\" : {}, \"dimplots_defaults\" : { \"group_by\" : \"seurat_clusters\" , \"split_by\" : None , \"subset\" : None , \"reduction\" : \"dim\" , \"devpars\" : { \"res\" : 100 }, }, \"dimplots\" : { \"Dimensional reduction plot\" : { \"label\" : True , }, }, } script = \"file://../scripts/scrna/SeuratClusterStats.R\" plugin_opts = { \"report\" : \"file://../reports/common.svelte\" , \"report_paging\" : 8 , } class ModuleScoreCalculator ( Proc ): DOCS \"\"\"Calculate the module scores for each cell The module scores are calculated by [`Seurat::AddModuleScore()`](https://satijalab.org/seurat/reference/addmodulescore) or [`Seurat::CellCycleScoring()`](https://satijalab.org/seurat/reference/cellcyclescoring) for cell cycle scores. The module scores are calculated as the average expression levels of each program on single cell level, subtracted by the aggregated expression of control feature sets. All analyzed features are binned based on averaged expression, and the control features are randomly selected from each bin. Input: srtobj: The seurat object loaded by `SeuratClustering` Output: rdsfile: The seurat object with module scores added to the metadata. Envs: defaults (ns): The default parameters for `modules`. - features: The features to calculate the scores. Multiple features should be separated by comma. You can also specify `cc.genes` or `cc.genes.updated.2019` to use the cell cycle genes to calculate cell cycle scores. If so, three columns will be added to the metadata, including `S.Score`, `G2M.Score` and `Phase`. Only one type of cell cycle scores can be calculated at a time. - nbin (type=int): Number of bins of aggregate expression levels for all analyzed features. - ctrl (type=int): Number of control features selected from the same bin per analyzed feature. - k (flag): Use feature clusters returned from `DoKMeans`. - assay: The assay to use. - seed (type=int): Set a random seed. - search (flag): Search for symbol synonyms for features in features that don't match features in object? - keep (flag): Keep the scores for each feature? Only works for non-cell cycle scores. - agg (choice): The aggregation function to use. Only works for non-cell cycle scores. - mean: The mean of the expression levels - median: The median of the expression levels - sum: The sum of the expression levels - max: The max of the expression levels - min: The min of the expression levels - var: The variance of the expression levels - sd: The standard deviation of the expression levels modules (type=json): The modules to calculate the scores. Keys are the names of the expression programs and values are the dicts inherited from `env.defaults`. Here are some examples - >>> { >>> \"CellCycle\": {\"features\": \"cc.genes.updated.2019\"}, >>> \"Exhaustion\": {\"features\": \"HAVCR2,ENTPD1,LAYN,LAG3\"}, >>> \"Activation\": {\"features\": \"IFNG\"}, >>> \"Proliferation\": {\"features\": \"STMN1,TUBB\"} >>> } For `CellCycle`, the columns `S.Score`, `G2M.Score` and `Phase` will be added to the metadata. `S.Score` and `G2M.Score` are the cell cycle scores for each cell, and `Phase` is the cell cycle phase for each cell. You can also add Diffusion Components (DC) to the modules >>> {\"DC\": {\"features\": 2, \"kind\": \"diffmap\"}} will perform diffusion map as a reduction and add the first 2 components as `DC_1` and `DC_2` to the metadata. `diffmap` is a shortcut for `diffusion_map`. Other key-value pairs will pass to [`destiny::DiffusionMap()`](https://www.rdocumentation.org/packages/destiny/versions/2.0.4/topics/DiffusionMap class). You can later plot the diffusion map by using `reduction = \"DC\"` in `env.dimplots` in `SeuratClusterStats`. This requires [`SingleCellExperiment`](https://bioconductor.org/packages/release/bioc/html/SingleCellExperiment.html) and [`destiny`](https://bioconductor.org/packages/release/bioc/html/destiny.html) R packages. post_mutaters (type=json): The mutaters to mutate the metadata after calculating the module scores. The mutaters will be applied in the order specified. This is useful when you want to create new scores based on the calculated module scores. \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"rdsfile:file:{{in.srtobj | stem}}.qs\" lang = config . lang . rscript envs = { \"defaults\" : { \"features\" : None , \"nbin\" : 24 , \"ctrl\" : 100 , \"k\" : False , \"assay\" : None , \"seed\" : 8525 , \"search\" : False , \"keep\" : False , \"agg\" : \"mean\" , }, \"modules\" : { # \"CellCycle\": {\"features\": \"cc.genes.updated.2019\"}, # \"Exhaustion\": {\"features\": \"HAVCR2,ENTPD1,LAYN,LAG3\"}, # \"Activation\": {\"features\": \"IFNG\"}, # \"Proliferation\": {\"features\": \"STMN1,TUBB\"}, }, \"post_mutaters\" : {}, } script = \"file://../scripts/scrna/ModuleScoreCalculator.R\" @mark ( DOCS deprecated = ( \"[ {proc.name} ] is deprecated, \" \"use [SeuratClusterStats] or [ClonalStats] instead.\" ) ) class CellsDistribution ( Proc ): \"\"\"Distribution of cells (i.e. in a TCR clone) from different groups for each cluster This generates a set of pie charts with proportion of cells in each cluster Rows are the cells identities (i.e. TCR clones or TCR clusters), columns are groups (i.e. clinic groups). Examples: ```toml [CellsDistribution.envs.mutaters] # Add Patient1_Tumor_Expanded column with CDR3.aa that # expands in Tumor of patient 1 Patient1_Tumor_Expanded = ''' expanded(., region, \"Tumor\", subset = patient == \"Lung1\", uniq = FALSE) ''' [CellsDistribution.envs.cases.Patient1_Tumor_Expanded] cells_by = \"Patient1_Tumor_Expanded\" cells_orderby = \"desc(CloneSize)\" group_by = \"region\" group_order = [ \"Tumor\", \"Normal\" ] ``` ![CellsDistribution_example](https://pwwang.github.io/immunopipe/latest/processes/images/CellsDistribution_example.png) Input: srtobj: The seurat object in RDS format Output: outdir: The output directory. The results for each case will be saved in a subdirectory. Envs: mutaters (type=json): The mutaters to mutate the metadata Keys are the names of the mutaters and values are the R expressions passed by `dplyr::mutate()` to mutate the metadata. cluster_orderby: The order of the clusters to show on the plot. An expression passed to `dplyr::summarise()` on the grouped data frame (by `seurat_clusters`). The summary stat will be passed to `dplyr::arrange()` to order the clusters. It's applied on the whole meta.data before grouping and subsetting. For example, you can order the clusters by the activation score of the cluster: `desc(mean(ActivationScore, na.rm = TRUE))`, suppose you have a column `ActivationScore` in the metadata. group_by: The column name in metadata to group the cells for the columns of the plot. group_order (list): The order of the groups (columns) to show on the plot cells_by: The column name in metadata to group the cells for the rows of the plot. If your cell groups have overlapping cells, you can also use multiple columns, separated by comma (`,`). These columns will be concatenated to form the cell groups. For the overlapping cells, they will be counted multiple times for different groups. So make sure the cell group names in different columns are unique. cells_order (list): The order of the cells (rows) to show on the plot cells_orderby: An expression passed to `dplyr::arrange()` to order the cells (rows) of the plot. Only works when `cells-order` is not specified. The data frame passed to `dplyr::arrange()` is grouped by `cells_by` before ordering. You can have multiple expressions separated by semicolon (`;`). The expessions will be parsed by `rlang::parse_exprs()`. 4 extra columns were added to the metadata for ordering the rows in the plot: * `CloneSize`: The size (number of cells) of clones (identified by `cells_by`) * `CloneGroupSize`: The clone size in each group (identified by `group_by`) * `CloneClusterSize`: The clone size in each cluster (identified by `seurat_clusters`) * `CloneGroupClusterSize`: The clone size in each group and cluster (identified by `group_by` and `seurat_clusters`) cells_n (type=int): The max number of groups to show for each cell group identity (row). Ignored if `cells_order` is specified. subset: An expression to subset the cells, will be passed to `dplyr::filter()` on metadata. This will be applied prior to `each`. descr: The description of the case, will be shown in the report. hm_devpars (ns): The device parameters for the heatmaps. - res (type=int): The resolution of the heatmaps. - height (type=int): The height of the heatmaps. - width (type=int): The width of the heatmaps. devpars (ns): The device parameters for the plots of pie charts. - res (type=int): The resolution of the plots - height (type=int): The height of the plots - width (type=int): The width of the plots each: The column name in metadata to separate the cells into different plots. prefix_each (flag): Whether to prefix the `each` column name to the value as the case/section name. section: The section to show in the report. This allows different cases to be put in the same section in report. Only works when `each` is not specified. overlap (list): Plot the overlap of cell groups (values of `cells_by`) in different cases under the same section. The section must have at least 2 cases, each case should have a single `cells_by` column. cases (type=json;order=99): If you have multiple cases, you can specify them here. Keys are the names of the cases and values are the options above except `mutaters`. If some options are not specified, the options in `envs` will be used. If no cases are specified, a default case will be used with case name `DEFAULT`. Requires: r-seurat: - check: {{proc.lang}} -e \"library(Seurat)\" r-dplyr: - check: {{proc.lang}} -e \"library(dplyr)\" r-tidyr: - check: {{proc.lang}} -e \"library(tidyr)\" \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outdir:dir:{{in.srtobj | stem}}.cells_distribution\" lang = config . lang . rscript envs = { \"mutaters\" : {}, \"cluster_orderby\" : None , \"group_by\" : None , \"group_order\" : [], \"cells_by\" : None , \"cells_order\" : [], \"cells_orderby\" : None , \"cells_n\" : 10 , \"subset\" : None , \"descr\" : None , \"devpars\" : {}, \"hm_devpars\" : {}, \"each\" : None , \"prefix_each\" : True , \"section\" : \"DEFAULT\" , \"overlap\" : [], \"cases\" : {}, } script = \"file://../scripts/scrna/CellsDistribution.R\" plugin_opts = { \"report\" : \"file://../reports/scrna/CellsDistribution.svelte\" , \"report_paging\" : 8 , } class SeuratMetadataMutater ( Proc ): DOCS \"\"\"Mutate the metadata of the seurat object Input: srtobj: The seurat object loaded by SeuratPreparing metafile: Additional metadata A tab-delimited file with columns as meta columns and rows as cells. Output: outfile: The seurat object with the additional metadata Envs: mutaters (type=json): The mutaters to mutate the metadata. The key-value pairs will be passed the `dplyr::mutate()` to mutate the metadata. Requires: r-seurat: - check: {{proc.lang}} <(echo \"library(Seurat)\") r-tibble: - check: {{proc.lang}} <(echo \"library(tibble)\") r-dplyr: - check: {{proc.lang}} <(echo \"library(dplyr)\") \"\"\" # noqa: E501 input = \"srtobj:file, metafile:file\" output = \"outfile:file:{{in.srtobj | stem}}.qs\" lang = config . lang . rscript envs = { \"mutaters\" : {}} script = \"file://../scripts/scrna/SeuratMetadataMutater.R\" @mark ( deprecated = \"[ {proc.name} ] is deprecated, use [SeuratClusterStats] instead.\" ) DOCS class DimPlots ( Proc ): \"\"\"Seurat - Dimensional reduction plots Input: srtobj: The seruat object in RDS format configfile: A toml configuration file with \"cases\" If this is given, `envs.cases` will be overriden name: The name of the job, used in report Output: outdir: The output directory Envs: cases: The cases for the dim plots Keys are the names and values are the arguments to `Seurat::Dimplots` \"\"\" input = \"srtobj:file, configfile:file, name:var\" output = \"outdir:dir:{{in.srtobj | stem}}.dimplots\" lang = config . lang . rscript script = \"file://../scripts/scrna/DimPlots.R\" envs = { \"cases\" : { \"Ident\" : { \"group.by\" : \"ident\" }}} plugin_opts = { \"report\" : \"file://../reports/scrna/DimPlots.svelte\" , \"report_toc\" : False , } class MarkersFinder ( Proc ): DOCS \"\"\"Find markers between different groups of cells When only `group_by` is specified as `\"seurat_clusters\"` in `envs.cases`, the markers will be found for all the clusters. You can also find the differentially expressed genes between any two groups of cells by setting `group_by` to a different column name in metadata. Follow `envs.cases` for more details. Input: srtobj: The seurat object loaded by `SeuratPreparing` If you have your `Seurat` object prepared by yourself, you can also use it here, but you should make sure that the object has been processed by `PrepSCTFindMarkers` if data is not normalized using `SCTransform`. Output: outdir: The output directory for the markers and plots Envs: ncores (type=int): Number of cores to use for parallel computing for some `Seurat` procedures. * Used in `future::plan(strategy = \"multicore\", workers = <ncores>)` to parallelize some Seurat procedures. * See also: <https://satijalab.org/seurat/articles/future_vignette.html> mutaters (type=json): The mutaters to mutate the metadata. You can also use the clone selectors to select the TCR clones/clusters. See <https://pwwang.github.io/scplotter/reference/clone_selectors.html>. group_by: The column name in metadata to group the cells. If only `group_by` is specified, and `ident-1` and `ident-2` are not specified, markers will be found for all groups in this column in the manner of \"group vs rest\" comparison. `NA` group will be ignored. If `None`, `Seurat::Idents(srtobj)` will be used, which is usually `\"seurat_clusters\"` after unsupervised clustering. ident_1: The first group of cells to compare When this is empty, the comparisons will be expanded to each group v.s. the rest of the cells in `group_by`. ident_2: The second group of cells to compare If not provided, the rest of the cells are used for `ident-2`. each: The column name in metadata to separate the cells into different cases. When this is specified, the case will be expanded for each value of the column in metadata. For example, when you have `envs.cases.\"Cluster Markers\".each = \"Sample\"`, then the case will be expanded as `envs.cases.\"Cluster Markers - Sample1\"`, `envs.cases.\"Cluster Markers - Sample2\"`, etc. You can specify `allmarker_plots` and `overlaps` to plot the markers for all cases in the same plot and plot the overlaps of the markers between different cases by values in this column. dbs (list): The dbs to do enrichment analysis for significant markers See below for all libraries. <https://maayanlab.cloud/Enrichr/#libraries> sigmarkers: An expression passed to `dplyr::filter()` to filter the significant markers for enrichment analysis. Available variables are `p_val`, `avg_log2FC`, `pct.1`, `pct.2` and `p_val_adj`. For example, `\"p_val_adj < 0.05 & abs(avg_log2FC) > 1\"` to select markers with adjusted p-value < 0.05 and absolute log2 fold change > 1. enrich_style (choice): The style of the enrichment analysis. The enrichment analysis will be done by `EnrichIt()` from [`enrichit`](https://pwwang.github.io/enrichit/). Two styles are available: - enrichr: `enrichr` style enrichment analysis (fisher's exact test will be used). - clusterprofiler: `clusterProfiler` style enrichment analysis (hypergeometric test will be used). - clusterProfiler: alias for `clusterprofiler` assay: The assay to use. error (flag): Error out if no/not enough markers are found or no pathways are enriched. If `False`, empty results will be returned. subset: An expression to subset the cells for each case. cache (type=auto): Where to cache the results. If `True`, cache to `outdir` of the job. If `False`, don't cache. Otherwise, specify the directory to cache to. rest (ns): Rest arguments for `Seurat::FindMarkers()`. Use `-` to replace `.` in the argument name. For example, use `min-pct` instead of `min.pct`. - <more>: See <https://satijalab.org/seurat/reference/findmarkers> allmarker_plots_defaults (ns): Default options for the plots for all markers when `ident-1` is not specified. - plot_type: The type of the plot. See <https://pwwang.github.io/biopipen.utils.R/reference/VizDEGs.html>. Available types are `violin`, `box`, `bar`, `ridge`, `dim`, `heatmap` and `dot`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: Other arguments passed to [`biopipen.utils::VizDEGs()`](https://pwwang.github.io/biopipen.utils.R/reference/VizDEGs.html). allmarker_plots (type=json): All marker plot cases. The keys are the names of the cases and the values are the dicts inherited from `allmarker_plots_defaults`. allenrich_plots_defaults (ns): Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: See <https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html>. allenrich_plots (type=json): Cases of the plots to generate for the enrichment analysis. The keys are the names of the cases and the values are the dicts inherited from `allenrich_plots_defaults`. The cases under `envs.cases` can inherit this options. marker_plots_defaults (ns): Default options for the plots to generate for the markers. - plot_type: The type of the plot. See <https://pwwang.github.io/biopipen.utils.R/reference/VizDEGs.html>. Available types are `violin`, `box`, `bar`, `ridge`, `dim`, `heatmap` and `dot`. There are two additional types available - `volcano_pct` and `volcano_log2fc`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: Other arguments passed to [`biopipen.utils::VizDEGs()`](https://pwwang.github.io/biopipen.utils.R/reference/VizDEGs.html). If `plot_type` is `volcano_pct` or `volcano_log2fc`, they will be passed to [`scplotter::VolcanoPlot()`](https://pwwang.github.io/plotthis/reference/VolcanoPlot.html). marker_plots (type=json): Cases of the plots to generate for the markers. Plot cases. The keys are the names of the cases and the values are the dicts inherited from `marker_plots_defaults`. The cases under `envs.cases` can inherit this options. enrich_plots_defaults (ns): Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See <https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html>. Available types are `bar`, `dot`, `lollipop`, `network`, `enrichmap` and `wordcloud`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: See <https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html>. enrich_plots (type=json): Cases of the plots to generate for the enrichment analysis. The keys are the names of the cases and the values are the dicts inherited from `enrich_plots_defaults`. The cases under `envs.cases` can inherit this options. overlaps_defaults (ns): Default options for investigating the overlapping of significant markers between different cases or comparisons. This means either `ident-1` should be empty, so that they can be expanded to multiple comparisons. - sigmarkers: The expression to filter the significant markers for each case. If not provided, `envs.sigmarkers` will be used. - plot_type (choice): The type of the plot to generate for the overlaps. - venn: Use `plotthis::VennDiagram()`. - upset: Use `plotthis::UpsetPlot()`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: More arguments pased to `plotthis::VennDiagram()` (<https://pwwang.github.io/plotthis/reference/venndiagram1.html>) or `plotthis::UpsetPlot()` (<https://pwwang.github.io/plotthis/reference/upsetplot1.html>) overlaps (type=json): Cases for investigating the overlapping of significant markers between different cases or comparisons. The keys are the names of the cases and the values are the dicts inherited from `overlaps_defaults`. There are two situations that we can perform overlaps: 1. If `ident-1` is not specified, the overlaps can be performed between different comparisons. 2. If `each` is specified, the overlaps can be performed between different cases, where in each case, `ident-1` must be specified. cases (type=json): If you have multiple cases for marker discovery, you can specify them here. The keys are the names of the cases and the values are the above options. If some options are not specified, the default values specified above (under `envs`) will be used. If no cases are specified, the default case will be added with the default values under `envs` with the name `Marker Discovery`. \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outdir:dir:{{in.srtobj | stem0}}.markers\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"mutaters\" : {}, \"group_by\" : None , \"ident_1\" : None , \"ident_2\" : None , \"each\" : None , \"dbs\" : [ \"KEGG_2021_Human\" , \"MSigDB_Hallmark_2020\" ], \"sigmarkers\" : \"p_val_adj < 0.05\" , \"enrich_style\" : \"enrichr\" , \"assay\" : None , \"error\" : False , \"subset\" : None , \"cache\" : config . path . tmpdir , \"rest\" : {}, \"allmarker_plots_defaults\" : { \"plot_type\" : None , \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, }, \"allmarker_plots\" : {}, \"allenrich_plots_defaults\" : { \"plot_type\" : \"heatmap\" , \"devpars\" : { \"res\" : 100 }, }, \"allenrich_plots\" : {}, \"marker_plots_defaults\" : { \"plot_type\" : None , \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, }, \"marker_plots\" : { \"Volcano Plot (diff_pct)\" : { \"plot_type\" : \"volcano_pct\" }, \"Volcano Plot (log2FC)\" : { \"plot_type\" : \"volcano_log2fc\" }, \"Dot Plot\" : { \"plot_type\" : \"dot\" }, }, \"enrich_plots_defaults\" : { \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, }, \"enrich_plots\" : { \"Bar Plot\" : { \"plot_type\" : \"bar\" , \"ncol\" : 1 , \"top_term\" : 10 }, }, \"overlaps_defaults\" : { \"sigmarkers\" : None , \"plot_type\" : \"venn\" , \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, }, \"overlaps\" : {}, \"cases\" : {}, } order = 5 script = \"file://../scripts/scrna/MarkersFinder.R\" plugin_opts = { \"report\" : \"file://../reports/scrna/MarkersFinder.svelte\" , \"report_paging\" : 8 , } class TopExpressingGenes ( Proc ): DOCS \"\"\"Find the top expressing genes in each cluster Input: srtobj: The seurat object in RDS or qs/qs2 format Output: outdir: The output directory for the tables and plots Envs: mutaters (type=json): The mutaters to mutate the metadata. You can also use the clone selectors to select the TCR clones/clusters. See <https://pwwang.github.io/scplotter/reference/clone_selectors.html>. ident: The group of cells to find the top expressing genes. The cells will be selected by the `group_by` column with this `ident` value in metadata. If not provided, the top expressing genes will be found for all groups of cells in the `group_by` column. group_by: The column name in metadata to group the cells. each: The column name in metadata to separate the cells into different cases. dbs (list): The dbs to do enrichment analysis for significant markers See below for all libraries. <https://maayanlab.cloud/Enrichr/#libraries> n (type=int): The number of top expressing genes to find. enrich_style (choice): The style of the enrichment analysis. The enrichment analysis will be done by `EnrichIt()` from [`enrichit`](https://pwwang.github.io/enrichit/). Two styles are available: - enrichr: `enrichr` style enrichment analysis (fisher's exact test will be used). - clusterprofiler: `clusterProfiler` style enrichment analysis (hypergeometric test will be used). - clusterProfiler: alias for `clusterprofiler` enrich_plots_defaults (ns): Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See <https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html>. Available types are `bar`, `dot`, `lollipop`, `network`, `enrichmap` and `wordcloud`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: See <https://pwwang.github.io/scplotter/reference/EnrichmentPlot.htmll>. enrich_plots (type=json): Cases of the plots to generate for the enrichment analysis. The keys are the names of the cases and the values are the dicts inherited from `enrich_plots_defaults`. The cases under `envs.cases` can inherit this options. subset: An expression to subset the cells for each case. cases (type=json): If you have multiple cases, you can specify them here. The keys are the names of the cases and the values are the above options except `mutaters`. If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the default values under `envs` with the name `Top Expressing Genes`. \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outdir:dir:{{in.srtobj | stem}}.top_expressing_genes\" lang = config . lang . rscript script = \"file://../scripts/scrna/TopExpressingGenes.R\" envs = { \"mutaters\" : {}, \"ident\" : None , \"group_by\" : None , \"each\" : None , \"dbs\" : [ \"KEGG_2021_Human\" , \"MSigDB_Hallmark_2020\" ], \"n\" : 250 , \"subset\" : None , \"enrich_style\" : \"enrichr\" , \"enrich_plots_defaults\" : { \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, }, \"enrich_plots\" : { \"Bar Plot\" : { \"plot_type\" : \"bar\" , \"ncol\" : 1 , \"top_term\" : 10 }, }, \"cases\" : {}, } plugin_opts = { \"report\" : \"file://../reports/common.svelte\" , \"report_paging\" : 8 , } class ExprImputation ( Proc ): DOCS \"\"\"This process imputes the dropout values in scRNA-seq data. It takes the Seurat object as input and outputs the Seurat object with imputed expression data. Reference: - [Linderman, George C., Jun Zhao, and Yuval Kluger. \"Zero-preserving imputation of scRNA-seq data using low-rank approximation.\" BioRxiv (2018): 397588.](https://www.nature.com/articles/s41467-021-27729-z) - [Li, Wei Vivian, and Jingyi Jessica Li. \"An accurate and robust imputation method scImpute for single-cell RNA-seq data.\" Nature communications 9.1 (2018): 997.](https://www.nature.com/articles/s41467-018-03405-7) - [Dijk, David van, et al. \"MAGIC: A diffusion-based imputation method reveals gene-gene interactions in single-cell RNA-sequencing data.\" BioRxiv (2017): 111591.](https://www.cell.com/cell/abstract/S0092-8674(18)30724-4) Input: infile: The input file in RDS/qs format of Seurat object Output: outfile: The output file in RDS format of Seurat object Note that with rmagic and alra, the original default assay will be renamed to `RAW` and the imputed RNA assay will be renamed to `RNA` and set as default assay. Envs: tool (choice): Either alra, scimpute or rmagic - alra: Use RunALRA() from Seurat - scimpute: Use scImpute() from scimpute - rmagic: Use magic() from Rmagic scimpute_args (ns): The arguments for scimpute - drop_thre (type=float): The dropout threshold - kcluster (type=int): Number of clusters to use - ncores (type=int): Number of cores to use - refgene: The reference gene file rmagic_args (ns): The arguments for rmagic - python: The python path where magic-impute is installed. - threshold (type=float): The threshold for magic imputation. Only the genes with dropout rates greater than this threshold (No. of cells with non-zero expression / total number of cells) will be imputed. alra_args (type=json): The arguments for `RunALRA()` Requires: r-scimpute: - if: {{proc.envs.tool == \"scimpute\"}} - check: {{proc.lang}} <(echo \"library(scImpute)\") r-rmagic: - if: {{proc.envs.tool == \"rmagic\"}} - check: | {{proc.lang}} <(\\ echo \"\\ tryCatch(\\ { setwd(dirname(Sys.getenv('CONDA_PREFIX'))) }, \\ error = function(e) NULL \\ ); \\ library(Rmagic)\\ \"\\ ) magic-impute: - if: {{proc.envs.tool == \"rmagic\"}} - check: {{proc.envs.rmagic_args.python}} -c \"import magic\") r-dplyr: - if: {{proc.envs.tool == \"scimpute\"}} - check: {{proc.lang}} <(echo \"library(dplyr)\") r-seurat: - check: {{proc.lang}} <(echo \"library(Seurat)\") r-seuratwrappers: - if: {{proc.envs.tool == \"alra\"}} - check: {{proc.lang}} <(echo \"library(SeuratWrappers)\") \"\"\" # noqa: E501 input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.imputed.qs\" lang = config . lang . rscript envs = { \"tool\" : \"alra\" , \"rmagic_args\" : { \"python\" : config . exe . magic_python , \"threshold\" : 0.5 }, \"scimpute_args\" : { \"drop_thre\" : 0.5 , \"kcluster\" : None , \"ncores\" : config . misc . ncores , \"refgene\" : config . ref . refgene , }, \"alra_args\" : {}, } script = \"file://../scripts/scrna/ExprImputation.R\" class SCImpute ( Proc ): DOCS \"\"\"Impute the dropout values in scRNA-seq data. Deprecated. Use `ExprImputation` instead. Input: infile: The input file for imputation Either a SeuratObject or a matrix of count/TPM groupfile: The file to subset the matrix or label the cells Could be an output from ImmunarchFilter Output: outfile: The output matrix Envs: infmt: The input format. Either `seurat` or `matrix` \"\"\" input = \"infile:file, groupfile:file\" output = [ \"outfile:file:{{in.infile | stem | replace: '.seurat', ''}}.\" \"{{envs.outfmt}}\" ] lang = config . lang . rscript envs = { \"infmt\" : \"seurat\" , # or matrix \"outfmt\" : \"txt\" , # or csv, rds \"type\" : \"count\" , # or TPM \"drop_thre\" : 0.5 , \"kcluster\" : None , # or Kcluster \"ncores\" : config . misc . ncores , \"refgene\" : config . ref . refgene , } script = \"file://../scripts/scrna/SCImpute.R\" class SeuratFilter ( Proc ): DOCS \"\"\"Filtering cells from a seurat object Input: srtobj: The seurat object in RDS filters: The filters to apply. Could be a file or string in TOML, or a python dictionary, with following keys: - mutaters: Create new columns in the metadata - filter: A R expression that will pass to `subset(sobj, subset = ...)` to filter the cells Output: outfile: The filtered seurat object in RDS Envs: invert: Invert the selection? Requires: r-seurat: - check: {{proc.lang}} <(echo \"library('Seurat')\") r-dplyr: - check: {{proc.lang}} <(echo \"library('dplyr')\") \"\"\" input = \"srtobj:file, filters:var\" output = \"outfile:file:{{in.srtobj | stem}}.filtered.RDS\" lang = config . lang . rscript envs = { \"invert\" : False } script = \"file://../scripts/scrna/SeuratFilter.R\" class SeuratSubset ( Proc ): DOCS \"\"\"Subset a seurat object into multiple seruat objects Input: srtobj: The seurat object in RDS subsets: The subsettings to apply. Could be a file or string in TOML, or a python dictionary, with following keys: - <name>: Name of the case mutaters: Create new columns in the metadata subset: A R expression that will pass to `subset(sobj, subset = ...)` groupby: The column to group by, each value will be a case If groupby is given, subset will be ignored, each value of the groupby column will be a case Output: outdir: The output directory with the subset seurat objects Envs: ignore_nas: Ignore NA values? Requires: r-seurat: - check: {{proc.lang}} <(echo \"library('Seurat')\") r-dplyr: - check: {{proc.lang}} <(echo \"library('dplyr')\") \"\"\" input = \"srtobj:file, subsets:var\" output = \"outdir:dir:{{in.srtobj | stem}}.subsets\" envs = { \"ignore_nas\" : True } lang = config . lang . rscript script = \"file://../scripts/scrna/SeuratSubset.R\" class SeuratSplit ( Proc ): DOCS \"\"\"Split a seurat object into multiple seruat objects Input: srtobj: The seurat object in RDS by: The metadata column to split by Output: outdir: The output directory with the subset seurat objects Envs: by: The metadata column to split by Ignored if `by` is given in the input recell: Rename the cell ids using the `by` column A string of R function taking the original cell ids and `by` \"\"\" input = \"srtobj:file, by:var\" output = \"outdir:dir:{{in.srtobj | stem}}.subsets\" envs = { \"by\" : None , \"recell\" : None , # \"function(cellid, by) {}\", } lang = config . lang . rscript script = \"file://../scripts/scrna/SeuratSplit.R\" class Subset10X ( Proc ): DOCS \"\"\"Subset 10X data, mostly used for testing Requires r-matrix to load matrix.mtx.gz Input: indir: The input directory Output: outdir: The output directory Envs: seed: The seed for random number generator nfeats: The number of features to keep. If <=1 then it will be the percentage of features to keep ncells: The number of cells to keep. If <=1 then it will be the percentage of cells to keep feats_to_keep: The features/genes to keep. The final features list will be `feats_to_keep` + `nfeats` \"\"\" input = \"indir:dir\" output = \"outdir:dir:{{in.indir | stem}}\" envs = { \"seed\" : 8525 , \"nfeats\" : 0.1 , \"ncells\" : 0.1 , \"feats_to_keep\" : [], } lang = config . lang . rscript script = \"file://../scripts/scrna/Subset10X.R\" class SeuratTo10X ( Proc ): DOCS \"\"\"Write a Seurat object to 10X format using `write10xCounts` from `DropletUtils` Input: srtobj: The seurat object in RDS Output: outdir: The output directory. When `envs.split_by` is specified, the subdirectories will be created for each distinct value of the column. Otherwise, the matrices will be written to the output directory. Envs: version: The version of 10X format \"\"\" input = \"srtobj:file\" output = \"outdir:dir:{{in.srtobj | stem}}\" envs = { \"version\" : \"3\" , \"split_by\" : None } lang = config . lang . rscript script = \"file://../scripts/scrna/SeuratTo10X.R\" class ScFGSEA ( Proc ): DOCS \"\"\"Gene set enrichment analysis for cells in different groups using `fgsea` This process allows us to do Gene Set Enrichment Analysis (GSEA) on the expression data, but based on variaties of grouping, including the from the meta data and the scTCR-seq data as well. The GSEA is done using the [fgsea](https://bioconductor.org/packages/release/bioc/html/fgsea.html) package, which allows to quickly and accurately calculate arbitrarily low GSEA P-values for a collection of gene sets. The fgsea package is based on the fast algorithm for preranked GSEA described in [Subramanian et al. 2005](https://www.pnas.org/content/102/43/15545). For each case, the process will generate a table with the enrichment scores for each gene set, and GSEA plots for the top gene sets. Input: srtobj: The seurat object in RDS format Output: outdir: The output directory for the results and plots Envs: ncores (type=int): Number of cores for parallelization Passed to `nproc` of `fgseaMultilevel()`. mutaters (type=json): The mutaters to mutate the metadata. The key-value pairs will be passed the `dplyr::mutate()` to mutate the metadata. You can also use the clone selectors to select the TCR clones/clusters. See <https://pwwang.github.io/scplotter/reference/clone_selectors.html>. group_by: The column name in metadata to group the cells. ident_1: The first group of cells to compare ident_2: The second group of cells to compare, if not provided, the rest of the cells that are not `NA`s in `group_by` column are used for `ident-2`. each: The column name in metadata to separate the cells into different subsets to do the analysis. subset: An expression to subset the cells. gmtfile: The pathways in GMT format, with the gene names/ids in the same format as the seurat object. One could also use a URL to a GMT file. For example, from <https://download.baderlab.org/EM_Genesets/current_release/Human/symbol/Pathways/>. method (choice): The method to do the preranking. - signal_to_noise: Signal to noise. The larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \"class marker\". - s2n: Alias of signal_to_noise. - abs_signal_to_noise: The absolute value of signal_to_noise. - abs_s2n: Alias of abs_signal_to_noise. - t_test: T test. Uses the difference of means scaled by the standard deviation and number of samples. - ratio_of_classes: Also referred to as fold change. Uses the ratio of class means to calculate fold change for natural scale data. - diff_of_classes: Difference of class means. Uses the difference of class means to calculate fold change for nature scale data - log2_ratio_of_classes: Log2 ratio of class means. Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. top (type=auto): Do gsea table and enrich plot for top N pathways. If it is < 1, will apply it to `padj`, selecting pathways with `padj` < `top`. eps (type=float): This parameter sets the boundary for calculating the p value. See <https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html> alleach_plots_defaults (ns): Default options for the plots to generate for all pathways. - plot_type: The type of the plot, currently either dot or heatmap (default) - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: See <https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html>. alleach_plots (type=json): Cases of the plots to generate for all pathways. The keys are the names of the cases and the values are the dicts inherited from `alleach_plots_defaults`. minsize (type=int): Minimal size of a gene set to test. All pathways below the threshold are excluded. maxsize (type=int): Maximal size of a gene set to test. All pathways above the threshold are excluded. rest (type=json;order=98): Rest arguments for [`fgsea()`](https://rdrr.io/bioc/fgsea/man/fgsea.html) See also <https://rdrr.io/bioc/fgsea/man/fgseaMultilevel.html> cases (type=json;order=99): If you have multiple cases, you can specify them here. The keys are the names of the cases and the values are the above options except `mutaters`. If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the name `GSEA`. Requires: bioconductor-fgsea: - check: {{proc.lang}} -e \"library(fgsea)\" r-seurat: - check: {{proc.lang}} -e \"library(seurat)\" \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outdir:dir:{{(in.casefile or in.srtobj) | stem0}}.fgsea\" lang = config . lang . rscript envs = { \"mutaters\" : {}, \"ncores\" : config . misc . ncores , \"group_by\" : None , \"ident_1\" : None , \"ident_2\" : None , \"each\" : None , \"subset\" : None , \"gmtfile\" : \"KEGG_2021_Human\" , \"method\" : \"s2n\" , \"top\" : 20 , \"minsize\" : 10 , \"maxsize\" : 100 , \"eps\" : 0 , \"alleach_plots_defaults\" : { \"plot_type\" : \"heatmap\" , \"devpars\" : { \"res\" : 100 }, }, \"alleach_plots\" : {}, \"rest\" : {}, \"cases\" : {}, } script = \"file://../scripts/scrna/ScFGSEA.R\" plugin_opts = { \"report\" : \"file://../reports/common.svelte\" , \"report_paging\" : 8 , } class CellTypeAnnotation ( Proc ): DOCS \"\"\"Annotate the cell clusters. Currently, four ways are supported: 1. Pass the cell type annotation directly 2. Use [`ScType`](https://github.com/IanevskiAleksandr/sc-type) 3. Use [`scCATCH`](https://github.com/ZJUFanLab/scCATCH) 4. Use [`hitype`](https://github.com/pwwang/hitype) The annotated cell types will replace the original `seurat_clusters` column in the metadata, so that the downstream processes will use the annotated cell types. The old `seurat_clusters` column will be renamed to `seurat_clusters_id`. If you are using `ScType`, `scCATCH`, or `hitype`, a text file containing the mapping from the old `seurat_clusters` to the new cell types will be generated and saved to `cluster2celltype.tsv` under `<workdir>/<pipline_name>/CellTypeAnnotation/0/output/`. Examples: ```toml [CellTypeAnnotation.envs] tool = \"direct\" cell_types = [\"CellType1\", \"CellType2\", \"-\", \"CellType4\"] ``` The cell types will be assigned as: ``` 0 -> CellType1 1 -> CellType2 2 -> 2 3 -> CellType4 ``` Input: sobjfile: The single-cell object in RDS/qs/qs2/h5ad format. Output: outfile: The rds/qs/qs2/h5ad file of seurat object with cell type annotated. A text file containing the mapping from the old `seurat_clusters` to the new cell types will be generated and saved to `cluster2celltype.tsv` under the job output directory. Envs: tool (choice): The tool to use for cell type annotation. - sctype: Use `scType` to annotate cell types. See <https://github.com/IanevskiAleksandr/sc-type> - hitype: Use `hitype` to annotate cell types. See <https://github.com/pwwang/hitype> - sccatch: Use `scCATCH` to annotate cell types. See <https://github.com/ZJUFanLab/scCATCH> - celltypist: Use `celltypist` to annotate cell types. See <https://github.com/Teichlab/celltypist> - direct: Directly assign cell types sctype_tissue: The tissue to use for `sctype`. Avaiable tissues should be the first column (`tissueType`) of `sctype_db`. If not specified, all rows in `sctype_db` will be used. sctype_db: The database to use for sctype. Check examples at <https://github.com/IanevskiAleksandr/sc-type/blob/master/ScTypeDB_full.xlsx> hitype_tissue: The tissue to use for `hitype`. Avaiable tissues should be the first column (`tissueType`) of `hitype_db`. If not specified, all rows in `hitype_db` will be used. hitype_db: The database to use for hitype. Compatible with `sctype_db`. See also <https://pwwang.github.io/hitype/articles/prepare-gene-sets.html> You can also use built-in databases, including `hitypedb_short`, `hitypedb_full`, and `hitypedb_pbmc3k`. cell_types (list): The cell types to use for direct annotation. You can use `\"-\"` or `\"\"` as the placeholder for the clusters that you want to keep the original cell types (`seurat_clusters`). If the length of `cell_types` is shorter than the number of clusters, the remaining clusters will be kept as the original cell types. You can also use `NA` to remove the clusters from downstream analysis. This only works when `envs.newcol` is not specified. /// Note If `tool` is `direct` and `cell_types` is not specified or an empty list, the original cell types will be kept and nothing will be changed. /// more_cell_types (type=json): The additional cell type annotations to add to the metadata. The keys are the new column names and the values are the cell types lists. The cell type lists work the same as `cell_types` above. This is useful when you want to keep multiple annotations of cell types. sccatch_args (ns): The arguments for `scCATCH::findmarkergene()` if `tool` is `sccatch`. - species: The specie of cells. - cancer: If the sample is from cancer tissue, then the cancer type may be defined. - tissue: Tissue origin of cells must be defined. - marker: The marker genes for cell type identification. - if_use_custom_marker (flag): Whether to use custom marker genes. If `True`, no `species`, `cancer`, and `tissue` are needed. - <more>: Other arguments for [`scCATCH::findmarkergene()`](https://rdrr.io/cran/scCATCH/man/findmarkergene.html). You can pass an RDS file to `sccatch_args.marker` to work as custom marker. If so, `if_use_custom_marker` will be set to `TRUE` automatically. celltypist_args (ns): The arguments for `celltypist::celltypist()` if `tool` is `celltypist`. - model: The path to model file. - python: The python path where celltypist is installed. - majority_voting: When true, it refines cell identities within local subclusters after an over-clustering approach at the cost of increased runtime. - over_clustering (type=auto): The column name in metadata to use as clusters for majority voting. Set to `False` to disable over-clustering. When `in.sobjfile` is rds/qs/qs2 (supposing we have a Seurat object), the default ident is used by default. Otherwise, it is False by default. - assay: When converting a Seurat object to AnnData, the assay to use. If input is h5seurat, this defaults to RNA. If input is Seurat object in RDS, this defaults to the default assay. merge (flag): Whether to merge the clusters with the same cell types. Otherwise, a suffix will be added to the cell types (ie. `.1`, `.2`, etc). newcol: The new column name to store the cell types. If not specified, the `seurat_clusters` column will be overwritten. If specified, the original `seurat_clusters` column will be kept and `Idents` will be kept as the original `seurat_clusters`. outtype (choice): The output file type. Currently only works for `celltypist`. An RDS file will be generated for other tools. - input: Use the same file type as the input. - rds: Use RDS file. - qs: Use qs2 file. - qs2: Use qs2 file. - h5ad: Use AnnData file. Requires: r-HGNChelper: - if: {{proc.envs.tool == 'sctype'}} - check: {{proc.lang}} -e \"library(HGNChelper)\" r-seurat: - if: {{proc.envs.tool == 'sctype'}} - check: {{proc.lang}} -e \"library(Seurat)\" r-dplyr: - if: {{proc.envs.tool == 'sctype'}} - check: {{proc.lang}} -e \"library(dplyr)\" r-openxlsx: - if: {{proc.envs.tool == 'sctype'}} - check: {{proc.lang}} -e \"library(openxlsx)\" \"\"\" # noqa: E501 input = \"sobjfile:file\" output = ( \"outfile:file:\" \"{{in.sobjfile | stem}}.annotated.\" \"{{- ext0(in.sobjfile) if envs.outtype == 'input' else envs.outtype -}}\" ) lang = config . lang . rscript envs = { \"tool\" : \"hitype\" , \"sctype_tissue\" : None , \"sctype_db\" : config . ref . sctype_db , \"cell_types\" : [], \"more_cell_types\" : None , \"sccatch_args\" : { \"species\" : None , \"cancer\" : \"Normal\" , \"tissue\" : None , \"marker\" : None , \"if_use_custom_marker\" : False , }, \"hitype_tissue\" : None , \"hitype_db\" : None , \"celltypist_args\" : { \"model\" : None , \"python\" : config . lang . python , \"majority_voting\" : True , \"over_clustering\" : None , \"assay\" : None , }, \"merge\" : False , \"newcol\" : None , \"outtype\" : \"input\" , } script = \"file://../scripts/scrna/CellTypeAnnotation.R\" class SeuratMap2Ref ( Proc ): DOCS \"\"\"Map the seurat object to reference See: <https://satijalab.org/seurat/articles/integration_mapping.html> and <https://satijalab.org/seurat/articles/multimodal_reference_mapping.html> Input: sobjfile: The seurat object Output: outfile: The rds file of seurat object with cell type annotated. Note that the reduction name will be `ref.umap` for the mapping. To visualize the mapping, you should use `ref.umap` as the reduction name. Envs: ncores (type=int;order=-100): Number of cores to use. When `split_by` is used, this will be the number of cores for each object to map to the reference. When `split_by` is not used, this is used in `future::plan(strategy = \"multicore\", workers = <ncores>)` to parallelize some Seurat procedures. See also: <https://satijalab.org/seurat/archive/v3.0/future_vignette.html> mutaters (type=json): The mutaters to mutate the metadata. This is helpful when we want to create new columns for `split_by`. use: A column name of metadata from the reference (e.g. `celltype.l1`, `celltype.l2`) to transfer to the query as the cell types (ident) for downstream analysis. This field is required. If you want to transfer multiple columns, you can use `envs.MapQuery.refdata`. ident: The name of the ident for query transferred from `envs.use` of the reference. ref: The reference seurat object file. Either an RDS file or a h5seurat file that can be loaded by `Seurat::LoadH5Seurat()`. The file type is determined by the extension. `.rds` or `.RDS` for RDS file, `.h5seurat` or `.h5` for h5seurat file. refnorm (choice): Normalization method the reference used. The same method will be used for the query. - LogNormalize: Using [`NormalizeData`](https://satijalab.org/seurat/reference/normalizedata). - SCTransform: Using [`SCTransform`](https://satijalab.org/seurat/reference/sctransform). - SCT: Alias of SCTransform. - auto: Automatically detect the normalization method. If the default assay of reference is `SCT`, then `SCTransform` will be used. split_by: The column name in metadata to split the query into multiple objects. This helps when the original query is too large to process. skip_if_normalized: Skip normalization if the query is already normalized. Since the object is supposed to be generated by `SeuratPreparing`, it is already normalized. However, a different normalization method may be used. If the reference is normalized by the same method as the query, the normalization can be skipped. Otherwise, the normalization cannot be skipped. The normalization method used for the query set is determined by the default assay. If `SCT`, then `SCTransform` is used; otherwise, `NormalizeData` is used. You can set this to `False` to force re-normalization (with or without the arguments previously used). SCTransform (ns): Arguments for [`SCTransform()`](https://satijalab.org/seurat/reference/sctransform) - do-correct-umi (flag): Place corrected UMI matrix in assay counts layer? - do-scale (flag): Whether to scale residuals to have unit variance? - do-center (flag): Whether to center residuals to have mean zero? - <more>: See <https://satijalab.org/seurat/reference/sctransform>. Note that the hyphen (`-`) will be transformed into `.` for the keys. NormalizeData (ns): Arguments for [`NormalizeData()`](https://satijalab.org/seurat/reference/normalizedata) - normalization-method: Normalization method. - <more>: See <https://satijalab.org/seurat/reference/normalizedata>. Note that the hyphen (`-`) will be transformed into `.` for the keys. FindTransferAnchors (ns): Arguments for [`FindTransferAnchors()`](https://satijalab.org/seurat/reference/findtransferanchors) - normalization-method (choice): Name of normalization method used. - LogNormalize: Log-normalize the data matrix - SCT: Scale data using the SCTransform method - auto: Automatically detect the normalization method. See `envs.refnorm`. - reference-reduction: Name of dimensional reduction to use from the reference if running the pcaproject workflow. Optionally enables reuse of precomputed reference dimensional reduction. - <more>: See <https://satijalab.org/seurat/reference/findtransferanchors>. Note that the hyphen (`-`) will be transformed into `.` for the keys. MapQuery (ns): Arguments for [`MapQuery()`](https://satijalab.org/seurat/reference/mapquery) - reference-reduction: Name of reduction to use from the reference for neighbor finding - reduction-model: `DimReduc` object that contains the umap model. - refdata (type=json): Extra data to transfer from the reference to the query. - <more>: See <https://satijalab.org/seurat/reference/mapquery>. Note that the hyphen (`-`) will be transformed into `.` for the keys. cache (type=auto): Whether to cache the information at different steps. If `True`, the seurat object will be cached in the job output directory, which will be not cleaned up when job is rerunning. The cached seurat object will be saved as `<signature>.<kind>.RDS` file, where `<signature>` is the signature determined by the input and envs of the process. See <https://github.com/satijalab/seurat/issues/7849>, <https://github.com/satijalab/seurat/issues/5358> and <https://github.com/satijalab/seurat/issues/6748> for more details also about reproducibility issues. To not use the cached seurat object, you can either set `cache` to `False` or delete the cached file at `<signature>.RDS` in the cache directory. plots (type=json): The plots to generate. The keys are the names of the plots and the values are the arguments for the plot. The arguments will be passed to `biopipen.utils::VizSeuratMap2Ref()` to generate the plots. The plots will be saved to the output directory. See <https://pwwang.github.io/biopipen.utils.R/reference/VizSeuratMap2Ref.html>. Requires: r-seurat: - check: {{proc.lang}} -e \"library(Seurat)\" \"\"\" # noqa: E501 input = \"sobjfile:file\" output = \"outfile:file:{{in.sobjfile | stem}}.qs\" lang = config . lang . rscript envs_depth = 3 envs = { \"ncores\" : config . misc . ncores , \"use\" : None , \"ident\" : \"seurat_clusters\" , \"mutaters\" : {}, \"ref\" : None , \"refnorm\" : \"auto\" , \"split_by\" : None , \"skip_if_normalized\" : True , \"SCTransform\" : { \"do-correct-umi\" : False , \"do-scale\" : False , \"do-center\" : True , }, \"NormalizeData\" : { \"normalization-method\" : \"LogNormalize\" , }, \"FindTransferAnchors\" : { # \"reference-reduction\": \"spca\", }, \"MapQuery\" : { # \"reference-reduction\": \"spca\", # \"reduction-model\": \"wnn.umap\", \"refdata\" : { # \"celltype-l1\": \"celltype.l1\", # \"celltype-l2\": \"celltype.l2\", # \"predicted_ADT\": \"ADT\", }, }, \"cache\" : config . path . tmpdir , \"plots\" : { \"Mapped Identity\" : { \"features\" : \" {ident} : {use} \" , }, \"Mapping Score\" : { \"features\" : \" {ident} .score\" , }, }, } script = \"file://../scripts/scrna/SeuratMap2Ref.R\" plugin_opts = { \"report\" : \"file://../reports/common.svelte\" } class RadarPlots ( Proc ): DOCS \"\"\"Radar plots for cell proportion in different clusters. This process generates the radar plots for the clusters of T cells. It explores the proportion of cells in different groups (e.g. Tumor vs Blood) in different T-cell clusters. Examples: Let's say we have a metadata like this: | Cell | Source | Timepoint | seurat_clusters | | ---- | ------ | --------- | --------------- | | A | Blood | Pre | 0 | | B | Blood | Pre | 0 | | C | Blood | Post | 1 | | D | Blood | Post | 1 | | E | Tumor | Pre | 2 | | F | Tumor | Pre | 2 | | G | Tumor | Post | 3 | | H | Tumor | Post | 3 | With configurations: ```toml [RadarPlots.envs] by = \"Source\" ``` Then we will have a radar plots like this: ![Radar plots](https://pwwang.github.io/immunopipe/latest/processes/images/RadarPlots-default.png) We can use `each` to separate the cells into different cases: ```toml [RadarPlots.envs] by = \"Source\" each = \"Timepoint\" ``` Then we will have two radar plots, one for `Pre` and one for `Post`: ![Radar plots](https://pwwang.github.io/immunopipe/latest/processes/images/RadarPlots-each.png) Using `cluster_order` to change the order of the clusters and show only the first 3 clusters: ```toml [RadarPlots.envs] by = \"Source\" cluster_order = [\"2\", \"0\", \"1\"] breaks = [0, 50, 100] # also change the breaks ``` ![Radar plots cluster_order](https://pwwang.github.io/immunopipe/latest/processes/images/RadarPlots-cluster_order.png) /// Attention All the plots used in the examples are just for demonstration purpose. The real plots will have different appearance. /// Input: srtobj: The seurat object in RDS or qs/qs2 format Output: outdir: The output directory for the plots Envs: mutaters (type=json): Mutaters to mutate the metadata of the seurat object. Keys are the column names and values are the expressions to mutate the columns. These new columns will be used to define your cases. by: Which column to use to separate the cells in different groups. `NA`s will be ignored. For example, If you have a column named `Source` that marks the source of the cells, and you want to separate the cells into `Tumor` and `Blood` groups, you can set `by` to `Source`. The there will be two curves in the radar plot, one for `Tumor` and one for `Blood`. each: A column with values to separate all cells in different cases When specified, the case will be expanded to multiple cases for each value in the column. If specified, `section` will be ignored, and the case name will be used as the section name. prefix_each (flag): Whether to prefix the `each` column name to the values as the case/section name. breakdown: An additional column with groups to break down the cells distribution in each cluster. For example, if you want to see the distribution of the cells in each cluster in different samples. In this case, you should have multiple values in each `by`. These values won't be plotted in the radar plot, but a barplot will be generated with the mean value of each group and the error bar. test (choice): The test to use to calculate the p values. If there are more than 2 groups in `by`, the p values will be calculated pairwise group by group. Only works when `breakdown` is specified and `by` has 2 groups or more. - wilcox: Wilcoxon rank sum test - t: T test - none: No test will be performed order (list): The order of the values in `by`. You can also limit (filter) the values we have in `by`. For example, if column `Source` has values `Tumor`, `Blood`, `Spleen`, and you only want to plot `Tumor` and `Blood`, you can set `order` to `[\"Tumor\", \"Blood\"]`. This will also have `Tumor` as the first item in the legend and `Blood` as the second item. colors: The colors for the groups in `by`. If not specified, the default colors will be used. Multiple colors can be separated by comma (`,`). You can specify `biopipen` to use the `biopipen` palette. ident: The column name of the cluster information. cluster_order (list): The order of the clusters. You may also use it to filter the clusters. If not given, all clusters will be used. If the cluster names are integers, use them directly for the order, even though a prefix `Cluster` is added on the plot. breaks (list;itype=int): breaks of the radar plots, from 0 to 100. If not given, the breaks will be calculated automatically. direction (choice): Direction to calculate the percentages. - inter-cluster: the percentage of the cells in all groups in each cluster (percentage adds up to 1 for each cluster). - intra-cluster: the percentage of the cells in all clusters. (percentage adds up to 1 for each group). section: If you want to put multiple cases into a same section in the report, you can set this option to the name of the section. Only used in the report. subset: The subset of the cells to do the analysis. bar_devpars (ns): The parameters for `png()` for the barplot - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot devpars (ns): The parameters for `png()` - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot cases (type=json): The cases for the multiple radar plots. Keys are the names of the cases and values are the arguments for the plots (`each`, `by`, `order`, `breaks`, `direction`, `ident`, `cluster_order` and `devpars`). If not cases are given, a default case will be used, with the key `DEFAULT`. The keys must be valid string as part of the file name. \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outdir:dir:{{in.srtobj | stem}}.radar_plots\" lang = config . lang . rscript script = \"file://../scripts/scrna/RadarPlots.R\" envs = { \"mutaters\" : {}, \"by\" : None , \"each\" : None , \"prefix_each\" : True , \"order\" : None , \"colors\" : \"biopipen\" , \"ident\" : \"seurat_clusters\" , \"cluster_order\" : [], \"breakdown\" : None , \"test\" : \"wilcox\" , \"breaks\" : [], \"direction\" : \"intra-cluster\" , \"section\" : \"DEFAULT\" , \"subset\" : None , \"bar_devpars\" : { \"res\" : 100 , \"width\" : 1200 , \"height\" : 800 , }, \"devpars\" : { \"res\" : 100 , \"width\" : 1200 , \"height\" : 1000 , }, \"cases\" : {}, } plugin_opts = { \"report\" : \"file://../reports/scrna/RadarPlots.svelte\" , } @mark ( deprecated = True ) DOCS class MetaMarkers ( Proc ): \"\"\"Find markers between three or more groups of cells, using one-way ANOVA or Kruskal-Wallis test. Sometimes, you may want to find the markers for cells from more than 2 groups. In this case, you can use this process to find the markers for the groups and do enrichment analysis for the markers. Each marker is examined using either one-way ANOVA or Kruskal-Wallis test. The p values are adjusted using the specified method. The significant markers are then used for enrichment analysis using [enrichr](https://maayanlab.cloud/Enrichr/) api. Other than the markers and the enrichment analysis as outputs, this process also generates violin plots for the top 10 markers. Input: srtobj: The seurat object loaded by `SeuratPreparing` Output: outdir: The output directory for the markers Envs: ncores (type=int): Number of cores to use to parallelize for genes mutaters (type=json): The mutaters to mutate the metadata The key-value pairs will be passed the `dplyr::mutate()` to mutate the metadata. group-by: The column name in metadata to group the cells. If only `group-by` is specified, and `idents` are not specified, markers will be found for all groups in this column. `NA` group will be ignored. idents: The groups of cells to compare, values should be in the `group-by` column. each: The column name in metadata to separate the cells into different cases. prefix_each (flag): Whether to add the `each` value as prefix to the case name. dbs (list): The dbs to do enrichment analysis for significant markers See below for all libraries. <https://maayanlab.cloud/Enrichr/#libraries> subset: The subset of the cells to do the analysis. An expression passed to `dplyr::filter()`. p_adjust (choice): The method to adjust the p values, which can be used to filter the significant markers. See also <https://rdrr.io/r/stats/p.adjust.html> - holm: Holm-Bonferroni method - hochberg: Hochberg method - hommel: Hommel method - bonferroni: Bonferroni method - BH: Benjamini-Hochberg method - BY: Benjamini-Yekutieli method - fdr: FDR method of Benjamini-Hochberg - none: No adjustment sigmarkers: An expression passed to `dplyr::filter()` to filter the significant markers for enrichment analysis. The default is `p.value < 0.05`. If `method = 'anova'`, the variables that can be used for filtering are: `sumsq`, `meansq`, `statistic`, `p.value` and `p_adjust`. If `method = 'kruskal'`, the variables that can be used for filtering are: `statistic`, `p.value` and `p_adjust`. section: The section name for the report. Worked only when `each` is not specified. Otherwise, the section name will be constructed from `each` and `group-by`. If `DEFAULT`, and it's the only section, it not included in the case/section names. method (choice): The method for the test. - anova: One-way ANOVA - kruskal: Kruskal-Wallis test cases (type=json): If you have multiple cases, you can specify them here. The keys are the names of the cases and the values are the above options except `ncores` and `mutaters`. If some options are not specified, the default values specified above will be used. If no cases are specified, the default case will be added with the default values under `envs` with the name `DEFAULT`. \"\"\" # noqa: E501 input = \"srtobj:file\" output = \"outdir:dir:{{in.srtobj | stem}}.meta_markers\" lang = config . lang . rscript script = \"file://../scripts/scrna/MetaMarkers.R\" envs = { \"ncores\" : config . misc . ncores , \"mutaters\" : {}, \"group-by\" : None , \"idents\" : None , \"each\" : None , \"subset\" : None , \"prefix_each\" : True , \"p_adjust\" : \"BH\" , \"dbs\" : [ \"KEGG_2021_Human\" , \"MSigDB_Hallmark_2020\" ], \"sigmarkers\" : \"p_adjust < 0.05\" , \"section\" : \"DEFAULT\" , \"method\" : \"anova\" , \"cases\" : {}, } plugin_opts = { \"report\" : \"file://../reports/scrna/MetaMarkers.svelte\" , \"report_paging\" : 8 , } class Seurat2AnnData ( Proc ): DOCS \"\"\"Convert seurat object to AnnData Input: sobjfile: The seurat object file, in RDS or qs/qs2 format Output: outfile: The AnnData file Envs: assay: The assay to use for AnnData. If not specified, the default assay will be used. \"\"\" input = \"sobjfile:file\" output = \"outfile:file:{{in.sobjfile | stem}}.h5ad\" lang = config . lang . rscript script = \"file://../scripts/scrna/Seurat2AnnData.R\" envs = { \"assay\" : None } class AnnData2Seurat ( Proc ): DOCS \"\"\"Convert AnnData to seurat object Input: adfile: The AnnData .h5ad file Output: outfile: The seurat object file in RDS or qs/qs2 format Envs: assay: The assay to use to convert to seurat object. dotplot_check (type=auto): Whether to do a check with a dot plot. (`scplotter::FeatureStatPlot(plot_type = \"dot\", ..)` will be used) to see if the conversion is successful. Set to `False` to disable the check. If `True`, top 10 variable genes will be used for the check. You can give a list of genes or a string of genes with comma (`,`) separated to use for the check. \"\"\" input = \"adfile:file\" output = \"outfile:file:{{in.adfile | stem}}.qs\" lang = config . lang . rscript envs = { \"assay\" : \"RNA\" , \"dotplot_check\" : True } script = \"file://../scripts/scrna/AnnData2Seurat.R\" class ScSimulation ( Proc ): DOCS \"\"\"Simulate single-cell data using splatter. See <https://www.bioconductor.org/packages/devel/bioc/vignettes/splatter/inst/doc/splatter.html#2_Quickstart> Input: seed: The seed for the simulation You could also use string as the seed, and the seed will be generated by `digest::digest2int()`. So this could also work as a unique identifier for the simulation (ie. Sample ID). Output: outfile: The output Seurat object/SingleCellExperiment in qs/qs2 format Envs: ngenes (type=int): The number of genes to simulate ncells (type=int): The number of cells to simulate nspikes (type=int): The number of spike-ins to simulate When `ngenes`, `ncells`, and `nspikes` are not specified, the default params from `mockSCE()` will be used. By default, `ngenes = 2000`, `ncells = 200`, and `nspikes = 100`. outtype (choice): The output file type. - seurat: Seurat object - singlecellexperiment: SingleCellExperiment object - sce: alias for `singlecellexperiment` method (choice): which simulation method to use. Options are: - single: produces a single population - groups: produces distinct groups (eg. cell types), or - paths: selects cells from continuous trajectories (eg. differentiation processes) params (ns): Other parameters for simulation. The parameters are initialized `splitEstimate(mockSCE())` and then updated with the given parameters. See <https://rdrr.io/bioc/splatter/man/SplatParams.html>. Hyphens (`-`) will be transformed into dots (`.`) for the keys. \"\"\" # noqa: E501 input = \"seed:var\" output = \"outfile:file:simulatied_{{in.seed}}.RDS\" lang = config . lang . rscript envs = { \"ngenes\" : None , \"ncells\" : None , \"nspikes\" : None , \"outtype\" : \"seurat\" , \"method\" : \"single\" , \"params\" : {}, } script = \"file://../scripts/scrna/ScSimulation.R\" class CellCellCommunication ( Proc ): DOCS \"\"\"Cell-cell communication inference This is implemented based on [LIANA](https://liana-py.readthedocs.io/en/latest/index.html), which is a Python package for cell-cell communication inference and provides a list of existing methods including [CellPhoneDB](https://github.com/ventolab/CellphoneDB), [Connectome](https://github.com/msraredon/Connectome/), log2FC, [NATMI](https://github.com/forrest-lab/NATMI), [SingleCellSignalR](https://github.com/SCA-IRCM/SingleCellSignalR), Rank_Aggregate, Geometric Mean, [scSeqComm](https://gitlab.com/sysbiobig/scseqcomm), and [CellChat](https://github.com/jinworks/CellChat). You can also try `python -c 'import liana; liana.mt.show_methods()'` to see the methods available. Note that this process does not do any visualization. You can use `CellCellCommunicationPlots` to visualize the results. Reference: - [Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9184522/). - [LIANA](https://www.biorxiv.org/content/10.1101/2023.08.19.553863v1). Input: sobjfile: The seurat object file in RDS or h5seurat format or AnnData file. Output: outfile: The output file with the 'liana_res' data frame. Stats are provided for both ligand and receptor entities, more specifically: ligand and receptor are the two entities that potentially interact. As a reminder, CCC events are not limited to secreted signalling, but we refer to them as ligand and receptor for simplicity. Also, in the case of heteromeric complexes, the ligand and receptor columns represent the subunit with minimum expression, while *_complex corresponds to the actual complex, with subunits being separated by _. source and target columns represent the source/sender and target/receiver cell identity for each interaction, respectively * `*_props`: represents the proportion of cells that express the entity. By default, any interactions in which either entity is not expressed in above 10%% of cells per cell type is considered as a false positive, under the assumption that since CCC occurs between cell types, a sufficient proportion of cells within should express the genes. * `*_means`: entity expression mean per cell type. * `lr_means`: mean ligand-receptor expression, as a measure of ligand-receptor interaction magnitude. * `cellphone_pvals`: permutation-based p-values, as a measure of interaction specificity. Envs: method (choice): The method to use for cell-cell communication inference. - CellPhoneDB: Use CellPhoneDB method. Magnitude Score: lr_means; Specificity Score: cellphone_pvals. - Connectome: Use Connectome method. - log2FC: Use log2FC method. - NATMI: Use NATMI method. - SingleCellSignalR: Use SingleCellSignalR method. - Rank_Aggregate: Use Rank_Aggregate method. - Geometric_Mean: Use Geometric Mean method. - scSeqComm: Use scSeqComm method. - CellChat: Use CellChat method. - cellphonedb: alias for `CellPhoneDB` - connectome: alias for `Connectome` - log2fc: alias for `log2FC` - natmi: alias for `NATMI` - singlesignaler: alias for `SingleCellSignalR` - rank_aggregate: alias for `Rank_Aggregate` - geometric_mean: alias for `Geometric_Mean` - scseqcomm: alias for `scSeqComm` - cellchat: alias for `CellChat` subset: An expression in string to subset the cells. When a `.rds` or `.h5seurat` file is provided for `in.sobjfile`, you can provide an expression in `R`, which will be passed to `base::subset()` in `R` to subset the cells. But you can always pass an expression in `python` to subset the cells. See <https://anndata.readthedocs.io/en/latest/tutorials/notebooks/getting-started.html#subsetting-using-metadata>. You should use `adata` to refer to the AnnData object. For example, `adata.obs.groups == \"g1\"` will subset the cells with `groups` equal to `g1`. subset_using: The method to subset the cells. - auto: Automatically detect the method to use. Note that this is not always accurate. We simply check if `[` is in the expression. If so, we use `python` to subset the cells; otherwise, we use `R`. - python: Use python to subset the cells. - r: Use R to subset the cells. split_by: The column name in metadata to split the cells to run the method separately. The results will be combined together with this column in the final output. assay: The assay to use for the analysis. Only works for Seurat object. seed (type=int): The seed for the random number generator. ncores (type=int): The number of cores to use. groupby: The column name in metadata to group the cells. Typically, this column should be the cluster id. species (choice): The species of the cells. - human: Human cells, the 'consensus' resource will be used. - mouse: Mouse cells, the 'mouseconsensus' resource will be used. expr_prop (type=float): Minimum expression proportion for the ligands and receptors (+ their subunits) in the corresponding cell identities. Set to 0 to return unfiltered results. min_cells (type=int): Minimum cells (per cell identity if grouped by `groupby`) to be considered for downstream analysis. n_perms (type=int): Number of permutations for the permutation test. Relevant only for permutation-based methods (e.g., `CellPhoneDB`). If `0` is passed, no permutation testing is performed. rscript: The path to the Rscript executable used to convert RDS file to AnnData. if `in.sobjfile` is an RDS file, it will be converted to AnnData file (h5ad). You need `Seurat`, `SeuratDisk` and `digest` installed. <more>: Other arguments for the method. The arguments are passed to the method directly. See the method documentation for more details and also `help(liana.mt.<method>.__call__)` in Python. \"\"\" # noqa: E501 input = \"sobjfile:file\" output = \"outfile:file:{{in.sobjfile | stem}}-ccc.txt\" lang = config . lang . python envs = { \"method\" : \"cellchat\" , \"assay\" : None , \"seed\" : 1337 , \"subset\" : None , \"subset_using\" : \"auto\" , \"split_by\" : None , \"ncores\" : config . misc . ncores , \"groupby\" : \"seurat_clusters\" , \"species\" : \"human\" , \"expr_prop\" : 0.1 , \"min_cells\" : 5 , \"n_perms\" : 1000 , \"rscript\" : config . lang . rscript , } script = \"file://../scripts/scrna/CellCellCommunication.py\" class CellCellCommunicationPlots ( Proc ): DOCS \"\"\"Visualization for cell-cell communication inference. Input: cccfile: The output file from `CellCellCommunication` Output: outdir: The output directory for the plots. Envs: subset: An expression to pass to `dplyr::filter()` to subset the ccc data. magnitude: The column name in the data to use as the magnitude of the communication. By default, the second last column will be used. See `li.mt.show_methods()` for the available methods in LIANA. or <https://liana-py.readthedocs.io/en/latest/notebooks/basic_usage.html#Tileplot> specificity: The column name in the data to use as the specificity of the communication. By default, the last column will be used. If the method doesn't have a specificity, set it to None. devpars (ns): The parameters for the plot. - res (type=int): The resolution of the plot - height (type=int): The height of the plot - width (type=int): The width of the plot more_formats (type=list): The additional formats to save the plots. descr: The description of the plot. cases (type=json): The cases for the plots. The keys are the names of the cases and the values are the arguments for the plots. The arguments include the ones inherited from `envs`. You can have a special `plot_type` `\"table\"` to generate a table for the ccc data to save as a text file and show in the report. If no cases are given, a default case will be used, with the key `Cell-Cell Communication`. <more>: Other arguments passed to [scplotter::CCCPlot](https://pwwang.github.io/scplotter/reference/CCCPlot.html) \"\"\" # noqa: E501 input = \"cccfile:file\" output = \"outdir:dir:{{in.cccfile | stem}}_plots\" lang = config . lang . rscript envs = { \"subset\" : None , \"magnitude\" : None , \"specificity\" : None , \"devpars\" : { \"res\" : 100 }, \"more_formats\" : [], \"descr\" : \"Cell-cell communication plot\" , \"cases\" : {}, } script = \"file://../scripts/scrna/CellCellCommunicationPlots.R\" plugin_opts = { \"report\" : \"file://../reports/common.svelte\" , } class ScVelo ( Proc ): DOCS \"\"\"Velocity analysis for single-cell RNA-seq data This process is implemented based on the Python package `scvelo` (v0.3.3). Note that it doesn't work with `numpy>=2`. Input: sobjfile: The seurat object file in RDS or h5seurat format or AnnData file. Output: outfile: The output object with the velocity embeddings and information. In either RDS, h5seurat or h5ad format, depending on the `envs.outtype`. There will be also plots generated in the output directory (parent directory of `outfile`). Note that these plots will not be used in the report, but can be used as supplementary information for the velocity analysis. To visualize the velocity embeddings, you can use the `SeuratClusterStats` process with `v_reduction` provided to one of the `envs.dimplots`. Envs: ncores (type=int): Number of cores to use. group_by: The column name in metadata to group the cells. Typically, this column should be the cluster id. mode (type=list): The mode to use for the velocity analysis. It should be a subset of `['deterministic', 'stochastic', 'dynamical']`, meaning that we can perform the velocity analysis in multiple modes. fitting_by (choice): The mode to use for fitting the velocities. - stochastic: Stochastic mode - deterministic: Deterministic mode min_shared_counts (type=int): Minimum number of counts (both unspliced and spliced) required for a gene. n_neighbors (type=int): The number of neighbors to use for the velocity graph. n_pcs (type=int): The number of PCs to use for the velocity graph. denoise (flag): Whether to denoise the data. denoise_topn (type=int): Number of genes with highest likelihood selected to infer velocity directions. kinetics (flag): Whether to compute the RNA velocity kinetics. kinetics_topn (type=int): Number of genes with highest likelihood selected to infer velocity directions. calculate_velocity_genes (flag): Whether to calculate the velocity genes. top_n (type=int): The number of top features to plot. rscript: The path to the Rscript executable used to convert RDS file to AnnData. if `in.sobjfile` is an RDS file, it will be converted to AnnData file (h5ad). You need `Seurat`, `SeuratDisk` and `digest` installed. outtype (choice): The output file type. - <input>: The same as the input file type. - h5seurat: h5seurat file - h5ad: h5ad file - qs: qs/qs2 file - qs2: qs2 file - rds: RDS file \"\"\" input = \"sobjfile:file\" output = ( \"outfile:file:{{in.sobjfile | stem}}-scvelo.\" \"{{ext0(in.sobjfile) if envs.outtype == '<input>' else envs.outtype}}\" ) lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"group_by\" : \"seurat_clusters\" , \"mode\" : [ \"deterministic\" , \"stochastic\" , \"dynamical\" ], \"fitting_by\" : \"stochastic\" , \"min_shared_counts\" : 30 , \"n_neighbors\" : 30 , \"n_pcs\" : 30 , \"denoise\" : False , \"denoise_topn\" : 3 , \"kinetics\" : False , \"kinetics_topn\" : 100 , \"calculate_velocity_genes\" : False , \"top_n\" : 6 , \"rscript\" : config . lang . rscript , \"outtype\" : \"<input>\" , } script = \"file://../scripts/scrna/ScVelo.py\" class Slingshot ( Proc ): DOCS \"\"\"Trajectory inference using Slingshot This process is implemented based on the R package `slingshot`. Input: sobjfile: The seurat object file in RDS or qs format. Output: outfile: The output object with the trajectory information. The lineages are stored in the metadata of the seurat object at columns `LineageX`, where X is the lineage number. The `BranchID` column contains the branch id for each cell. One can use `scplotter::CellDimPlot(object, lineages = c(\"Lineage1\", \"Lineage2\", ...))` to visualize the trajectories. Envs: group_by: The column name in metadata to group the cells. Typically, this column should be the cluster id. reduction: The nonlinear reduction to use for the trajectory analysis. dims (type=auto): The dimensions to use for the analysis. A list or a string with comma separated values. Consecutive numbers can be specified with a colon (`:`) or a dash (`-`). start: The starting group for the Slingshot analysis. end: The ending group for the Slingshot analysis. prefix: The prefix to add to the column names of the resulting pseudotime variable. reverse (flag): Logical value indicating whether to reverse the pseudotime variable. align_start (flag): Whether to align the starting pseudotime values at the maximum pseudotime. seed (type=int): The seed for the random number generator. \"\"\" # noqa: E501 input = \"sobjfile:file\" output = \"outfile:file:{{in.sobjfile | stem}}.qs\" lang = config . lang . rscript envs = { \"group_by\" : \"seurat_clusters\" , \"reduction\" : None , \"dims\" : [ 1 , 2 ], \"start\" : None , \"end\" : None , \"prefix\" : None , \"reverse\" : False , \"align_start\" : False , \"seed\" : 8525 , } script = \"file://../scripts/scrna/Slingshot.R\" class LoomTo10X ( Proc ): DOCS \"\"\"Convert Loom file to 10X format Input: loomfile: The Loom file Output: outdir: The output directory for the 10X format files, including the `matrix.mtx.gz`, `barcodes.tsv.gz` and `features.tsv.gz` files. \"\"\" input = \"loomfile:file\" output = \"outdir:dir:{{in.loomfile | stem}}.10X\" lang = config . lang . rscript script = \"file://../scripts/scrna/LoomTo10X.R\" class PseudoBulkDEG ( Proc ): DOCS \"\"\"Pseduo-bulk differential gene expression analysis This process performs differential gene expression analysis, instead of on single-cell level, on the pseudo-bulk data, aggregated from the single-cell data. Input: sobjfile: The seurat object file in RDS or qs/qs2 format. Output: outdir: The output containing the results of the differential gene expression analysis. Envs: ncores (type=int): Number of cores to use for parallelization. mutaters (type=json): Mutaters to mutate the metadata of the seurat object. Keys are the new column names and values are the expressions to mutate the columns. These new columns can be used to define your cases. You can also use the clone selectors to select the TCR clones/clusters. See <https://pwwang.github.io/scplotter/reference/clone_selectors.html>. each: The column name in metadata to separate the cells into different cases. When specified, the case will be expanded to multiple cases for each value in the column. cache (type=auto): Where to cache the results. If `True`, cache to `outdir` of the job. If `False`, don't cache. Otherwise, specify the directory to cache to. subset: An expression in string to subset the cells. aggregate_by: The column names in metadata to aggregate the cells. layer: The layer to pull and aggregate the data. assay: The assay to pull and aggregate the data. error (flag): Error out if no/not enough markers are found or no pathways are enriched. If `False`, empty results will be returned. group_by: The column name in metadata to group the cells. ident_1: The first identity to compare. ident_2: The second identity to compare. If not specified, the rest of the identities will be compared with `ident_1`. paired_by: The column name in metadata to mark the paired samples. For example, subject. If specified, the paired test will be performed. dbs (list): The databases to use for enrichment analysis. The databases are passed to `biopipen.utils::Enrichr()` to do the enrichment analysis. The default databases are `KEGG_2021_Human` and `MSigDB_Hallmark_2020`. See <https://maayanlab.cloud/Enrichr/#libraries> for the available libraries. sigmarkers: An expression passed to `dplyr::filter()` to filter the significant markers for enrichment analysis. The default is `p_val_adj < 0.05`. If `tool = 'DESeq2'`, the variables that can be used for filtering are: `baseMean`, `log2FC`, `lfcSE`, `stat`, `p_val`, `p_val_adj`. If `tool = 'edgeR'`, the variables that can be used for filtering are: `logCPM`, `log2FC`, `LR`, `p_val`, `p_val_adj`. enrich_style (choice): The style of the enrichment analysis. - enrichr: Use `enrichr`-style for the enrichment analysis. - clusterProfiler: Use `clusterProfiler`-style for the enrichment analysis. allmarker_plots_defaults (ns): Default options for the plots for all markers when `ident-1` is not specified. - plot_type: The type of the plot. See <https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html>. Available types are `violin`, `box`, `bar`, `ridge`, `dim`, `heatmap` and `dot`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - order_by: an expression to order the markers, passed by `dplyr::arrange()`. - genes: The number of top genes to show or an expression passed to `dplyr::filter()` to filter the genes. - <more>: Other arguments passed to [`scplotter::FeatureStatPlot()`](https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html). allmarker_plots (type=json): All marker plot cases. The keys are the names of the cases and the values are the dicts inherited from `allmarker_plots_defaults`. allenrich_plots_defaults (ns): Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: See <https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html>. allenrich_plots (type=json): Cases of the plots to generate for the enrichment analysis. The keys are the names of the cases and the values are the dicts inherited from `allenrich_plots_defaults`. The cases under `envs.cases` can inherit this options. marker_plots_defaults (ns): Default options for the plots to generate for the markers. - plot_type: The type of the plot. See <https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html>. Available types are `violin`, `box`, `bar`, `ridge`, `dim`, `heatmap` and `dot`. There are two additional types available - `volcano_pct` and `volcano_log2fc`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - order_by: an expression to order the markers, passed by `dplyr::arrange()`. - genes: The number of top genes to show or an expression passed to `dplyr::filter()` to filter the genes. - <more>: Other arguments passed to [`scplotter::FeatureStatPlot()`](https://pwwang.github.io/scplotter/reference/FeatureStatPlot.html). If `plot_type` is `volcano_pct` or `volcano_log2fc`, they will be passed to [`scplotter::VolcanoPlot()`](https://pwwang.github.io/plotthis/reference/VolcanoPlot.html). marker_plots (type=json): Cases of the plots to generate for the markers. Plot cases. The keys are the names of the cases and the values are the dicts inherited from `marker_plots_defaults`. The cases under `envs.cases` can inherit this options. enrich_plots_defaults (ns): Default options for the plots to generate for the enrichment analysis. - plot_type: The type of the plot. See <https://pwwang.github.io/scplotter/reference/EnrichmentPlot.html>. Available types are `bar`, `dot`, `lollipop`, `network`, `enrichmap` and `wordcloud`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: See <https://pwwang.github.io/scplotter/reference/EnrichmentPlot.htmll>. enrich_plots (type=json): Cases of the plots to generate for the enrichment analysis. The keys are the names of the cases and the values are the dicts inherited from `enrich_plots_defaults`. The cases under `envs.cases` can inherit this options. overlaps_defaults (ns): Default options for investigating the overlapping of significant markers between different cases or comparisons. This means either `ident-1` should be empty, so that they can be expanded to multiple comparisons. - sigmarkers: The expression to filter the significant markers for each case. If not provided, `envs.sigmarkers` will be used. - plot_type (choice): The type of the plot to generate for the overlaps. - venn: Use `plotthis::VennDiagram()`. - upset: Use `plotthis::UpsetPlot()`. - more_formats (type=list): The extra formats to save the plot in. - save_code (flag): Whether to save the code to generate the plot. - devpars (ns): The device parameters for the plots. - res (type=int): The resolution of the plots. - height (type=int): The height of the plots. - width (type=int): The width of the plots. - <more>: More arguments pased to `plotthis::VennDiagram()` (<https://pwwang.github.io/plotthis/reference/venndiagram1.html>) or `plotthis::UpsetPlot()` (<https://pwwang.github.io/plotthis/reference/upsetplot1.html>) overlaps (type=json): Cases for investigating the overlapping of significant markers between different cases or comparisons. The keys are the names of the cases and the values are the dicts inherited from `overlaps_defaults`. There are two situations that we can perform overlaps: 1. If `ident-1` is not specified, the overlaps can be performed between different comparisons. 2. If `each` is specified, the overlaps can be performed between different cases, where in each case, `ident-1` must be specified. tool (choice): The method to use for the differential expression analysis. - DESeq2: Use DESeq2 for the analysis. - edgeR: Use edgeR for the analysis. plots_defaults (ns): The default parameters for the plots. - <more>: Parameters passed to `biopipen.utils::VizBulkDEGs()`. See: <https://pwwang.github.io/biopipen.utils.R/reference/VizBulkDEGs.html> plots (type=json): The parameters for the plots. The keys are the names of the plots and the values are the parameters for the plots. The parameters will override the defaults in `plots_defaults`. If not specified, no plots will be generated. cases (type=json): The cases for the analysis. The keys are the names of the cases and the values are the arguments for the analysis. The arguments include the ones inherited from `envs`. If no cases are specified, a default case will be added with the name `DEG Analysis` and the default values specified above. \"\"\" # noqa: E501 input = \"sobjfile:file\" output = \"outdir:dir:{{in.sobjfile | stem}}.pseudobulk_deg\" lang = config . lang . rscript script = \"file://../scripts/scrna/PseudoBulkDEG.R\" envs = { \"ncores\" : config . misc . ncores , \"mutaters\" : {}, \"cache\" : config . path . tmpdir , \"each\" : None , \"subset\" : None , \"aggregate_by\" : None , \"layer\" : \"counts\" , \"assay\" : \"RNA\" , \"error\" : False , \"group_by\" : None , \"ident_1\" : None , \"ident_2\" : None , \"paired_by\" : None , \"tool\" : \"DESeq2\" , \"dbs\" : [ \"KEGG_2021_Human\" , \"MSigDB_Hallmark_2020\" ], \"sigmarkers\" : \"p_val_adj < 0.05\" , \"enrich_style\" : \"enrichr\" , \"allmarker_plots_defaults\" : { \"plot_type\" : None , \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, \"order_by\" : \"desc(abs(log2FC))\" , \"genes\" : 10 , }, \"allmarker_plots\" : {}, \"allenrich_plots_defaults\" : { \"plot_type\" : \"heatmap\" , \"devpars\" : { \"res\" : 100 }, }, \"allenrich_plots\" : {}, \"marker_plots_defaults\" : { \"plot_type\" : None , \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, \"order_by\" : \"desc(abs(log2FC))\" , \"genes\" : 10 , }, \"marker_plots\" : { \"Volcano Plot\" : { \"plot_type\" : \"volcano\" }, }, \"enrich_plots_defaults\" : { \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, }, \"enrich_plots\" : { \"Bar Plot\" : { \"plot_type\" : \"bar\" , \"ncol\" : 1 , \"top_term\" : 10 }, }, \"overlaps_defaults\" : { \"sigmarkers\" : None , \"plot_type\" : \"venn\" , \"more_formats\" : [], \"save_code\" : False , \"devpars\" : { \"res\" : 100 }, }, \"overlaps\" : {}, \"cases\" : {}, } plugin_opts = { \"report\" : \"file://../reports/common.svelte\" , \"report_paging\" : 8 , }","title":"biopipen.ns.scrna"},{"location":"api/source/biopipen.ns.scrna_metabolic_landscape/","text":"SOURCE CODE biopipen.ns. scrna_metabolic_landscape DOCS \"\"\"Metabolic landscape analysis for scRNA-seq data\"\"\" from __future__ import annotations from pathlib import Path from typing import Type from diot import Diot # type: ignore from datar.tibble import tibble from pipen.utils import mark from pipen_args import ProcGroup from pipen_annotate import annotate from ..core.config import config from ..core.proc import Proc class MetabolicPathwayActivity ( Proc ): DOCS \"\"\"This process calculates the pathway activities in different groups and subsets. The cells are first grouped by subsets and then the metabolic activities are examined for each groups in different subsets. For each subset, a heatmap and a violin plot will be generated. The heatmap shows the pathway activities for each group and each metabolic pathway ![MetabolicPathwayActivity_heatmap](https://pwwang.github.io/immunopipe/latest/processes/images/MetabolicPathwayActivity_heatmap.png){: width=\"80%\"} The violin plot shows the distribution of the pathway activities for each group ![MetabolicPathwayActivity_violin](https://pwwang.github.io/immunopipe/latest/processes/images/MetabolicPathwayActivity_violin.png){: width=\"45%\"} Input: sobjfile: The Seurat object file. It should be loaded as a Seurat object Output: outdir: The output directory. It will contain the pathway activity score files and plots. Envs: ntimes (type=int): Number of permutations to estimate the p-values ncores (type=int;pgarg): Number of cores to use for parallelization Defaults to `ScrnaMetabolicLandscape.ncores` gmtfile (pgarg): The GMT file with the metabolic pathways. Defaults to `ScrnaMetabolicLandscape.gmtfile` subset_by (pgarg;readonly): Subset the data by the given column in the metadata. For example, `Response`. `NA` values will be removed in this column. Defaults to `ScrnaMetabolicLandscape.subset_by` If None, the data will not be subsetted. group_by (pgarg;readonly): Group the data by the given column in the metadata. For example, `cluster`. Defaults to `ScrnaMetabolicLandscape.group_by` plots (type=json): The plots to generate. Names will be used as the prefix for the output files. Values will be a dictionary with the following keys: * `plot_type` is the type of plot to generate. One of `heatmap`, `box`, `violin` or `merged_heatmap` (all subsets in one plot). * `devpars` is a dictionary with the device parameters for the plot. * Other arguments for `plotthis::Heatmap()`, `plotthis::BoxPlot()` or `plotthis::ViolinPlot()`, depending on the `plot_type`. cases (type=json): Multiple cases for the analysis. If you only have one case, you can specify the parameters directly to `envs.ntimes`, `envs.subset_by`, `envs.group_by`, `envs.group1`, `envs.group2`, and `envs.plots`. The name of the case will be `envs.subset_by`. If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from `envs.ntimes`, `envs.subset_by`, `envs.group_by`, `envs.group1`, `envs.group2`, and `envs.plots`. \"\"\" # noqa: E501 input = \"sobjfile:file\" output = \"outdir:dir:{{in.sobjfile | stem}}.pathwayactivity\" envs = { \"ntimes\" : 5000 , \"ncores\" : config . misc . ncores , \"gmtfile\" : None , \"subset_by\" : None , \"group_by\" : None , \"plots\" : { \"Pathway Activity (violin plot)\" : { \"plot_type\" : \"violin\" , \"add_box\" : True , \"devpars\" : { \"res\" : 100 }, }, \"Pathway Activity (heatmap)\" : { \"plot_type\" : \"heatmap\" , \"devpars\" : { \"res\" : 100 }, }, }, \"cases\" : {}, } lang = config . lang . rscript script = ( \"file://../scripts/scrna_metabolic_landscape/MetabolicPathwayActivity.R\" ) plugin_opts = { \"report\" : \"file://../reports/scrna_metabolic_landscape/MetabolicPathwayActivity.svelte\" } class MetabolicFeatures ( Proc ): DOCS \"\"\"This process performs enrichment analysis for the metabolic pathways for each group in each subset. The enrichment analysis is done with [`fgsea`](https://bioconductor.org/packages/release/bioc/html/fgsea.html) package or the [`GSEA_R`](https://github.com/GSEA-MSigDB/GSEA_R) package. Input: sobjfile: The Seurat object file in rds. It should be loaded as a Seurat object Output: outdir: The output directory. It will contain the GSEA results and plots. Envs: ncores (type=int;pgarg): Number of cores to use for parallelization for the comparisons for each subset and group. Defaults to `ScrnaMetabolicLandscape.ncores`. prerank_method (choice): Method to use for gene preranking. Signal to noise: the larger the differences of the means (scaled by the standard deviations); that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \u201cclass marker.\u201d. Absolute signal to noise: the absolute value of the signal to noise. T test: Uses the difference of means scaled by the standard deviation and number of samples. Ratio of classes: Uses the ratio of class means to calculate fold change for natural scale data. Diff of classes: Uses the difference of class means to calculate fold change for nature scale data Log2 ratio of classes: Uses the log2 ratio of class means to calculate fold change for natural scale data. This is the recommended statistic for calculating fold change for log scale data. - signal_to_noise: Signal to noise - s2n: Alias of signal_to_noise - abs_signal_to_noise: absolute signal to noise - abs_s2n: Alias of abs_signal_to_noise - t_test: T test - ratio_of_classes: Also referred to as fold change - diff_of_classes: Difference of class means - log2_ratio_of_classes: Log2 ratio of class means gmtfile (pgarg): The GMT file with the metabolic pathways. Defaults to `ScrnaMetabolicLandscape.gmtfile` subset_by (pgarg;readonly): Subset the data by the given column in the metadata. For example, `Response`. `NA` values will be removed in this column. Defaults to `ScrnaMetabolicLandscape.subset_by` If None, the data will not be subsetted. group_by (pgarg;readonly): Group the data by the given column in the metadata. For example, `cluster`. Defaults to `ScrnaMetabolicLandscape.group_by` comparisons (type=list): The comparison groups to use for the analysis. If not provided, each group in the `group_by` column will be used to compare with the other groups. If a single group is provided as an element, it will be used to compare with all the other groups. For example, if we have `group_by = \"cluster\"` and we have `1`, `2` and `3` in the `group_by` column, we could have `comparisons = [\"1\", \"2\"]`, which will compare the group `1` with groups `2` and `3`, and the group `2` with groups `1` and `3`. We could also have `comparisons = [\"1:2\", \"1:3\"]`, which will compare the group `1` with group `2` and group `1` with group `3`. fgsea_args (type=json): Other arguments for the `fgsea::fgsea()` function. For example, `{\"minSize\": 15, \"maxSize\": 500}`. See <https://rdrr.io/bioc/fgsea/man/fgsea.html> for more details. plots (type=json): The plots to generate. Names will be used as the title for the plot. Values will be the arguments passed to `biopipen.utils::VizGSEA()` function. See <https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html>. A key `level` is supported to specify the level of the plot. Possible values are `case`, which includes all subsets and groups in the case; `subset`, which includes all groups in the subset; otherwise, it will plot for the groups. For `case`/`subset` level plots, current `plot_type` only \"dot\" is supported for now, then the values will be passed to `plotthis::DotPlot()` cases (type=json): Multiple cases for the analysis. If you only have one case, you can specify the parameters directly to `envs.prerank_method`, `envs.subset_by`, `envs.group_by`, `envs.comparisons`, `envs.fgsea_args` and `envs.plots`. The name of this default case will be `envs.subset_by`. If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from `envs.prerank_method`, `envs.subset_by`, `envs.group_by`, `envs.comparisons`, `envs.fgsea_args` and `envs.plots`. \"\"\" # noqa: E501 input = \"sobjfile:file\" output = \"outdir:dir:{{in.sobjfile | stem}}.pathwayfeatures\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"prerank_method\" : \"signal_to_noise\" , \"gmtfile\" : None , \"subset_by\" : None , \"group_by\" : None , \"comparisons\" : [], \"fgsea_args\" : {}, \"plots\" : { \"Summary Plot\" : { \"plot_type\" : \"summary\" , \"top_term\" : 10 , \"devpars\" : { \"res\" : 100 }, }, \"Enrichment Plots\" : { \"plot_type\" : \"gsea\" , \"top_term\" : 10 , \"devpars\" : { \"res\" : 100 }, }, }, \"cases\" : {}, } script = \"file://../scripts/scrna_metabolic_landscape/MetabolicFeatures.R\" plugin_opts = { \"report\" : \"file://../reports/scrna_metabolic_landscape/MetabolicFeatures.svelte\" } class MetabolicPathwayHeterogeneity ( Proc ): DOCS \"\"\"Calculate Metabolic Pathway heterogeneity. For each subset, the normalized enrichment score (NES) of each metabolic pathway is calculated for each group. The NES is calculated by comparing the enrichment score of the subset to the enrichment scores of the same subset in the permutations. The p-value is calculated by comparing the NES to the NESs of the same subset in the permutations. The heterogeneity can be reflected by the NES values and the p-values in different groups for the metabolic pathways. ![MetabolicPathwayHeterogeneity](https://pwwang.github.io/immunopipe/latest/processes/images/MetabolicPathwayHeterogeneity.png) Envs: gmtfile (pgarg): The GMT file with the metabolic pathways. Defaults to `ScrnaMetabolicLandscape.gmtfile` select_pcs (type=float): Select the PCs to use for the analysis. pathway_pval_cutoff (type=float): The p-value cutoff to select the enriched pathways ncores (type=int;pgarg): Number of cores to use for parallelization Defaults to `ScrnaMetabolicLandscape.ncores` subset_by (pgarg;readonly): Subset the data by the given column in the metadata. For example, `Response`. `NA` values will be removed in this column. Defaults to `ScrnaMetabolicLandscape.subset_by` If None, the data will not be subsetted. group_by (pgarg;readonly): Group the data by the given column in the metadata. For example, `cluster`. Defaults to `ScrnaMetabolicLandscape.group_by` fgsea_args (type=json): Other arguments for the `fgsea::fgsea()` function. For example, `{\"minSize\": 15, \"maxSize\": 500}`. See <https://rdrr.io/bioc/fgsea/man/fgsea.html> for more details. plots (type=json): The plots to generate. Names will be used as the title for the plot. Values will be the arguments passed to `biopipen.utils::VizGSEA()` function. See <https://pwwang.github.io/biopipen.utils.R/reference/VizGSEA.html>. cases (type=json): Multiple cases for the analysis. If you only have one case, you can specify the parameters directly to `envs.subset_by`, `envs.group_by`, `envs.fgsea_args`, `envs.plots`, `envs.select_pcs`, and `envs.pathway_pval_cutoff`. The name of this default case will be `envs.subset_by`. If you have multiple cases, you can specify the parameters for each case in a dictionary. The keys will be the names of the cases and the values will be dictionaries with the parameters for each case, where the values will be inherited from `envs.subset_by`, `envs.group_by`, `envs.fgsea_args`, `envs.plots`, `envs.select_pcs`, and `envs.pathway_pval_cutoff`. \"\"\" # noqa: E501 input = \"sobjfile:file\" output = \"outdir:dir:{{in.sobjfile | stem}}.pathwayhetero\" lang = config . lang . rscript envs = { \"gmtfile\" : None , \"select_pcs\" : 0.8 , \"pathway_pval_cutoff\" : 0.01 , \"ncores\" : config . misc . ncores , \"subset_by\" : None , \"group_by\" : None , \"fgsea_args\" : { \"scoreType\" : \"std\" , \"nproc\" : 1 }, \"plots\" : { \"Pathway Heterogeneity\" : { \"plot_type\" : \"dot\" , \"devpars\" : { \"res\" : 100 }, }, }, \"cases\" : {}, } script = ( \"file://../scripts/scrna_metabolic_landscape/\" \"MetabolicPathwayHeterogeneity.R\" ) plugin_opts = { \"report\" : ( \"file://../reports/scrna_metabolic_landscape/\" \"MetabolicPathwayHeterogeneity.svelte\" ) } class ScrnaMetabolicLandscape ( ProcGroup ): DOCS \"\"\"Metabolic landscape analysis for scRNA-seq data An abstract from <https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape> See docs here for more details <https://pwwang.github.io/biopipen/pipelines/scrna_metabolic_landscape> Reference: Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Args: metafile: Either a metafile or an rds file of a Seurat object. If it is a metafile, it should have two columns: `Sample` and `RNAData`. `Sample` should be the first column with unique identifiers for the samples and `RNAData` indicates where the barcodes, genes, expression matrices are. The data will be loaded and an unsupervised clustering will be done. Currently only 10X data is supported. If it is an rds file, the seurat object will be used directly is_seurat (flag): Whether the input `metafile` is a seurat object. If `metafile` is specified directly, this option will be ignored and will be inferred from the file extension. If `metafile` is not specified, meaning `<pipeline>.procs.MetabolicInput` is dependent on other processes, this option will be used to determine whether the input is a seurat object or not. noimpute (flag): Whether to do imputation for the dropouts. If True, the values will be left as is. gmtfile: The GMT file with the metabolic pathways. The gene names should match the gene names in the gene list in RNAData or the Seurat object. You can also provide a URL to the GMT file. For example, from <https://download.baderlab.org/EM_Genesets/current_release/Human/symbol/>. subset_by (pgarg;readonly): Subset the data by the given column in the metadata. For example, `Response`. `NA` values will be removed in this column. If None, the data will not be subsetted. group_by (pgarg;readonly): Group the data by the given column in the metadata. For example, `cluster`. mutaters (type=json): Add new columns to the metadata for grouping/subsetting. They are passed to `sobj@meta.data |> mutate(...)`. For example, `{\"timepoint\": \"if_else(treatment == 'control', 'pre', 'post')\"}` will add a new column `timepoint` to the metadata with values of `pre` and `post` based on the `treatment` column. ncores (type=int): Number of cores to use for parallelization for each process \"\"\" DEFAULTS = Diot ( metafile = None , is_seurat = None , gmtfile = None , mutaters = None , noimpute = True , ncores = config . misc . ncores , subset_by = None , group_by = None , ) def post_init ( self ): DOCS \"\"\"Load runtime processes\"\"\" if self . opts . metafile : suffix = Path ( self . opts . metafile ) . suffix self . opts . is_seurat = suffix in ( \".rds\" , \".RDS\" , \".qs\" , \".qs2\" ) @ProcGroup . add_proc # type: ignore def p_input ( self ) -> Type [ Proc ]: \"\"\"Build MetabolicInputs process\"\"\" from .misc import File2Proc @mark ( board_config_hidden = True ) class MetabolicInput ( File2Proc ): \"\"\"This process takes Seurat object as input and pass it to the next processes in the `ScrnaMetabolicLandscape` group. There is no configuration for this process. \"\"\" if self . opts . metafile : input_data = [ self . opts . metafile ] return MetabolicInput @ProcGroup . add_proc # type: ignore def p_preparing ( self ) -> Type [ Proc ] | None : \"\"\"Build SeuratPreparing process\"\"\" if self . opts . is_seurat : return None from .scrna import SeuratPreparing class MetabolicSeuratPreparing ( SeuratPreparing ): requires = self . p_input return MetabolicSeuratPreparing @ProcGroup . add_proc # type: ignore def p_clustering ( self ) -> Type [ Proc ]: \"\"\"Build SeuratClustering process\"\"\" if self . opts . is_seurat : return self . p_input # type: ignore from .scrna import SeuratClustering class MetabolicSeuratClustering ( SeuratClustering ): requires = self . p_preparing return MetabolicSeuratClustering @ProcGroup . add_proc # type: ignore def p_mutater ( self ) -> Type [ Proc ]: \"\"\"Build SeuratMetadataMutater process\"\"\" if not self . opts . mutaters : return self . p_clustering # type: ignore from .scrna import SeuratMetadataMutater class MetabolicSeuratMetadataMutater ( SeuratMetadataMutater ): requires = self . p_clustering input_data = lambda ch : tibble ( srtobj = ch . iloc [:, 0 ], metafile = [ None ], ) envs = { \"mutaters\" : self . opts . mutaters } return MetabolicSeuratMetadataMutater @ProcGroup . add_proc # type: ignore def p_expr_impute ( self ) -> Type [ Proc ]: \"\"\"Build process\"\"\" if self . opts . noimpute : return self . p_mutater # type: ignore from .scrna import ExprImputation @annotate . format_doc ( indent = 3 ) # type: ignore class MetabolicExprImputation ( ExprImputation ): \"\"\"{{Summary}} You can turn off the imputation by setting the `noimpute` option of the process group to `True`. \"\"\" requires = self . p_mutater return MetabolicExprImputation @ProcGroup . add_proc # type: ignore def p_pathway_activity ( self ) -> Type [ Proc ]: \"\"\"Build MetabolicPathwayActivity process\"\"\" return Proc . from_proc ( # type: ignore MetabolicPathwayActivity , \"MetabolicPathwayActivity\" , requires = self . p_expr_impute , # type: ignore order =- 1 , envs_depth = 5 , envs = { \"ncores\" : self . opts . ncores , \"gmtfile\" : self . opts . gmtfile , \"group_by\" : self . opts . group_by , \"subset_by\" : self . opts . subset_by , }, ) @ProcGroup . add_proc # type: ignore def p_pathway_heterogeneity ( self ) -> Type [ Proc ]: \"\"\"Build MetabolicPathwayHeterogeneity process\"\"\" return Proc . from_proc ( # type: ignore MetabolicPathwayHeterogeneity , \"MetabolicPathwayHeterogeneity\" , requires = self . p_mutater , # type: ignore envs_depth = 5 , envs = { \"ncores\" : self . opts . ncores , \"gmtfile\" : self . opts . gmtfile , \"group_by\" : self . opts . group_by , \"subset_by\" : self . opts . subset_by , }, ) @ProcGroup . add_proc # type: ignore def p_features ( self ) -> Type [ Proc ]: \"\"\"Build MetabolicFeatures process\"\"\" return Proc . from_proc ( # type: ignore MetabolicFeatures , \"MetabolicFeatures\" , requires = self . p_expr_impute , # type: ignore envs_depth = 5 , envs = { \"ncores\" : self . opts . ncores , \"gmtfile\" : self . opts . gmtfile , \"group_by\" : self . opts . group_by , \"subset_by\" : self . opts . subset_by , }, ) if __name__ == \"__main__\" : ScrnaMetabolicLandscape () . as_pipen () . run ()","title":"biopipen.ns.scrna_metabolic_landscape"},{"location":"api/source/biopipen.ns.snp/","text":"SOURCE CODE biopipen.ns. snp DOCS \"\"\"Plink processes\"\"\" from ..core.proc import Proc from ..core.config import config class PlinkSimulation ( Proc ): DOCS \"\"\"Simulate SNPs using PLINK v2 See also <https://www.cog-genomics.org/plink/2.0/input#simulate> and <https://pwwang.github.io/biopipen/api/biopipen.ns.snp/#biopipen.ns.snp.PlinkSimulation> Input: configfile: Configuration file containing the parameters for the simulation. The configuration file (in toml, yaml or json format) should contain a dictionary of parameters. The parameters are listed in `envs` except `ncores`, which is used for parallelization. You can set parameters in `envs` and override them in the configuration file. Output: outdir: Output directory containing the simulated data `plink_sim.bed`, `plink_sim.bim`, and `plink_sim.fam` will be generated. gtmat: Genotype matrix file containing the simulated data with rows representing SNPs and columns representing samples. Envs: nsnps (type=int): Number of SNPs to simulate ncases (type=int): Number of cases to simulate nctrls (type=int): Number of controls to simulate plink: Path to PLINK v2 seed (type=int): Random seed. If not set, seed will not be set. label: Prefix label for the SNPs. prevalence (type=float): Disease prevalence. minfreq (type=float): Minimum allele frequency. maxfreq (type=float): Maximum allele frequency. hetodds (type=float): Odds ratio for heterozygous genotypes. homodds (type=float): Odds ratio for homozygous genotypes. missing (type=float): Proportion of missing genotypes. args (ns): Additional arguments to pass to PLINK. - <more>: see <https://www.cog-genomics.org/plink/2.0/input#simulate>. transpose_gtmat (flag): If set, the genotype matrix (`out.gtmat`) will be transposed. sample_prefix: Use this prefix for the sample names. If not set, the sample names will be `per0_per0`, `per1_per1`, `per2_per2`, etc. If set, the sample names will be `prefix0`, `prefix1`, `prefix2`, etc. This only affects the sample names in the genotype matrix file (`out.gtmat`). \"\"\" input = \"configfile:file\" output = [ \"outdir:dir:{{in.configfile | stem}}.plink_sim\" , \"gtmat:file:{{in.configfile | stem}}.plink_sim/\" \"{{in.configfile | stem}}-gtmat.txt\" , ] lang = config . lang . python envs = { \"nsnps\" : None , \"ncases\" : None , \"nctrls\" : None , \"plink\" : config . exe . plink , \"seed\" : None , \"label\" : \"SNP\" , \"prevalence\" : 0.01 , \"minfreq\" : 0.0 , \"maxfreq\" : 1.0 , \"hetodds\" : 1.0 , \"homodds\" : 1.0 , \"missing\" : 0.0 , \"args\" : {}, \"transpose_gtmat\" : False , \"sample_prefix\" : None , } script = \"file://../scripts/snp/PlinkSimulation.py\" class MatrixEQTL ( Proc ): DOCS \"\"\"Run Matrix eQTL See also <https://www.bios.unc.edu/research/genomic_software/Matrix_eQTL/> Input: geno: Genotype matrix file with rows representing SNPs and columns representing samples. expr: Expression matrix file with rows representing genes and columns representing samples. cov: Covariate matrix file with rows representing covariates and columns representing samples. Output: alleqtls: Matrix eQTL output file cisqtls: The cis-eQTL file if `snppos` and `genepos` are provided. Otherwise it'll be empty. Envs: model (choice): The model to use. - linear: Linear model - modelLINEAR: Same as `linear` - anova: ANOVA model - modelANOVA: Same as `anova` pval (type=float): P-value threshold for eQTLs match_samples (flag): Match samples in the genotype and expression matrices. If True, an error will be raised if samples from `in.geno`, `in.expr`, and `in.cov` (if provided) are not the same. If False, common samples will be used to subset the matrices. transp (type=float): P-value threshold for trans-eQTLs. If cis-eQTLs are not enabled (`snppos` and `genepos` are not set), this defaults to 1e-5. If cis-eQTLs are enabled, this defaults to `None`, which will disable trans-eQTL analysis. fdr (flag): Do FDR calculation or not (save memory if not). snppos: The path of the SNP position file. It could be a BED, GFF, VCF or a tab-delimited file with `snp`, `chr`, `pos` as the first 3 columns. genepos: The path of the gene position file. It could be a BED or GFF file. dist (type=int): Distance threshold for cis-eQTLs. transpose_geno (flag): If set, the genotype matrix (`in.geno`) will be transposed. transpose_expr (flag): If set, the expression matrix (`in.expr`) will be transposed. transpose_cov (flag): If set, the covariate matrix (`in.cov`) will be transposed. \"\"\" input = \"geno:file, expr:file, cov:file\" output = [ \"alleqtls:file:{{in.geno | stem}}.alleqtls.txt\" , \"cisqtls:file:{{in.geno | stem}}.cisqtls.txt\" , ] lang = config . lang . rscript envs = { \"model\" : \"linear\" , \"pval\" : 1e-3 , \"match_samples\" : False , \"transp\" : None , \"fdr\" : False , \"snppos\" : None , \"genepos\" : config . ref . refgene , \"dist\" : 250000 , \"transpose_geno\" : False , \"transpose_expr\" : False , \"transpose_cov\" : False , } script = \"file://../scripts/snp/MatrixEQTL.R\" class PlinkFromVcf ( Proc ): DOCS \"\"\"Convert VCF to PLINK format. The PLINK format consists of 3 files: `.bed`, `.bim`, and `.fam`. Requires PLINK v2 TODO: Handle sex when sex chromosomes are included. Input: invcf: VCF file Output: outdir: Output directory containing the PLINK files Envs: plink: Path to PLINK v2 tabix: Path to tabix ncores (type=int): Number of cores/threads to use, will pass to plink `--threads` option vcf_half_call (choice): The current VCF standard does not specify how '0/.' and similar GT values should be interpreted. - error: error out and reports the line number of the anomaly - e: alias for `error` - haploid: treat half-calls as haploid/homozygous - h: alias for `haploid` - missing: treat half-calls as missing - m: alias for `missing` - reference: treat the missing part as reference - r: alias for `reference` double_id (flag): set both FIDs and IIDs to the VCF/BCF sample ID. vcf_filter (auto): skip variants which failed one or more filters tracked by the FILTER field. If True, only FILTER with `PASS` or `.` will be kept. Multiple filters can be specified by separating them with space or as a list. vcf_idspace_to: convert all spaces in sample IDs to this character. set_missing_var_ids: update variant IDs using a template string, with a '@' where the chromosome code should go, and a '#' where the base-pair position belongs. You can also specify `\\\\$r` and `\\\\$a` for the reference and alternate alleles, respectively. See <https://www.cog-genomics.org/plink/2.0/data#set_all_var_ids> max_alleles (type=int): Maximum number of alleles per variant. <more>: see <https://www.cog-genomics.org/plink/2.0/> for more options. Note that `_` will be replaced by `-` in the argument names. \"\"\" # noqa: E501 input = \"invcf:file\" output = \"outdir:dir:{{in.invcf.stem | regex_replace: ' \\\\ .gz$', ''}}\" lang = config . lang . python envs = { \"plink\" : config . exe . plink2 , \"tabix\" : config . exe . tabix , \"ncores\" : config . misc . ncores , \"vcf_half_call\" : \"missing\" , \"double_id\" : True , \"vcf_filter\" : True , \"vcf_idspace_to\" : \"_\" , \"set_missing_var_ids\" : \"@_#\" , \"max_alleles\" : 2 , } script = \"file://../scripts/snp/PlinkFromVcf.py\" class Plink2GTMat ( Proc ): DOCS \"\"\"Convert PLINK files to genotype matrix. Requires PLINK v2. The .raw/.traw file is generated by plink and then transformed to a genotype matrix file. See <https://www.cog-genomics.org/plink/2.0/formats#raw> and <https://www.cog-genomics.org/plink/2.0/formats#traw> for more information. The allelic dosage is used as the values of genotype matrix. \"--keep-allele-order\" is used to keep the allele order consistent with the reference allele first. This way, the genotype of homozygous reference alleles will be encoded as 2, heterozygous as 1, and homozygous alternate alleles as 0. This is the PLINK dosage encoding. If you want to use this encoding, you can set `envs.gtcoding` to `plink`. Otherwise, the default encoding is `vcf`, which will encode the genotype as 0, 1, and 2 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. Note that `envs.gtcoding = \"vcf\"` only works for biallelic variants for now. Input: indir: Input directory containing the PLINK files. Including `.bed`, `.bim`, and `.fam` files Output: outfile: Genotype matrix file with rows representing SNPs and columns representing samples if `envs.transpose` is `False`. Envs: plink: Path to PLINK v2.0 ncores (type=int): Number of cores/threads to use, will pass to plink `--threads` option transpose (flag): If set, the genotype matrix (`out.outfile`) is transposed. samid: what to use as sample ID. Placeholders include `{fid}` and `{iid}` for family and individual IDs, respectively. varid: what to use as variant ID. Placeholders include `{chr}`, `{pos}`, `{rs}`, `{ref}`, and `{alt}` for chromosome, position, rsID, reference allele, and alternate allele, respectively. trans_chr: A dictionary to translate chromosome numbers to chromosome names. missing_id: what to use as the rs if missing. gtcoding (choice): The genotype coding to use. - vcf: 0/1/2 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. - plink: 2/1/0 for homozygous reference, heterozygous, and homozygous alternate alleles, respectively. \"\"\" input = \"indir:dir\" output = \"outfile:file:{{in.indir | stem}}-gtmat.txt\" lang = config . lang . python envs = { \"plink\" : config . exe . plink2 , \"ncores\" : config . misc . ncores , \"transpose\" : False , \"samid\" : \" {fid} _ {iid} \" , \"varid\" : \" {chr} _ {pos} _ {varid} _ {ref} _ {alt} \" , \"trans_chr\" : { \"23\" : \"X\" , \"24\" : \"Y\" , \"25\" : \"XY\" , \"26\" : \"M\" }, \"missing_id\" : \"NA\" , \"gtcoding\" : \"vcf\" , } script = \"file://../scripts/snp/Plink2GTMat.py\" class PlinkIBD ( Proc ): DOCS \"\"\"Run PLINK IBD analysis (identity by descent) See also <https://www.cog-genomics.org/plink/1.9/ibd> This has to run with PLINK v1.9. Plink v2 does not support IBD analysis yet. Input: indir: Input directory containing the PLINK files. Including `.bed`, `.bim`, and `.fam` files Output: outdir: Output file containing the IBD results. Including [`.genome`](https://www.cog-genomics.org/plink/2.0/formats#genome) file for the original IBD report from PLINK, and `.ibd.png` for the heatmap of `PI_HAT` values. Envs: plink: Path to PLINK v1.9 ncores (type=int): Number of cores/threads to use, will pass to plink `--threads` option highld: High LD regions to be excluded from the analysis. If not set, no regions will be excluded. samid: what to use as sample ID. Placeholders include `{fid}` and `{iid}` for family and individual IDs, respectively indep (type=auto): LD pruning parameters. Either a list of numerics or a string concatenated by `,` to specify 1) consider a window of N SNPs (e.g. 50), 2) calculate LD between each pair of SNPs in the window (e.g. 5), 3) remove one of a pair of SNPs if the LD is greater than X (e.g. 0.2). pihat (type=float): PI_HAT threshold for IBD analysis. See also <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5007749/> plot (flag): If set, plot the heatmap of `PI_HAT` values. anno: The annotation file for the samples, used to plot on the heatmap. Names must match the ones that are transformed by `args.samid`. seed (type=int): Random seed for the analysis. devpars (ns): The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot \"\"\" input = \"indir:dir\" output = \"outdir:dir:{{in.indir | stem}}.ibd\" lang = config . lang . rscript envs = { \"plink\" : config . exe . plink , \"ncores\" : config . misc . ncores , \"highld\" : None , \"samid\" : \" {fid} _ {iid} \" , \"indep\" : [ 50 , 5 , 0.2 ], \"pihat\" : 0.1875 , \"plot\" : True , \"anno\" : None , \"seed\" : 8525 , \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, } script = \"file://../scripts/snp/PlinkIBD.R\" plugin_opts = { \"report\" : \"file://../reports/snp/PlinkIBD.svelte\" } class PlinkHWE ( Proc ): DOCS \"\"\"Hardy-Weinberg Equilibrium report and filtering See also <https://www.cog-genomics.org/plink/2.0/basic_stats#hardy> Input: indir: Input directory containing the PLINK files. Including `.bed`, `.bim`, and `.fam` files Output: outdir: Output file containing the HWE results. Including [`.hwe`](https://www.cog-genomics.org/plink/2.0/formats#hwe) file for the original HWE report from PLINK and `.hardy.fail` for the variants that failed the HWE test. It also includes binary files `.bed`, `.bim`, and `.fam` Envs: plink: Path to PLINK v2 ncores (type=int): Number of cores/threads to use, will pass to plink `--threads` option cutoff (type=float): P-value cutoff for HWE test plot (flag): If set, plot the distribution of HWE p-values. devpars (ns): The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot \"\"\" input = \"indir:dir\" output = \"outdir:dir:{{in.indir | stem}}.hwe\" lang = config . lang . rscript envs = { \"plink\" : config . exe . plink2 , \"ncores\" : config . misc . ncores , \"cutoff\" : 1e-5 , \"plot\" : True , \"devpars\" : { \"width\" : 1000 , \"height\" : 800 , \"res\" : 100 }, } script = \"file://../scripts/snp/PlinkHWE.R\" plugin_opts = { \"report\" : \"file://../reports/snp/PlinkHWE.svelte\" } class PlinkHet ( Proc ): DOCS \"\"\"Calculation of sample heterozygosity. Input: indir: Input directory containing the PLINK files. Including `.bed`, `.bim`, and `.fam` files Output: outdir: Output file containing the heterozygosity results. Including [`.het`](https://www.cog-genomics.org/plink/2.0/formats#het) file for the original heterozygosity report from PLINK and `.het.fail` for the samples that failed the heterozygosity test. It also includes binary files `.bed`, `.bim`, and `.fam` Envs: plink: Path to PLINK v2, at least v2.00a5.10 ncores (type=int): Number of cores/threads to use, will pass to plink `--threads` option cutoff (type=float): Heterozygosity cutoff, samples with heterozygosity beyond `mean - cutoff * sd` or `mean + cutoff * sd` will be considered as outliers. plot (flag): If set, plot the distribution of heterozygosity values. devpars (ns): The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot \"\"\" input = \"indir:dir\" output = \"outdir:dir:{{in.indir | stem}}.het\" lang = config . lang . rscript envs = { \"plink\" : config . exe . plink2 , \"ncores\" : config . misc . ncores , \"cutoff\" : 3.0 , \"plot\" : True , \"devpars\" : { \"width\" : 1000 , \"height\" : 800 , \"res\" : 100 }, } script = \"file://../scripts/snp/PlinkHet.R\" plugin_opts = { \"report\" : \"file://../reports/snp/PlinkHet.svelte\" } class PlinkCallRate ( Proc ): DOCS \"\"\"Calculation of call rate for the samples and variants. Input: indir: Input directory containing the PLINK files. Including `.bed`, `.bim`, and `.fam` files Output: outdir: Output file containing the call rate results. Including [`.imiss`](https://www.cog-genomics.org/plink/2.0/formats#imiss) file for missing calls for samples, [`.lmiss`](https://www.cog-genomics.org/plink/2.0/formats#lmiss) for missing calls for variants, `.samplecr.fail` for the samples fail sample call rate cutoff (`args.samplecr`), and `.varcr.fail` for the SNPs fail snp call rate cutoff (`args.varcr`). It also includes binary files `.bed`, `.bim`, and `.fam`. Envs: plink: Path to PLINK v2 ncores (type=int): Number of cores/threads to use, will pass to plink `--threads` option samplecr (type=float): Sample call rate cutoff varcr (type=float): Variant call rate cutoff max_iter (type=int): Maximum number of iterations to run the call rate calculation. Since the sample and variant call rates are affected by each other, it may be necessary to iterate the calculation to get the stable results. plot (flag): If set, plot the distribution of call rates. devpars (ns): The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot \"\"\" input = \"indir:dir\" output = \"outdir:dir:{{in.indir | stem}}.callrate\" lang = config . lang . rscript envs = { \"plink\" : config . exe . plink2 , \"ncores\" : config . misc . ncores , \"samplecr\" : 0.95 , \"varcr\" : 0.95 , \"max_iter\" : 3 , \"plot\" : True , \"devpars\" : { \"width\" : 1000 , \"height\" : 800 , \"res\" : 100 }, } script = \"file://../scripts/snp/PlinkCallRate.R\" plugin_opts = { \"report\" : \"file://../reports/snp/PlinkCallRate.svelte\" } class PlinkFilter ( Proc ): DOCS \"\"\"Filter samples and variants for PLINK files. Input: indir: Input directory containing the PLINK files. Including `.bed`, `.bim`, and `.fam` files samples_file: File containing the sample IDs. variants_file: File containing the variant IDs or regions. Output: outdir: Output directory containing the filtered PLINK files. Including `.bed`, `.bim`, and `.fam` files Envs: plink: Path to PLINK v2 ncores (type=int): Number of cores/threads to use, will pass to plink `--threads` option samples (auto): Sample IDs. If both FID and IID should be provided and separatedby `/`. Otherwise, assuming the same FID and IID. A list of sample IDs or string concatenated by `,`. If either `in.samples_file` or `envs.samples_file` is set, this will be ignored. variants (auto): Variant IDs. A list of variant IDs or string concatenated by `,`. If either `in.variants_file` or `envs.variants_file` is set, this will be ignored. samples_file: File containing the sample IDs. If `in.samples_file` is set, this will be ignored. variants_file: File containing the variant IDs. If `in.variants_file` is set, this will be ignored. keep (flag): Use `samples`/`variants`/`samples_file`/`variants_file` to only keep the specified samples/variants, instead of removing them. vfile_type (choice): The type of the variants file. - id: Variant IDs - bed0: 0-based BED file - bed1: 1-based BED file chr: Chromosome to keep. For example, `1-4 22 XY` will keep chromosomes 1 to 4, 22, and XY. not_chr: Chromosome to remove. For example, `1-4 22 XY` will remove chromosomes 1 to 4, 22, and XY. autosome (flag): Excludes all unplaced and non-autosomal variants autosome_xy (flag): Does `autosome` but does not exclude the pseudo-autosomal region of X. snps_only (auto): Excludes all variants with one or more multi-character allele codes. With 'just-acgt', variants with single-character allele codes outside of {'A', 'C', 'G', 'T', 'a', 'c', 'g', 't', <missing code>} are also excluded. \"\"\" input = [ \"indir:dir\" , \"samples_file:file\" , \"variants_file:file\" , ] output = \"outdir:dir:{{in.indir | stem}}.filtered\" lang = config . lang . python envs = { \"plink\" : config . exe . plink2 , \"ncores\" : config . misc . ncores , \"samples\" : None , \"variants\" : None , \"samples_file\" : None , \"variants_file\" : None , \"keep\" : False , \"vfile_type\" : \"id\" , \"chr\" : None , \"not_chr\" : None , \"autosome\" : False , \"autosome_xy\" : False , \"snps_only\" : False , } script = \"file://../scripts/snp/PlinkFilter.py\" class PlinkFreq ( Proc ): DOCS \"\"\"Calculate allele frequencies for the variants. Input: indir: Input directory containing the PLINK files. Including `.bed`, `.bim`, and `.fam` files Output: outdir: Output file containing the allele frequency results. By default, it includes [`.afreq`](https://www.cog-genomics.org/plink/2.0/formats#afreq) file for the allele frequency report from PLINK. Modifiers can be added to change this behavior. See `envs.modifier` for more information. When `envs.filter != no`, it also includes binary files `.bed`, `.bim`, and `.fam` after filtering with `envs.cutoff`. Envs: plink: Path to PLINK v2 ncores (type=int): Number of cores/threads to use, will pass to plink `--threads` option modifier (choice): The modifier of `--freq` to control the output behavior. - none: No modifier, only the `.afreq` file will be generated. `MAF` (minor allele frequency) will be added in addition to the `REF_FREQ` and `ALT1_FREQ` columns. Check `.afreqx` for the added columns. - counts: write allele count report to `.acount`. See <https://www.cog-genomics.org/plink/2.0/formats#afreq>. `ALT1`, `ALT1_CT`, and `REF_CT` are added. Check `.acountx` for the added columns. - x: write genotype count report to `.gcount` Like `--freqx` in v1.9, `--geno-counts` will be run to generate the genotype counts. `ALT1`, `HET_REF_ALT1_CT`, and `HOM_ALT1_CT` are added. Check `.gcountx` for the added columns. gz (flag): If set, compress the output files. cutoff (auto): Cutoffs to mark or filter the variants. If a float is given, default column will be used based on the modifier. For `modifier=\"none\"`, it defaults to `MAF`. For `modifier=\"counts\"`, it defaults to `ALT1_CT`. For `modifier=\"x\"`, it defaults to `HOM_ALT1_CT`. Or this could be a dictionary to specify the column names and cutoffs. For example, `{\"MAF\": 0.05}`. filter (auto): The direction of filtering variants based on `cutoff`. If a single value is given, it will apply to all columns provided in `cutoff`. If a dictionary is given, it will apply to the corresponding column. If a column cannot be found in the dictionary, it defaults to `no`. no: Do not filter variants (no binary files are generated in outdir). gt: Filter variants with MAF greater than `cutoff`. lt: Filter variants with MAF less than `cutoff`. ge: Filter variants with MAF greater than or equal to `cutoff`. le: Filter variants with MAF less than or equal to `cutoff`. plot (flag): If set, plot the distribution of allele frequencies. devpars (ns): The device parameters for the plot. - width (type=int): Width of the plot - height (type=int): Height of the plot - res (type=int): Resolution of the plot \"\"\" input = \"indir:dir\" output = \"outdir:dir:{{in.indir | stem}}.freq\" lang = config . lang . rscript envs = { \"plink\" : config . exe . plink2 , \"ncores\" : config . misc . ncores , \"modifier\" : \"none\" , \"gz\" : False , \"cutoff\" : {}, \"filter\" : {}, \"plot\" : True , \"devpars\" : { \"width\" : 1000 , \"height\" : 800 , \"res\" : 100 }, } script = \"file://../scripts/snp/PlinkFreq.R\" plugin_opts = { \"report\" : \"file://../reports/snp/PlinkFreq.svelte\" } class PlinkUpdateName ( Proc ): DOCS \"\"\"Update variant names in PLINK files. See also <https://www.cog-genomics.org/plink/2.0/data#update_map>. Input: indir: Input directory containing the PLINK files. Including `.bed`, `.bim`, and `.fam` files namefile: File containing the variant names to update. Either a file containing two columns, the first column is the old variant name, and the second column is the new variant name. Or a VCF file containing the variant names to update. When a VCF file is given, the chromosome, position, and reference and alternate alleles will be used to match the variants. Output: outdir: Output directory containing the updated PLINK files. Including `.bed`, `.bim`, and `.fam` files Envs: ncores: Number of cores/threads to use, will pass to plink `--threads` option plink: Path to PLINK v2 bcftools: Path to bcftools match_alt (choice): How to match alternate alleles when `in.namefile` is a VCF file. - exact: Matches alternate alleles exactly. - all: Matches alternate alleles regardless of the order. `chr1:100:A:T,G` matches `chr1:100:A:G,T` or `chr1:100:A:T,G`. - any: Matches any alternate allele. For example, `chr1:100:A:T,G` matches `chr1:100:A:G,C` - first_included: Matches when the first allele is included. For example, `chr1:100:A:T,G` matches `chr1:100:A:C,T`. - first: Match first alternate allele For example, `chr1:100:A:T,G` matches `chr1:100:A:T`. - none: Do not match alternate alleles \"\"\" input = \"indir:dir, namefile:file\" output = \"outdir:dir:{{in.indir | stem}}.newnames\" lang = config . lang . python envs = { \"ncores\" : config . misc . ncores , \"plink\" : config . exe . plink2 , \"bcftools\" : config . exe . bcftools , \"match_alt\" : \"exact\" , } script = \"file://../scripts/snp/PlinkUpdateName.py\"","title":"biopipen.ns.snp"},{"location":"api/source/biopipen.ns.stats/","text":"SOURCE CODE biopipen.ns. stats DOCS \"\"\"Provides processes for statistics.\"\"\" from ..core.proc import Proc from ..core.config import config class ChowTest ( Proc ): DOCS \"\"\"Massive Chow tests. See Also https://en.wikipedia.org/wiki/Chow_test Input: infile: The input data file. The rows are samples and the columns are features. It must be tab-delimited. ``` Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 ``` groupfile: The group file. The rows are the samples and the columns are the groupings. It must be tab-delimited. ``` Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 ``` fmlfile: The formula file. The first column is grouping and the second column is the formula. It must be tab-delimited. ``` Group Formula ... # Other columns to be added to outfile G1 Fn ~ F1 + Fx + Fy # Fx, Fy could be covariates G1 Fn ~ F2 + Fx + Fy ... Gk Fn ~ F3 + Fx + Fy ``` Output: outfile: The output file. It is a tab-delimited file with the first column as the grouping and the second column as the p-value. ``` Group Formula ... Pooled Groups SSR SumSSR Fstat Pval Padj G1 Fn ~ F1 0.123 2 1 0.123 0.123 0.123 0.123 G1 Fn ~ F2 0.123 2 1 0.123 0.123 0.123 0.123 ... Gk Fn ~ F3 0.123 2 1 0.123 0.123 0.123 0.123 ``` Envs: padj (choice): The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. transpose_input (flag): Whether to transpose the input file. transpose_group (flag): Whether to transpose the group file. \"\"\" input = \"infile:file, groupfile:file, fmlfile:file\" output = \"outfile:file:{{in.infile | stem}}.chowtest.txt\" lang = config . lang . rscript envs = { \"padj\" : \"none\" , \"transpose_input\" : False , \"transpose_group\" : False , } script = \"file://../scripts/stats/ChowTest.R\" class Mediation ( Proc ): DOCS \"\"\"Mediation analysis. The flowchart of mediation analysis: ![Mediation Analysis](https://library.virginia.edu/sites/default/files/inline-images/mediation_flowchart-1.png) Reference: - <https://library.virginia.edu/data/articles/introduction-to-mediation-analysis> - <https://en.wikipedia.org/wiki/Mediation_(statistics)> - <https://tilburgsciencehub.com/topics/analyze/regression/linear-regression/mediation-analysis/> - <https://ademos.people.uic.edu/Chapter14.html> Input: infile: The input data file. The rows are samples and the columns are features. It must be tab-delimited. ``` Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 ``` fmlfile: The formula file. ``` Case M Y X Cov Model_M Model_Y Case1 F1 F2 F3 F4,F5 glm lm ... ``` Where Y is the outcome variable, X is the predictor variable, M is the mediator variable, and Case is the case name. Model_M and Model_Y are the models for M and Y, respectively. `envs.cases` will be ignored if this is provided. Output: outfile: The output file. Columns to help understand the results: Total Effect: a total effect of X on Y (without M) (`Y ~ X`). ADE: A Direct Effect of X on Y after taking into account a mediation effect of M (`Y ~ X + M`). ACME: The Mediation Effect, the total effect minus the direct effect, which equals to a product of a coefficient of X in the second step and a coefficient of M in the last step. The goal of mediation analysis is to obtain this indirect effect and see if it's statistically significant. Envs: ncores (type=int): Number of cores to use for parallelization for cases. sims (type=int): Number of Monte Carlo draws for nonparametric bootstrap or quasi-Bayesian approximation. Will be passed to `mediation::mediate` function. args (ns): Other arguments passed to `mediation::mediate` function. - <more>: More arguments passed to `mediation::mediate` function. See: <https://rdrr.io/cran/mediation/man/mediate.html> padj (choice): The method for (ACME) p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. cases (type=json): The cases for mediation analysis. Ignored if `in.fmlfile` is provided. A json/dict with case names as keys and values as a dict of M, Y, X, Cov, Model_M, Model_Y. For example: ```json { \"Case1\": { \"M\": \"F1\", \"Y\": \"F2\", \"X\": \"F3\", \"Cov\": \"F4,F5\", \"Model_M\": \"glm\", \"Model_Y\": \"lm\" }, ... } ``` transpose_input (flag): Whether to transpose the input file. \"\"\" # noqa: E501 input = \"infile:file, fmlfile:file\" output = \"outfile:file:{{in.infile | stem}}.mediation.txt\" lang = config . lang . rscript envs = { \"ncores\" : config . misc . ncores , \"sims\" : 1000 , \"args\" : {}, \"padj\" : \"none\" , \"cases\" : {}, \"transpose_input\" : False , } script = \"file://../scripts/stats/Mediation.R\" class LiquidAssoc ( Proc ): DOCS \"\"\"Liquid association tests. See Also https://github.com/gundt/fastLiquidAssociation Requires https://github.com/pwwang/fastLiquidAssociation Input: infile: The input data file. The rows are samples and the columns are features. It must be tab-delimited. ``` Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 ``` The features (columns) will be tested pairwise, which will be the X and Y columns in the result of `fastMLA` covfile: The covariate file. The rows are the samples and the columns are the covariates. It must be tab-delimited. If provided, the data in `in.infile` will be adjusted by covariates by regressing out the covariates and the residuals will be used for liquid association tests. groupfile: The group file. The rows are the samples and the columns are the groupings. It must be tab-delimited. ``` Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 ``` This will be served as the Z column in the result of `fastMLA` This can be omitted. If so, `envs.nvec` should be specified, which is to select column from `in.infile` as Z. fmlfile: The formula file. The 3 columns are X3, X12 and X21. The results will be filtered based on the formula. It must be tab-delimited without header. Output: outfile: The output file. ``` X12 X21 X3 rhodiff MLA value estimates san.se wald Pval model C38 C46 C5 0.87 0.32 0.67 0.20 10.87 0 F C46 C38 C5 0.87 0.32 0.67 0.20 10.87 0 F C27 C39 C4 0.94 0.34 1.22 0.38 10.03 0 F ``` Envs: nvec: The column index (1-based) of Z in `in.infile`, if `in.groupfile` is omitted. You can specify multiple columns by comma-seperated values, or a range of columns by `-`. For example, `1,3,5-7,9`. It also supports column names. For example, `F1,F3`. `-` is not supported for column names. x: Similar as `nvec`, but limit X group to given features. The rest of features (other than X and Z) in `in.infile` will be used as Y. The features in `in.infile` will still be tested pairwise, but only features in X and Y will be kept. topn (type=int): Number of results to return by `fastMLA`, ordered from highest `|MLA|` value descending. The default of the package is 2000, but here we set to 1e6 to return as many results as possible (also good to do pvalue adjustment). rvalue (type=float): Tolerance value for LA approximation. Lower values of rvalue will cause a more thorough search, but take longer. cut (type=int): Value passed to the GLA function to create buckets (equal to number of buckets+1). Values placing between 15-30 samples per bucket are optimal. Must be a positive integer>1. By default, `max(ceiling(nrow(data)/22), 4)` is used. ncores (type=int): Number of cores to use for parallelization. padj (choice): The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. transpose_input (flag): Whether to transpose the input file. transpose_group (flag): Whether to transpose the group file. transpose_cov (flag): Whether to transpose the covariate file. xyz_names: The names of X12, X21 and X3 in the final output file. Separated by comma. For example, `X12,X21,X3`. \"\"\" input = \"infile:file, covfile:file, groupfile:file, fmlfile:file\" output = \"outfile:file:{{in.infile | stem}}.liquidassoc.txt\" lang = config . lang . rscript envs = { \"nvec\" : None , \"x\" : None , \"topn\" : 1e6 , \"rvalue\" : 0.5 , \"cut\" : 20 , \"ncores\" : config . misc . ncores , \"padj\" : \"none\" , \"transpose_input\" : False , \"transpose_group\" : False , \"transpose_cov\" : False , \"xyz_names\" : None , } script = \"file://../scripts/stats/LiquidAssoc.R\" class DiffCoexpr ( Proc ): DOCS \"\"\"Differential co-expression analysis. See also <https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-497> and <https://github.com/DavisLaboratory/dcanr/blob/8958d61788937eef3b7e2b4118651cbd7af7469d/R/inference_methods.R#L199>. Input: infile: The input data file. The rows are samples and the columns are features. It must be tab-delimited. ``` Sample F1 F2 F3 ... Fn S1 1.2 3.4 5.6 7.8 S2 2.3 4.5 6.7 8.9 ... Sm 5.6 7.8 9.0 1.2 ``` groupfile: The group file. The rows are the samples and the columns are the groupings. It must be tab-delimited. ``` Sample G1 G2 G3 ... Gk S1 0 1 0 0 S2 2 1 0 NA # exclude this sample ... Sm 1 0 0 0 ``` Output: outfile: The output file. It is a tab-delimited file with the first column as the feature pair and the second column as the p-value. ``` Group Feature1 Feature2 Pval Padj G1 F1 F2 0.123 0.123 G1 F1 F3 0.123 0.123 ... ``` Envs: method (choice): The method used to calculate the differential co-expression. - pearson: Pearson correlation. - spearman: Spearman correlation. beta: The beta value for the differential co-expression analysis. padj (choice): The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. perm_batch (type=int): The number of permutations to run in each batch seed (type=int): The seed for random number generation ncores (type=int): The number of cores to use for parallelization transpose_input (flag): Whether to transpose the input file. transpose_group (flag): Whether to transpose the group file. \"\"\" # noqa: E501 input = \"infile:file, groupfile:file\" output = \"outfile:file:{{in.infile | stem}}.diffcoexpr.txt\" lang = config . lang . rscript envs = { \"method\" : \"pearson\" , \"beta\" : 6 , \"padj\" : \"none\" , \"perm_batch\" : 20 , \"seed\" : 8525 , \"ncores\" : config . misc . ncores , \"transpose_input\" : False , \"transpose_group\" : False , } script = \"file://../scripts/stats/DiffCoexpr.R\" class MetaPvalue ( Proc ): DOCS \"\"\"Calulation of meta p-values. If there is only one input file, only the p-value adjustment will be performed. Input: infiles: The input files. Each file is a tab-delimited file with multiple columns. There should be ID column(s) to match the rows in other files and p-value column(s) to be combined. The records will be full-joined by ID. When only one file is provided, only the pvalue adjustment will be performed when `envs.padj` is not `none`, otherwise the input file will be copied to `out.outfile`. Output: outfile: The output file. It is a tab-delimited file with the first column as the ID and the second column as the combined p-value. ``` ID ID1 ... Pval Padj a x ... 0.123 0.123 b y ... 0.123 0.123 ... ``` Envs: id_cols: The column names used in all `in.infiles` as ID columns. Multiple columns can be specified by comma-seperated values. For example, `ID1,ID2`, where `ID1` is the ID column in the first file and `ID2` is the ID column in the second file. If `id_exprs` is specified, this should be a single column name for the new ID column in each `in.infiles` and the final `out.outfile`. id_exprs: The R expressions for each `in.infiles` to get ID column(s). pval_cols: The column names used in all `in.infiles` as p-value columns. Different columns can be specified by comma-seperated values for each `in.infiles`. For example, `Pval1,Pval2`. method (choice): The method used to calculate the meta-pvalue. - fisher: Fisher's method. - sumlog: Sum of logarithms (same as Fisher's method) - logitp: Logit method. - sumz: Sum of z method (Stouffer's method). - meanz: Mean of z method. - meanp: Mean of p method. - invt: Inverse t method. - sump: Sum of p method (Edgington's method). - votep: Vote counting method. - wilkinsonp: Wilkinson's method. - invchisq: Inverse chi-square method. keep_single (flag): Whether to keep the original p-value when there is only one p-value. na: The method to handle NA values. -1 to skip the record. Otherwise NA will be replaced by the given value. padj (choice): The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. \"\"\" input = \"infiles:files\" output = \"outfile:file:{{in.infiles | first | stem}}.metapval.txt\" lang = config . lang . rscript envs = { \"id_cols\" : None , \"id_exprs\" : None , \"pval_cols\" : None , \"method\" : \"fisher\" , \"na\" : - 1 , \"keep_single\" : True , \"padj\" : \"none\" , } script = \"file://../scripts/stats/MetaPvalue.R\" class MetaPvalue1 ( Proc ): DOCS \"\"\"Calulation of meta p-values. Unlike `MetaPvalue`, this process only accepts one input file. The p-values will be grouped by the ID columns and combined by the selected method. Input: infile: The input file. The file is a tab-delimited file with multiple columns. There should be ID column(s) to group the rows where p-value column(s) to be combined. Output: outfile: The output file. It is a tab-delimited file with the first column as the ID and the second column as the combined p-value. ``` ID ID1 ... Pval Padj a x ... 0.123 0.123 b y ... 0.123 0.123 ... ``` Envs: id_cols: The column names used in `in.infile` as ID columns. Multiple columns can be specified by comma-seperated values. For example, `ID1,ID2`. pval_col: The column name used in `in.infile` as p-value column. method (choice): The method used to calculate the meta-pvalue. - fisher: Fisher's method. - sumlog: Sum of logarithms (same as Fisher's method) - logitp: Logit method. - sumz: Sum of z method (Stouffer's method). - meanz: Mean of z method. - meanp: Mean of p method. - invt: Inverse t method. - sump: Sum of p method (Edgington's method). - votep: Vote counting method. - wilkinsonp: Wilkinson's method. - invchisq: Inverse chi-square method. na: The method to handle NA values. -1 to skip the record. Otherwise NA will be replaced by the given value. keep_single (flag): Whether to keep the original p-value when there is only one p-value. padj (choice): The method for p-value adjustment. - none: No p-value adjustment (no Padj column in outfile). - holm: Holm-Bonferroni method. - hochberg: Hochberg method. - hommel: Hommel method. - bonferroni: Bonferroni method. - BH: Benjamini-Hochberg method. - BY: Benjamini-Yekutieli method. - fdr: FDR correction method. \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.metapval.txt\" lang = config . lang . rscript envs = { \"id_cols\" : None , \"pval_col\" : None , \"method\" : \"fisher\" , \"na\" : - 1 , \"keep_single\" : True , \"padj\" : \"none\" , } script = \"file://../scripts/stats/MetaPvalue1.R\"","title":"biopipen.ns.stats"},{"location":"api/source/biopipen.ns.tcgamaf/","text":"SOURCE CODE biopipen.ns. tcgamaf DOCS \"\"\"Processes for TCGA MAF files.\"\"\" from ..core.proc import Proc from ..core.config import config class Maf2Vcf ( Proc ): DOCS \"\"\"Converts a MAF file to a VCF file. This is a wrapper around the `maf2vcf` script from the `maf2vcf` package. Input: infile: The input MAF file Output: outfile: Output multi-sample VCF containing all TN-pairs outdir: Path to output directory where VCFs will be stored, one per TN-pair Envs: perl: Path to perl to run `maf2vcf.pl` samtools: Path to samtools to be used in `maf2vcf.pl` args: Other arguments to pass to the script \"\"\" input = \"infile:file\" output = [ 'outfile:file:{{in.infile | stem}}.vcfs/' '{{in.infile | stem}}.multisample.vcf' , 'outdir:dir:{{in.infile | stem}}.vcfs' ] lang = config . lang . python envs = { \"perl\" : config . lang . perl , \"samtools\" : config . exe . samtools , \"ref\" : config . ref . reffa , \"args\" : { \"per-tn-vcfs\" : True }, } script = \"file://../scripts/tcgamaf/Maf2Vcf.py\" class MafAddChr ( Proc ): DOCS \"\"\"Adds the `chr` prefix to chromosome names in a MAF file if not present. Input: infile: The input MAF file Output: outfile: The output MAF file \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | stem}}.maf\" lang = config . lang . python script = \"file://../scripts/tcgamaf/MafAddChr.py\"","title":"biopipen.ns.tcgamaf"},{"location":"api/source/biopipen.ns.tcr/","text":"SOURCE CODE biopipen.ns. tcr DOCS \"\"\"Tools to analyze single-cell TCR sequencing data\"\"\" from pipen.utils import mark from ..core.defaults import SCRIPT_DIR from ..core.proc import Proc from ..core.config import config @mark ( deprecated = \" {proc.name} is deprecated, use ScRepLoading instead.\" ) DOCS class ImmunarchLoading ( Proc ): \"\"\"Immuarch - Loading data Load the raw data into [`immunarch`](https://immunarch.com) object, using [`immunarch::repLoad()`](https://immunarch.com/reference/repLoad.html). For the data path specified at `TCRData` in the input file, we will first find `filtered_contig_annotations.csv` and `filtered_config_annotations.csv.gz` in the path. If neighter of them exists, we will find `all_contig_annotations.csv` and `all_contig_annotations.csv.gz` in the path and a warning will be raised (You can find it at `./.pipen/<pipeline-name>/ImmunarchLoading/0/job.stderr`). If none of the files exists, an error will be raised. This process will also generate a text file with the information for each cell. The file will be saved at `./.pipen/<pipeline-name>/ImmunarchLoading/0/output/<prefix>.tcr.txt`. The file can be used by the `SeuratMetadataMutater` process to integrate the TCR-seq data into the `Seurat` object for further integrative analysis. `envs.metacols` can be used to specify the columns to be exported to the text file. Input: metafile: The meta data of the samples A tab-delimited file Two columns are required: * `Sample` to specify the sample names. * `TCRData` to assign the path of the data to the samples, and this column will be excluded as metadata. Immunarch is able to fetch the sample names from the names of the target files. However, 10x data yields result like `filtered_contig_annotations.csv`, which doesn't have any name information. Output: rdsfile: The RDS file with the data and metadata, which can be processed by other `immunarch` functions. metatxt: The meta data at cell level, which can be used to attach to the Seurat object Envs: prefix: The prefix to the barcodes. You can use placeholder like `{Sample}_` to use the meta data from the `immunarch` object. The prefixed barcodes will be saved in `out.metatxt`. The `immunarch` object keeps the original barcodes, but the prefix is saved at `immdata$prefix`. /// Note This option is useful because the barcodes for the cells from scRNA-seq data are usually prefixed with the sample name, for example, `Sample1_AAACCTGAGAAGGCTA-1`. However, the barcodes for the cells from scTCR-seq data are usually not prefixed with the sample name, for example, `AAACCTGAGAAGGCTA-1`. So we need to add the prefix to the barcodes for the scTCR-seq data, and it is easier for us to integrate the data from different sources later. /// tmpdir: The temporary directory to link all data files. `Immunarch` scans a directory to find the data files. If the data files are not in the same directory, we can link them to a temporary directory and pass the temporary directory to `Immunarch`. This option is useful when the data files are in different directories. mode: Either \"single\" for single chain data or \"paired\" for paired chain data. For `single`, only TRB chain will be kept at `immdata$data`, information for other chains will be saved at `immdata$tra` and `immdata$multi`. extracols (list): The extra columns to be exported to the text file. You can refer to the [immunarch documentation](https://immunarch.com/articles/v2_data.html#immunarch-data-format) to get a sense for the full list of the columns. The columns may vary depending on the data source. The columns from `immdata$meta` and some core columns, including `Barcode`, `CDR3.aa`, `Clones`, `Proportion`, `V.name`, `J.name`, and `D.name` will be exported by default. You can use this option to specify the extra columns to be exported. \"\"\" # noqa: E501 input = \"metafile:file\" output = [ \"rdsfile:file:{{in.metafile | stem}}.immunarch.RDS\" , \"metatxt:file:{{in.metafile | stem}}.tcr.txt\" , ] lang = config . lang . rscript envs = { \"tmpdir\" : config . path . tmpdir , \"prefix\" : \" {Sample} _\" , \"mode\" : \"paired\" , \"extracols\" : [], } script = \"file://../scripts/tcr/ImmunarchLoading.R\" @mark ( deprecated = True ) DOCS class ImmunarchFilter ( Proc ): \"\"\"Immunarch - Filter data See <https://immunarch.com/articles/web_only/repFilter_v3.html> Input: immdata: The data loaded by `immunarch::repLoad()` filterfile: A config file in TOML. A dict of configurations with keys as the names of the group and values dicts with following keys. See `envs.filters` Output: outfile: The filtered `immdata` groupfile: Also a group file with rownames as cells and column names as each of the keys in `in.filterfile` or `envs.filters`. The values will be subkeys of the dicts in `in.filterfile` or `envs.filters`. Envs: filters: The filters to filter the data You can have multiple cases (groups), the names will be the keys of this dict, values are also dicts with keys the methods supported by `immunarch::repFilter()`. There is one more method `by.count` supported to filter the count matrix. For `by.meta`, `by.repertoire`, `by.rep`, `by.clonotype` or `by.col` the values will be passed to `.query` of `repFilter()`. You can also use the helper functions provided by `immunarch`, including `morethan`, `lessthan`, `include`, `exclude` and `interval`. If these functions are not used, `include(value)` will be used by default. For `by.count`, the value of `filter` will be passed to `dplyr::filter()` to filter the count matrix. You can also specify `ORDER` to define the filtration order, which defaults to 0, higher `ORDER` gets later executed. Each subkey/subgroup must be exclusive For example: >>> { >>> \"name\": \"BM_Post_Clones\", >>> \"filters\" { >>> \"Top_20\": { >>> \"SAVE\": True, # Save the filtered data to immdata >>> \"by.meta\": {\"Source\": \"BM\", \"Status\": \"Post\"}, >>> \"by.count\": { >>> \"ORDER\": 1, \"filter\": \"TOTAL %%in%% TOTAL[1:20]\" >>> } >>> }, >>> \"Rest\": { >>> \"by.meta\": {\"Source\": \"BM\", \"Status\": \"Post\"}, >>> \"by.count\": { >>> \"ORDER\": 1, \"filter\": \"!TOTAL %%in%% TOTAL[1:20]\" >>> } >>> } >>> } prefix: The prefix will be added to the cells in the output file Placeholders like `{Sample}_` can be used to from the meta data metacols: The extra columns to be exported to the group file. \"\"\" input = \"immdata:file, filterfile:file\" output = \"\"\" outfile:file:{{in.immdata | stem}}.RDS, groupfile:file:{ % i f in.filterfile -%} {{- in.filterfile | toml_load | attr: \"name\" | append: \".txt\" -}} { %- e lse -%} {{- envs.filters | attr: \"name\" | append: \".txt\" -}} { %- e ndif -%} \"\"\" envs = { \"prefix\" : \" {Sample} _\" , \"filters\" : {}, \"metacols\" : [ \"Clones\" , \"Proportion\" , \"CDR3.aa\" ], } lang = config . lang . rscript script = \"file://../scripts/tcr/ImmunarchFilter.R\" @mark ( deprecated = \" {proc.name} is deprecated, use ClonalStats instead.\" ) DOCS class Immunarch ( Proc ): \"\"\"Exploration of Single-cell and Bulk T-cell/Antibody Immune Repertoires See <https://immunarch.com/articles/web_only/v3_basic_analysis.html> After [`ImmunarchLoading`](!!#biopipennstcrimmunarchloading) loads the raw data into an [immunarch](https://immunarch.com) object, this process wraps the functions from [`immunarch`](https://immunarch.com) to do the following: - Basic statistics, provided by [`immunarch::repExplore`](https://immunarch.com/reference/repExplore.html), such as number of clones or distributions of lengths and counts. - The clonality of repertoires, provided by [`immunarch::repClonality`](https://immunarch.com/reference/repClonality.html) - The repertoire overlap, provided by [`immunarch::repOverlap`](https://immunarch.com/reference/repOverlap.html) - The repertoire overlap, including different clustering procedures and PCA, provided by [`immunarch::repOverlapAnalysis`](https://immunarch.com/reference/repOverlapAnalysis.html) - The distributions of V or J genes, provided by [`immunarch::geneUsage`](https://immunarch.com/reference/geneUsage.html) - The diversity of repertoires, provided by [`immunarch::repDiversity`](https://immunarch.com/reference/repDiversity.html) - The dynamics of repertoires across time points/samples, provided by [`immunarch::trackClonotypes`](https://immunarch.com/reference/trackClonotypes.html) - The spectratype of clonotypes, provided by [`immunarch::spectratype`](https://immunarch.com/reference/spectratype.html) - The distributions of kmers and sequence profiles, provided by [`immunarch::getKmers`](https://immunarch.com/reference/getKmers.html) - The V-J junction circos plots, implemented within the script of this process. Environment Variable Design: With different sets of arguments, a single function of the above can perform different tasks. For example, `repExplore` can be used to get the statistics of the size of the repertoire, the statistics of the length of the CDR3 region, or the statistics of the number of the clonotypes. Other than that, you can also have different ways to visualize the results, by passing different arguments to the [`immunarch::vis`](https://immunarch.com/reference/vis.html) function. For example, you can pass `.by` to `vis` to visualize the results of `repExplore` by different groups. Before we explain each environment variable in details in the next section, we will give some examples here to show how the environment variables are organized in order for a single function to perform different tasks. ```toml # Repertoire overlapping [Immunarch.envs.overlaps] # The method to calculate the overlap, passed to `repOverlap` method = \"public\" ``` What if we want to calculate the overlap by different methods at the same time? We can use the following configuration: ```toml [Immunarch.envs.overlaps.cases] Public = { method = \"public\" } Jaccard = { method = \"jaccard\" } ``` Then, the `repOverlap` function will be called twice, once with `method = \"public\"` and once with `method = \"jaccard\"`. We can also use different arguments to visualize the results. These arguments will be passed to the `vis` function: ```toml [Immunarch.envs.overlaps.cases.Public] method = \"public\" vis_args = { \"-plot\": \"heatmap2\" } [Immunarch.envs.overlaps.cases.Jaccard] method = \"jaccard\" vis_args = { \"-plot\": \"heatmap2\" } ``` `-plot` will be translated to `.plot` and then passed to `vis`. If multiple cases share the same arguments, we can use the following configuration: ```toml [Immunarch.envs.overlaps] vis_args = { \"-plot\": \"heatmap2\" } [Immunarch.envs.overlaps.cases] Public = { method = \"public\" } Jaccard = { method = \"jaccard\" } ``` For some results, there are futher analysis that can be performed. For example, for the repertoire overlap, we can perform clustering and PCA (see also <https://immunarch.com/articles/web_only/v4_overlap.html>): ```R imm_ov1 <- repOverlap(immdata$data, .method = \"public\", .verbose = F) repOverlapAnalysis(imm_ov1, \"mds\") %>% vis() repOverlapAnalysis(imm_ov1, \"tsne\") %>% vis() ``` In such a case, we can use the following configuration: ```toml [Immunarch.envs.overlaps] method = \"public\" [Immunarch.envs.overlaps.analyses.cases] MDS = { \"-method\": \"mds\" } TSNE = { \"-method\": \"tsne\" } ``` Then, the `repOverlapAnalysis` function will be called twice on the result generated by `repOverlap(immdata$data, .method = \"public\")`, once with `.method = \"mds\"` and once with `.method = \"tsne\"`. We can also use different arguments to visualize the results. These arguments will be passed to the `vis` function: ```toml [Immunarch.envs.overlaps] method = \"public\" [Immunarch.envs.overlaps.analyses] # See: <https://immunarch.com/reference/vis.immunr_hclust.html> vis_args = { \"-plot\": \"best\" } [Immunarch.envs.overlaps.analyses.cases] MDS = { \"-method\": \"mds\" } TSNE = { \"-method\": \"tsne\" } ``` Generally, you don't need to specify `cases` if you only have one case. A default case will be created for you. For multiple cases, the arguments at the same level as `cases` will be inherited by all cases. Examples: ```toml [Immunarch.envs.kmers] k = 5 ``` ![Immunarch kmers](https://immunarch.com/articles/web_only/v9_kmers_files/figure-html/unnamed-chunk-4-1.png) ```toml [Immunarch.envs.kmers] # Shared by cases k = 5 [Immunarch.envs.kmers.cases] Head5 = { head = 5, -position = \"stack\" } Head10 = { head = 10, -position = \"fill\" } Head30 = { head = 30, -position = \"dodge\" } ``` ![Immunarch kmers](https://immunarch.com/articles/web_only/v9_kmers_files/figure-html/unnamed-chunk-6-1.png) With motif profiling: ```toml [Immunarch.envs.kmers] k = 5 [Immnuarch.envs.kmers.profiles.cases] TextPlot = { method = \"self\", vis_args = { \"-plot\": \"text\" } } SeqPlot = { method = \"self\", vis_args = { \"-plot\": \"seq\" } } ``` ![Immunarch kmers](https://immunarch.com/articles/web_only/v9_kmers_files/figure-html/unnamed-chunk-10-1.png) Input: immdata: The data loaded by `immunarch::repLoad()` metafile: A cell-level metafile, where the first column must be the cell barcodes that match the cell barcodes in `immdata`. The other columns can be any metadata that you want to use for the analysis. The loaded metadata will be left-joined to the converted cell-level data from `immdata`. This can also be a Seurat object RDS file. If so, the `sobj@meta.data` will be used as the metadata. Output: outdir: The output directory Envs: mutaters (type=json;order=-9): The mutaters passed to `dplyr::mutate()` on expanded cell-level data to add new columns. The keys will be the names of the columns, and the values will be the expressions. The new names can be used in `volumes`, `lens`, `counts`, `top_clones`, `rare_clones`, `hom_clones`, `gene_usages`, `divs`, etc. prefix: The prefix to the barcodes. You can use placeholder like `{Sample}_` The prefixed barcodes will be used to match the barcodes in `in.metafile`. Not used if `in.metafile` is not specified. If `None` (default), `immdata$prefix` will be used. volumes (ns): Explore clonotype volume (sizes). - by: Groupings when visualize clonotype volumes, passed to the `.by` argument of `vis(imm_vol, .by = <values>)`. Multiple columns should be separated by `,`. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.volumes` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.volume.by`, `envs.volume.devpars`. lens (ns): Explore clonotype CDR3 lengths. - by: Groupings when visualize clonotype lengths, passed to the `.by` argument of `vis(imm_len, .by = <values>)`. Multiple columns should be separated by `,`. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.lens` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.lens.by`, `envs.lens.devpars`. counts (ns): Explore clonotype counts. - by: Groupings when visualize clonotype counts, passed to the `.by` argument of `vis(imm_count, .by = <values>)`. Multiple columns should be separated by `,`. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.counts` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.counts.by`, `envs.counts.devpars`. top_clones (ns): Explore top clonotypes. - by: Groupings when visualize top clones, passed to the `.by` argument of `vis(imm_top, .by = <values>)`. Multiple columns should be separated by `,`. - marks (list;itype=int): A numerical vector with ranges of the top clonotypes. Passed to the `.head` argument of `repClonoality()`. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.top_clones` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.top_clones.by`, `envs.top_clones.marks` and `envs.top_clones.devpars`. rare_clones (ns): Explore rare clonotypes. - by: Groupings when visualize rare clones, passed to the `.by` argument of `vis(imm_rare, .by = <values>)`. Multiple columns should be separated by `,`. - marks (list;itype=int): A numerical vector with ranges of abundance for the rare clonotypes in the dataset. Passed to the `.bound` argument of `repClonoality()`. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.rare_clones` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.rare_clones.by`, `envs.rare_clones.marks` and `envs.rare_clones.devpars`. hom_clones (ns): Explore homeo clonotypes. - by: Groupings when visualize homeo clones, passed to the `.by` argument of `vis(imm_hom, .by = <values>)`. Multiple columns should be separated by `,`. - marks (ns): A dict with the threshold of the half-closed intervals that mark off clonal groups. Passed to the `.clone.types` arguments of `repClonoality()`. The keys could be: - Rare (type=float): the rare clonotypes - Small (type=float): the small clonotypes - Medium (type=float): the medium clonotypes - Large (type=float): the large clonotypes - Hyperexpanded (type=float): the hyperexpanded clonotypes - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.hom_clones` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.hom_clones.by`, `envs.hom_clones.marks` and `envs.hom_clones.devpars`. overlaps (ns): Explore clonotype overlaps. - method (choice): The method to calculate overlaps. - public: number of public clonotypes between two samples. - overlap: a normalised measure of overlap similarity. It is defined as the size of the intersection divided by the smaller of the size of the two sets. - jaccard: conceptually a percentage of how many objects two sets have in common out of how many objects they have total. - tversky: an asymmetric similarity measure on sets that compares a variant to a prototype. - cosine: a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. - morisita: how many times it is more likely to randomly select two sampled points from the same quadrat (the dataset is covered by a regular grid of changing size) then it would be in the case of a random distribution generated from a Poisson process. Duplicate objects are merged with their counts are summed up. - inc+public: incremental overlaps of the N most abundant clonotypes with incrementally growing N using the public method. - inc+morisita: incremental overlaps of the N most abundant clonotypes with incrementally growing N using the morisita method. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - vis_args (type=json): Other arguments for the plotting functions `vis(imm_ov, ...)`. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - analyses (ns;order=8): Perform overlap analyses. - method: Plot the samples with these dimension reduction methods. The methods could be `hclust`, `tsne`, `mds` or combination of them, such as `mds+hclust`. You can also set to `none` to skip the analyses. They could also be combined, for example, `mds+hclust`. See <https://immunarch.com/reference/repOverlapAnalysis.html>. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.overlaps.analyses` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.overlaps.analyses.method`, `envs.overlaps.analyses.vis_args` and `envs.overlaps.analyses.devpars`. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.overlaps` will be used. If NO cases are specified, the default case will be added, with the key the default method and the values of `envs.overlaps.method`, `envs.overlaps.vis_args`, `envs.overlaps.devpars` and `envs.overlaps.analyses`. gene_usages (ns): Explore gene usages. - top (type=int): How many top (ranked by total usage across samples) genes to show in the plots. Use `0` to use all genes. - norm (flag): If True then use proportions of genes, else use counts of genes. - by: Groupings to show gene usages, passed to the `.by` argument of `vis(imm_gu_top, .by = <values>)`. Multiple columns should be separated by `,`. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - analyses (ns;order=8): Perform gene usage analyses. - method: The method to control how the data is going to be preprocessed and analysed. One of `js`, `cor`, `cosine`, `pca`, `mds` and `tsne`. Can also be combined with following methods for the actual analyses: `hclust`, `kmeans`, `dbscan`, and `kruskal`. For example: `cosine+hclust`. You can also set to `none` to skip the analyses. See <https://immunarch.com/articles/web_only/v5_gene_usage.html>. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.gene_usages.analyses` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.gene_usages.analyses.method`, `envs.gene_usages.analyses.vis_args` and `envs.gene_usages.analyses.devpars`. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.gene_usages` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.gene_usages.top`, `envs.gene_usages.norm`, `envs.gene_usages.by`, `envs.gene_usages.vis_args`, `envs.gene_usages.devpars` and `envs.gene_usages.analyses`. spects (ns): Spectratyping analysis. - quant: Select the column with clonal counts to evaluate. Set to `id` to count every clonotype once. Set to `count` to take into the account number of clones per clonotype. Multiple columns should be separated by `,`. - col: A string that specifies the column(s) to be processed. The output is one of the following strings, separated by the plus sign: \"nt\" for nucleotide sequences, \"aa\" for amino acid sequences, \"v\" for V gene segments, \"j\" for J gene segments. E.g., pass \"aa+v\" for spectratyping on CDR3 amino acid sequences paired with V gene segments, i.e., in this case a unique clonotype is a pair of CDR3 amino acid and V gene segment. Clonal counts of equal clonotypes will be summed up. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.spects` will be used. By default, a `By_Clonotype` case will be added, with the values of `quant = \"id\"` and `col = \"nt\"`, and a `By_Num_Clones` case will be added, with the values of `quant = \"count\"` and `col = \"aa+v\"`. divs (ns): Parameters to control the diversity analysis. - method (choice): The method to calculate diversity. - chao1: a nonparameteric asymptotic estimator of species richness. (number of species in a population). - hill: Hill numbers are a mathematically unified family of diversity indices. (differing only by an exponent q). - div: true diversity, or the effective number of types. It refers to the number of equally abundant types needed for the average proportional abundance of the types to equal that observed in the dataset of interest where all types may not be equally abundant. - gini.simp: The Gini-Simpson index. It is the probability of interspecific encounter, i.e., probability that two entities represent different types. - inv.simp: Inverse Simpson index. It is the effective number of types that is obtained when the weighted arithmetic mean is used to quantify average proportional abundance of types in the dataset of interest. - gini: The Gini coefficient. It measures the inequality among values of a frequency distribution (for example levels of income). A Gini coefficient of zero expresses perfect equality, where all values are the same (for example, where everyone has the same income). A Gini coefficient of one (or 100 percents) expresses maximal inequality among values (for example where only one person has all the income). - d50: The D50 index. It is the number of types that are needed to cover 50%% of the total abundance. - raref: Species richness from the results of sampling through extrapolation. - by: The variables (column names) to group samples. Multiple columns should be separated by `,`. - plot_type (choice): The type of the plot, works when `by` is specified. Not working for `raref`. - box: Boxplot - bar: Barplot with error bars - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - args (type=json): Other arguments for `repDiversity()`. Do not include the preceding `.` and use `-` instead of `.` in the argument names. For example, `do-norm` will be compiled to `.do.norm`. See all arguments at <https://immunarch.com/reference/repDiversity.html>. - order (list): The order of the values in `by` on the x-axis of the plots. If not specified, the values will be used as-is. - test (ns): Perform statistical tests between each pair of groups. Does NOT work for `raref`. - method (choice): The method to perform the test - none: No test - t.test: Welch's t-test - wilcox.test: Wilcoxon rank sum test - padjust (choice): The method to adjust p-values. Defaults to `none`. - bonferroni: one-step correction - holm: step-down method using Bonferroni adjustments - hochberg: step-up method (independent) - hommel: closed method based on Simes tests (non-negative) - BH: Benjamini & Hochberg (non-negative) - BY: Benjamini & Yekutieli (negative) - fdr: Benjamini & Hochberg (non-negative) - none: no correction. - separate_by: A column name used to separate the samples into different plots. - split_by: A column name used to split the samples into different subplots. Like `separate_by`, but the plots will be put in the same figure. y-axis will be shared, even if `align_y` is `False` or `ymin`/`ymax` are not specified. `ncol` will be ignored. - split_order: The order of the values in `split_by` on the x-axis of the plots. It can also be used for `separate_by` to control the order of the plots. Values can be separated by `,`. - align_x (flag): Align the x-axis of multiple plots. Only works for `raref`. - align_y (flag): Align the y-axis of multiple plots. - ymin (type=float): The minimum value of the y-axis. The minimum value of the y-axis for plots splitting by `separate_by`. `align_y` is forced `True` when both `ymin` and `ymax` are specified. - ymax (type=float): The maximum value of the y-axis. The maximum value of the y-axis for plots splitting by `separate_by`. `align_y` is forced `True` when both `ymin` and `ymax` are specified. Works when both `ymin` and `ymax` are specified. - log (flag): Indicate whether we should plot with log-transformed x-axis using `vis(.log = TRUE)`. Only works for `raref`. - ncol (type=int): The number of columns of the plots. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If NO cases are specified, the default case will be added, with the name of `envs.div.method`. The values specified in `envs.div` will be used as the defaults for the cases here. trackings (ns): Parameters to control the clonotype tracking analysis. - targets: Either a set of CDR3AA seq of clonotypes to track (separated by `,`), or simply an integer to track the top N clonotypes. - subject_col: The column name in meta data that contains the subjects/samples on the x-axis of the alluvial plot. If the values in this column are not unique, the values will be merged with the values in `subject_col` to form the x-axis. This defaults to `Sample`. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - subjects (list): A list of values from `subject_col` to show in the alluvial plot on the x-axis. If not specified, all values in `subject_col` will be used. This also specifies the order of the x-axis. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments (`target`, `subject_col`, and `subjects`). If any of these arguments are not specified, the values in `envs.trackings` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.trackings.target`, `envs.trackings.subject_col`, and `envs.trackings.subjects`. kmers (ns): Arguments for kmer analysis. - k (type=int): The length of kmer. - head (type=int): The number of top kmers to show. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - subset: Subset the data before calculating the clonotype volumes. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data. - profiles (ns;order=8): Arguments for sequence profilings. - method (choice): The method for the position matrix. For more information see <https://en.wikipedia.org/wiki/Position_weight_matrix>. - freq: position frequency matrix (PFM) - a matrix with occurences of each amino acid in each position. - prob: position probability matrix (PPM) - a matrix with probabilities of each amino acid in each position. - wei: position weight matrix (PWM) - a matrix with log likelihoods of PPM elements. - self: self-information matrix (SIM) - a matrix with self-information of elements in PWM. - vis_args (type=json): Other arguments for the plotting functions. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.kmers.profiles` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.kmers.profiles.method`, `envs.kmers.profiles.vis_args` and `envs.kmers.profiles.devpars`. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the default case will be added, with the name `DEFAULT` and the values of `envs.kmers.k`, `envs.kmers.head`, `envs.kmers.vis_args` and `envs.kmers.devpars`. vj_junc (ns): Arguments for VJ junction circos plots. This analysis is not included in `immunarch`. It is a separate implementation using [`circlize`](https://github.com/jokergoo/circlize). - by: Groupings to show VJ usages. Typically, this is the `Sample` column, so that the VJ usages are shown for each sample. But you can also use other columns, such as `Subject` to show the VJ usages for each subject. Multiple columns should be separated by `,`. - by_clones (flag): If True, the VJ usages will be calculated based on the distinct clonotypes, instead of the individual cells. - subset: Subset the data before plotting VJ usages. The whole data will be expanded to cell level, and then subsetted. Clone sizes will be re-calculated based on the subsetted data, which will affect the VJ usages at cell level (by_clones=False). - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the plot. - height (type=int): The height of the plot. - res (type=int): The resolution of the plot. - cases (type=json;order=9): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.vj_junc` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.vj_junc.by`, `envs.vj_junc.by_clones` `envs.vj_junc.subset` and `envs.vj_junc.devpars`. \"\"\" # noqa: E501 input = \"immdata:file,metafile:file\" output = \"outdir:dir:{{in.immdata | stem}}.immunarch\" lang = config . lang . rscript envs = { \"mutaters\" : {}, \"prefix\" : None , # basic statistics \"volumes\" : { \"by\" : None , \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"cases\" : {}, }, \"lens\" : { \"by\" : None , \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"cases\" : {}, }, \"counts\" : { \"by\" : None , \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"cases\" : {}, }, # clonality \"top_clones\" : { \"by\" : None , \"marks\" : [ 10 , 100 , 1000 , 3000 , 10000 , 30000 , 1e5 ], \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"cases\" : {}, }, \"rare_clones\" : { \"by\" : None , \"marks\" : [ 1 , 3 , 10 , 30 , 100 ], \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"cases\" : {}, }, \"hom_clones\" : { \"by\" : None , \"marks\" : dict ( Rare = 1e-5 , Small = 1e-4 , Medium = 1e-3 , Large = 0.01 , Hyperexpanded = 1.0 , ), \"subset\" : None , \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"cases\" : {}, }, # overlapping \"overlaps\" : { \"method\" : \"public\" , \"vis_args\" : {}, \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"analyses\" : { \"method\" : \"none\" , \"vis_args\" : {}, \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"cases\" : {}, }, \"cases\" : {}, }, # gene usage \"gene_usages\" : { \"top\" : 30 , \"norm\" : False , \"by\" : None , \"vis_args\" : {}, \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"analyses\" : { \"method\" : \"none\" , \"vis_args\" : {}, \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"cases\" : {}, }, \"cases\" : {}, }, # Spectratyping \"spects\" : { \"quant\" : None , \"col\" : None , \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"cases\" : { \"By_Clonotype\" : dict ( quant = \"id\" , col = \"nt\" ), \"By_Num_Clones\" : dict ( quant = \"count\" , col = \"aa+v\" ), }, }, # Diversity \"divs\" : { \"method\" : \"gini\" , \"by\" : None , \"plot_type\" : \"bar\" , \"args\" : {}, \"order\" : [], \"test\" : { \"method\" : \"none\" , \"padjust\" : \"none\" , }, \"separate_by\" : None , \"split_by\" : None , \"split_order\" : None , \"align_x\" : False , \"align_y\" : False , \"log\" : False , \"devpars\" : { \"width\" : 800 , \"height\" : 800 , \"res\" : 100 , }, \"subset\" : None , \"ncol\" : 2 , \"ymin\" : None , \"ymax\" : None , \"cases\" : {}, }, # Clonotype tracking \"trackings\" : { \"targets\" : None , # Do not do trackings by default \"subject_col\" : \"Sample\" , \"subjects\" : [], \"subset\" : None , \"cases\" : {}, }, # Kmer analysis \"kmers\" : { \"k\" : 5 , \"head\" : 10 , \"vis_args\" : {}, \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"subset\" : None , \"profiles\" : { \"method\" : \"self\" , \"vis_args\" : {}, \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"cases\" : {}, }, \"cases\" : {}, }, # VJ junction \"vj_junc\" : { \"by\" : \"Sample\" , \"by_clones\" : True , \"devpars\" : { \"width\" : 800 , \"height\" : 800 , \"res\" : 100 }, \"subset\" : None , \"cases\" : {}, }, } script = \"file://../scripts/tcr/Immunarch.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/Immunarch.svelte\" , \"report_paging\" : 3 , \"poplog_max\" : 999 , } @mark ( deprecated = \" {proc.name} is deprecated, use ClonalStats instead.\" ) DOCS class SampleDiversity ( Proc ): \"\"\"Sample diversity and rarefaction analysis This is part of Immunarch, in case we have multiple dataset to compare. Input: immdata: The data loaded by `immunarch::repLoad()` Output: outdir: The output directory Envs: div_methods: Methods to calculate diversities It is a dict, keys are the method names, values are the groupings. Each one is a case, multiple columns for a case are separated by `,` For example: `{\"div\": [\"Status\", \"Sex\", \"Status,Sex\"]}` will run true diversity for samples grouped by `Status`, `Sex`, and both. The diversity for each sample without grouping will also be added anyway. Supported methods: `chao1`, `hill`, `div`, `gini.simp`, `inv.simp`, `gini`, and `raref`. See also <https://immunarch.com/articles/web_only/v6_diversity.html>. devpars: The parameters for the plotting device It is a dict, and keys are the methods and values are dicts with width, height and res that will be passed to `png()` If not provided, 1000, 1000 and 100 will be used. \"\"\" input = \"immdata:file\" output = \"outdir:dir:{{in.immdata | stem}}.diversity\" lang = config . lang . rscript envs = { \"div_methods\" : { \"chao1\" : [], \"hill\" : [], \"div\" : [], \"gini.simp\" : [], \"inv.simp\" : [], \"gini\" : [], \"raref\" : [], }, \"devpars\" : {}, } script = \"file://../scripts/tcr/SampleDiversity.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/SampleDiversity.svelte\" , } @mark ( deprecated = \" {proc.name} is deprecated, use ClonalStats instead.\" ) DOCS class CloneResidency ( Proc ): \"\"\"Identification of clone residency This process is used to investigate the residency of clones in groups, typically two samples (e.g. tumor and normal) from the same patient. But it can be used for any two groups of clones. There are three types of output from this process - Count tables of the clones in the two groups | CDR3_aa | Tumor | Normal | |------------------|-------|--------| | CASSYGLSWGSYEQYF | 306 | 55 | | CASSVTGAETQYF | 295 | 37 | | CASSVPSAHYNEQFF | 197 | 9 | | ... | ... | ... | - Residency plots showing the residency of clones in the two groups ![CloneResidency_residency](https://pwwang.github.io/immunopipe/latest/processes/images/CloneResidency.png) The points in the plot are jittered to avoid overplotting. The x-axis is the residency in the first group and the y-axis is the residency in the second group. The size of the points are relative to the normalized size of the clones. You may identify different types of clones in the plot based on their residency in the two groups: - Collapsed (The clones that are collapsed in the second group) - Dual (The clones that are present in both groups with equal size) - Expanded (The clones that are expanded in the second group) - First Group Multiplet (The clones only in the First Group with size > 1) - Second Group Multiplet (The clones only in the Second Group with size > 1) - First Group Singlet (The clones only in the First Group with size = 1) - Second Group Singlet (The clones only in the Second Group with size = 1) This idea is borrowed from this paper: > [Wu, Thomas D., et al. \"Peripheral T cell expansion predicts tumour infiltration and clinical response.\" Nature 579.7798 (2020): 274-278.](https://www.nature.com/articles/s41586-020-2056-8) - Venn diagrams showing the overlap of the clones in the two groups ![CloneResidency_venn](https://pwwang.github.io/immunopipe/latest/processes/images/CloneResidency_venn.png){: width=\"60%\"} Input: immdata: The data loaded by `immunarch::repLoad()` metafile: A cell-level metafile, where the first column must be the cell barcodes that match the cell barcodes in `immdata`. The other columns can be any metadata that you want to use for the analysis. The loaded metadata will be left-joined to the converted cell-level data from `immdata`. This can also be a Seurat object RDS file. If so, the `sobj@meta.data` will be used as the metadata. Output: outdir: The output directory Envs: subject (list): The key of subject in metadata. The clone residency will be examined for this subject/patient group: The key of group in metadata. This usually marks the samples that you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, instead of venn diagram, upset plots will be used. order (list): The order of the values in `group`. In scatter/residency plots, `X` in `X,Y` will be used as x-axis and `Y` will be used as y-axis. You can also have multiple orders. For example: `[\"X,Y\", \"X,Z\"]`. If you only have two groups, you can set `order = [\"X\", \"Y\"]`, which will be the same as `order = [\"X,Y\"]`. section: How the subjects aligned in the report. Multiple subjects with the same value will be grouped together. Useful for cohort with large number of samples. mutaters (type=json): The mutaters passed to `dplyr::mutate()` on the cell-level data converted from `in.immdata`. If `in.metafile` is provided, the mutaters will be applied to the joined data. The keys will be the names of the new columns, and the values will be the expressions. The new names can be used in `subject`, `group`, `order` and `section`. subset: The filter passed to `dplyr::filter()` to filter the data for the cells before calculating the clone residency. For example, `Clones > 1` to filter out singletons. prefix: The prefix of the cell barcodes in the `Seurat` object. upset_ymax: The maximum value of the y-axis in the upset bar plots. upset_trans: The transformation to apply to the y axis of upset bar plots. For example, `log10` or `sqrt`. If not specified, the y axis will be plotted as is. Note that the position of the bar plots will be dodged instead of stacked when the transformation is applied. See also <https://github.com/tidyverse/ggplot2/issues/3671> cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be used as the names of the cases. The values will be passed to the corresponding arguments. If no cases are specified, the default case will be added, with the name `DEFAULT` and the values of `envs.subject`, `envs.group`, `envs.order` and `envs.section`. These values are also the defaults for the other cases. \"\"\" # noqa: E501 input = \"immdata:file,metafile:file\" output = \"outdir:dir:{{in.immdata | stem}}.cloneov\" lang = config . lang . rscript envs = { \"subject\" : [], \"group\" : None , \"order\" : [], \"section\" : None , \"mutaters\" : {}, \"subset\" : None , \"prefix\" : \" {Sample} _\" , \"upset_ymax\" : None , \"upset_trans\" : None , \"cases\" : {}, } script = \"file://../scripts/tcr/CloneResidency.R\" order = 2 plugin_opts = { \"report\" : \"file://../reports/tcr/CloneResidency.svelte\" } @mark ( deprecated = True ) DOCS class Immunarch2VDJtools ( Proc ): \"\"\"Convert immuarch format into VDJtools input formats. This process converts the [`immunarch`](https://immunarch.com/) object to the [`VDJtools`](https://vdjtools-doc.readthedocs.io/en/master/) input files, in order to perform the VJ gene usage analysis by [`VJUsage`](!!#biopipennstcrvjusage) process. This process will generally generate a tab-delimited file for each sample, with the following columns. - `count`: The number of reads for this clonotype - `frequency`: The frequency of this clonotype - `CDR3nt`: The nucleotide sequence of the CDR3 region - `CDR3aa`: The amino acid sequence of the CDR3 region - `V`: The V gene - `D`: The D gene - `J`: The J gene See also: <https://vdjtools-doc.readthedocs.io/en/master/input.html#vdjtools-format>. This process has no environment variables. Input: immdata: The data loaded by `immunarch::repLoad()` Output: outdir: The output directory containing the vdjtools input for each sample \"\"\" # noqa: E501 input = \"immdata:file\" output = \"outdir:dir:{{in.immdata | stem}}.vdjtools_input\" lang = config . lang . rscript script = \"file://../scripts/tcr/Immunarch2VDJtools.R\" @mark ( deprecated = True ) DOCS class ImmunarchSplitIdents ( Proc ): \"\"\"Split the data into multiple immunarch datasets by Idents from Seurat Note that only the cells in both the `immdata` and `sobjfile` will be kept. Requires `immunarch >= 0.9.0` to use `select_clusters()` Input: immdata: The data loaded by `immunarch::repLoad()` sobjfile: The Seurat object file. You can set a different ident by `Idents(sobj) <- \"new_ident\"` to split the data by the new ident, where `\"new_ident\"` is the an existing column in meta data Output: outdir: The output directory containing the RDS files of the splitted immunarch datasets Envs: prefix: The prefix of the cell barcodes in the `Seurat` object. Once could use a fixed prefix, or a placeholder with the column name in meta data. For example, `\"{Sample}_\"` will replace the placeholder with the value of the column `Sample` in meta data. sample_col: The column name in meta data that contains the sample name \"\"\" input = \"immdata:file, sobjfile:file\" output = \"outdir:dir:{{in.immdata | stem}}.splitidents\" lang = config . lang . rscript envs = { \"prefix\" : \" {Sample} _\" , \"sample_col\" : \"Sample\" } script = \"file://../scripts/tcr/ImmunarchSplitIdents.R\" @mark ( deprecated = \" {proc.name} is deprecated, use ClonalStats instead.\" ) DOCS class VJUsage ( Proc ): \"\"\"Circos-style V-J usage plot displaying the frequency of various V-J junctions using vdjtools. This process performs the VJ gene usage analysis using [`VDJtools`](https://vdjtools-doc.readthedocs.io/en/master/). It wraps the [`PlotFancyVJUsage`](https://vdjtools-doc.readthedocs.io/en/master/basic.html#plotfancyvjusage) command in `VDJtools`. The output will be a V-J junction circos plot for a single sample. Arcs correspond to different V and J segments, scaled to their frequency in sample. Ribbons represent V-J pairings and their size is scaled to the pairing frequency (weighted in present case). ![VJUsage](https://vdjtools-doc.readthedocs.io/en/master/_images/basic-fancyvj.png){: width=\"80%\" } Input: infile: The input file, in vdjtools input format Output: outfile: The V-J usage plot Envs: vdjtools: The path to the `VDJtools` executable. vdjtools_patch (hidden): The patch file for `VDJtools`. It's delivered with the pipeline ([`biopipen`][3] package). * You don't need to provide this file, unless you want to use a different patch file by yourself. * See the issue with `VDJtools` [here](https://github.com/mikessh/vdjtools/issues/139). \"\"\" # noqa: E501 input = \"infile:file\" output = ( \"outfile:file:{{ in.infile | stem | replace: '.vdjtools', '' }}\" \".fancyvj.wt.png\" ) lang = config . lang . rscript envs = { \"vdjtools\" : config . exe . vdjtools , \"vdjtools_patch\" : str ( SCRIPT_DIR / \"tcr\" / \"vdjtools-patch.sh\" ), } order = 3 script = \"file://../scripts/tcr/VJUsage.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/VJUsage.svelte\" } @mark ( deprecated = True ) DOCS class Attach2Seurat ( Proc ): \"\"\"Attach the clonal information to a Seurat object as metadata Input: immfile: The immunarch object in RDS sobjfile: The Seurat object file in RDS Output: outfile: The Seurat object with the clonal information as metadata Envs: prefix: The prefix to the barcodes. You can use placeholder like `{Sample}_` to use the meta data from the immunarch object metacols: Which meta columns to attach \"\"\" input = \"immfile:file, sobjfile:file\" output = \"outfile:file:{{in.sobjfile | basename}}\" lang = config . lang . rscript envs = { \"prefix\" : \" {Sample} _\" , \"metacols\" : [ \"Clones\" , \"Proportion\" , \"CDR3.aa\" ], } script = \"file://../scripts/tcr/Attach2Seurat.R\" class TCRClustering ( Proc ): DOCS \"\"\"Cluster the TCR clones by their CDR3 sequences This process is used to cluster TCR clones based on their CDR3 sequences. It uses either [GIANA](https://github.com/s175573/GIANA) > Zhang, Hongyi, Xiaowei Zhan, and Bo Li. > \"GIANA allows computationally-efficient TCR clustering and multi-disease > repertoire classification by isometric transformation.\" > Nature communications 12.1 (2021): 1-11. Or [ClusTCR](https://github.com/svalkiers/clusTCR) > Sebastiaan Valkiers, Max Van Houcke, Kris Laukens, Pieter Meysman, > ClusTCR: a Python interface for rapid clustering of large sets of CDR3 > sequences with unknown antigen specificity, > Bioinformatics, 2021. Both methods are based on the [Faiss Clustering Library](https://github.com/facebookresearch/faiss), for efficient similarity search and clustering of dense vectors, so both methods yield similar results. A text file will be generated with the cluster assignments for each cell, together with the `immunarch` object (in `R`) with the cluster assignments at `TCR_Clsuter` column. This information will then be merged to a `Seurat` object for further downstream analysis. The cluster assignments are prefixed with `S_` or `M_` to indicate whether a cluster has only one unique CDR3 sequence or multiple CDR3 sequences. Note that a cluster with `S_` prefix may still have multiple cells, as the same CDR3 sequence may be shared by multiple cells. Input: screpfile: The TCR data object loaded by `scRepertoire::CombineTCR()` or `scRepertoire::CombineExpression()` Output: outfile: The `scRepertoire` object in qs with TCR cluster information. Column `TCR_Cluster` will be added to the metadata. Envs: tool (choice): The tool used to do the clustering, either [GIANA](https://github.com/s175573/GIANA) or [ClusTCR](https://github.com/svalkiers/clusTCR). For GIANA, using TRBV mutations is not supported - GIANA: by Li lab at UT Southwestern Medical Center - ClusTCR: by Sebastiaan Valkiers, etc python: The path of python with `GIANA`'s dependencies installed or with `clusTCR` installed. Depending on the `tool` you choose. within_sample (flag): Whether to cluster the TCR clones within each sample. When `in.screpfile` is a `Seurat` object, the samples are marked by the `Sample` column in the metadata. args (type=json): The arguments for the clustering tool For GIANA, they will be passed to `python GIAna.py` See <https://github.com/s175573/GIANA#usage>. For ClusTCR, they will be passed to `clustcr.Clustering(...)` See <https://svalkiers.github.io/clusTCR/docs/clustering/how-to-use.html#clustering>. chain (choice): The TCR chain to use for clustering. - alpha: TCR alpha chain (the first sequence in CTaa, separated by `_`) - beta: TCR beta chain (the second sequence in CTaa, separated by `_`) - both: Both TCR alpha and beta chains Requires: clusTCR: - if: {{ proc.envs.tool == 'ClusTCR' }} - check: {{ proc.envs.python }} -c \"import clustcr\" \"\"\" # noqa: E501 input = \"screpfile:file\" output = \"outfile:file:{{in.screpfile | stem}}.tcr_clustered.qs\" lang = config . lang . rscript envs = { \"tool\" : \"GIANA\" , # or ClusTCR \"python\" : config . lang . python , \"within_sample\" : True , # whether to cluster the TCR clones within each sample \"args\" : {}, \"chain\" : \"both\" , # alpha, beta, both } script = \"file://../scripts/tcr/TCRClustering.R\" @mark ( deprecated = \" {proc.name} is deprecated, use ClonalStats instead.\" ) DOCS class TCRClusterStats ( Proc ): \"\"\"Statistics of TCR clusters, generated by `TCRClustering`. The statistics include - The number of cells in each cluster (cluster size) - Sample diversity using TCR clusters instead of TCR clones - Shared TCR clusters between samples Examples: ### Cluster size ```toml [TCRClusterStats.envs.cluster_size] by = \"Sample\" ``` ![Cluster_size](https://pwwang.github.io/immunopipe/latest/processes/images/TCRClusteringStats_cluster_size.png){: width=\"80%\"} ### Shared clusters ```toml [TCRClusterStats.envs.shared_clusters] numbers_on_heatmap = true heatmap_meta = [\"region\"] ``` ![Shared_clusters](https://pwwang.github.io/immunopipe/latest/processes/images/TCRClusteringStats_shared_clusters.png){: width=\"80%\"} ### Sample diversity ```toml [TCRClusterStats.envs.sample_diversity] method = \"gini\" ``` ![Sample_diversity](https://pwwang.github.io/immunopipe/latest/processes/images/TCRClusteringStats_sample_diversity.png){: width=\"80%\"} Compared to the sample diversity using TCR clones: ![Sample_diversity](https://pwwang.github.io/immunopipe/latest/processes/images/Immunarch_sample_diversity.png){: width=\"80%\"} Input: immfile: The immunarch object with TCR clusters attached Output: outdir: The output directory containing the stats and reports Envs: cluster_size (ns): The distribution of size of each cluster. - by: The variables (column names) used to fill the histogram. Only a single column is supported. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.cluster_size` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT`. shared_clusters (ns): Stats about shared TCR clusters - numbers_on_heatmap (flag): Whether to show the numbers on the heatmap. - heatmap_meta (list): The columns of metadata to show on the heatmap. - cluster_rows (flag): Whether to cluster the rows on the heatmap. - sample_order: The order of the samples on the heatmap. Either a string separated by `,` or a list of sample names. This only works for columns if `cluster_rows` is `True`. - grouping: The groups to investigate the shared clusters. If specified, venn diagrams will be drawn instead of heatmaps. In such case, `numbers_on_heatmap` and `heatmap_meta` will be ignored. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.shared_clusters` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT`. sample_diversity (ns): Sample diversity using TCR clusters instead of clones. - by: The variables (column names) to group samples. Multiple columns should be separated by `,`. - method (choice): The method to calculate diversity. - gini: The Gini coefficient. It measures the inequality among values of a frequency distribution (for example levels of income). - gini.simp: The Gini-Simpson index. It is the probability of interspecific encounter, i.e., probability that two entities represent different types. - inv.simp: Inverse Simpson index. It is the effective number of types that is obtained when the weighted arithmetic mean is used to quantify average proportional abundance of types in the dataset of interest. - div: true diversity, or the effective number of types. It refers to the number of equally abundant types needed for the average proportional abundance of the types to equal that observed in the dataset of interest where all types may not be equally abundant. - devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device - cases (type=json): If you have multiple cases, you can use this argument to specify them. The keys will be the names of the cases. The values will be passed to the corresponding arguments above. If any of these arguments are not specified, the values in `envs.sample_diversity` will be used. If NO cases are specified, the default case will be added, with the name `DEFAULT`. Requires: r-immunarch: - check: {{proc.lang}} -e \"library(immunarch)\" \"\"\" # noqa: E501 input = \"immfile:file\" output = \"outdir:dir:{{in.immfile | stem}}.tcrclusters_stats\" lang = config . lang . rscript envs = { \"cluster_size\" : { \"by\" : \"Sample\" , \"devpars\" : { \"width\" : 1000 , \"height\" : 900 , \"res\" : 100 }, \"cases\" : {}, }, \"shared_clusters\" : { \"numbers_on_heatmap\" : True , \"heatmap_meta\" : [], \"cluster_rows\" : True , \"sample_order\" : None , \"cluster_rows\" : True , \"grouping\" : None , \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"cases\" : {}, }, \"sample_diversity\" : { \"by\" : None , \"method\" : \"gini\" , \"devpars\" : { \"width\" : 1000 , \"height\" : 1000 , \"res\" : 100 }, \"cases\" : {}, }, } script = \"file://../scripts/tcr/TCRClusterStats.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/TCRClusterStats.svelte\" , } @mark ( deprecated = True ) DOCS class CloneSizeQQPlot ( Proc ): \"\"\"QQ plot of the clone sizes QQ plots for clones sizes of pairs of samples Input: immdata: The data loaded by `immunarch::repLoad()` Output: outdir: The output directory Envs: subject: The key of subject in metadata, defining the pairs. The clone residency will be examined for this subject/patient group: The key of group in metadata. This usually marks the samples that you want to compare. For example, Tumor vs Normal, post-treatment vs baseline It doesn't have to be 2 groups always. If there are more than 3 groups, for example, [A, B, C], the QQ plots will be generated for all the combinations of 2 groups, i.e., [A, B], [A, C], [B, C] order: The order of the values in `group`. Early-ordered group will be used as x-axis in scatter plots If there are more than 2 groups, for example, [A, B, C], the QQ plots will be drawn for pairs: B ~ A, C ~ B. diag: Whether to draw the diagonal line in the QQ plot on: The key of the metadata to use for the QQ plot. One/Both of `[\"Clones\", \"Proportion\"]` \"\"\" input = \"immdata:file\" output = \"outdir:dir:{{in.immdata | stem}}.qqplots\" lang = config . lang . rscript envs = { \"subject\" : [], \"group\" : None , \"order\" : [], \"diag\" : True , \"on\" : [ \"Clones\" , \"Proportion\" ], } script = \"file://../scripts/tcr/CloneSizeQQPlot.R\" order = 3 plugin_opts = { \"report\" : \"file://../reports/tcr/CloneSizeQQPlot.svelte\" } class CDR3AAPhyschem ( Proc ): DOCS \"\"\"CDR3 AA physicochemical feature analysis The idea is to perform a regression between two groups of cells (e.g. Treg vs Tconv) at different length of CDR3 AA sequences. The regression will be performed for each physicochemical feature of the AA (hydrophobicity, volume and isolectric point). Reference: - [Stadinski, Brian D., et al. \"Hydrophobic CDR3 residues promote the development of self-reactive T cells.\" Nature immunology 17.8 (2016): 946-955.](https://www.nature.com/articles/ni.3491) - [Lagattuta, Kaitlyn A., et al. \"Repertoire analyses reveal T cell antigen receptor sequence features that influence T cell fate.\" Nature immunology 23.3 (2022): 446-457.](https://www.nature.com/articles/s41590-022-01129-x) - [Wimley, W. C. & White, S. H. Experimentally determined hydrophobicity scale for proteins at membrane - interfaces. Nat. Struct. Biol. 3, 842-848 (1996).](https://www.nature.com/articles/nsb1096-842) - [Handbook of chemistry & physics 72nd edition. (CRC Press, 1991).](https://books.google.com/books?hl=en&lr=&id=bNDMBQAAQBAJ&oi=fnd&pg=PP1&dq=Hdbk+of+chemistry+%26+physics&ots=H9fzwhwz-C&sig=EXHI9N3q4OW9TYEBWlldqkvADfM#v=onepage&q=Hdbk%20of%20chemistry%20%26%20physics&f=false) - [Zamyatnin, A. A. Protein volume in solution. Prog. Biophys. Mol. Biol. 24, 107-123 (1972).](https://www.sciencedirect.com/science/article/pii/0079610772900053) Input: scrfile: The data loaded by `ScRepCombiningExpression`, saved in RDS or qs/qs2 format. The data is actually generated by `scRepertiore::combineExpression()`. The data must have both TRA and TRB chains. Output: outdir: The output directory Envs: group: The key of group in metadata to define the groups to compare. For example, `CellType`, which has cell types annotated for each cell in the combined object (immdata + Seurat metadata) comparison (type=auto): A dict of two groups, with keys as the group names and values as the group labels. For example, ```toml Treg = [\"CD4 CTL\", \"CD4 Naive\", \"CD4 TCM\", \"CD4 TEM\"] Tconv = \"Tconv\" ``` Or simply a list of two groups, for example, `[\"Treg\", \"Tconv\"]` when they are both in the `group` column. target: Which group to use as the target group. The target group will be labeled as 1, and the other group will be labeled as 0 in the regression. If not specified, the first group in `comparison` will be used as the target group. each (auto): A column, or a list of columns or a string of columns separated by comma. The columns will be used to split the data into multiple groups and the regression will be applied to each group separately. If not provided, all the cells will be used. \"\"\" # noqa: E501 input = \"scrfile:file\" output = \"outdir:dir:{{in.immdata | stem}}.cdr3aaphyschem\" lang = config . lang . rscript envs = { \"group\" : None , \"comparison\" : None , \"target\" : None , \"each\" : None , } script = \"file://../scripts/tcr/CDR3AAPhyschem.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/CDR3AAPhyschem.svelte\" } class TESSA ( Proc ): DOCS \"\"\"Tessa is a Bayesian model to integrate T cell receptor (TCR) sequence profiling with transcriptomes of T cells. Enabled by the recently developed single cell sequencing techniques, which provide both TCR sequences and RNA sequences of each T cell concurrently, Tessa maps the functional landscape of the TCR repertoire, and generates insights into understanding human immune response to diseases. As the first part of tessa, BriseisEncoder is employed prior to the Bayesian algorithm to capture the TCR sequence features and create numerical embeddings. We showed that the reconstructed Atchley Factor matrices and CDR3 sequences, generated through the numerical embeddings, are highly similar to their original counterparts. The CDR3 peptide sequences are constructed via a RandomForest model applied on the reconstructed Atchley Factor matrices. See <https://github.com/jcao89757/TESSA> When finished, two columns will be added to the `meta.data` of the `Seurat` object: - `TESSA_Cluster`: The cluster assignments from TESSA. - `TESSA_Cluster_Size`: The number of cells in each cluster. These columns can be then used for further downstream analysis to explore the functional landscape of the TCR repertoire. Reference: - 'Mapping the Functional Landscape of TCR Repertoire.', Zhang, Z., Xiong, D., Wang, X. et al. 2021. [link](https://www.nature.com/articles/s41592-020-01020-3) - 'Deep learning-based prediction of the T cell receptor-antigen binding specificity.', Lu, T., Zhang, Z., Zhu, J. et al. 2021. [link](https://www.nature.com/articles/s42256-021-00383-2) Input: screpdata: The data loaded by `ScRepCombiningExpression`, saved in RDS or qs/qs2 format. The data is actually generated by `scRepertiore::combineExpression()`. The data must have both TRA and TRB chains. Output: outfile: a qs fileof a Seurat object, with `TESSA_Cluster` and `TESSA_Cluster_Size` added to the `meta.data` Envs: python: The path of python with `TESSA`'s dependencies installed within_sample (flag): Whether the TCR networks are constructed only within TCRs from the same sample/patient (True) or with all the TCRs in the meta data matrix (False). assay: Which assay to use to extract the expression matrix. Only works if `in.srtobj` is an RDS file of a Seurat object. By default, if `SCTransform` is performed, `SCT` will be used. predefined_b (flag): Whether use the predefined `b` or not. Please check the paper of tessa for more details about the b vector. If True, the tessa will not update b in the MCMC iterations. max_iter (type=int): The maximum number of iterations for MCMC. save_tessa (flag): Save tessa detailed results to seurat object? It will be saved to `sobj@misc$tessa`. \"\"\" input = \"screpdata:file\" output = \"outfile:file:{{in.screpdata | stem}}.tessa.qs\" lang = config . lang . rscript envs = { \"python\" : config . lang . python , \"assay\" : None , \"within_sample\" : False , \"predefined_b\" : False , \"max_iter\" : 1000 , \"save_tessa\" : False , } script = \"file://../scripts/tcr/TESSA.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/TESSA.svelte\" } class TCRDock ( Proc ): DOCS \"\"\"Using TCRDock to predict the structure of MHC-peptide-TCR complexes See <https://github.com/phbradley/TCRdock>. Input: configfile: The config file for TCRDock It's should be a toml file with the keys listed in `envs`, including `organism`, `mhc_class`, `mhc`, `peptide`, `va`, `ja`, `vb`, `jb`, `cdr3a`, and `cdr3b`. The values will overwrite the values in `envs`. Output: outdir: The output directory containing the results Envs: organism: The organism of the TCR, peptide and MHC mhc_class (type=int): The MHC class, either `1` or `2` mhc: The MHC allele, e.g., `A*02:01` peptide: The peptide sequence va: The V alpha gene ja: The J alpha gene vb: The V beta gene jb: The J beta gene cdr3a: The CDR3 alpha sequence cdr3b: The CDR3 beta sequence python: The path of python with dependencies for `tcrdock` installed. If not provided, `TCRDock.lang` will be used (the same interpreter used for the wrapper script). It could also be a list to specify, for example, a python in a conda environment (e.g., `[\"conda\", \"run\", \"-n\", \"myenv\", \"python\"]`). tmpdir: The temporary directory used to clone the `tcrdock` source code if `envs.tcrdock` is not provided. tcrdock: The path to the `tcrdock` source code repo. You need to clone the source code from the github repository. <https://github.com/phbradley/TCRdock> at revision c5a7af42eeb0c2a4492a4d4fe803f1f9aafb6193 at main branch. You also have to run `download_blast.py` after cloning to download the blast database in the directory. If not provided, we will clone the source code to the `envs.tmpdir` directory and run the `download_blast.py` script. model_name: The model name to use model_file: The model file to use. If provided as a relative path, it should be relative to the `<envs.data_dir>/params/`, otherwise, it should be the full path. data_dir: The data directory that contains the model files. The model files should be in the `params` subdirectory. \"\"\" input = \"configfile:file\" output = \"outdir:dir:{{in.configfile | stem}}.tcrdock\" lang = config . lang . python envs = { \"tcrdock\" : None , \"organism\" : \"human\" , \"mhc_class\" : 1 , \"mhc\" : \"A*02:01\" , \"peptide\" : None , \"va\" : None , \"ja\" : None , \"vb\" : None , \"jb\" : None , \"cdr3a\" : None , \"cdr3b\" : None , \"python\" : None , \"model_name\" : \"model_2_ptm_ft4\" , \"model_file\" : \"tcrpmhc_run4_af_mhc_params_891.pkl\" , \"data_dir\" : None , } script = \"file://../scripts/tcr/TCRDock.py\" class ScRepLoading ( Proc ): DOCS \"\"\"Load the single cell TCR/BCR data into a `scRepertoire` compatible object This process loads the single cell TCR/BCR data into a `scRepertoire` (>= v2.0.8, < v2.3.2) compatible object. Later, `scRepertoire::combineExpression` can be used to combine the expression data with the TCR/BCR data. For the data path specified at `TCRData`/`BCRData` in the input file (`in.metafile`), will be used to find the TCR/BCR data files and `scRepertoire::loadContigs()` will be used to load the data. A directory can be specified in `TCRData`/`BCRData`, then `scRepertoire::loadContigs()` will be used directly to load the data from the directory. Otherwise if a file is specified, it will be symbolically linked to a directory for `scRepertoire::loadContigs()` to load. Note that when the file name can not be recognized by `scRepertoire::loadContigs()`, `envs.format` must be set for the correct format of the data. Input: metafile: The meta data of the samples A tab-delimited file Two columns are required: * `Sample` to specify the sample names. * `TCRData`/`BCRData` to assign the path of the data to the samples, and this column will be excluded as metadata. Output: outfile: The `scRepertoire` compatible object in qs/qs2 format Envs: type (choice): The type of the data to load. - TCR: T cell receptor data - BCR: B cell receptor data - auto: Automatically detect the type from the metadata. If `auto` is selected, the type will be determined by the presence of `TCRData` or `BCRData` columns in the metadata. If both columns are present, `TCR` will be selected by default. combineTCR (type=json): The extra arguments for `scRepertoire::combineTCR` function. See also <https://www.borch.dev/uploads/screpertoire/reference/combinetcr> combineBCR (type=json): The extra arguments for `scRepertoire::combineBCR` function. See also <https://www.borch.dev/uploads/screpertoire/reference/combinebcr> exclude (auto): The columns to exclude from the metadata to add to the object. A list of column names to exclude or a string with column names separated by `,`. By default, `BCRData`, `TCRData` and `RNAData` will be excluded. tmpdir: The temporary directory to store the symbolic links to the TCR/BCR data files. format (choice): The format of the TCR/BCR data files. - 10X: 10X Genomics data, which is usually in a directory with `filtered_contig_annotations.csv` file. - AIRR: AIRR format, which is usually in a file with `airr_rearrangement.tsv` file. - BD: Becton Dickinson data, which is usually in a file with `Contigs_AIRR.tsv` file. - Dandelion: Dandelion data, which is usually in a file with `all_contig_dandelion.tsv` file. - Immcantation: Immcantation data, which is usually in a file with `data.tsv` file. - JSON: JSON format, which is usually in a file with `.json` extension. - ParseBio: ParseBio data, which is usually in a file with `barcode_report.tsv` file. - MiXCR: MiXCR data, which is usually in a file with `clones.tsv` file. - Omniscope: Omniscope data, which is usually in a file with `.csv` extension. - TRUST4: TRUST4 data, which is usually in a file with `barcode_report.tsv` file. - WAT3R: WAT3R data, which is usually in a file with `barcode_results.csv` file. See also: <https://rdrr.io/github/ncborcherding/scRepertoire/man/loadContigs.html> If not provided, the format will be guessed from the file name by `scRepertoire::loadContigs()`. \"\"\" # noqa: E501 input = \"metafile:file\" output = \"outfile:file:{{in.metafile | stem}}.scRep.qs\" lang = config . lang . rscript envs = { \"type\" : \"auto\" , # or TCR/BCR \"combineTCR\" : { \"samples\" : True }, \"combineBCR\" : { \"samples\" : True }, \"exclude\" : [ \"BCRData\" , \"TCRData\" , \"RNAData\" ], \"format\" : None , \"tmpdir\" : config . path . tmpdir , } script = \"file://../scripts/tcr/ScRepLoading.R\" class ScRepCombiningExpression ( Proc ): DOCS \"\"\"Combine the scTCR/BCR data with the expression data This process combines the scTCR/BCR data with the expression data using `scRepertoire::combineExpression` function. The expression data should be in `Seurat` format. The `scRepertoire` object should be a combined contig object, usually generated by `scRepertoire::combineTCR` or `scRepertoire::combineBCR`. See also: <https://www.borch.dev/uploads/screpertoire/reference/combineexpression>. Input: screpfile: The `scRepertoire` object in RDS/qs format srtobj: The `Seurat` object, saved in RDS/qs format Output: outfile: The `Seurat` object with the TCR/BCR data combined In addition to the meta columns added by `scRepertoire::combineExpression()`, a new column `VDJ_Presence` will be added to the metadata. It indicates whether the cell has a TCR/BCR sequence or not. The value is `TRUE` if the cell has a TCR/BCR sequence, and `FALSE` otherwise. Envs: cloneCall: How to call the clone - VDJC gene (gene), CDR3 nucleotide (nt), CDR3 amino acid (aa), VDJC gene + CDR3 nucleotide (strict) or a custom variable in the data. chain: indicate if both or a specific chain should be used e.g. \"both\", \"TRA\", \"TRG\", \"IGH\", \"IGL\". group_by: The column label in the combined clones in which clone frequency will be calculated. NULL or \"none\" will keep the format of input.data. proportion (flag): Whether to proportion (TRUE) or total frequency (FALSE) of the clone based on the group_by variable. filterNA (flag): Method to subset Seurat/SCE object of barcodes without clone information cloneSize (type=json): The bins for the grouping based on proportion or frequency. If proportion is FALSE and the cloneSizes are not set high enough based on frequency, the upper limit of cloneSizes will be automatically updated. addLabel (flag): This will add a label to the frequency header, allowing the user to try multiple group_by variables or recalculate frequencies after subsetting the data. \"\"\" input = \"screpfile:file,srtobj:file\" output = \"outfile:file:{{in.screpfile | stem}}.qs\" lang = config . lang . rscript envs = { \"cloneCall\" : \"aa\" , \"chain\" : \"both\" , \"group_by\" : \"Sample\" , \"proportion\" : True , \"filterNA\" : False , \"cloneSize\" : { \"Rare\" : 1e-04 , \"Small\" : 0.001 , \"Medium\" : 0.01 , \"Large\" : 0.1 , \"Hyperexpanded\" : 1 , }, \"addLabel\" : False , } script = \"file://../scripts/tcr/ScRepCombiningExpression.R\" class ClonalStats ( Proc ): DOCS \"\"\"Visualize the clonal information. Using [`scplotter`](https://github.com/pwwang/scplotter) to visualize the clonal information. Input: screpfile: The `scRepertoire` object in RDS/qs format Output: outdir: The output directory containing the plots Envs: mutaters (type=json;order=-9): The mutaters passed to `dplyr::mutate()` to add new variables. When the object loaded form `in.screpfile` is a list, the mutaters will be applied to each element. The keys are the names of the new variables, and the values are the expressions. When it is a `Seurat` object, typically an output of `scRepertoire::combineExpression()`, the mutaters will be applied to the `meta.data`. viz_type (choice): The type of visualization to generate. - volume: The volume of the clones using [`ClonalVolumePlot`](https://pwwang.github.io/scplotter/reference/ClonalVolumePlot.html) - abundance: The abundance of the clones using [`ClonalAbundancePlot`](https://pwwang.github.io/scplotter/reference/ClonalAbundancePlot.html) - length: The length of the CDR3 sequences using [`ClonalLengthPlot`](https://pwwang.github.io/scplotter/reference/ClonalLengthPlot.html) - residency: The residency of the clones using [`ClonalResidencyPlot`](https://pwwang.github.io/scplotter/reference/ClonalResidencyPlot.html) - dynamics: The dynamics of the clones using [`ClonalDynamicsPlot`](https://pwwang.github.io/scplotter/reference/ClonalDynamicsPlot.html) - composition: The composition of the clones using [`ClonalCompositionPlot`](https://pwwang.github.io/scplotter/reference/ClonalCompositionPlot.html) - overlap: The overlap of the clones using [`ClonalOverlapPlot`](https://pwwang.github.io/scplotter/reference/ClonalOverlapPlot.html) - diversity: The diversity of the clones using [`ClonalDiversityPlot`](https://pwwang.github.io/scplotter/reference/ClonalDiversityPlot.html) - geneusage: The gene usage of the clones using [`ClonalGeneUsagePlot`](https://pwwang.github.io/scplotter/reference/ClonalGeneUsagePlot.html) - positional: The positional information of the clones using [`ClonalPositionalPlot`](https://pwwang.github.io/scplotter/reference/ClonalPositionalPlot.html) - kmer: The kmer information of the clones using [`ClonalKmerPlot`](https://pwwang.github.io/scplotter/reference/ClonalKmerPlot.html) - rarefaction: The rarefaction curve of the clones using [`ClonalRarefactionPlot`](https://pwwang.github.io/scplotter/reference/ClonalRarefactionPlot.html) subset: An expression to subset the data before plotting. Similar to `mutaters`, it will be applied to each element by `dplyr::filter()` if the object loaded form `in.screpfile` is a list; otherwise, it will be applied to `subset(sobj, subset = <expr>)` if the object is a `Seurat` object. devpars (ns): The parameters for the plotting device. - width (type=int): The width of the device - height (type=int): The height of the device - res (type=int): The resolution of the device more_formats (list): The extra formats to save the plots in, other than PNG. save_code (flag): Whether to save the code used to generate the plots Note that the data directly used to generate the plots will also be saved in an `rda` file. Be careful if the data is large as it may take a lot of disk space. descr: The description of the plot, used to show in the report. <more>: The arguments for the plot function See the documentation of the corresponding plot function for the details cases (type=json): The cases to generate the plots if we have multiple cases. The keys are the names of the cases, and the values are the arguments for the plot function. The arguments in `envs` will be used if not specified in `cases`, except for `mutaters`. Sections can be specified as the prefix of the case name, separated by `::`. For example, if you have a case named `Clonal Volume::Case1`, the plot will be put in the section `Clonal Volume`. By default, when there are multiple cases for the same 'viz_type', the name of the 'viz_type' will be used as the default section name (for example, when 'viz_type' is 'volume', the section name will be 'Clonal Volume'). When there is only a single case, the section name will default to 'DEFAULT', which will not be shown in the report. \"\"\" # noqa: E501 input = \"screpfile:file\" output = \"outdir:dir:{{in.screpfile | stem}}.clonalstats\" lang = config . lang . rscript envs = { \"mutaters\" : {}, \"subset\" : None , \"viz_type\" : None , \"devpars\" : { \"width\" : None , \"height\" : None , \"res\" : 100 }, \"more_formats\" : [], \"save_code\" : False , \"descr\" : None , \"cases\" : { \"Clonal Volume\" : { \"viz_type\" : \"volume\" }, \"Clonal Abundance\" : { \"viz_type\" : \"abundance\" }, \"CDR3 Length\" : { \"viz_type\" : \"length\" }, \"Clonal Diversity\" : { \"viz_type\" : \"diversity\" }, } } script = \"file://../scripts/tcr/ClonalStats.R\" plugin_opts = { \"report\" : \"file://../reports/tcr/ClonalStats.svelte\" }","title":"biopipen.ns.tcr"},{"location":"api/source/biopipen.ns.vcf/","text":"SOURCE CODE biopipen.ns. vcf DOCS \"\"\"Tools to handle VCF files\"\"\" from ..core.proc import Proc from ..core.config import config class VcfLiftOver ( Proc ): DOCS \"\"\"Liftover a VCF file using GATK Input: invcf: The input VCF file Output: outvcf: The output VCF file Envs: gatk: The path to gatk4, which should be installed via conda chain: The map chain file for liftover tmpdir: Directory for temporary storage of working files args: Other CLI arguments for `gatk LiftoverVcf` \"\"\" input = \"invcf:file\" output = \"outvcf:file:{{in.invcf | basename}}\" envs = { \"gatk\" : config . exe . gatk4 , \"chain\" : config . path . liftover_chain , \"tmpdir\" : config . path . tmpdir , \"reffa\" : config . ref . reffa , \"args\" : {}, } lang = config . lang . bash script = \"file://../scripts/vcf/VcfLiftOver.sh\" class VcfFilter ( Proc ): DOCS \"\"\"Filter records in vcf file Input: invcf: The input vcf file, could be bgzipped. Output: outfile: The filtered vcf file. If `in.invcf` is bgzipped, then this will be bgzipped. Envs: filters: A dict of filters with keys the filter names. >>> # Typically >>> lambda variant: <expression> Things to notice 1. Filters should return `False` to get variant filtered out 2. See https://brentp.github.io/cyvcf2/docstrings.html#cyvcf2.cyvcf2.Variant For what you can do with the variant 3. The filter python functions should be in string representation 4. Builtin filters can have parameters `{\"QUAL\": 30}` 5. List of builtin filters. Specify them like: `{\"FILTER\": params}` `SNPONLY`: keeps only SNPs (`{\"SNPONLY\": False}` to filter SNPs out) `QUAL`: keeps variants with QUAL>=param (`{\"QUAL\": (30, False)}`) to keep only variants with QUAL<30 filter_descs: Descriptions for the filters. Will be saved to the header of the output vcf file helper: Some helper code for the filters keep: Keep the variants not passing the filters? \"\"\" # noqa: E501 input = \"invcf:file\" output = \"outfile:file:{{in.invcf | basename}}\" lang = config . lang . python envs = { \"filters\" : {}, \"keep\" : True , \"helper\" : \"\" , \"filter_descs\" : {}, } script = \"file://../scripts/vcf/VcfFilter.py\" class VcfIndex ( Proc ): DOCS \"\"\"Index VCF files. If they are already index, use the index files Input: infile: The input VCF file Output: outfile: The output VCF file (bgzipped) outidx: The index file of the output VCF file Envs: tabix: Path to tabix \"\"\" input = \"infile:file\" output = \"\"\" { %- i f in.infile.endswith(\".gz\") %} outfile:file:{{in.infile | basename}}, outidx:file:{{in.infile | basename | append: \".tbi\"}} { %- e lse -%} outfile:file:{{in.infile | basename | append: \".gz\"}}, outidx:file:{{in.infile | basename | append: \".gz.tbi\"}} { % e ndif -%} \"\"\" lang = config . lang . python envs = { \"tabix\" : config . exe . tabix , \"ncores\" : config . misc . ncores , } script = \"file://../scripts/vcf/VcfIndex.py\" class Vcf2Bed ( Proc ): DOCS \"\"\"Convert Vcf file to Bed file Input: infile: The vcf file Output: outfile: The converted bed file Envs: inbase: The coordinate base of the vcf file outbase: The coordinate base of the base file Requires: cyvcf2: - check: {{proc.lang}} -c \"import cyvcf2\" \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | stem0}}.bed\" lang = config . lang . python envs = { \"inbase\" : 1 , \"outbase\" : 0 } script = \"file://../scripts/vcf/Vcf2Bed.py\" class VcfDownSample ( Proc ): DOCS \"\"\"Down-sample VCF files to keep only a subset of variants in there Input: infile: The input VCF file Output: outfile: The output VCF file with subet variants Gzipped if `in.infile` is gzipped Envs: n: Fraction/Number of variants to keep If `n > 1`, it is the number. If `n <= 1`, it is the fraction. \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" envs = { \"n\" : 0 } lang = config . lang . bash script = \"file://../scripts/vcf/VcfDownSample.sh\" class VcfSplitSamples ( Proc ): DOCS \"\"\"Split a VCF file into multiple VCF files, one for each sample Input: infile: The input VCF file Output: outdir: The output directory containing the split VCF files Envs: bcftools: Path to bcftools gz: Gzip the output VCF files? Has to be True if `envs.index` is True index: Index the output VCF files? ncores: Number of cores, used to extract samples, but not to index private: Keep sites where only the sample carries an non-ref allele. That means, sites with genotypes like `0/0` will be removed. \"\"\" input = \"infile:file\" output = \"outdir:dir:{{in.infile | stem}}.splitsamples\" lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"gz\" : True , \"index\" : True , \"ncores\" : config . misc . ncores , \"private\" : True , } script = \"file://../scripts/vcf/VcfSplitSamples.py\" class VcfIntersect ( Proc ): DOCS \"\"\"Find variants in both VCF files Input: infile1: The first VCF file infile2: The second VCF file Output: outfile: The output VCF file with subet variants in both files Envs: bcftools: Path to bcftools gz: Gzip the output VCF files? Has to be True if `envs.index` is True index: Index the output VCF files? keep_as: Keep the variants as presented in the first (0) or the second (1) file? collapse: How to match the variants in the two files? Will be passed to `bcftools isec -c` option. See also https://samtools.github.io/bcftools/bcftools.html#common_options - none: only records with identical REF and ALT alleles are compatible - some: only records where some subset of ALT alleles match are compatible - all: all records are compatible, regardless of whether the ALT alleles match or not. - snps: any SNP records are compatible, regardless of whether the ALT alleles match or not. - indels: any indel records are compatible, regardless of whether the ALT alleles match or not. - both: abbreviates `snps` and `indels` - id: only records with identical ID are compatible \"\"\" input = \"infile1:file, infile2:file\" output = \"\"\" outfile:file:{{in.infile1 | stem0}}.intersect.{{in.infile2 | stem0}}.vcf { %- i f envs.gz -%}.gz{ %- e ndif -%} \"\"\" lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"gz\" : True , \"index\" : True , \"collapse\" : \"all\" , } script = \"file://../scripts/vcf/VcfIntersect.py\" class VcfFix ( Proc ): DOCS \"\"\"Fix some issues with VCF files Input: infile: The input VCF file Output: outfile: The output VCF file Envs: fixes: A list of fixes to apply. Each one is a dict with keys `kind`, `id`, `regex` and `fix` `kind`: The kind of fix. Including `filter` the FILTERs in the header, `info` the info INFOs in the header, `contig` the contig lines in the header `format` the FORMATs in the header, `colnames` the column names in the header `header` general header item `variant` the variants `None` matches everything `id`: The ID the match. If `kind` is `filter`, `info`, `contig` or `format`, then it matches the `ID` of the item. If `kind` is `variant`, then it matches the `ID` of the variant. If a list is given, then it matches any of the IDs in the list. `regex`: The regular expression to match. When `id` is given, this is ignored. `append`: Whether to append a record instead of to replace an existing one. When it is True, `kind` has to not be `None` `fix`: The fix to apply in the format of a lambda function (in string), with a single argument. The function should either return a string (raw representation) for the record, the record itself, `None`, `False`. If `None` is returned, the original record is used. if `False`, the record is removed. If `append` is `True`, then the function should either return a string or an object. And the argument is `None` The argument is a different object based on different `kind`s. When `kind` is `None`, the argument is the plain line of the record with line ending. When `kind` is `info` or `format`, the record is a dict with keys `ID`, `Description`, `Type` and `Number`. When `kind` is `filter`, the record is a dict with keys `ID` and `Description`. When `kind` is `contig`, the record is a dict with keys `ID` and `length`. When `kind` is `header`, the record is a dict with `key` the name of the header and `value` the value of the header. When `kind` is `colnames`, the record is a list of column names. When `kind` is `variant`, the record is a dict with keys `CHROM`, `POS`, `REF`, `ALT`, `QUAL`, `FILTER`, `INFO`, `FORMAT` and `SAMPLES`. `INFO` is a dict with key-value pairs and `SAMPLES` are a list of values for each sample. Each value is also a list of values for each FORMAT. If a record matches multiple fixes, the first one is applied. helpers: raw code the provide some helpers for the fixes The code will automatically dedented if given as a string. A list of strings is also supported and will be joined with newlines. \"\"\" input = \"infile:file\" output = \"outfile:file:{{in.infile | basename}}\" lang = config . lang . python envs = { \"fixes\" : [], \"helpers\" : \"\" } script = \"file://../scripts/vcf/VcfFix.py\" class VcfAnno ( Proc ): DOCS \"\"\"Annotate a VCF file using vcfanno https://github.com/brentp/vcfanno Input: infile: The input VCF file conffile: The configuration file for vcfanno or configuration dict itself Output: outfile: The output VCF file Envs: vcfanno: Path to vcfanno ncores: Number of cores to use conffile: configuration file for vcfanno or configuration dict itself This is ignored when `conffile` is given as input args: Additional arguments to pass to vcfanno Requires: - name: vcfanno check: | {{proc.envs.vcfanno}} --help \"\"\" input = \"infile:file, conffile\" output = \"outfile:file:{{in.infile | stem0}}.{{envs.tool}}.vcf\" lang = config . lang . python envs = { \"vcfanno\" : config . exe . vcfanno , \"ncores\" : config . misc . ncores , \"conffile\" : {}, \"args\" : { \"permissive-overlap\" : True }, } script = \"file://../scripts/vcf/VcfAnno.py\" class TruvariBench ( Proc ): DOCS \"\"\"Run `truvari bench` to compare a VCF with CNV calls and base CNV standards Requires truvari v4+ See https://github.com/ACEnglish/truvari/wiki/bench Input: compvcf: The VCF file with CNV calls to compare basevcf: The VCF file with standard CNVs Output: outdir: The output directory Envs: truvari: Path to truvari `<other>`: Ohter `truvari bench` arguments Requires: truvari: - check: {{proc.envs.truvari}} version \"\"\" input = \"compvcf:file, basevcf:file\" output = \"outdir:dir:{{in.compvcf | stem0 | append: '.truvari_bench'}}\" envs = { \"truvari\" : config . exe . truvari , \"ref\" : config . ref . reffa , \"refdist\" : 500 , \"pctseq\" : 0.7 , \"pctsize\" : 0.7 , \"pctovl\" : 0.0 , \"typeignore\" : False , \"multimatch\" : False , } lang = config . lang . bash script = \"file://../scripts/vcf/TruvariBench.sh\" class TruvariBenchSummary ( Proc ): DOCS \"\"\"Summarise the statistics from `TruvariBench` for multiple jobs (VCFs) Input: indirs: The input directories, which should be the output directories of `TruvariBench` Output: outdir: The output directory, including the summary table and plots Envs: plots: The stats to plot with barplots. Candidates are `TP-base`, `TP-call`, `FP`, `FN`, `precision`, `recall`, `f1`, `base cnt`, `call cnt`, `TP-call_TP-gt`, `TP-call_FP-gt`, `TP-base_TP-gt`, `TP-base_FP-gt`, and `gt_concordance` See https://github.com/ACEnglish/truvari/wiki/bench devpars: The parameters to use for the plots. Requires: r-ggprism: - check: {{proc.lang}} -e \"library(ggprism)\" r-rjson: - check: {{proc.lang}} -e \"library(rjson)\" r-dplyr: - check: {{proc.lang}} -e \"library(dplyr)\" r-ggplot2: - check: {{proc.lang}} -e \"library(ggplot2)\" \"\"\" input = \"indirs:files\" input_data = lambda ch : [ list ( ch . iloc [:, 0 ])] output = \"outdir:dir:truvari_bench.summary\" lang = config . lang . rscript envs = { \"plots\" : [ \"comp cnt\" , \"base cnt\" , \"precision\" , \"recall\" , \"f1\" ], \"devpars\" : None , } script = \"file://../scripts/vcf/TruvariBenchSummary.R\" plugin_opts = { \"report\" : \"file://../reports/vcf/TruvariBenchSummary.svelte\" } class TruvariConsistency ( Proc ): DOCS \"\"\"Run `truvari consistency` to check consistency of CNV calls See https://github.com/ACEnglish/truvari/wiki/consistency Requires truvari v4+ Input: vcfs: The vcf files with CNV calls Output: outfile: The output file with the report Envs: truvari: Path to truvari heatmap: Whether to generate a heatmap of the consistency Set to False to disable annofile: The annotation file for the heatmap, multiple columns but the first column must be the sample name. Note that the stem of the vcf file name from consistency file will be used. These annotations will be added as row annotations. Other options see also `biopipen.ns.plot.Heatmap`. \"\"\" input = \"vcfs:files\" output = ( \"outdir:dir:\" \"{{in.vcfs | first | stem0 | append: '.etc.truvari_consistency'}}\" ) lang = config . lang . rscript envs = { \"truvari\" : config . exe . truvari , \"heatmap\" : {}} script = \"file://../scripts/vcf/TruvariConsistency.R\" plugin_opts = { \"report\" : \"file://../reports/vcf/TruvariConsistency.svelte\" } class BcftoolsAnnotate ( Proc ): DOCS \"\"\"Add or remove annotations from VCF files See also: <https://samtools.github.io/bcftools/bcftools.html#annotate> Input: infile: The input VCF file annfile: The annotation file. Currently only VCF files are supported. Output: outfile: The VCF file with annotations added or removed. Envs: bcftools: Path to bcftools tabix: Path to tabix, used to index infile and annfile annfile: The annotation file. If `in.annfile` is provided, this is ignored ncores (type=int): Number of cores (`--threads`) to use columns (auto): Comma-separated or list of columns or tags to carry over from the annotation file. Overrides `-c, --columns` remove (auto): Remove the specified columns from the input file header (list): Headers to be added gz (flag): Whether to gzip the output file index (flag): Whether to index the output file (tbi) (`envs.gz` forced to True) <more>: Other arguments for `bcftools annotate` See also <https://samtools.github.io/bcftools/bcftools.html#annotate> Note that the underscore `_` will be replaced with dash `-` in the argument name. \"\"\" input = \"infile:file, annfile:file\" output = ( \"outfile:file:{{in.infile | stem: 'gz'}}.vcf\" \"{{'.gz' if envs.index or envs.gz else ''}}\" ) lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"tabix\" : config . exe . tabix , \"annfile\" : None , \"columns\" : [], \"remove\" : [], \"header\" : [], \"gz\" : True , \"index\" : True , \"ncores\" : config . misc . ncores , } script = \"file://../scripts/vcf/BcftoolsAnnotate.py\" class BcftoolsFilter ( Proc ): DOCS \"\"\"Apply fixed threshold filters to VCF files Input: infile: The input VCF file Output: outfile: The filtered VCF file. If the `in.infile` is gzipped, this is gzipped as well. Envs: bcftools: Path to bcftools tabix: Path to tabix, used to index infile/outfile ncores (type=int): Number of cores (`--threads`) to use keep: Whether we should keep the filtered variants or not. If True, the filtered variants will be kept in the output file, but with a new FILTER. includes: and excludes: include/exclude only sites for which EXPRESSION is true. See: <https://samtools.github.io/bcftools/bcftools.html#expressions> If provided, `envs.include/exclude` will be ignored. If `str`/`list` used, The filter names will be `Filter_<type>_<index>`. A dict is used where keys are filter names and values are expressions gz (flag): Whether to gzip the output file index (flag): Whether to index the output file (tbi) (`envs.gz` forced to True) <more>: Other arguments for `bcftools filter` See also <https://samtools.github.io/bcftools/bcftools.html#filter> \"\"\" input = \"infile:file\" output = ( \"outfile:file:{{in.infile | stem: 'gz'}}.vcf\" \"{{'.gz' if envs.index or envs.gz else ''}}\" ) lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"tabix\" : config . exe . tabix , \"ncores\" : config . misc . ncores , \"keep\" : True , \"includes\" : None , \"excludes\" : None , \"gz\" : True , \"index\" : True , } script = \"file://../scripts/vcf/BcftoolsFilter.py\" class BcftoolsSort ( Proc ): DOCS \"\"\"Sort VCF files using `bcftools sort`. `bcftools sort` is used to sort VCF files by chromosome and position based on the order of contigs in the header. Here we provide a chrsize file to first sort the contigs in the header and then sort the VCF file using `bcftools sort`. Input: infile: The input VCF file Output: outfile: The sorted VCF file. Envs: bcftools: Path to bcftools tabix: Path to tabix, used to index infile/outfile ncores (type=int): Number of cores (`--threads`) to use gz (flag): Whether to gzip the output file index (flag): Whether to index the output file (tbi) (`envs.gz` forced to True) chrsize: The chromosome size file, from which the chromosome order is used to sort the contig in the header first. If not provided, `bcftools sort` will be used directly. notfound (choice): What if the contig in the VCF file is not found in the `chrsize` file. - error: Report error - remove: Remove the contig from the header. Note that if there are records with the removed contig, an error will be raised by `bcftools sort` - start: Move the contig to the start of the contigs from `chrsize` - end: Move the contig to the end of the contigs from `chrsize` <more>: Other arguments for `bcftools sort`. For example `max_mem`. See also <https://samtools.github.io/bcftools/bcftools.html#sort> \"\"\" input = \"infile:file\" output = ( \"outfile:file:{{in.infile | stem: 'gz'}}.vcf\" \"{{'.gz' if envs.index or envs.gz else ''}}\" ) lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"tabix\" : config . exe . tabix , \"ncores\" : config . misc . ncores , \"chrsize\" : config . ref . chrsize , \"notfound\" : \"remove\" , \"gz\" : True , \"index\" : True , } script = \"file://../scripts/vcf/BcftoolsSort.py\" class BcftoolsMerge ( Proc ): DOCS \"\"\"Merge multiple VCF files using `bcftools merge`. Input: infiles: The input VCF files Output: outfile: The merged VCF file. Envs: bcftools: Path to bcftools tabix: Path to tabix, used to index infile/outfile ncores (type=int): Number of cores (`--threads`) to use gz (flag): Whether to gzip the output file index (flag): Whether to index the output file (tbi) (`envs.gz` forced to True) <more>: Other arguments for `bcftools merge`. See also <https://samtools.github.io/bcftools/bcftools.html#merge> \"\"\" input = \"infiles:files\" output = ( \"outfile:file:{{in.infiles | first | stem | append: '_etc_merged'}}.vcf\" \"{{'.gz' if envs.index or envs.gz else ''}}\" ) lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"tabix\" : config . exe . tabix , \"ncores\" : config . misc . ncores , \"gz\" : True , \"index\" : True , } script = \"file://../scripts/vcf/BcftoolsMerge.py\" class BcftoolsView ( Proc ): DOCS \"\"\"View, subset and filter VCF files by position and filtering expression. Also convert between VCF and BCF. Input: infile: The input VCF file regions_file: The region file used to subset the input VCF file. samples_file: The samples file used to subset the input VCF file. Output: outfile: The output VCF file. Envs: bcftools: Path to bcftools tabix: Path to tabix, used to index infile/outfile ncores (type=int): Number of cores (`--threads`) to use regions_file: The region file used to subset the input VCF file. If `in.regions_file` is provided, this is ignored. samples_file: The samples file used to subset the input VCF file. If `in.samples_file` is provided, this is ignored. gz (flag): Whether to gzip the output file index (flag): Whether to index the output file (tbi) (`envs.gz` forced to True) <more>: Other arguments for `bcftools view`. See also https://samtools.github.io/bcftools/bcftools.html#view Note that the underscore `_` will be replaced with dash `-` in the argument name. \"\"\" input = \"infile:file, regions_file:file, samples_file:file\" output = ( \"outfile:file:{{in.infile | stem: 'gz'}}.vcf\" \"{{'.gz' if envs.index or envs.gz else ''}}\" ) lang = config . lang . python envs = { \"bcftools\" : config . exe . bcftools , \"tabix\" : config . exe . tabix , \"ncores\" : config . misc . ncores , \"regions_file\" : None , \"samples_file\" : None , \"gz\" : True , \"index\" : True , } script = \"file://../scripts/vcf/BcftoolsView.py\"","title":"biopipen.ns.vcf"},{"location":"api/source/biopipen.ns.web/","text":"SOURCE CODE biopipen.ns. web DOCS \"\"\"Get data from the web\"\"\" from ..core.proc import Proc from ..core.config import config class Download ( Proc ): DOCS \"\"\"Download data from URLs Input: url: The URL to download data from Output: outfile: The file downloaded Envs: tool (choice): Which tool to use to download the data - wget: Use wget - aria2c: Use aria2c - urllib: Use python's urllib - aria: Alias for aria2c wget: Path to wget aria2c: Path to aria2c args: The arguments to pass to the tool ncores: The number of cores to use Requires: wget: Only required when envs.tool == \"wget\" - check: {{proc.envs.wget}} --version aria2c: Only required when envs.tool == \"aria2c\" - check: {{proc.envs.aria2c}} --version \"\"\" input = \"url\" output = ( \"outfile:file:\" \"\"\"{{in.url | basename | url_decode | slugify: separator='.', lowercase=False, regex_pattern='[^-a-zA-Z0-9_]+' }}\"\"\" ) lang = config . lang . python envs = { \"tool\" : \"wget\" , # or aria2c, python \"wget\" : config . exe . wget , \"aria2c\" : config . exe . aria2c , \"args\" : {}, \"ncores\" : config . misc . ncores , } script = \"file://../scripts/web/Download.py\" class DownloadList ( Proc ): DOCS \"\"\"Download data from URLs in a file. This does not work by iterating over the URLs in the file. The whole file is passed to `wget` or `aria2c` at once. Input: urlfile: The file containing the URLs to download data from Output: outdir: The directory containing the downloaded files Envs: tool (choice): Which tool to use to download the data - wget: Use wget - aria2c: Use aria2c - urllib: Use python's urllib - aria: Alias for aria2c wget: Path to wget aria2c: Path to aria2c args: The arguments to pass to the tool ncores: The number of cores to use Requires: wget: Only required when envs.tool == \"wget\" - check: {{proc.envs.wget}} --version aria2c: Only required when envs.tool == \"aria2c\" - check: {{proc.envs.aria2c}} --version \"\"\" input = \"urlfile:file\" output = \"outdir:dir:{{in.urlfile | stem}}.downloaded\" lang = config . lang . python envs = { \"tool\" : \"wget\" , # or aria2c \"wget\" : config . exe . wget , \"aria2c\" : config . exe . aria2c , \"args\" : {}, \"ncores\" : config . misc . ncores , } script = \"file://../scripts/web/DownloadList.py\" class GCloudStorageDownloadFile ( Proc ): DOCS \"\"\"Download file from Google Cloud Storage Before using this, make sure you have the `gcloud` tool installed and logged in with the appropriate credentials using `gcloud auth login`. Also make sure you have [`google-crc32c`](https://pypi.org/project/google-crc32c/) installed to verify the integrity of the downloaded files. Input: url: The URL to download data from. It should be in the format gs://bucket/path/to/file Output: outfile: The file downloaded Envs: gcloud: Path to gcloud args (ns): Other arguments to pass to the `gcloud storage cp` command - do_not_decompress (flag): Do not decompress the file. - <more>: More arguments to pass to the `gcloud storage cp` command See `gcloud storage cp --help` for more information \"\"\" input = \"url:var\" output = \"outfile:file:{{in.url | replace: 'gs://', '/' | basename}}\" lang = config . lang . python envs = { \"gcloud\" : config . exe . gcloud , \"args\" : { \"do_not_decompress\" : True }, } script = \"file://../scripts/web/GCloudStorageDownloadFile.py\" class GCloudStorageDownloadBucket ( Proc ): DOCS \"\"\"Download all files from a Google Cloud Storage bucket Before using this, make sure you have the `gcloud` tool installed and logged in with the appropriate credentials using `gcloud auth login`. Note that this will not use the `--recursive` flag of `gcloud storage cp`. The files will be listed and downloaded one by one so that they can be parallelized. Also make sure you have [`google-crc32c`](https://pypi.org/project/google-crc32c/) installed to verify the integrity of the downloaded files. Input: url: The URL to download data from. It should be in the format gs://bucket Output: outdir: The directory containing the downloaded files Envs: gcloud: Path to gcloud keep_structure (flag): Keep the directory structure of the bucket ncores (type=int): The number of cores to use to download the files in parallel args (ns): Other arguments to pass to the `gcloud storage cp` command - do_not_decompress (flag): Do not decompress the file. - <more>: More arguments to pass to the `gcloud storage cp` command See `gcloud storage cp --help` for more information \"\"\" input = \"url:var\" output = \"outdir:dir:{{in.url | replace: 'gs://', ''}}\" lang = config . lang . python envs = { \"gcloud\" : config . exe . gcloud , \"keep_structure\" : True , \"ncores\" : config . misc . ncores , \"args\" : { \"do_not_decompress\" : True }, } script = \"file://../scripts/web/GCloudStorageDownloadBucket.py\"","title":"biopipen.ns.web"},{"location":"api/source/biopipen.utils.common_docstrs/","text":"SOURCE CODE biopipen.utils. common_docstrs DOCS \"\"\"Common docstrings for biopipen procs.\"\"\" import textwrap from typing import Callable def indent_docstr ( docstr : str , indent : str ) -> str : DOCS \"\"\"Indent the docstring. Args: docstr: The docstring. indent: The indent. Returns: The indented docstring. \"\"\" return textwrap . indent ( docstr , indent ) . strip () def format_placeholder ( ** kwargs ) -> Callable [[ type ], type ]: DOCS \"\"\"A decorator to format a docstring placeholder. Args: **kwargs: The docstring placeholder. Returns: The decorated function. \"\"\" def decorator ( klass : type ) -> type : if not klass . __doc__ : return klass klass . __doc__ = klass . __doc__ % kwargs return klass return decorator # MUTATE_HELPERS_CLONESIZE = \"\"\" # There are also also 4 helper functions, `expanded`, `collapsed`, `emerged` and `vanished`, # which can be used to identify the expanded/collpased/emerged/vanished groups (i.e. TCR clones). # See also <https://pwwang.github.io/immunopipe/configurations/#mutater-helpers>. # For example, you can use # `{\"Patient1_Tumor_Collapsed_Clones\": \"expanded(., Source, 'Tumor', subset = Patent == 'Patient1', uniq = FALSE)\"}` # to create a new column in metadata named `Patient1_Tumor_Collapsed_Clones` # with the collapsed clones in the tumor sample (compared to the normal sample) of patient 1. # The values in this columns for other clones will be `NA`. # Those functions take following arguments: # * `df`: The metadata data frame. You can use the `.` to refer to it. # * `group.by`: The column name in metadata to group the cells. # * `idents`: The first group or both groups of cells to compare (value in `group.by` column). If only the first group is given, the rest of the cells (with non-NA in `group.by` column) will be used as the second group. # * `subset`: An expression to subset the cells, will be passed to `dplyr::filter()`. Default is `TRUE` (no filtering). # * `each`: A column name (without quotes) in metadata to split the cells. # Each comparison will be done for each value in this column (typically each patient or subject). # * `id`: The column name in metadata for the group ids (i.e. `CDR3.aa`). # * `compare`: Either a (numeric) column name (i.e. `Clones`) in metadata to compare between groups, or `.n` to compare the number of cells in each group. # If numeric column is given, the values should be the same for all cells in the same group. # This will not be checked (only the first value is used). # It is helpful to use `Clones` to use the raw clone size from TCR data, in case the cells are not completely mapped to RNA data. # Also if you have `subset` set or `NA`s in `group.by` column, you should use `.n` to compare the number of cells in each group. # * `uniq`: Whether to return unique ids or not. Default is `TRUE`. If `FALSE`, you can mutate the meta data frame with the returned ids. For example, `df |> mutate(expanded = expanded(...))`. # * `debug`: Return the data frame with intermediate columns instead of the ids. Default is `FALSE`. # * `order`: The expression passed to `dplyr::arrange()` to order intermediate dataframe and get the ids in order accordingly. # The intermediate dataframe includes the following columns: # * `<id>`: The ids of clones (i.e. `CDR3.aa`). # * `<each>`: The values in `each` column. # * `ident_1`: The size of clones in the first group. # * `ident_2`: The size of clones in the second group. # * `.diff`: The difference between the sizes of clones in the first and second groups. # * `.sum`: The sum of the sizes of clones in the first and second groups. # * `.predicate`: Showing whether the clone is expanded/collapsed/emerged/vanished. # * `include_emerged`: Whether to include the emerged group for `expanded` (only works for `expanded`). Default is `FALSE`. # * `include_vanished`: Whether to include the vanished group for `collapsed` (only works for `collapsed`). Default is `FALSE`. # You can also use `top()` to get the top clones (i.e. the clones with the largest size) in each group. # For example, you can use # `{\"Patient1_Top10_Clones\": \"top(subset = Patent == 'Patient1', uniq = FALSE)\"}` # to create a new column in metadata named `Patient1_Top10_Clones`. # The values in this columns for other clones will be `NA`. # This function takes following arguments: # * `df`: The metadata data frame. You can use the `.` to refer to it. # * `id`: The column name in metadata for the group ids (i.e. `CDR3.aa`). # * `n`: The number of top clones to return. Default is `10`. # If n < 1, it will be treated as the percentage of the size of the group. # Specify `0` to get all clones. # * `compare`: Either a (numeric) column name (i.e. `Clones`) in metadata to compare between groups, or `.n` to compare the number of cells in each group. # If numeric column is given, the values should be the same for all cells in the same group. # This will not be checked (only the first value is used). # It is helpful to use `Clones` to use the raw clone size from TCR data, in case the cells are not completely mapped to RNA data. # Also if you have `subset` set or `NA`s in `group.by` column, you should use `.n` to compare the number of cells in each group. # * `subset`: An expression to subset the cells, will be passed to `dplyr::filter()`. Default is `TRUE` (no filtering). # * `each`: A column name (without quotes) in metadata to split the cells. # Each comparison will be done for each value in this column (typically each patient or subject). # * `uniq`: Whether to return unique ids or not. Default is `TRUE`. If `FALSE`, you can mutate the meta data frame with the returned ids. For example, `df |> mutate(expanded = expanded(...))`. # * `debug`: Return the data frame with intermediate columns instead of the ids. Default is `FALSE`. # * `with_ties`: Whether to include ties (i.e. clones with the same size as the last clone) or not. Default is `FALSE`. # \"\"\" # ENVS_SECTION_EACH = \"\"\" # The `section` is used to collect cases and put the results under the same directory and the same section in report. # When `each` for a case is specified, the `section` will be ignored and case name will be used as `section`. # The cases will be the expanded values in `each` column. When `prefix_each` is True, the column name specified by `each` will be prefixed to each value as directory name and expanded case name. # \"\"\"","title":"biopipen.utils.common_docstrs"},{"location":"api/source/biopipen.utils.gene/","text":"SOURCE CODE biopipen.utils. gene DOCS \"\"\"Do gene name conversion\"\"\" from __future__ import annotations import re import contextlib import pandas as pd from mygene import MyGeneInfo mygene = MyGeneInfo () class QueryGenesNotFound ( ValueError ): DOCS \"\"\"When genes cannot be found\"\"\" def gene_name_conversion ( DOCS genes : list [ str ], infmt : str | list [ str ], outfmt : str , dup : str = \"first\" , species : str = \"human\" , notfound : str = \"na\" , suppress_messages : bool = False , ): \"\"\"Convert gene names using MyGeneInfo Args: genes: A character/integer vector of gene names/ids species: A character vector of species names infmt: A character vector of input gene name formats See the available scopes at https://docs.mygene.info/en/latest/doc/data.html#available-fields You can use ensg as a shortcut for ensembl.gene outfmt: A character vector of output gene name formats dup: How to deal with duplicate gene names found. first: keep the first one (default), sorted by score descendingly last: keep the last one, sorted by score descendingly all: keep all of them, each will be a separate row <X>: combine them into a single string, separated by X notfound: How to deal with gene names that are not found error: stop with an error message use-query: use the query gene name as the converted gene name skip: skip the gene names that are not found ignore: Same as \"skip\" na: use NA as the converted gene name (default) suppress_messages: Suppress the messages while querying Returns: A dataframe with the query gene names and the converted gene names When a gene name is not found, the converted name will be \"NA\" When duplicate gene names are found, the one with the highest score will be kept \"\"\" notfound = notfound . lower () if notfound not in ( \"error\" , \"use-query\" , \"skip\" , \"ignore\" , \"na\" ): raise ValueError ( \"`notfound` of `gene_name_conversion` must be one of \" \"'error', 'use-query', 'skip', 'ignore', 'na'\" ) if infmt in [ \"ensg\" , \"ensmusg\" ]: infmt = \"ensembl.gene\" if outfmt in [ \"ensg\" , \"ensmusg\" ]: outfmt = \"ensembl.gene\" orig_genes = genes [:] if infmt == \"ensembl.gene\" : # Remove version numbers from ensembl gene ids genes = [ re . sub ( \" \\\\ ..*\" , \"\" , gene ) for gene in genes ] query_df = pd . DataFrame ({ \"query\" : genes , \"orig\" : orig_genes }) if suppress_messages : with contextlib . redirect_stdout ( None ): out = mygene . querymany ( genes , scopes = infmt , fields = outfmt , species = species , as_dataframe = True , df_index = False , ) else : out = mygene . querymany ( genes , scopes = infmt , fields = outfmt , species = species , as_dataframe = True , df_index = False , ) if out . shape [ 0 ] == 0 : return pd . DataFrame ({ \"query\" : genes , \"converted\" : [ \"NA\" ] * len ( genes )}) if dup == \"first\" : out = ( out . sort_values ( \"_score\" , ascending = False ) . groupby ( \"query\" ) . head ( 1 ) . reset_index ( drop = True ) ) elif dup == \"last\" : out = ( out . sort_values ( \"_score\" , ascending = False ) . groupby ( \"query\" ) . tail ( 1 ) . reset_index ( drop = True ) ) elif dup != \"all\" : out = ( out . sort_values ( \"_score\" , ascending = False ) . groupby ( \"query\" ) . agg ({ outfmt : lambda x : f \" { dup } \" . join ([ str ( x ) for x in x . unique ()])}) . reset_index () ) out = pd . merge ( query_df , out , on = \"query\" , how = \"left\" ) out = out . drop ( columns = [ \"query\" ]) . rename ( columns = { \"orig\" : \"query\" }) if notfound == \"error\" : if out [ outfmt ] . isnull () . any (): nagenes = out [ out [ outfmt ] . isnull ()][ \"query\" ] . tolist () raise QueryGenesNotFound ( f \"Query genes not found: { ',' . join ( nagenes ) } \" ) elif notfound == \"use-query\" : out [ outfmt ] = out [ outfmt ] . combine_first ( out [ \"query\" ]) elif notfound in [ \"skip\" , \"ignore\" ]: out = out . dropna ( subset = [ outfmt ]) else : # notfound == \"na\" out [ outfmt ] = out [ outfmt ] . fillna ( \"NA\" ) return out","title":"biopipen.utils.gene"},{"location":"api/source/biopipen.utils/","text":"SOURCE CODE biopipen. utils DOCS","title":"biopipen.utils"},{"location":"api/source/biopipen.utils.misc/","text":"SOURCE CODE biopipen.utils. misc DOCS from __future__ import annotations from pathlib import Path import os import sys import logging from subprocess import Popen from typing import List , Callable , Any from biopipen.core.filters import dict_to_cli_args # noqa: F401 logger = logging . getLogger ( \"biopipen_job\" ) logger . setLevel ( logging . DEBUG ) _handler = logging . StreamHandler ( sys . stdout ) # Use same log format as in R # {sprintf(\"%-7s\", level)} [{format(time, \"%Y-%m-%d %H:%M:%S\")}] {msg} # so the logs can be populated by pipen-poplog _handler . setFormatter ( logging . Formatter ( \" %(levelname)-7s [ %(asctime)s ] %(message)s \" , datefmt = \"%Y-%m- %d %H:%M:%S\" , ) ) logger . addHandler ( _handler ) def exec_code ( code , global_vars = None , local_vars = None , return_var = None ): global_vars = global_vars or {} local_vars = local_vars or {} exec ( code , global_vars , local_vars ) if return_var is not None : return local_vars [ return_var ] return None def run_command ( DOCS cmd : str | List [ Any ], fg : bool = False , wait : bool = True , print_command : bool = True , print_command_handler : Callable = print , ** kwargs , ) -> Popen | str : \"\"\"Run a command. Args: cmd: A string or list of strings representing the command to run. fg: Whether to run the command in the foreground. Redirects stdout and stderr to the current process. wait: Whether to wait for the command to finish. The command will be waited for if `fg` is `True`. print_command: Whether to print the command before running it. print_command_handler: The function to use to print the command. kwargs: Keyword arguments to pass to `subprocess.Popen`. Returns: The `Popen` object, or str when `stdout` is `RETURN` or `return`. \"\"\" import shlex from subprocess import PIPE , STDOUT if isinstance ( cmd , list ): cmd = [ str ( c ) for c in cmd ] if print_command : print_command_handler ( \"RUNNING COMMAND:\" ) if isinstance ( cmd , str ): print_command_handler ( f \" { cmd } \\n \" ) else : print_command_handler ( f \" { shlex . join ( cmd ) } \\n \" ) # flush the output if print_command_handler is print if print_command_handler is print : sys . stdout . flush () if isinstance ( cmd , str ): kwargs [ \"shell\" ] = True if kwargs . get ( \"stdin\" ) is True : kwargs [ \"stdin\" ] = PIPE return_stdout = False if kwargs . get ( \"stdout\" ) is True : kwargs [ \"stdout\" ] = PIPE elif kwargs . get ( \"stdout\" ) in ( \"RETURN\" , \"return\" ): kwargs [ \"stdout\" ] = PIPE return_stdout = True elif isinstance ( kwargs . get ( \"stdout\" ), ( str , Path )): if isinstance ( kwargs [ \"stdout\" ], str ): kwargs [ \"stdout\" ] = Path ( kwargs [ \"stdout\" ]) kwargs [ \"stdout\" ] = kwargs [ \"stdout\" ] . open ( \"w\" ) kwargs [ \"close_fds\" ] = True if kwargs . get ( \"stderr\" ) is True : kwargs [ \"stderr\" ] = PIPE elif kwargs . get ( \"stderr\" ) in ( \"STDOUT\" , \"stdout\" ): kwargs [ \"stderr\" ] = STDOUT if fg : if kwargs . get ( \"stdout\" ) or kwargs . get ( \"stderr\" ): raise ValueError ( \"Cannot redirect stdout or stderr when running in foreground\" ) kwargs [ \"stdout\" ] = sys . stdout kwargs [ \"stderr\" ] = sys . stderr kwargs [ \"universal_newlines\" ] = True if \"env\" in kwargs : kwargs [ \"env\" ] = { ** os . environ , ** kwargs [ \"env\" ]} try : p = Popen ( cmd , ** kwargs ) except Exception as e : raise RuntimeError ( f \"Failed to run command: { e } \\n \" f \"Command (list): { cmd } \\n \" f \"Command (str): { shlex . join ( cmd ) } \" ) if fg or wait or return_stdout : rc = p . wait () if rc != 0 : raise RuntimeError ( f \"Failed to run command: rc= { rc } \\n \" f \"Command (list): { cmd } \\n \" f \"Command (str): { shlex . join ( cmd ) } \" ) if return_stdout : return p . stdout . read () . decode () # type: ignore return p return p","title":"biopipen.utils.misc"},{"location":"api/source/biopipen.utils.reference/","text":"SOURCE CODE biopipen.utils. reference DOCS \"\"\"Utilities for indexing reference files\"\"\" from __future__ import annotations import tempfile from os import PathLike from pathlib import Path from typing import Literal from ..core.config import config from biopipen.utils.misc import run_command def gztype ( gzfile ): import binascii with open ( gzfile , \"rb\" ) as f : flag = binascii . hexlify ( f . read ( 4 )) if flag == b \"1f8b0804\" : return \"bgzip\" if flag == b \"1f8b0808\" : return \"gzip\" return \"flat\" def tabix_index ( DOCS infile : str | PathLike , preset : Literal [ \"gff\" , \"bed\" , \"sam\" , \"vcf\" , \"gaf\" ], tmpdir : Literal [ False ] | str | PathLike | None = None , tabix : str = config . exe . tabix , ) -> str | PathLike : \"\"\"Index input file using tabix 1. Try to check if there is an index file in the same directory where infile is. 2. If so, return the infile 3. Otherwise, check if infile is bgzipped, if not bgzip it and save it in tmpdir 4. Index the bgzipped file and return the bgzipped file Args: infile: The input file to be indexed preset: The preset used to index the file tmpdir: The directory to link the infile there and index it If False, try to index the infile directly. The directory where the infile is should be writable. tabix: The path to tabix Returns: The infile itself or re-bgzipped infile. This file comes with the index file in the same directory \"\"\" infile = Path ( infile ) gt = gztype ( infile ) index_file = infile . with_suffix ( infile . suffix + \".tbi\" ) # if index file exists, and it's newer than the infile, return infile if ( gt == \"bgzip\" and index_file . is_file () and index_file . stat () . st_mtime > infile . resolve () . stat () . st_mtime ): # only bgzipped file is possible to have index file return infile if tmpdir is False : # index the infile directly run_command ([ tabix , \"-p\" , preset , infile ], fg = True ) return infile if tmpdir is None : from hashlib import md5 # use a hash of infile to create the tempdir tmpdir = Path ( tempfile . gettempdir ()) . joinpath ( f \"biopipen_tabix_index_ { md5 ( str ( infile ) . encode ()) . hexdigest () } \" ) else : tmpdir = Path ( tmpdir ) tmpdir . mkdir ( exist_ok = True , parents = True ) # /path/to/some.vcf -> some.vcf # /path/to/some.vcf.gz -> some.vcf basename = infile . stem if infile . name . endswith ( \".gz\" ) else infile . name # try bgzip infile new_infile = tmpdir / ( basename + \".gz\" ) if gt == \"gzip\" : # re-bgzip run_command ( [ \"gunzip\" , \"-f\" , \"-c\" , infile ], stdout = new_infile . with_suffix ( \"\" ), ) run_command ([ \"bgzip\" , \"-f\" , new_infile . with_suffix ( \"\" )], fg = True ) elif gt == \"flat\" : run_command ([ \"bgzip\" , \"-f\" , \"-c\" , infile ], stdout = new_infile ) else : if new_infile . is_symlink (): new_infile . unlink () # directory of infile may not have write permission new_infile . symlink_to ( infile ) new_index_file = new_infile . with_suffix ( new_infile . suffix + \".tbi\" ) if ( new_index_file . is_file () and new_index_file . stat () . st_mtime > infile . resolve () . stat () . st_mtime ): return new_infile run_command ([ tabix , \"-p\" , preset , new_infile ], fg = True ) return new_infile def _run_bam_index ( bam , idxfile = None , tool = \"samtools\" , samtools = config . exe . samtools , sambamba = config . exe . sambamba , ncores = 1 , ): if tool == \"samtools\" : cmd = [ samtools , \"index\" , \"-@\" , ncores , bam , idxfile ] else : cmd = [ sambamba , \"index\" , \"-t\" , ncores , bam , idxfile ] run_command ( cmd , fg = True ) def bam_index ( DOCS bam : str | Path , bamdir : Path | str = tempfile . gettempdir (), tool : str = \"samtools\" , samtools : str = config . exe . samtools , sambamba : str = config . exe . sambamba , ncores : int = 1 , ext : str = \".bam.bai\" , force : bool = False , ) -> Path : \"\"\"Index a bam file First look for the index file in the same directory as the bam file, if found, return the bam file. Otherwise, generate a symbolic link of the bam file in bamdir, and generate a index there, return the path to the symbolic link Args: bam: The path to the bam file bamdir: If index file can't be found in the directory as the bam file, create a symbolic link to the bam file, and generate the index here tool: The tool used to generate the index file, either `samtools` or `sambamba` samtools: The path to samtools sambamba: The path to sambamba ncores: Number of cores (threads) used to generate the index file ext: The ext of the index file, default `.bam.bai`, in case, `.bai` is also treated as index file force: Force to generate the index file, with given bamfile. Don't check if the index file exists. Returns: The bam file if index exists in the directory as the bam file. Otherwise symbolic link to the bam file in bamdir. \"\"\" bam = Path ( bam ) indexfile = bam . with_suffix ( ext ) if force : _run_bam_index ( bam , indexfile , tool , samtools , sambamba , ncores , ) return bam if indexfile . is_file (): return bam linkfile = Path ( bamdir ) . joinpath ( bam . name ) indexfile = linkfile . with_suffix ( ext ) if linkfile . exists () and not linkfile . samefile ( bam ): linkfile . unlink () if indexfile . exists (): indexfile . unlink () if not linkfile . exists (): linkfile . symlink_to ( bam ) if indexfile . is_file (): return linkfile _run_bam_index ( linkfile , indexfile , tool , samtools , sambamba , ncores , ) return linkfile","title":"biopipen.utils.reference"},{"location":"api/source/biopipen.utils.vcf/","text":"SOURCE CODE biopipen.utils. vcf DOCS from __future__ import annotations from typing import Any , Sequence class HeaderItem ( dict ): DOCS \"\"\"The base class of header items\"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . raw = None @classmethod def from_str ( cls , line : str ): obj = cls () obj . raw = line line = line . rstrip ( \" \\r\\n \" ) items = line [ line . find ( \"<\" ) + 1 : - 1 ] . split ( \",\" , 3 ) for item in items : key , value = item . split ( \"=\" , 1 ) if key == \"Description\" : value = value [ 1 : - 1 ] obj [ key ] = value return obj def __setattr__ ( self , name : str , value : Any ) -> None : return super () . __setitem__ ( name , value ) def __getattr__ ( self , name : str ) -> Any : return super () . __getitem__ ( name ) class HeaderInfo ( HeaderItem ): DOCS \"\"\"The INFO items in the header\"\"\" kind = \"info\" def __str__ ( self ): return ( f \"##INFO=<ID= { self [ 'ID' ] } ,\" f \"Number= { self [ 'Number' ] } ,\" f \"Type= { self [ 'Type' ] } ,\" f \"Description= \\\" { self [ 'Description' ] } \\\" >\" ) @staticmethod def is_type ( raw : str ) -> bool : return raw . startswith ( \"##INFO\" ) class HeaderFormat ( HeaderItem ): DOCS \"\"\"The FORMAT items in the header\"\"\" kind = \"format\" def __str__ ( self ): return ( f \"##FORMAT=<ID= { self [ 'ID' ] } ,\" f \"Number= { self [ 'Number' ] } ,\" f \"Type= { self [ 'Type' ] } ,\" f \"Description= \\\" { self [ 'Description' ] } \\\" >\" ) @staticmethod def is_type ( raw : str ) -> bool : return raw . startswith ( \"##FORMAT\" ) class HeaderFilter ( HeaderItem ): DOCS \"\"\"The FILTER items in the header\"\"\" kind = \"filter\" def __str__ ( self ): return ( f \"##FILTER=<ID= { self [ 'ID' ] } ,\" f \"Description= \\\" { self [ 'Description' ] } \\\" >\" ) @staticmethod def is_type ( raw : str ) -> bool : return raw . startswith ( \"##FILTER\" ) class HeaderContig ( HeaderItem ): DOCS \"\"\"The contig items in the header\"\"\" kind = \"contig\" def __str__ ( self ): return f \"##contig=<ID= { self [ 'ID' ] } ,\" f \"length= { self [ 'length' ] } >\" @staticmethod def is_type ( raw : str ) -> bool : return raw . startswith ( \"##contig\" ) class HeaderGeneral ( HeaderItem ): DOCS \"\"\"The general items in the header\"\"\" kind = \"header\" @classmethod def from_str ( cls , line : str ): obj = cls () obj . raw = line line = line . rstrip ( \" \\r\\n \" ) obj [ \"key\" ], obj [ \"value\" ] = line [ 2 :] . split ( \"=\" , 1 ) return obj def __str__ ( self ): return f \"## { self [ 'key' ] } = { self [ 'value' ] } \" @staticmethod def is_type ( raw : str ) -> bool : if not raw . startswith ( \"##\" ): return False key = raw [ 2 :] . split ( \"=\" , 1 )[ 0 ] return key not in ( \"INFO\" , \"FILTER\" , \"FORMAT\" , \"contig\" ) class Fields ( list ): DOCS \"\"\"The fields/column names\"\"\" kind = \"fields\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . raw = None @classmethod def from_str ( cls , line : str ): obj = cls () obj . raw = line line = line . rstrip ( \" \\r\\n \" ) obj . extend ( line [ 1 :] . split ( \" \\t \" )) return obj def __str__ ( self ): return \"#\" + \" \\t \" . join ( self ) @property def samples ( self ): return self [ 9 :] @staticmethod def is_type ( raw : str ) -> bool : return raw . startswith ( \"#CHROM\" ) class Info ( dict ): DOCS \"\"\"The INFO of the variant\"\"\" @classmethod def from_str ( cls , infostr : str ): obj = cls () for part in infostr . split ( \";\" ): # a flag if \"=\" not in part : obj [ part ] = True else : name , value = part . split ( \"=\" , 1 ) obj [ name ] = value return obj def __str__ ( self ) -> str : return \";\" . join ( k if v is True else f \" { k } = { v } \" for k , v in self . items () if v is not False ) class Format ( list ): DOCS \"\"\"The FORMAT of the variant\"\"\" @classmethod def from_str ( cls , formatstr : str ): return cls ( formatstr . split ( \":\" )) def __str__ ( self ) -> str : return \":\" . join ( self ) class Alt ( list ): DOCS \"\"\"The ALT of the variant\"\"\" @classmethod def from_str ( cls , altstr ): return cls ( altstr . split ( \",\" )) def __str__ ( self ) -> str : return \",\" . join ( self ) class Filter ( list ): DOCS \"\"\"The FILTER of the variant\"\"\" @classmethod def from_str ( cls , filtstr : str ): return cls ( filtstr . split ( \";\" )) def __str__ ( self ) -> str : return \";\" . join ( self ) class Sample ( dict ): DOCS \"\"\"One sample of the variant\"\"\" def __init__ ( self , values : Sequence [ str ], format : Format ): super () . __init__ () self . _format = format for name , value in zip ( format , values ): self [ name ] = value @property def format ( self ): return self . _format @classmethod def from_str ( cls , value_str : str , format : Format ): return cls ( value_str . split ( \":\" ), format ) @classmethod def from_strs ( cls , value_strs : Sequence [ str ], format : Format ): return cls ( value_strs , format ) def __str__ ( self ) -> str : values = [ self [ fmt ] for fmt in self . _format ] return \":\" . join ( values ) class Samples ( list ): DOCS \"\"\"The samples of the variant\"\"\" def __init__ ( self , samples : Sequence [ Sample ], format : Format ): super () . __init__ ( samples ) self . _format = format @property def format ( self ): return self . _format @classmethod def from_str ( cls , sample_str : str , format : Format ): return cls ( [ Sample . from_str ( sam_str , format ) for sam_str in sample_str . split ( \" \\t \" ) ], format , ) @classmethod def from_strs ( cls , sample_strs : Sequence [ str ], format : Format ): return cls ( [ Sample . from_str ( sam_str , format ) for sam_str in sample_strs ], format , ) @classmethod def from_strss ( cls , sample_strss : Sequence [ Sequence [ str ]], format : Format ): return cls ( [ Sample . from_strs ( sam_strs , format ) for sam_strs in sample_strss ], format , ) def __str__ ( self ) -> str : return \" \\t \" . join ( str ( s ) for s in self ) class Variant : kind = \"variant\" def __init__ ( self , chrom : str , pos : int , id : str , ref : str , alt : Alt , qual : str , filter : Filter , info : Info , format : Format , samples : Samples , ): self . chrom = chrom self . pos = pos self . id = id self . ref = ref self . alt = alt self . qual = qual self . filter = filter self . info = info self . format = format self . samples = samples self . raw = None @classmethod def from_strs ( cls , chrom : str , pos : int | str , id : str , ref : str , alt : str | Sequence [ str ], qual : str , filter : str | Sequence [ str ], info : str | dict , format : str | Sequence [ str ], samples : str | Sequence [ str ] | Sequence [ Sequence [ str ]], ): format = ( Format . from_str ( format ) if isinstance ( format , str ) else Format ( format ) ) if isinstance ( samples , str ): samples = Samples . from_str ( samples , format ) elif isinstance ( samples [ 0 ], str ): samples = Samples . from_strs ( samples , format ) # type: ignore else : samples = Samples . from_strss ( samples , format ) obj = cls ( chrom , int ( pos ), id , ref , Alt . from_str ( alt ) if isinstance ( alt , str ) else Alt ( alt ), qual , Filter . from_str ( filter ) if isinstance ( filter , str ) else Filter ( filter ), Info . from_str ( info ) if isinstance ( info , str ) else Info ( info ), format , samples , ) return obj @classmethod def from_str ( cls , variant_line : str ): raw = variant_line variant_line = variant_line . rstrip ( \" \\r\\n \" ) items = variant_line . split ( \" \\t \" ) chrom = items [ 0 ] pos = int ( items [ 1 ]) id = items [ 2 ] ref = items [ 3 ] alt = Alt . from_str ( items [ 4 ]) qual = items [ 5 ] filter = Filter . from_str ( items [ 6 ]) info = Info . from_str ( items [ 7 ]) format = Format . from_str ( items [ 8 ]) samples = Samples . from_strs ( items [ 9 :], format ) obj = cls ( chrom , pos , id , ref , alt , qual , filter , info , format , samples , ) obj . raw = raw return obj def __str__ ( self ): return ( f \" { self . chrom } \\t { self . pos } \\t { self . id } \\t { self . ref } \\t \" f \" { self . alt } \\t { self . qual } \\t { self . filter } \\t { self . info } \\t \" f \" { self . format } \\t { self . samples } \" ) def __repr__ ( self ): return f \"Variant( { self . chrom } , { self . pos } , { self . id } )\" @staticmethod def is_type ( raw : str ) -> bool : return not raw . startswith ( \"#\" )","title":"biopipen.utils.vcf"},{"location":"pipelines/cellranger_pipeline/","text":"CellRanger pipeline Including two pipelines: CellRangerCountPipeline and CellRangerVdjPipeline . Pipeline overview Each pipeline contains two processes. CellRangerCount / CellRangerVdj : Run cellranger count/vdj on each sample. CellRangerSummary : Summarize the results from each sample. Input files Each sample should have a set of fastq files, in the format of: [ # fastq files for sample 1 [ \"sample1_S1_L001_R1_001.fastq.gz\" , \"sample1_S1_L001_R2_001.fastq.gz\" , \"sample1_S1_L002_R1_001.fastq.gz\" , \"sample1_S1_L002_R2_001.fastq.gz\" , ], # fastq files for sample 2 [ \"sample2_S1_L001_R1_001.fastq.gz\" , \"sample2_S1_L001_R2_001.fastq.gz\" , \"sample2_S1_L002_R1_001.fastq.gz\" , \"sample2_S1_L002_R2_001.fastq.gz\" , ], ... ] If the ids cannot be inferred from the fastq file names, or you want to use a different id than the inferred one, you can specify the ids in the input: input = [ # fastq files for sample 1 [ \"sample1_S1_L001_R1_001.fastq.gz\" , \"sample1_S1_L001_R2_001.fastq.gz\" , \"sample1_S1_L002_R1_001.fastq.gz\" , \"sample1_S1_L002_R2_001.fastq.gz\" , ], # fastq files for sample 2 [ \"sample2_S1_L001_R1_001.fastq.gz\" , \"sample2_S1_L001_R2_001.fastq.gz\" , \"sample2_S1_L002_R1_001.fastq.gz\" , \"sample2_S1_L002_R2_001.fastq.gz\" , ], ... ] ids = [ \"sampleA\" , \"sampleB\" , ... ] Configurations input : The input fastq files for each sample. See Input files for details. ids : The ids for each sample. See Input files for details. Other than the input, you should provide other configurations to the processes to each individual process. Check the documentation of each process for more details. CellRangerCount CellRangerVdj CellRangerSummary Reference To run the pipeline, you need to provide the reference genome for the cellranger pipeline. You can provide the reference genome in the configuration: [CellRangerCount.envs] ref = \"/path/to/reference\" To obtain the reference genome, please refer to: https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/inputs/cr-inputs-overview#count https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/inputs/cr-inputs-overview#vdj You may also make your own reference by cellranger mkref for gene expression. See the cellranger documentation: https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/inputs/cr-3p-references Also check out docker/cellranger_pipeline/make-examples.sh to see how the references are prepared. Docker image You can use the docker image [ biopipen/cellranger-pipeline ] to run the pipeline. The image contains the cellranger software and the biopipen package. It is also built with an example dataset for you to test the pipeline: /example/example-data/Sample_X_S1_L001_R1_001.fastq.gz /example/example-data/Sample_X_S1_L001_R2_001.fastq.gz /example/example-data/Sample_Y_S1_L001_R1_001.fastq.gz /example/example-data/Sample_Y_S1_L001_R2_001.fastq.gz /example/example-data/Sample_Y_S1_L002_R1_001.fastq.gz /example/example-data/Sample_Y_S1_L002_R2_001.fastq.gz A sample configuration file is also provided at /biopipen/docker/cellranger_pipeline/CellrangerCountPipeline.config.toml . Note that the docker image was not built with the reference genome. You need to provide the reference genome by yourself.","title":"CellRanger Pipeline"},{"location":"pipelines/cellranger_pipeline/#cellranger-pipeline","text":"Including two pipelines: CellRangerCountPipeline and CellRangerVdjPipeline .","title":"CellRanger pipeline"},{"location":"pipelines/cellranger_pipeline/#pipeline-overview","text":"Each pipeline contains two processes. CellRangerCount / CellRangerVdj : Run cellranger count/vdj on each sample. CellRangerSummary : Summarize the results from each sample.","title":"Pipeline overview"},{"location":"pipelines/cellranger_pipeline/#input-files","text":"Each sample should have a set of fastq files, in the format of: [ # fastq files for sample 1 [ \"sample1_S1_L001_R1_001.fastq.gz\" , \"sample1_S1_L001_R2_001.fastq.gz\" , \"sample1_S1_L002_R1_001.fastq.gz\" , \"sample1_S1_L002_R2_001.fastq.gz\" , ], # fastq files for sample 2 [ \"sample2_S1_L001_R1_001.fastq.gz\" , \"sample2_S1_L001_R2_001.fastq.gz\" , \"sample2_S1_L002_R1_001.fastq.gz\" , \"sample2_S1_L002_R2_001.fastq.gz\" , ], ... ] If the ids cannot be inferred from the fastq file names, or you want to use a different id than the inferred one, you can specify the ids in the input: input = [ # fastq files for sample 1 [ \"sample1_S1_L001_R1_001.fastq.gz\" , \"sample1_S1_L001_R2_001.fastq.gz\" , \"sample1_S1_L002_R1_001.fastq.gz\" , \"sample1_S1_L002_R2_001.fastq.gz\" , ], # fastq files for sample 2 [ \"sample2_S1_L001_R1_001.fastq.gz\" , \"sample2_S1_L001_R2_001.fastq.gz\" , \"sample2_S1_L002_R1_001.fastq.gz\" , \"sample2_S1_L002_R2_001.fastq.gz\" , ], ... ] ids = [ \"sampleA\" , \"sampleB\" , ... ]","title":"Input files"},{"location":"pipelines/cellranger_pipeline/#configurations","text":"input : The input fastq files for each sample. See Input files for details. ids : The ids for each sample. See Input files for details. Other than the input, you should provide other configurations to the processes to each individual process. Check the documentation of each process for more details. CellRangerCount CellRangerVdj CellRangerSummary","title":"Configurations"},{"location":"pipelines/cellranger_pipeline/#reference","text":"To run the pipeline, you need to provide the reference genome for the cellranger pipeline. You can provide the reference genome in the configuration: [CellRangerCount.envs] ref = \"/path/to/reference\" To obtain the reference genome, please refer to: https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/inputs/cr-inputs-overview#count https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/inputs/cr-inputs-overview#vdj You may also make your own reference by cellranger mkref for gene expression. See the cellranger documentation: https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/inputs/cr-3p-references Also check out docker/cellranger_pipeline/make-examples.sh to see how the references are prepared.","title":"Reference"},{"location":"pipelines/cellranger_pipeline/#docker-image","text":"You can use the docker image [ biopipen/cellranger-pipeline ] to run the pipeline. The image contains the cellranger software and the biopipen package. It is also built with an example dataset for you to test the pipeline: /example/example-data/Sample_X_S1_L001_R1_001.fastq.gz /example/example-data/Sample_X_S1_L001_R2_001.fastq.gz /example/example-data/Sample_Y_S1_L001_R1_001.fastq.gz /example/example-data/Sample_Y_S1_L001_R2_001.fastq.gz /example/example-data/Sample_Y_S1_L002_R1_001.fastq.gz /example/example-data/Sample_Y_S1_L002_R2_001.fastq.gz A sample configuration file is also provided at /biopipen/docker/cellranger_pipeline/CellrangerCountPipeline.config.toml . Note that the docker image was not built with the reference genome. You need to provide the reference genome by yourself.","title":"Docker image"},{"location":"pipelines/cnvkit_pipeline/","text":"CNVkit pipeline The pipeline decouples cnvkit.py batch so that we get detailed control over each step. Pipeline overview The pipeline consists of the following steps: cnvkit.py access to generate a BED file of accessible regions if not given Guess baits from bam files if baitfile is not given cnvkit.py autobin to generate target and antitarget files cnvkit.py coverage to generate coverage files for target region cnvkit.py coverage to generate coverage files for antitarget region cnvkit.py reference to generate a reference.cnn file using normal samples (or a \"flat\" reference file if no normal samples are given) cnvkit.py fix to combine the uncorrected target and antitarget coverage tables (.cnn) and correct for biases in regional coverage and GC content, according to the given reference. cnvkit.py segment to infer discrete copy number segments from the given coverage table: cnvkit.py call to call copy number alterations from the given segments file cnvkit.py scatter to generate scatter plots of log2 ratios cnvkit.py diagram to generate a diagram of copy number alterations on all chromosomes cnvkit.py heatmap to generate a heatmap of segment-level log2 ratios cnvkit.py heatmap to generate a heatmap of bin-level log2 ratios See also the flowchart below: Input files metafile : a tab-separated file (see the next section) containing sample information baitfile : Potentially targeted genomic regions. E.g. all possible exons for the reference genome. This is optional when method is wgs . accfile : The accessible genomic regions. If not given, use cnvkit.py access to generate one. You can control the details by configuration items [CNVkitAccess.envs] Configurations Special configurations access_excludes : File(s) with regions to be excluded for cnvkit.py access . guessbaits_guided : Whether to use guided mode for guessing baits. metacols : The column names for each type of information in metafile group : The column name in the metafile that indicates the sample group Default: Group purity : The column name in the metafile that indicates the sample purity. Default: Purity snpvcf : The column name in the metafile that indicates the path to the SNP VCF file. Default: SnpVcf bam : The column name in the metafile that indicates the path to the BAM file. Default: Bam vcf_sample_id : The column name in the metafile that indicates the sample ID in the VCF file. Default: VcfSampleId vcf_normal_id : The column name in the metafile that indicates the normal sample ID in the VCF file. Default: VcfNormalId sex : The column name in the metafile that indicates the sample sex. Default: Sex guess_baits : The column name in the metafile that indicates whether to guess the bait file from the bam files. Default: GuessBaits guessbaits : Guess the bait file from the bam files, either guided or unguided. If False , baitfile is used. Otherwise, if baitfile is given, use it (guided), otherwise use accfile (unguided). The bam files with metacols.guess_baits column set to True , TRUE , true , 1 , Yes , YES , or yes will be used to guess the bait file. case : The group name of samples in metacols.group to call CNVs for. If not specified, use all samples. In such a case, control must not be specified, as we are using a flat reference. control : The group name of samples in metacols.group to use as reference if not specified, use a flat reference. Global configurations The options that are used by multiple processes (can be overriden individually by [<proc>.envs.xxx] ): cnvkit : the path to the cnvkit.py executable, defaults to config.exe.cnvkit from ./.biopipen.toml or ~/.biopipen.toml . rscript : Path to the Rscript excecutable to use for running R code. Requires DNAcopy to be installed in R, defaults to config.lang.rscript samtools : Path to samtools, used for guessing bait file. convert : Linux convert command to convert pdf to png So that they can be embedded in the HTML report. ncores : number of cores to use, defaults to config.misc.ncores reffa : the reference genome (e.g. hg19.fa), defaults to config.ref.reffa Used by CNVkitAccess , CNVkitAutobin and CNVkitReference annotate : Use gene models from this file to assign names to the target regions. Format: UCSC refFlat.txt or ensFlat.txt file (preferred), or BED, interval list, GFF, or similar. Defaults to config.ref.refflat short_names : Reduce multi-accession bait labels to be short and consistent method : Sequencing protocol: hybridization capture ('hybrid'), targeted amplicon sequencing ('amplicon'), or whole genome sequencing ('wgs'). Determines whether and how to use antitarget bins. male_reference : Use or assume a male reference (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). Used by CNVkitReference , CNVkitCall , CNVkitHeatmapCns and CNVkitHeatmapCnr . drop_low_coverage : Drop very-low-coverage bins before segmentation to avoid false-positive deletions in poor-quality tumor samples. Used by CNVkitSegment and CNVkitCall no_gc : Skip GC correction for cnvkit.py reference/fix . no_edge : Skip edge-effect correction for cnvkit.py reference/fix . no_rmask : Skip RepeatMasker correction for cnvkit.py reference/fix . no_* options are used by CNVkitReference and CNVkitFix min_variant_depth : Minimum read depth for a SNV to be displayed in the b-allele frequency plot. Used by CNVkitSegment and CNVkitCall zygosity_freq : Ignore VCF's genotypes (GT field) and instead infer zygosity from allele frequencies. Used by CNVkitSegment and CNVkitCall Process-specific configurations The options that are used by a single process. See the process-specific documentation for details. You can configure them by [<proc>.envs.xxx] in the config file. CNVkitAccess CNVkitGuessBaits CNVkitAutobin CNVkitCoverageTarget CNVkitCoverageAntitarget CNVkitReference CNVkitFix CNVkitSegment CNVkitScatter CNVkitDiagram CNVkitHeatmapCns CNVkitHeatmapCnr CNVkitHeatmapCall The metafile A metafile should be with the following columns: Sample: The sample_id used for target/antitarget files. If not provided, the sample_id will be the first part of basename of the bam file. For exapmle: D123.tumor.bam -> D123 <bam> : The path to the bam file, better using absolute path. <group> : The type of the sample, defining the tumor/normal samples. <sex> : Guess each sample from coverage of X and Y chromosomes if not given. <purity> : Estimated tumor cell fraction, a.k.a. purity or cellularity. <snpvcf> : file name containing variants for segmentation by allele frequencies. <vcf_sample_id> : Sample ID in the VCF file. <vcf_normal_id> : Normal sample ID in the VCF file. <guess_baits> : Guess the bait file from the bam file","title":"CNVkit Pipeline"},{"location":"pipelines/cnvkit_pipeline/#cnvkit-pipeline","text":"The pipeline decouples cnvkit.py batch so that we get detailed control over each step.","title":"CNVkit pipeline"},{"location":"pipelines/cnvkit_pipeline/#pipeline-overview","text":"The pipeline consists of the following steps: cnvkit.py access to generate a BED file of accessible regions if not given Guess baits from bam files if baitfile is not given cnvkit.py autobin to generate target and antitarget files cnvkit.py coverage to generate coverage files for target region cnvkit.py coverage to generate coverage files for antitarget region cnvkit.py reference to generate a reference.cnn file using normal samples (or a \"flat\" reference file if no normal samples are given) cnvkit.py fix to combine the uncorrected target and antitarget coverage tables (.cnn) and correct for biases in regional coverage and GC content, according to the given reference. cnvkit.py segment to infer discrete copy number segments from the given coverage table: cnvkit.py call to call copy number alterations from the given segments file cnvkit.py scatter to generate scatter plots of log2 ratios cnvkit.py diagram to generate a diagram of copy number alterations on all chromosomes cnvkit.py heatmap to generate a heatmap of segment-level log2 ratios cnvkit.py heatmap to generate a heatmap of bin-level log2 ratios See also the flowchart below:","title":"Pipeline overview"},{"location":"pipelines/cnvkit_pipeline/#input-files","text":"metafile : a tab-separated file (see the next section) containing sample information baitfile : Potentially targeted genomic regions. E.g. all possible exons for the reference genome. This is optional when method is wgs . accfile : The accessible genomic regions. If not given, use cnvkit.py access to generate one. You can control the details by configuration items [CNVkitAccess.envs]","title":"Input files"},{"location":"pipelines/cnvkit_pipeline/#configurations","text":"","title":"Configurations"},{"location":"pipelines/cnvkit_pipeline/#special-configurations","text":"access_excludes : File(s) with regions to be excluded for cnvkit.py access . guessbaits_guided : Whether to use guided mode for guessing baits. metacols : The column names for each type of information in metafile group : The column name in the metafile that indicates the sample group Default: Group purity : The column name in the metafile that indicates the sample purity. Default: Purity snpvcf : The column name in the metafile that indicates the path to the SNP VCF file. Default: SnpVcf bam : The column name in the metafile that indicates the path to the BAM file. Default: Bam vcf_sample_id : The column name in the metafile that indicates the sample ID in the VCF file. Default: VcfSampleId vcf_normal_id : The column name in the metafile that indicates the normal sample ID in the VCF file. Default: VcfNormalId sex : The column name in the metafile that indicates the sample sex. Default: Sex guess_baits : The column name in the metafile that indicates whether to guess the bait file from the bam files. Default: GuessBaits guessbaits : Guess the bait file from the bam files, either guided or unguided. If False , baitfile is used. Otherwise, if baitfile is given, use it (guided), otherwise use accfile (unguided). The bam files with metacols.guess_baits column set to True , TRUE , true , 1 , Yes , YES , or yes will be used to guess the bait file. case : The group name of samples in metacols.group to call CNVs for. If not specified, use all samples. In such a case, control must not be specified, as we are using a flat reference. control : The group name of samples in metacols.group to use as reference if not specified, use a flat reference.","title":"Special configurations"},{"location":"pipelines/cnvkit_pipeline/#global-configurations","text":"The options that are used by multiple processes (can be overriden individually by [<proc>.envs.xxx] ): cnvkit : the path to the cnvkit.py executable, defaults to config.exe.cnvkit from ./.biopipen.toml or ~/.biopipen.toml . rscript : Path to the Rscript excecutable to use for running R code. Requires DNAcopy to be installed in R, defaults to config.lang.rscript samtools : Path to samtools, used for guessing bait file. convert : Linux convert command to convert pdf to png So that they can be embedded in the HTML report. ncores : number of cores to use, defaults to config.misc.ncores reffa : the reference genome (e.g. hg19.fa), defaults to config.ref.reffa Used by CNVkitAccess , CNVkitAutobin and CNVkitReference annotate : Use gene models from this file to assign names to the target regions. Format: UCSC refFlat.txt or ensFlat.txt file (preferred), or BED, interval list, GFF, or similar. Defaults to config.ref.refflat short_names : Reduce multi-accession bait labels to be short and consistent method : Sequencing protocol: hybridization capture ('hybrid'), targeted amplicon sequencing ('amplicon'), or whole genome sequencing ('wgs'). Determines whether and how to use antitarget bins. male_reference : Use or assume a male reference (i.e. female samples will have +1 log-CNR of chrX; otherwise male samples would have -1 chrX). Used by CNVkitReference , CNVkitCall , CNVkitHeatmapCns and CNVkitHeatmapCnr . drop_low_coverage : Drop very-low-coverage bins before segmentation to avoid false-positive deletions in poor-quality tumor samples. Used by CNVkitSegment and CNVkitCall no_gc : Skip GC correction for cnvkit.py reference/fix . no_edge : Skip edge-effect correction for cnvkit.py reference/fix . no_rmask : Skip RepeatMasker correction for cnvkit.py reference/fix . no_* options are used by CNVkitReference and CNVkitFix min_variant_depth : Minimum read depth for a SNV to be displayed in the b-allele frequency plot. Used by CNVkitSegment and CNVkitCall zygosity_freq : Ignore VCF's genotypes (GT field) and instead infer zygosity from allele frequencies. Used by CNVkitSegment and CNVkitCall","title":"Global configurations"},{"location":"pipelines/cnvkit_pipeline/#process-specific-configurations","text":"The options that are used by a single process. See the process-specific documentation for details. You can configure them by [<proc>.envs.xxx] in the config file. CNVkitAccess CNVkitGuessBaits CNVkitAutobin CNVkitCoverageTarget CNVkitCoverageAntitarget CNVkitReference CNVkitFix CNVkitSegment CNVkitScatter CNVkitDiagram CNVkitHeatmapCns CNVkitHeatmapCnr CNVkitHeatmapCall","title":"Process-specific configurations"},{"location":"pipelines/cnvkit_pipeline/#the-metafile","text":"A metafile should be with the following columns: Sample: The sample_id used for target/antitarget files. If not provided, the sample_id will be the first part of basename of the bam file. For exapmle: D123.tumor.bam -> D123 <bam> : The path to the bam file, better using absolute path. <group> : The type of the sample, defining the tumor/normal samples. <sex> : Guess each sample from coverage of X and Y chromosomes if not given. <purity> : Estimated tumor cell fraction, a.k.a. purity or cellularity. <snpvcf> : file name containing variants for segmentation by allele frequencies. <vcf_sample_id> : Sample ID in the VCF file. <vcf_normal_id> : Normal sample ID in the VCF file. <guess_baits> : Guess the bait file from the bam file","title":"The metafile"},{"location":"pipelines/scrna_metabolic/","text":"scrna_metabolic Metabolic landscape analysis for single-cell RNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape Reference Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12. Run the pipeline Run from CLI: pipen run scrna_metabolic_landscape ScrnaMetabolicLandscape [ options ] Serve as part of a pipeline: from biopipen.ns.scrna_metabolic_landscape import ScrnaMetabolicLandscape pipeline = ScrnaMetabolicLandscape ( < options > ) # You can specify dependencies so that the whole metabolic landscape pipeline # works as a part of another pipeline Inputs metafile : Either a metafile or an rds file of a Seurat object. If it is a metafile, it should have two columns: Sample and RNAData . Sample should be the first column with unique identifiers for the samples and RNAData indicates where the barcodes, genes, expression matrices are. The data will be loaded and an unsupervised clustering will be done. Currently only 10X data is supported. If it is an rds file, the seurat object will be used directly. gmtfile : The GMT file with the metabolic pathways. The gene names should match the gene names in the gene list in RNAData or the Seurat object. You can also provide a URL to the GMT file. group_by : Group the data by the given column in the metadata. For example, cluster . subset_by : (Optional) Subset the data by the given column in the metadata. For example, Response . NA values will be removed in this column. If None, the data will not be subsetted. mutaters : (Optional) Add new columns to the metadata for grouping/subsetting. They are passed to sobj@meta.data |> mutate(...) . For example, {\"timepoint\": \"if_else(treatment == 'control', 'pre', 'post')\"} will add a new column timepoint to the metadata with values of pre and post based on the treatment column. cases : (Optional) Multiple cases for the analysis. If you have multiple different grouping/subsetting scenarios, you can specify them here. Each case can have its own subset_by , group_by , and analysis parameters. Advanced Configuration Multiple Cases You can define multiple analysis cases with different grouping/subsetting strategies: [ScrnaMetabolicLandscape] metafile = \"test_data/scrna_metabolic/seurat_obj.rds\" gmtfile = \"test_data/scrna_metabolic/KEGG_metabolism.gmt\" [ScrnaMetabolicLandscape.MetabolicPathwayActivity.envs.cases] \"By Treatment\" = { group_by = \"seurat_clusters\" , subset_by = \"treatment\" } \"By Response\" = { group_by = \"seurat_clusters\" , subset_by = \"response\" } Custom Plotting Each process supports customizable plots: [ScrnaMetabolicLandscape.MetabolicPathwayActivity.envs.plots] \"Custom Heatmap\" = { plot_type = \"heatmap\" , show_row_names = true , devpars = { width = 1200 , height = 800 , res = 150 } } \"Custom Violin\" = { plot_type = \"violin\" , add_box = true , devpars = { width = 1000 , height = 600 } } [ScrnaMetabolicLandscape.MetabolicFeatures.envs.plots] \"Top 5 Summary\" = { plot_type = \"summary\" , top_term = 5 , level = \"subset\" } A step-by-step example Prepare the seurat object Using the data from: Yost KE, Satpathy AT, Wells DK, Qi Y et al. Clonal replacement of tumor-specific T cells following PD-1 blockade. Nat Med 2019 Aug;25(8):1251-1259. PMID: 31359002 library ( Seurat ) # Download data (tcell rds and metadata) from # https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123813 count_file <- \"test_data/scrna_metabolic/GSE123813_bcc_scRNA_counts.txt.gz\" meta_file <- \"test_data/scrna_metabolic/GSE123813_bcc_all_metadata.txt.gz\" counts <- read.table ( count_file , header = TRUE , row.names = 1 , sep = \"\\t\" , check.names = F ) metadata <- read.table ( meta_file , header = TRUE , row.names = 1 , sep = \"\\t\" , check.names = F ) # Subset 1000 cells for just demo purpose counts = counts [, sample ( 1 : ncol ( counts ), 1000 )] metadata = metadata [ colnames ( counts ),] # Create seurat object seurat_obj = CreateSeuratObject ( counts = counts ) seurat_obj @ meta.data = cbind ( seurat_obj @ meta.data , metadata [ rownames ( seurat_obj @ meta.data ),] ) seurat_obj = NormalizeData ( seurat_obj ) all.genes <- rownames ( seurat_obj ) seurat_obj <- ScaleData ( seurat_obj , features = all.genes ) seurat_obj <- FindVariableFeatures ( object = seurat_obj ) seurat_obj <- RunPCA ( seurat_obj , features = VariableFeatures ( object = seurat_obj )) # Output exceeds the size limit. Open the full output data in a text editor # Warning message: # \"Feature names cannot have underscores ('_'), replacing with dashes ('-')\" # Centering and scaling data matrix # PC_ 1 # Positive: CALD1, EMP2, SPARC, CAV1, EMP1, ACTN1, KRT17, BGN, DSP, RAB13 # RND3, S100A16, KRT14, S100A14, S100A2, CD9, FHL2, IER3, GJB2, TRIM29 # TSC22D1, KRT15, SFN, FSTL1, DDR1, IGFBP7, JUP, PTRF, KRT5, FOSL1 # Negative: CD74, CRIP1, RARRES3, RGS1, LAT, CD69, NKG7, HLA-DPA1, TNFRSF4, CD27 # GZMA, HLA-DRB1, CXCR6, CTSW, GPR183, LDLRAD4, ICOS, HIST1H4C, GZMK, AC092580.4 # SLC9A3R1, CCR7, S100A4, HLA-DPB1, HMGB2, GBP5, SELL, CXCR3, LAG3, HLA-DRB5 # PC_ 2 # Positive: DSP, DSC3, SFN, SERPINB5, KRT17, S100A14, KRT15, KRT16, IRF6, TRIM29 # KRT14, LGALS7B, JUP, PERP, TACSTD2, GJB2, KRT5, KRT6B, DDR1, S100A2 # PKP1, CDH3, KRT6A, FXYD3, GJB3, MPZL2, CXADR, DSC2, DSG3, GRHL3 # Negative: COL1A2, COL3A1, LUM, COL6A1, MXRA8, FN1, CTSK, COL1A1, COL6A3, DCN # MMP2, COL6A2, PRRX1, FKBP10, TNFAIP6, FAP, PCOLCE, PDGFRB, NNMT, AEBP1 # C1S, CCDC80, SFRP2, RCN3, PDPN, SERPINF1, COL5A2, CTHRC1, COL5A1, COL12A1 # PC_ 3 # Positive: LYZ, TYROBP, SPI1, FCER1G, KYNU, C15orf48, BCL2A1, HLA-DRA, CD68, AIF1 # CST3, CTSZ, SERPINA1, CSF2RA, HLA-DRB5, SLC7A11, LST1, MS4A7, ALDH2, FAM49A # IFI30, HLA-DRB1, GPR157, PLEK, HLA-DPB1, IL1B, CD86, CCDC88A, HLA-DPA1, RNF144B # Negative: MT2A, MT1X, BGN, MT1E, COL6A2, COL1A2, COL6A1, COL5A2, MXRA8, DCN # COL12A1, COL3A1, COL1A1, LUM, MFAP2, PCOLCE, MT1F, MMP2, COL5A1, COL6A3 # PRRX1, C1S, AEBP1, CTSK, FAP, LAT, PDGFRB, RARRES3, THY1, EFEMP2 # GJB3, LYPD3, GRHL3, PVRL4, KRT16, TACSTD2, GJB5, SERPINB13, MPZL2, KRT23 # Negative: UBE2C, GTSE1, BIRC5, RRM2, CCNA2, TYMS, TOP2A, TK1, DLGAP5, MKI67 # PKMYT1, CENPA, KIFC1, CDCA5, UHRF1, ASF1B, AURKB, FAM111B, TROAP, CKAP2L # HJURP, ESCO2, FOXM1, CDK1, ZWINT, E2F2, CLSPN, HIST1H1B, CDT1, MCM10 # By default, the pipeline will do the clustering using the SeuratClustering process # If you want to do the clustering yourself, you can set `is_seurat = true` # when running the pipeline, which will skip the clustering step. seurat_obj <- FindNeighbors ( seurat_obj , dims = 1 : 10 ) seurat_obj <- FindClusters ( seurat_obj , resolution = 0.5 ) head ( Idents ( seurat_obj )) # Computing nearest neighbor graph # Computing SNN # Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck # Number of nodes: 1000 # Number of edges: 30631 # Running Louvain algorithm... # Maximum modularity in 10 random starts: 0.8795 # Number of communities: 9 # Elapsed time: 0 seconds # bcc.su009.post.tcell_CCATTCGCAATCTACG 0 # bcc.su004.pre.tcell_ACAGCTACACTTCTGC 0 # bcc.su006.pre.tcell_CGTGTAAAGTGACTCT 1 # bcc.su001.post.tcell_CCTTTCTGTACCGTTA 2 # bcc.su007.pre.tcell_ATTTCTGAGAAGGACA 1 # bcc.su009.pre.tcell_AGACGTTTCCTGCAGG8 8 # Levels: # '0''1''2''3''4''5''6''7''8' # Check the meta.data head ( seurat_obj @ meta.data ) # orig.ident nCount_RNA nFeature_RNA patient treatment sort cluster UMAP1 UMAP2 RNA_snn_res.0.5 seurat_clusters # <fct> <dbl> <int> <chr> <chr> <chr> <chr> <dbl> <dbl> <fct> <fct> # bcc.su009.post.tcell_CCATTCGCAATCTACG bcc.su009.post.tcell 4242 1726 su009 post CD45+ CD3+ CD8_mem_T_cells -9.1816435 0.7484789 0 0 # bcc.su004.pre.tcell_ACAGCTACACTTCTGC bcc.su004.pre.tcell 3159 1466 su004 pre CD45+ CD3+ CD8_ex_T_cells 4.1067562 3.3754938 0 0 # bcc.su006.pre.tcell_CGTGTAAAGTGACTCT bcc.su006.pre.tcell 3279 1401 su006 pre CD45+ CD3+ Tregs 0.4816457 11.9388428 1 1 # bcc.su001.post.tcell_CCTTTCTGTACCGTTA bcc.su001.post.tcell 5057 2218 su001 post CD45+ CD3+ CD8_ex_T_cells 2.3045983 7.4856248 2 2 # bcc.su007.pre.tcell_ATTTCTGAGAAGGACA bcc.su007.pre.tcell 3701 1413 su007 pre CD45+ CD3+ CD8_mem_T_cells -1.9617293 5.5546365 1 1 # bcc.su009.pre.tcell_AGACGTTTCCTGCAGG bcc.su009.pre.tcell 3891 1593 su009 pre CD45+ CD3+ Tcell_prolif 5.2243228 -1.0460106 8 8 # save seurat object saveRDS ( seurat_obj , \"test_data/scrna_metabolic/seurat_obj.rds\" ) Prepare the pathway file A set of collected metabolic pathways can be found here: https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape/blob/master/Data/KEGG_metabolism.gmt Download and save it to test_data/scrna_metabolic/KEGG_metabolism.gmt Prepare the configuration file Save at test_data/scrna_metabolic/config.toml : # Pipeline configuration [ScrnaMetabolicLandscape] metafile = \"test_data/scrna_metabolic/seurat_obj.rds\" gmtfile = \"test_data/scrna_metabolic/KEGG_metabolism.gmt\" group_by = \"seurat_clusters\" subset_by = \"timepoint\" [ScrnaMetabolicLandscape.mutaters] timepoint = \"if_else(patient != 'su001', NA_character_, treatment)\" # Optional: Configure individual processes [ScrnaMetabolicLandscape.MetabolicPathwayActivity.envs] ntimes = 1000 [ScrnaMetabolicLandscape.MetabolicFeatures.envs.plots] \"Summary Plot\" = { plot_type = \"summary\" , top_term = 5 } Run the pipeline pipen run scrna_metabolic_landscape ScrnaMetabolicLandscape --config test_data/scrna_metabolic/config.toml Check out the results/reports The results can be found at ./ScrnaMetabolicLandscape_results/ , and reports can be found at ./ScrnaMetabolicLandscape_results/REPORTS . To check out the reports, open ./ScrnaMetabolicLandscape_results/REPORTS/index.html in your browser. There are 3 parts of the results: MetabolicPathwayActivity : The pathway activities for groups (defined by group_by in the configuration) for each subset (defined by subset_by ) MetabolicPathwayHeterogeneity : Pathway heterogeneity for groups for each subset MetabolicFeatures : The pathway enrichment analysis in detail for each group against the rest of the groups in each subset.","title":"Scrna metabolic"},{"location":"pipelines/scrna_metabolic/#scrna_metabolic","text":"Metabolic landscape analysis for single-cell RNA-seq data An abstract from https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape","title":"scrna_metabolic"},{"location":"pipelines/scrna_metabolic/#reference","text":"Xiao, Zhengtao, Ziwei Dai, and Jason W. Locasale. \"Metabolic landscape of the tumor microenvironment at single cell resolution.\" Nature communications 10.1 (2019): 1-12.","title":"Reference"},{"location":"pipelines/scrna_metabolic/#run-the-pipeline","text":"","title":"Run the pipeline"},{"location":"pipelines/scrna_metabolic/#run-from-cli","text":"pipen run scrna_metabolic_landscape ScrnaMetabolicLandscape [ options ]","title":"Run from CLI:"},{"location":"pipelines/scrna_metabolic/#serve-as-part-of-a-pipeline","text":"from biopipen.ns.scrna_metabolic_landscape import ScrnaMetabolicLandscape pipeline = ScrnaMetabolicLandscape ( < options > ) # You can specify dependencies so that the whole metabolic landscape pipeline # works as a part of another pipeline","title":"Serve as part of a pipeline:"},{"location":"pipelines/scrna_metabolic/#inputs","text":"metafile : Either a metafile or an rds file of a Seurat object. If it is a metafile, it should have two columns: Sample and RNAData . Sample should be the first column with unique identifiers for the samples and RNAData indicates where the barcodes, genes, expression matrices are. The data will be loaded and an unsupervised clustering will be done. Currently only 10X data is supported. If it is an rds file, the seurat object will be used directly. gmtfile : The GMT file with the metabolic pathways. The gene names should match the gene names in the gene list in RNAData or the Seurat object. You can also provide a URL to the GMT file. group_by : Group the data by the given column in the metadata. For example, cluster . subset_by : (Optional) Subset the data by the given column in the metadata. For example, Response . NA values will be removed in this column. If None, the data will not be subsetted. mutaters : (Optional) Add new columns to the metadata for grouping/subsetting. They are passed to sobj@meta.data |> mutate(...) . For example, {\"timepoint\": \"if_else(treatment == 'control', 'pre', 'post')\"} will add a new column timepoint to the metadata with values of pre and post based on the treatment column. cases : (Optional) Multiple cases for the analysis. If you have multiple different grouping/subsetting scenarios, you can specify them here. Each case can have its own subset_by , group_by , and analysis parameters.","title":"Inputs"},{"location":"pipelines/scrna_metabolic/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"pipelines/scrna_metabolic/#multiple-cases","text":"You can define multiple analysis cases with different grouping/subsetting strategies: [ScrnaMetabolicLandscape] metafile = \"test_data/scrna_metabolic/seurat_obj.rds\" gmtfile = \"test_data/scrna_metabolic/KEGG_metabolism.gmt\" [ScrnaMetabolicLandscape.MetabolicPathwayActivity.envs.cases] \"By Treatment\" = { group_by = \"seurat_clusters\" , subset_by = \"treatment\" } \"By Response\" = { group_by = \"seurat_clusters\" , subset_by = \"response\" }","title":"Multiple Cases"},{"location":"pipelines/scrna_metabolic/#custom-plotting","text":"Each process supports customizable plots: [ScrnaMetabolicLandscape.MetabolicPathwayActivity.envs.plots] \"Custom Heatmap\" = { plot_type = \"heatmap\" , show_row_names = true , devpars = { width = 1200 , height = 800 , res = 150 } } \"Custom Violin\" = { plot_type = \"violin\" , add_box = true , devpars = { width = 1000 , height = 600 } } [ScrnaMetabolicLandscape.MetabolicFeatures.envs.plots] \"Top 5 Summary\" = { plot_type = \"summary\" , top_term = 5 , level = \"subset\" }","title":"Custom Plotting"},{"location":"pipelines/scrna_metabolic/#a-step-by-step-example","text":"","title":"A step-by-step example"},{"location":"pipelines/scrna_metabolic/#prepare-the-seurat-object","text":"Using the data from: Yost KE, Satpathy AT, Wells DK, Qi Y et al. Clonal replacement of tumor-specific T cells following PD-1 blockade. Nat Med 2019 Aug;25(8):1251-1259. PMID: 31359002 library ( Seurat ) # Download data (tcell rds and metadata) from # https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE123813 count_file <- \"test_data/scrna_metabolic/GSE123813_bcc_scRNA_counts.txt.gz\" meta_file <- \"test_data/scrna_metabolic/GSE123813_bcc_all_metadata.txt.gz\" counts <- read.table ( count_file , header = TRUE , row.names = 1 , sep = \"\\t\" , check.names = F ) metadata <- read.table ( meta_file , header = TRUE , row.names = 1 , sep = \"\\t\" , check.names = F ) # Subset 1000 cells for just demo purpose counts = counts [, sample ( 1 : ncol ( counts ), 1000 )] metadata = metadata [ colnames ( counts ),] # Create seurat object seurat_obj = CreateSeuratObject ( counts = counts ) seurat_obj @ meta.data = cbind ( seurat_obj @ meta.data , metadata [ rownames ( seurat_obj @ meta.data ),] ) seurat_obj = NormalizeData ( seurat_obj ) all.genes <- rownames ( seurat_obj ) seurat_obj <- ScaleData ( seurat_obj , features = all.genes ) seurat_obj <- FindVariableFeatures ( object = seurat_obj ) seurat_obj <- RunPCA ( seurat_obj , features = VariableFeatures ( object = seurat_obj )) # Output exceeds the size limit. Open the full output data in a text editor # Warning message: # \"Feature names cannot have underscores ('_'), replacing with dashes ('-')\" # Centering and scaling data matrix # PC_ 1 # Positive: CALD1, EMP2, SPARC, CAV1, EMP1, ACTN1, KRT17, BGN, DSP, RAB13 # RND3, S100A16, KRT14, S100A14, S100A2, CD9, FHL2, IER3, GJB2, TRIM29 # TSC22D1, KRT15, SFN, FSTL1, DDR1, IGFBP7, JUP, PTRF, KRT5, FOSL1 # Negative: CD74, CRIP1, RARRES3, RGS1, LAT, CD69, NKG7, HLA-DPA1, TNFRSF4, CD27 # GZMA, HLA-DRB1, CXCR6, CTSW, GPR183, LDLRAD4, ICOS, HIST1H4C, GZMK, AC092580.4 # SLC9A3R1, CCR7, S100A4, HLA-DPB1, HMGB2, GBP5, SELL, CXCR3, LAG3, HLA-DRB5 # PC_ 2 # Positive: DSP, DSC3, SFN, SERPINB5, KRT17, S100A14, KRT15, KRT16, IRF6, TRIM29 # KRT14, LGALS7B, JUP, PERP, TACSTD2, GJB2, KRT5, KRT6B, DDR1, S100A2 # PKP1, CDH3, KRT6A, FXYD3, GJB3, MPZL2, CXADR, DSC2, DSG3, GRHL3 # Negative: COL1A2, COL3A1, LUM, COL6A1, MXRA8, FN1, CTSK, COL1A1, COL6A3, DCN # MMP2, COL6A2, PRRX1, FKBP10, TNFAIP6, FAP, PCOLCE, PDGFRB, NNMT, AEBP1 # C1S, CCDC80, SFRP2, RCN3, PDPN, SERPINF1, COL5A2, CTHRC1, COL5A1, COL12A1 # PC_ 3 # Positive: LYZ, TYROBP, SPI1, FCER1G, KYNU, C15orf48, BCL2A1, HLA-DRA, CD68, AIF1 # CST3, CTSZ, SERPINA1, CSF2RA, HLA-DRB5, SLC7A11, LST1, MS4A7, ALDH2, FAM49A # IFI30, HLA-DRB1, GPR157, PLEK, HLA-DPB1, IL1B, CD86, CCDC88A, HLA-DPA1, RNF144B # Negative: MT2A, MT1X, BGN, MT1E, COL6A2, COL1A2, COL6A1, COL5A2, MXRA8, DCN # COL12A1, COL3A1, COL1A1, LUM, MFAP2, PCOLCE, MT1F, MMP2, COL5A1, COL6A3 # PRRX1, C1S, AEBP1, CTSK, FAP, LAT, PDGFRB, RARRES3, THY1, EFEMP2 # GJB3, LYPD3, GRHL3, PVRL4, KRT16, TACSTD2, GJB5, SERPINB13, MPZL2, KRT23 # Negative: UBE2C, GTSE1, BIRC5, RRM2, CCNA2, TYMS, TOP2A, TK1, DLGAP5, MKI67 # PKMYT1, CENPA, KIFC1, CDCA5, UHRF1, ASF1B, AURKB, FAM111B, TROAP, CKAP2L # HJURP, ESCO2, FOXM1, CDK1, ZWINT, E2F2, CLSPN, HIST1H1B, CDT1, MCM10 # By default, the pipeline will do the clustering using the SeuratClustering process # If you want to do the clustering yourself, you can set `is_seurat = true` # when running the pipeline, which will skip the clustering step. seurat_obj <- FindNeighbors ( seurat_obj , dims = 1 : 10 ) seurat_obj <- FindClusters ( seurat_obj , resolution = 0.5 ) head ( Idents ( seurat_obj )) # Computing nearest neighbor graph # Computing SNN # Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck # Number of nodes: 1000 # Number of edges: 30631 # Running Louvain algorithm... # Maximum modularity in 10 random starts: 0.8795 # Number of communities: 9 # Elapsed time: 0 seconds # bcc.su009.post.tcell_CCATTCGCAATCTACG 0 # bcc.su004.pre.tcell_ACAGCTACACTTCTGC 0 # bcc.su006.pre.tcell_CGTGTAAAGTGACTCT 1 # bcc.su001.post.tcell_CCTTTCTGTACCGTTA 2 # bcc.su007.pre.tcell_ATTTCTGAGAAGGACA 1 # bcc.su009.pre.tcell_AGACGTTTCCTGCAGG8 8 # Levels: # '0''1''2''3''4''5''6''7''8' # Check the meta.data head ( seurat_obj @ meta.data ) # orig.ident nCount_RNA nFeature_RNA patient treatment sort cluster UMAP1 UMAP2 RNA_snn_res.0.5 seurat_clusters # <fct> <dbl> <int> <chr> <chr> <chr> <chr> <dbl> <dbl> <fct> <fct> # bcc.su009.post.tcell_CCATTCGCAATCTACG bcc.su009.post.tcell 4242 1726 su009 post CD45+ CD3+ CD8_mem_T_cells -9.1816435 0.7484789 0 0 # bcc.su004.pre.tcell_ACAGCTACACTTCTGC bcc.su004.pre.tcell 3159 1466 su004 pre CD45+ CD3+ CD8_ex_T_cells 4.1067562 3.3754938 0 0 # bcc.su006.pre.tcell_CGTGTAAAGTGACTCT bcc.su006.pre.tcell 3279 1401 su006 pre CD45+ CD3+ Tregs 0.4816457 11.9388428 1 1 # bcc.su001.post.tcell_CCTTTCTGTACCGTTA bcc.su001.post.tcell 5057 2218 su001 post CD45+ CD3+ CD8_ex_T_cells 2.3045983 7.4856248 2 2 # bcc.su007.pre.tcell_ATTTCTGAGAAGGACA bcc.su007.pre.tcell 3701 1413 su007 pre CD45+ CD3+ CD8_mem_T_cells -1.9617293 5.5546365 1 1 # bcc.su009.pre.tcell_AGACGTTTCCTGCAGG bcc.su009.pre.tcell 3891 1593 su009 pre CD45+ CD3+ Tcell_prolif 5.2243228 -1.0460106 8 8 # save seurat object saveRDS ( seurat_obj , \"test_data/scrna_metabolic/seurat_obj.rds\" )","title":"Prepare the seurat object"},{"location":"pipelines/scrna_metabolic/#prepare-the-pathway-file","text":"A set of collected metabolic pathways can be found here: https://github.com/LocasaleLab/Single-Cell-Metabolic-Landscape/blob/master/Data/KEGG_metabolism.gmt Download and save it to test_data/scrna_metabolic/KEGG_metabolism.gmt","title":"Prepare the pathway file"},{"location":"pipelines/scrna_metabolic/#prepare-the-configuration-file","text":"Save at test_data/scrna_metabolic/config.toml : # Pipeline configuration [ScrnaMetabolicLandscape] metafile = \"test_data/scrna_metabolic/seurat_obj.rds\" gmtfile = \"test_data/scrna_metabolic/KEGG_metabolism.gmt\" group_by = \"seurat_clusters\" subset_by = \"timepoint\" [ScrnaMetabolicLandscape.mutaters] timepoint = \"if_else(patient != 'su001', NA_character_, treatment)\" # Optional: Configure individual processes [ScrnaMetabolicLandscape.MetabolicPathwayActivity.envs] ntimes = 1000 [ScrnaMetabolicLandscape.MetabolicFeatures.envs.plots] \"Summary Plot\" = { plot_type = \"summary\" , top_term = 5 }","title":"Prepare the configuration file"},{"location":"pipelines/scrna_metabolic/#run-the-pipeline_1","text":"pipen run scrna_metabolic_landscape ScrnaMetabolicLandscape --config test_data/scrna_metabolic/config.toml","title":"Run the pipeline"},{"location":"pipelines/scrna_metabolic/#check-out-the-resultsreports","text":"The results can be found at ./ScrnaMetabolicLandscape_results/ , and reports can be found at ./ScrnaMetabolicLandscape_results/REPORTS . To check out the reports, open ./ScrnaMetabolicLandscape_results/REPORTS/index.html in your browser. There are 3 parts of the results: MetabolicPathwayActivity : The pathway activities for groups (defined by group_by in the configuration) for each subset (defined by subset_by ) MetabolicPathwayHeterogeneity : Pathway heterogeneity for groups for each subset MetabolicFeatures : The pathway enrichment analysis in detail for each group against the rest of the groups in each subset.","title":"Check out the results/reports"}]}